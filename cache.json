{"2025-02-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.10391v1","updated":"2025-02-14T18:59:51Z","published":"2025-02-14T18:59:51Z","title":"MM-RLHF: The Next Step Forward in Multimodal LLM Alignment","summary":"  Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.\n","authors":["Yi-Fan Zhang","Tao Yu","Haochen Tian","Chaoyou Fu","Peiyan Li","Jianshu Zeng","Wulin Xie","Yang Shi","Huanyu Zhang","Junkang Wu","Xue Wang","Yibo Hu","Bin Wen","Fan Yang","Zhang Zhang","Tingting Gao","Di Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2502.10391v1.pdf","comment":"Project Page: https://mm-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2502.10388v1","updated":"2025-02-14T18:59:28Z","published":"2025-02-14T18:59:28Z","title":"Aspect-Oriented Summarization for Psychiatric Short-Term Readmission\n  Prediction","summary":"  Recent progress in large language models (LLMs) has enabled the automated\nprocessing of lengthy documents even without supervised training on a\ntask-specific dataset. Yet, their zero-shot performance in complex tasks as\nopposed to straightforward information extraction tasks remains suboptimal. One\nfeasible approach for tasks with lengthy, complex input is to first summarize\nthe document and then apply supervised fine-tuning to the summary. However, the\nsummarization process inevitably results in some loss of information. In this\nstudy we present a method for processing the summaries of long documents aimed\nto capture different important aspects of the original document. We hypothesize\nthat LLM summaries generated with different aspect-oriented prompts contain\ndifferent \\textit{information signals}, and we propose methods to measure these\ndifferences. We introduce approaches to effectively integrate signals from\nthese different summaries for supervised training of transformer models. We\nvalidate our hypotheses on a high-impact task -- 30-day readmission prediction\nfrom a psychiatric discharge -- using real-world data from four hospitals, and\nshow that our proposed method increases the prediction performance for the\ncomplex task of predicting patient outcome.\n","authors":["WonJin Yoon","Boyu Ren","Spencer Thomas","Chanwhi Kim","Guergana Savova","Mei-Hua Hall","Timothy Miller"],"pdf_url":"https://arxiv.org/pdf/2502.10388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10378v1","updated":"2025-02-14T18:57:04Z","published":"2025-02-14T18:57:04Z","title":"Unknown Word Detection for English as a Second Language (ESL) Learners\n  Using Gaze and Pre-trained Language Models","summary":"  English as a Second Language (ESL) learners often encounter unknown words\nthat hinder their text comprehension. Automatically detecting these words as\nusers read can enable computing systems to provide just-in-time definitions,\nsynonyms, or contextual explanations, thereby helping users learn vocabulary in\na natural and seamless manner. This paper presents EyeLingo, a\ntransformer-based machine learning method that predicts the probability of\nunknown words based on text content and eye gaze trajectory in real time with\nhigh accuracy. A 20-participant user study revealed that our method can achieve\nan accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time\nreading assistance prototype to show the effectiveness of EyeLingo. The user\nstudy shows improvement in willingness to use and usefulness compared to\nbaseline methods.\n","authors":["Jiexin Ding","Bowen Zhao","Yuntao Wang","Xinyun Liu","Rui Hao","Ishan Chatterjee","Yuanchun Shi"],"pdf_url":"https://arxiv.org/pdf/2502.10378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10373v1","updated":"2025-02-14T18:51:40Z","published":"2025-02-14T18:51:40Z","title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation\n  Models","summary":"  Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.\n","authors":["William Chen","Jinchuan Tian","Yifan Peng","Brian Yan","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.10373v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.10361v1","updated":"2025-02-14T18:42:07Z","published":"2025-02-14T18:42:07Z","title":"Enhancing Multilingual LLM Pretraining with Model-Based Data Selection","summary":"  Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets.\n","authors":["Bettina Messmer","Vinko Sabolƒçec","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2502.10361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00843v2","updated":"2025-02-14T18:35:03Z","published":"2024-10-30T04:20:10Z","title":"The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation","summary":"  Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.\n","authors":["Reza Moravej","Saurabh Bodhe","Zhanguang Zhang","Didier Chetelat","Dimitrios Tsaras","Yingxue Zhang","Hui-Ling Zhen","Jianye Hao","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.00843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10352v1","updated":"2025-02-14T18:31:39Z","published":"2025-02-14T18:31:39Z","title":"Agentic Verification for Ambiguous Query Disambiguation","summary":"  In this work, we tackle the challenge of disambiguating queries in\nretrieval-augmented generation (RAG) to diverse yet answerable interpretations.\nState-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse\ninterpretations are generated by an LLM, later used as search queries to\nretrieve supporting passages. Such a process may introduce noise in either\ninterpretations or retrieval, particularly in enterprise settings, where LLMs\n-- trained on static data -- may struggle with domain-specific disambiguations.\nThus, a post-hoc verification phase is introduced to prune noises. Our\ndistinction is to unify diversification with verification by incorporating\nfeedback from retriever and generator early on. This joint approach improves\nboth efficiency and robustness by reducing reliance on multiple retrieval and\ninference steps, which are susceptible to cascading errors. We validate the\nefficiency and effectiveness of our method, Verified-Diversification with\nConsolidation (VERDICT), on the widely adopted ASQA benchmark to achieve\ndiverse yet verifiable interpretations. Empirical results show that VERDICT\nimproves grounding-aware F1 score by an average of 23% over the strongest\nbaseline across different backbone LLMs.\n","authors":["Youngwon Lee","Seung-won Hwang","Ruofan Wu","Feng Yan","Danmei Xu","Moutasem Akkad","Zhewei Yao","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2502.10352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v3","updated":"2025-02-14T18:15:01Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Sch√∂lkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10341v1","updated":"2025-02-14T18:02:37Z","published":"2025-02-14T18:02:37Z","title":"Organize the Web: Constructing Domains Enhances Pre-Training Data\n  Curation","summary":"  Modern language models are trained on large, unstructured datasets consisting\nof trillions of tokens and obtained by crawling the web. The unstructured\nnature makes it difficult to reason about their contents and develop systematic\napproaches to data curation. In this paper, we unpack monolithic web corpora by\ndeveloping taxonomies of their contents and organizing them into domains. We\nintroduce WebOrganizer, a framework for organizing web pages in terms of both\ntheir topic and format. Using these two complementary notions of domains, we\nautomatically annotate pre-training data by distilling annotations from a large\nlanguage model into efficient classifiers. This allows us to study how data\nfrom different domains should be mixed to improve models on downstream tasks,\nand we show that we can combine insights about effective topics and formats to\nfurther boost performance. We demonstrate that our domain mixing also improves\nexisting methods that select data based on quality. Furthermore, we study and\ncompare how quality-based methods will implicitly change the domain mixture.\nOverall, our work demonstrates that constructing and mixing domains provides a\nvaluable complement to quality-based data curation methods, opening new avenues\nfor effective and insightful pre-training data curation.\n","authors":["Alexander Wettig","Kyle Lo","Sewon Min","Hannaneh Hajishirzi","Danqi Chen","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2502.10341v1.pdf","comment":"Project page: https://weborganizer.allen.ai"},{"id":"http://arxiv.org/abs/2502.10339v1","updated":"2025-02-14T17:59:58Z","published":"2025-02-14T17:59:58Z","title":"STAR: Spectral Truncation and Rescale for Model Merging","summary":"  Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.\n","authors":["Yu-Ang Lee","Ching-Yun Ko","Tejaswini Pedapati","I-Hsin Chung","Mi-Yen Yeh","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10339v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.10338v1","updated":"2025-02-14T17:55:43Z","published":"2025-02-14T17:55:43Z","title":"Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering","summary":"  Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.\n","authors":["Nick Ferguson","Liane Guillou","Alan Bundy","Kwabena Nuamah"],"pdf_url":"https://arxiv.org/pdf/2502.10338v1.pdf","comment":"8 pages. Accepted to the Workshop on Planning in the Era of LLMs\n  (LM4Plan @ AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.10416v2","updated":"2025-02-14T17:40:13Z","published":"2024-12-09T20:03:14Z","title":"SuperMerge: An Approach For Gradient-Based Model Merging","summary":"  Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic,\nmonolithic, and possess the superpower to simultaneously support thousands of\ntasks. However, high-throughput applications often prefer smaller task-specific\nmodels because of their lower latency and cost. One challenge of using\ntask-specific models is the incremental need for solving newer tasks after the\nmodel is already deployed for existing tasks. A straightforward solution\nrequires fine-tuning the model again for both existing and new tasks, which is\ncomputationally expensive and time-consuming. To address this issue, we propose\na model merging based approach called SUPERMERGE. SUPERMERGE is a\ngradient-based method to systematically merge several fine-tuned models trained\non existing and new tasks. SUPERMERGE is designed to be lightweight and fast,\nand the merged model achieves similar performance to fully fine-tuned models on\nall tasks. Furthermore, we proposed a hierarchical model merging strategy to\nreduce the peak space requirement without sacrificing the performance of the\nmerged model. We experimentally demonstrate that SUPERMERGE outperforms\nexisting model merging methods on common natural language processing and\ncomputer vision tasks.\n","authors":["Haoyu Yang","Zheng Zhang","Saket Sathe"],"pdf_url":"https://arxiv.org/pdf/2412.10416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10291v2","updated":"2025-02-14T17:37:35Z","published":"2024-06-13T03:26:30Z","title":"ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents","summary":"  Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.\n","authors":["Hao Kang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.10291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10297v1","updated":"2025-02-14T16:59:05Z","published":"2025-02-14T16:59:05Z","title":"DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders","summary":"  Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers.\n","authors":["Julien Siems","Timur Carstensen","Arber Zela","Frank Hutter","Massimiliano Pontil","Riccardo Grazzi"],"pdf_url":"https://arxiv.org/pdf/2502.10297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17115v2","updated":"2025-02-14T16:44:08Z","published":"2024-09-25T17:28:13Z","title":"Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale","summary":"  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX\n","authors":["Fan Zhou","Zengzhi Wang","Qian Liu","Junlong Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17115v2.pdf","comment":"47 pages, 13 figures, 34 tables"},{"id":"http://arxiv.org/abs/2502.08859v2","updated":"2025-02-14T16:40:15Z","published":"2025-02-13T00:18:34Z","title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","summary":"  As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.\n","authors":["Clinton J. Wang","Dean Lee","Cristina Menghini","Johannes Mols","Jack Doughty","Adam Khoja","Jayson Lynch","Sean Hendryx","Summer Yue","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06426v2","updated":"2025-02-14T16:32:54Z","published":"2024-11-10T11:08:28Z","title":"SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains","summary":"  As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.\n","authors":["Bijoy Ahmed Saiem","MD Sadik Hossain Shanto","Rakib Ahsan","Md Rafi ur Rashid"],"pdf_url":"https://arxiv.org/pdf/2411.06426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13610v2","updated":"2025-02-14T16:27:25Z","published":"2024-10-17T14:46:22Z","title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling","summary":"  Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.\n","authors":["Yakun Zhu","Shaohang Wei","Xu Wang","Kui Xue","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13610v2.pdf","comment":"NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.10266v1","updated":"2025-02-14T16:23:39Z","published":"2025-02-14T16:23:39Z","title":"Are Large Language Models the future crowd workers of Linguistics?","summary":"  Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.\n","authors":["Iris Ferrazzo"],"pdf_url":"https://arxiv.org/pdf/2502.10266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10263v1","updated":"2025-02-14T16:16:02Z","published":"2025-02-14T16:16:02Z","title":"Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers","summary":"  Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.\n","authors":["Aivin V. Solatorio","Rafael Macalaba","James Liounis"],"pdf_url":"https://arxiv.org/pdf/2502.10263v1.pdf","comment":"Project GitHub repository at https://github.com/worldbank/ai4data-use"},{"id":"http://arxiv.org/abs/2502.10250v1","updated":"2025-02-14T15:59:33Z","published":"2025-02-14T15:59:33Z","title":"VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models","summary":"  Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.\n","authors":["Gokul Karthik Kumar","Iheb Chaabane","Kebin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.10250v1.pdf","comment":"Accepted at PAKDD 2025"},{"id":"http://arxiv.org/abs/2502.10248v1","updated":"2025-02-14T15:58:10Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v1.pdf","comment":"35 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.07780v2","updated":"2025-02-14T15:46:51Z","published":"2024-06-12T00:19:40Z","title":"A Critical Look At Tokenwise Reward-Guided Text Generation","summary":"  Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.\n","authors":["Ahmad Rashid","Ruotian Wu","Julia Grosse","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2406.07780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14050v3","updated":"2025-02-14T15:39:29Z","published":"2024-12-18T17:05:08Z","title":"Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation","summary":"  Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.\n","authors":["Vera Neplenbroek","Arianna Bisazza","Raquel Fern√°ndez"],"pdf_url":"https://arxiv.org/pdf/2412.14050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02844v3","updated":"2025-02-14T15:32:00Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19637v2","updated":"2025-02-14T15:20:42Z","published":"2024-10-25T15:39:34Z","title":"A distributional simplicity bias in the learning dynamics of\n  transformers","summary":"  The remarkable capability of over-parameterised neural networks to generalise\neffectively has been explained by invoking a ``simplicity bias'': neural\nnetworks prevent overfitting by initially learning simple classifiers before\nprogressing to more complex, non-linear functions. While simplicity biases have\nbeen described theoretically and experimentally in feed-forward networks for\nsupervised learning, the extent to which they also explain the remarkable\nsuccess of transformers trained with self-supervised techniques remains\nunclear. In our study, we demonstrate that transformers, trained on natural\nlanguage data, also display a simplicity bias. Specifically, they sequentially\nlearn many-body interactions among input tokens, reaching a saturation point in\nthe prediction error for low-degree interactions while continuing to learn\nhigh-degree interactions. To conduct this analysis, we develop a procedure to\ngenerate \\textit{clones} of a given natural language data set, which rigorously\ncapture the interactions between tokens up to a specified order. This approach\nopens up the possibilities of studying how interactions of different orders in\nthe data affect learning, in natural language processing and beyond.\n","authors":["Riccardo Rende","Federica Gerace","Alessandro Laio","Sebastian Goldt"],"pdf_url":"https://arxiv.org/pdf/2410.19637v2.pdf","comment":"10 pages, 5 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.10202v1","updated":"2025-02-14T14:56:19Z","published":"2025-02-14T14:56:19Z","title":"Can Post-Training Quantization Benefit from an Additional QLoRA\n  Integration?","summary":"  Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance.\n","authors":["Xiliang Zhu","Elena Khasanova","Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10202v1.pdf","comment":"Accepted to NAACL 2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.10201v1","updated":"2025-02-14T14:52:41Z","published":"2025-02-14T14:52:41Z","title":"Prediction hubs are context-informed frequent tokens in LLMs","summary":"  Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens.\n","authors":["Beatrix M. G. Nielsen","Iuri Macocco","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2502.10201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01237v2","updated":"2025-02-14T14:47:26Z","published":"2025-01-02T12:55:27Z","title":"Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction","summary":"  Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases.\n","authors":["Alexander Brinkmann","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2501.01237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15451v2","updated":"2025-02-14T14:03:43Z","published":"2025-01-26T08:45:37Z","title":"STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection","summary":"  The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese.\n","authors":["Zewen Bai","Yuanyuan Sun","Shengdi Yin","Junyu Lu","Jingjie Zeng","Haohao Zhu","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2501.15451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09078v3","updated":"2025-02-14T13:46:53Z","published":"2024-12-12T09:01:18Z","title":"Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning","summary":"  Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.\n","authors":["Zhenni Bi","Kai Han","Chuanjian Liu","Yehui Tang","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09078v3.pdf","comment":"Code will be available at\n  https://github.com/iamhankai/Forest-of-Thought"},{"id":"http://arxiv.org/abs/2502.10162v1","updated":"2025-02-14T13:46:14Z","published":"2025-02-14T13:46:14Z","title":"Revisiting Generalization Power of a DNN in Terms of Symbolic\n  Interactions","summary":"  This paper aims to analyze the generalization power of deep neural networks\n(DNNs) from the perspective of interactions. Unlike previous analysis of a\nDNN's generalization power in a highdimensional feature space, we find that the\ngeneralization power of a DNN can be explained as the generalization power of\nthe interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a\nspindle-shaped distribution. Furthermore, our theory can effectively\ndisentangle these two types of interactions from a DNN. We have verified that\nour theory can well match real interactions in a DNN in experiments.\n","authors":["Lei Cheng","Junpeng Zhang","Qihan Ren","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10162v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.19198"},{"id":"http://arxiv.org/abs/2410.14391v2","updated":"2025-02-14T13:15:13Z","published":"2024-10-18T11:52:10Z","title":"Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation","summary":"  Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation.\n","authors":["Wafaa Mohammed","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.14391v2.pdf","comment":"9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.10347v2","updated":"2025-02-14T13:13:33Z","published":"2024-10-14T10:00:49Z","title":"A Unified Approach to Routing and Cascading for LLMs","summary":"  The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection.\n","authors":["Jasper Dekoninck","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.10347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10140v1","updated":"2025-02-14T13:10:39Z","published":"2025-02-14T13:10:39Z","title":"Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages","summary":"  Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage.\n","authors":["Daniil Gurgurov","Ivan Vykopal","Josef van Genabith","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2502.10140v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2411.12118v2","updated":"2025-02-14T12:56:45Z","published":"2024-11-18T23:12:13Z","title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers","summary":"  In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.\n","authors":["Tiberiu Musat"],"pdf_url":"https://arxiv.org/pdf/2411.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14497v2","updated":"2025-02-14T12:38:15Z","published":"2025-01-24T13:53:54Z","title":"Evaluating and Improving Graph to Text Generation with Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.\n","authors":["Jie He","Yijun Yang","Wanqiu Long","Deyi Xiong","Victor Gutierrez-Basulto","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2501.14497v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.04348v2","updated":"2025-02-14T11:46:43Z","published":"2025-02-04T15:16:17Z","title":"Prompt-based Depth Pruning of Large Language Models","summary":"  Depth pruning aims to reduce the inference cost of a large language model\nwithout any hardware-specific complications, by simply removing several less\nimportant transformer blocks. However, our empirical findings suggest that the\nimportance of a transformer block may be highly task-dependent -- a block that\nis crucial for a task can be removed without degrading the accuracy on another\ntask. Based on this observation, we develop a dynamic depth pruning algorithm,\ncoined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which\nblocks to omit from the model based on the input prompt. PuDDing operates by\ntraining a lightweight router to predict the best omission set among a set of\noptions, where this option set has also been constructed in a data-driven\nmanner. Empirical results on commonsense reasoning benchmarks demonstrate that\nPuDDing effectively accelerates the inference language models, and achieves\nbetter on-task performance than static depth pruning baselines.\n","authors":["Juyun Wee","Minjae Park","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2502.04348v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.07016v3","updated":"2025-02-14T11:01:27Z","published":"2024-06-11T07:16:34Z","title":"Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary","summary":"  Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.\n","authors":["Dmitry Kobak","Rita Gonz√°lez-M√°rquez","Em≈ëke-√Ågnes Horv√°t","Jan Lause"],"pdf_url":"https://arxiv.org/pdf/2406.07016v3.pdf","comment":"v3: Updating the manuscript to include all PubMed abstracts until the\n  end of 2024"},{"id":"http://arxiv.org/abs/2502.10064v1","updated":"2025-02-14T10:41:42Z","published":"2025-02-14T10:41:42Z","title":"Hands-off Image Editing: Language-guided Editing without any\n  Task-specific Labeling, Masking or even Training","summary":"  Instruction-guided image editing consists in taking an image and an\ninstruction and deliverring that image altered according to that instruction.\nState-of-the-art approaches to this task suffer from the typical scaling up and\ndomain adaptation hindrances related to supervision as they eventually resort\nto some kind of task-specific labelling, masking or training. We propose a\nnovel approach that does without any such task-specific supervision and offers\nthus a better potential for improvement. Its assessment demonstrates that it is\nhighly effective, achieving very competitive performance.\n","authors":["Rodrigo Santos","Ant√≥nio Branco","Jo√£o Silva","Jo√£o Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2502.10064v1.pdf","comment":"Published in COLING 2025"},{"id":"http://arxiv.org/abs/2502.10061v1","updated":"2025-02-14T10:32:29Z","published":"2025-02-14T10:32:29Z","title":"Annotating Compositionality Scores for Irish Noun Compounds is Hard Work","summary":"  Noun compounds constitute a challenging construction for NLP applications,\ngiven their variability in idiomaticity and interpretation. In this paper, we\npresent an analysis of compound nouns identified in Irish text of varied\ndomains by expert annotators, focusing on compositionality as a key feature,\nbut also domain specificity, as well as familiarity and confidence of the\nannotator giving the ratings. Our findings and the discussion that ensued\ncontributes towards a greater understanding of how these constructions appear\nin Irish language, and how they might be treated separately from English noun\ncompounds.\n","authors":["Abigail Walsh","Teresa Clifford","Emma Daly","Jane Dunne","Brian Davis","Gear√≥id √ì Cleirc√≠n"],"pdf_url":"https://arxiv.org/pdf/2502.10061v1.pdf","comment":"6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.10058v1","updated":"2025-02-14T10:21:10Z","published":"2025-02-14T10:21:10Z","title":"MTLM: an Innovative Language Model Training Paradigm for ASR","summary":"  Pre-training Transformer-based language models (LMs) on a large amount of\ntext has proven crucial for improving automatic speech recognition (ASR)\nperformance. Generally, traditional LMs are unidirectional and unable to access\nthe context on the right. This paper proposes a method for training LMs that\nenable traditional unidirectional LMs to fully utilize left and right contexts.\nCompared with the unidirectional LMs, our LM facilitates ASR to transcribe\nhypotheses more consistently and in a more semantically unambiguous way, as it\nincorporates richer contextual representations. Finally, our experimental\nresults on the LibriSpeech corpus demonstrate that our model outperforms\ntraditional unidirectional LMs, whether n-best rescoring or shallow fusion is\nused as the decoding algorithm.\n","authors":["Qingliang Meng","Pengju Ren","Tian Li","Changsong Dai"],"pdf_url":"https://arxiv.org/pdf/2502.10058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02370v4","updated":"2025-02-14T10:04:55Z","published":"2024-09-04T01:40:20Z","title":"Do Large Language Models Possess Sensitive to Sentiment?","summary":"  Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized.\n","authors":["Yang Liu","Xichou Zhu","Zhou Shen","Yi Liu","Min Li","Yujun Chen","Benzi John","Zhenzhen Ma","Tao Hu","Zhi Li","Zhiyang Xu","Wei Luo","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02370v4.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.02375v4","updated":"2025-02-14T10:02:14Z","published":"2024-09-04T01:51:37Z","title":"How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review","summary":"  The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.\n","authors":["Yang Liu","Xichou Zhu","Zhou Shen","Yi Liu","Min Li","Yujun Chen","Benzi John","Zhenzhen Ma","Tao Hu","Zhi Li","Bolong Yang","Manman Wang","Zongxing Xie","Peng Liu","Dan Cai","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02375v4.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.10051v1","updated":"2025-02-14T10:00:20Z","published":"2025-02-14T10:00:20Z","title":"ORI: O Routing Intelligence","summary":"  Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models.\n","authors":["Ahmad Shadid","Rahul Kumar","Mohit Mayank"],"pdf_url":"https://arxiv.org/pdf/2502.10051v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.01584v2","updated":"2025-02-14T09:56:31Z","published":"2024-09-03T03:42:56Z","title":"Towards Cross-Lingual Explanation of Artwork in Large-scale Vision\n  Language Models","summary":"  As the performance of Large-scale Vision Language Models (LVLMs) improves,\nthey are increasingly capable of responding in multiple languages, and there is\nan expectation that the demand for explanations generated by LVLMs will grow.\nHowever, pre-training of Vision Encoder and the integrated training of LLMs\nwith Vision Encoder are mainly conducted using English training data, leaving\nit uncertain whether LVLMs can completely handle their potential when\ngenerating explanations in languages other than English. In addition,\nmultilingual QA benchmarks that create datasets using machine translation have\ncultural differences and biases, remaining issues for use as evaluation tasks.\nTo address these challenges, this study created an extended dataset in multiple\nlanguages without relying on machine translation. This dataset that takes into\naccount nuances and country-specific phrases was then used to evaluate the\ngeneration explanation abilities of LVLMs. Furthermore, this study examined\nwhether Instruction-Tuning in resource-rich English improves performance in\nother languages. Our findings indicate that LVLMs perform worse in languages\nother than English compared to English. In addition, it was observed that LVLMs\nstruggle to effectively manage the knowledge learned from English data. Our\ndataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt\n","authors":["Shintaro Ozaki","Kazuki Hayashi","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2409.01584v2.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2406.04344v3","updated":"2025-02-14T09:51:46Z","published":"2024-06-06T17:59:56Z","title":"Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models","summary":"  Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.\n","authors":["Tim Z. Xiao","Robert Bamler","Bernhard Sch√∂lkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.04344v3.pdf","comment":"Published in Transactions on Machine Learning Research (116 pages, 32\n  figures, v3: refined the paper structure and added more empirical results)"},{"id":"http://arxiv.org/abs/2502.03930v2","updated":"2025-02-14T09:49:57Z","published":"2025-02-06T10:09:49Z","title":"DiTAR: Diffusion Transformer Autoregressive Modeling for Speech\n  Generation","summary":"  Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness.\n","authors":["Dongya Jia","Zhuo Chen","Jiawei Chen","Chenpeng Du","Jian Wu","Jian Cong","Xiaobin Zhuang","Chumin Li","Zhen Wei","Yuping Wang","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03930v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.16345v2","updated":"2025-02-14T09:32:11Z","published":"2024-11-25T12:44:02Z","title":"Preference Optimization for Reasoning with Pseudo Feedback","summary":"  Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.\n","authors":["Fangkai Jiao","Geyang Guo","Xingxing Zhang","Nancy F. Chen","Shafiq Joty","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.16345v2.pdf","comment":"28 pages, 11 figures. ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10013v1","updated":"2025-02-14T08:47:10Z","published":"2025-02-14T08:47:10Z","title":"Probabilistic Lexical Manifold Construction in Large Language Models via\n  Hierarchical Vector Field Interpolation","summary":"  Hierarchical vector field interpolation introduces a structured probabilistic\nframework for lexical representation, ensuring that word embeddings transition\nsmoothly across a continuous manifold rather than being constrained to discrete\ntoken mappings. The proposed methodology constructs a probabilistic function\nspace where word representations adhere to topological consistency, mitigating\nrepresentational discontinuities commonly observed in transformer-based\nembeddings. Empirical evaluations reveal that probabilistic constraints enhance\nlexical coherence by refining contextual relationships, leading to improvements\nin semantic stability across multiple linguistic distributions. The application\nof divergence minimization techniques ensures that interpolated embeddings\nmaintain probabilistic consistency while preserving computational feasibility\nfor large-scale implementations. Experimental findings demonstrate that\ninterpolated lexical manifolds improve representation density alignment,\nreducing anisotropic distortions in contextual embedding distributions.\nComparative analyses with standard transformer-based models highlight that\nstructured interpolation yields more stable representations, particularly in\ntasks requiring fine-grained semantic differentiation. The statistical\nevaluation of embedding divergence confirms that probabilistic lexical\nmanifolds reduce representational inconsistencies while maintaining coherence\nacross varying scales of contextual abstraction. An assessment of computational\nefficiency reveals that while interpolation introduces minor processing\noverhead, the structured representation learning approach remains scalable for\npractical deployment.\n","authors":["Clive Pendleton","Ewan Harrington","Giles Fairbrother","Jasper Arkwright","Nigel Fenwick","Richard Katrix"],"pdf_url":"https://arxiv.org/pdf/2502.10013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16466v4","updated":"2025-02-14T08:44:16Z","published":"2023-11-28T04:07:34Z","title":"The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry","summary":"  Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. We analyzed over 1\nmillion complaints and identified a significant increase in LLM usage following\nthe release of ChatGPT. We find that LLM usage is associated with an increased\nlikelihood of obtaining relief from financial firms. To investigate this\nrelationship, we employ an instrumental variable approach to mitigate\nendogeneity concerns around LLM adoption. Although instrumental variables\nsuggest a potential causal link, they cannot fully capture all unobserved\nheterogeneity. To further establish this causal relationship, we conducted\ncontrolled experiments, which support that LLMs can enhance the clarity and\npersuasiveness of consumer narratives, thereby increasing the likelihood of\nobtaining relief. Our findings suggest that facilitating access to LLMs can\nhelp firms better understand consumer concerns and level the playing field\namong consumers. This underscores the importance of policies promoting\ntechnological accessibility, enabling all consumers to effectively voice their\nconcerns.\n","authors":["Minkyu Shin","Jin Kim","Jiwoong Shin"],"pdf_url":"https://arxiv.org/pdf/2311.16466v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12475v2","updated":"2025-02-14T08:40:39Z","published":"2024-12-17T02:22:24Z","title":"RareAgents: Advancing Rare Disease Care through LLM-Empowered\n  Multi-disciplinary Team","summary":"  Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the vast number of diseases. The\ninvolvement of multiple organs and systems, and the shortage of specialized\ndoctors with relevant experience make diagnosing and treating rare diseases\nmore challenging than common diseases. Recently, agents powered by large\nlanguage models (LLMs) have demonstrated notable applications across various\ndomains. In the medical field, some agent methods have outperformed direct\nprompts in question-answering tasks from medical examinations. However, current\nagent frameworks are not well-adapted to real-world clinical scenarios,\nespecially those involving the complex demands of rare diseases. To bridge this\ngap, we introduce RareAgents, the first LLM-driven multi-disciplinary team\nframework designed specifically for the complex clinical context of rare\ndiseases. RareAgents integrates advanced Multidisciplinary Team (MDT)\ncoordination, memory mechanisms, and medical tools utilization, leveraging\nLlama-3.1-8B/70B as the base model. Experimental results show that RareAgents\noutperforms state-of-the-art domain-specific models, GPT-4o, and current agent\nframeworks in differential diagnosis and medication recommendation for rare\ndiseases. Furthermore, we contribute a novel rare disease dataset,\nMIMIC-IV-Ext-Rare, to support further advancements in this field.\n","authors":["Xuanzhong Chen","Ye Jin","Xiaohao Mao","Lun Wang","Shuyang Zhang","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00675v2","updated":"2025-02-14T08:38:16Z","published":"2025-02-02T05:25:03Z","title":"ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction,\n  and Column Exploration","summary":"  Text-to-SQL systems have unlocked easier access to critical data insights by\nenabling natural language queries over structured databases. However, deploying\nsuch systems in enterprise environments remains challenging due to factors such\nas large, complex schemas (> 3000 columns), diverse SQL dialects (e.g.,\nBigQuery, Snowflake) and sophisticated query requirements (e.g.,\ntransformation, analytics). Current state-of-the-art performance on the Spider\n2.0 dataset -- a benchmark built to mimic such complex environments -- remains\nlimited at 20%. Key limitations include inadequate instruction-following, poor\nlong-context comprehension, weak self-refinement, and insufficient\ndialect-specific knowledge. To address these gaps, we propose ReFoRCE\n(Self-Refinement Agent with Format Restriction and Column Exploration) which\nintroduces (1) table compression to mitigate long-context limitations (2)\nformat restriction to ensure accurate answer format, and (3) iterative column\nexploration for enhanced schema understanding. Additionally, it employs\nself-refinement pipeline consisting of (1) parallelized workflows with voting\nmechanisms and (2) a Common Table Expression (CTE) based refinement approach to\nhandle unresolved cases. ReFoRCE achieves state-of-the-art results scoring\n31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.\n","authors":["Minghang Deng","Ashwin Ramachandran","Canwen Xu","Lanxiang Hu","Zhewei Yao","Anupam Datta","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00675v2.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.10003v1","updated":"2025-02-14T08:34:26Z","published":"2025-02-14T08:34:26Z","title":"SciClaimHunt: A Large Dataset for Evidence-based Scientific Claim\n  Verification","summary":"  Verifying scientific claims presents a significantly greater challenge than\nverifying political or news-related claims. Unlike the relatively broad\naudience for political claims, the users of scientific claim verification\nsystems can vary widely, ranging from researchers testing specific hypotheses\nto everyday users seeking information on a medication. Additionally, the\nevidence for scientific claims is often highly complex, involving technical\nterminology and intricate domain-specific concepts that require specialized\nmodels for accurate verification. Despite considerable interest from the\nresearch community, there is a noticeable lack of large-scale scientific claim\nverification datasets to benchmark and train effective models. To bridge this\ngap, we introduce two large-scale datasets, SciClaimHunt and SciClaimHunt_Num,\nderived from scientific research papers. We propose several baseline models\ntailored for scientific claim verification to assess the effectiveness of these\ndatasets. Additionally, we evaluate models trained on SciClaimHunt and\nSciClaimHunt_Num against existing scientific claim verification datasets to\ngauge their quality and reliability. Furthermore, we conduct human evaluations\nof the claims in proposed datasets and perform error analysis to assess the\neffectiveness of the proposed baseline models. Our findings indicate that\nSciClaimHunt and SciClaimHunt_Num serve as highly reliable resources for\ntraining models in scientific claim verification.\n","authors":["Sujit Kumar","Anshul Sharma","Siddharth Hemant Khincha","Gargi Shroff","Sanasam Ranbir Singh","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.10003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10001v1","updated":"2025-02-14T08:33:31Z","published":"2025-02-14T08:33:31Z","title":"EmbBERT-Q: Breaking Memory Barriers in Embedded NLP","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM.\n","authors":["Riccardo Bravin","Massimo Pavan","Hazem Hesham Yousef Shalby","Fabrizio Pittorino","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2502.10001v1.pdf","comment":"24 pages, 4 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.09992v1","updated":"2025-02-14T08:23:51Z","published":"2025-02-14T08:23:51Z","title":"Large Language Diffusion Models","summary":"  Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs.\n","authors":["Shen Nie","Fengqi Zhu","Zebin You","Xiaolu Zhang","Jingyang Ou","Jun Hu","Jun Zhou","Yankai Lin","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09990v1","updated":"2025-02-14T08:22:51Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09977v1","updated":"2025-02-14T08:04:22Z","published":"2025-02-14T08:04:22Z","title":"LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  - No Silver Bullet for LC or RAG Routing","summary":"  Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2,326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}.\n","authors":["Kuan Li","Liwen Zhang","Yong Jiang","Pengjun Xie","Fei Huang","Shuai Wang","Minhao Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09977v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.09969v1","updated":"2025-02-14T07:55:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tur"],"pdf_url":"https://arxiv.org/pdf/2502.09969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19324v2","updated":"2025-02-14T07:30:00Z","published":"2025-01-31T17:19:57Z","title":"Reward-Guided Speculative Decoding for Efficient LLM Reasoning","summary":"  We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.\n","authors":["Baohao Liao","Yuhui Xu","Hanze Dong","Junnan Li","Christof Monz","Silvio Savarese","Doyen Sahoo","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2501.19324v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2502.09956v1","updated":"2025-02-14T07:28:08Z","published":"2025-02-14T07:28:08Z","title":"KGGen: Extracting Knowledge Graphs from Plain Text with Language Models","summary":"  Recent interest in building foundation models for KGs has highlighted a\nfundamental challenge: knowledge-graph data is relatively scarce. The\nbest-known KGs are primarily human-labeled, created by pattern-matching, or\nextracted using early NLP techniques. While human-generated KGs are in short\nsupply, automatically extracted KGs are of questionable quality. We present a\nsolution to this data scarcity problem in the form of a text-to-KG generator\n(KGGen), a package that uses language models to create high-quality graphs from\nplaintext. Unlike other KG extractors, KGGen clusters related entities to\nreduce sparsity in extracted KGs. KGGen is available as a Python library\n(\\texttt{pip install kg-gen}), making it accessible to everyone. Along with\nKGGen, we release the first benchmark, Measure of of Information in Nodes and\nEdges (MINE), that tests an extractor's ability to produce a useful KG from\nplain text. We benchmark our new tool against existing extractors and\ndemonstrate far superior performance.\n","authors":["Belinda Mo","Kyssen Yu","Joshua Kazdan","Proud Mpala","Lisa Yu","Chris Cundy","Charilaos Kanatsoulis","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2502.09956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09944v1","updated":"2025-02-14T06:47:37Z","published":"2025-02-14T06:47:37Z","title":"Self-Supervised Learning for Neural Topic Models with\n  Variance-Invariance-Covariance Regularization","summary":"  In our study, we propose a self-supervised neural topic model (NTM) that\ncombines the power of NTMs and regularized self-supervised learning methods to\nimprove performance. NTMs use neural networks to learn latent topics hidden\nbehind the words in documents, enabling greater flexibility and the ability to\nestimate more coherent topics compared to traditional topic models. On the\nother hand, some self-supervised learning methods use a joint embedding\narchitecture with two identical networks that produce similar representations\nfor two augmented versions of the same input. Regularizations are applied to\nthese representations to prevent collapse, which would otherwise result in the\nnetworks outputting constant or redundant representations for all inputs. Our\nmodel enhances topic quality by explicitly regularizing latent topic\nrepresentations of anchor and positive samples. We also introduced an\nadversarial data augmentation method to replace the heuristic sampling method.\nWe further developed several variation models including those on the basis of\nan NTM that incorporates contrastive learning with both positive and negative\nsamples. Experimental results on three datasets showed that our models\noutperformed baselines and state-of-the-art models both quantitatively and\nqualitatively.\n","authors":["Weiran Xu","Kengo Hirami","Koji Eguchi"],"pdf_url":"https://arxiv.org/pdf/2502.09944v1.pdf","comment":"Preprint accepted in Springer Knowledge and Information Systems\n  (KAIS), in press"},{"id":"http://arxiv.org/abs/2502.09940v1","updated":"2025-02-14T06:34:08Z","published":"2025-02-14T06:34:08Z","title":"A Preliminary Exploration with GPT-4o Voice Mode","summary":"  With the rise of multimodal large language models, GPT-4o stands out as a\npioneering model, driving us to evaluate its capabilities. This report assesses\nGPT-4o across various tasks to analyze its audio processing and reasoning\nabilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and\nmusic understanding, performing well in tasks like intent classification,\nspoken command classification, semantic and grammatical reasoning.,\nmultilingual speech recognition, and singing analysis. It also shows greater\nrobustness against hallucinations than other large audio-language models\n(LALMs). However, it struggles with tasks such as audio duration prediction and\ninstrument classification. Additionally, GPT-4o's safety mechanisms cause it to\ndecline tasks like speaker identification, age classification, MOS prediction,\nand audio deepfake detection. Notably, the model exhibits a significantly\ndifferent refusal rate when responding to speaker verification tasks on\ndifferent datasets. This is likely due to variations in the accompanying\ninstructions or the quality of the input audio, suggesting the sensitivity of\nits built-in safeguards. Finally, we acknowledge that model performance varies\nwith evaluation protocols. This report only serves as a preliminary exploration\nof the current state of LALMs.\n","authors":["Yu-Xiang Lin","Chih-Kai Yang","Wei-Chih Chen","Chen-An Li","Chien-yu Huang","Xuanjun Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2502.09940v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.08943v2","updated":"2025-02-14T06:10:00Z","published":"2025-02-13T03:43:33Z","title":"Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis","summary":"  Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.\n","authors":["Wenbo Zhang","Hengrui Cai","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08943v2.pdf","comment":"10 pages, 1 table, 4 Figures"},{"id":"http://arxiv.org/abs/2502.09933v1","updated":"2025-02-14T06:05:12Z","published":"2025-02-14T06:05:12Z","title":"MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot\n  In-Context Inductive Reasoning","summary":"  Inductive Reasoning (IR), the ability to summarize rules from examples and\napply on new ones, has long been viewed as a primal ability for general\nintelligence and widely studied by cognitive science and AI researchers. Many\nbenchmarks have been proposed to measure such ability for Large Language Models\n(LLMs); however, they focus on few-shot (usually $<$10) setting and lack\nevaluation for aggregating many pieces of information from long contexts. On\nthe other hand, the ever-growing context length of LLMs have brought forth the\nnovel paradigm of many-shot In-Context Learning (ICL), which addresses new\ntasks with hundreds to thousands of examples without expensive and inefficient\nfine-tuning. However, many-shot evaluations are mostly focused on\nclassification (a very limited aspect of IR), and popular long-context LLM\ntasks such as Needle-In-A-Haystack (NIAH) seldom require complicated\nintelligence for integrating many pieces of information. To fix the issues from\nboth worlds, we propose MIR-Bench, the first many-shot in-context inductive\nreasoning benchmark that asks LLM to induce output via input-output examples\nfrom underlying functions with diverse data format. Based on MIR-Bench, we\nstudy many novel problems for inductive reasoning and many-shot ICL, including\nrobustness against erroneous shots and the effect of Chain-of-Thought (CoT),\nand acquired insightful findings.\n","authors":["Kai Yan","Zhan Ling","Kang Liu","Yifan Yang","Ting-Han Fan","Lingfeng Shen","Zhengyin Du","Jiecao Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09933v1.pdf","comment":"32 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.18279v9","updated":"2025-02-14T05:42:16Z","published":"2024-11-27T12:13:39Z","title":"Large Language Model-Brained GUI Agents: A Survey","summary":"  GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.\n","authors":["Chaoyun Zhang","Shilin He","Jiaxu Qian","Bowen Li","Liqun Li","Si Qin","Yu Kang","Minghua Ma","Guyue Liu","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18279v9.pdf","comment":"The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration"},{"id":"http://arxiv.org/abs/2407.08952v2","updated":"2025-02-14T04:56:16Z","published":"2024-07-12T03:15:01Z","title":"Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection","summary":"  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.\n","authors":["Ye Liu","Jiajun Zhu","Xukai Liu","Haoyu Tang","Yanghai Zhang","Kai Zhang","Xiaofang Zhou","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02619v9","updated":"2025-02-14T04:43:31Z","published":"2024-02-04T21:33:18Z","title":"Arithmetic in Transformers Explained","summary":"  While recent work has shown transformers can learn addition, previous models\nexhibit poor prediction accuracy and are limited to small numbers. Furthermore,\nthe relationship between single-task and multitask arithmetic capabilities\nremains unexplored. In this work, we analyze 44 autoregressive transformer\nmodels trained on addition, subtraction, or both. These include 16\naddition-only models, 2 subtraction-only models, 8 \"mixed\" models trained to\nperform addition and subtraction, and 14 mixed models initialized with\nparameters from an addition-only model. The models span 5- to 15-digit\nquestions, 2 to 4 attention heads, and 2 to 3 layers. We show that the addition\nmodels converge on a common logical algorithm, with most models achieving\n>99.999% prediction accuracy. We provide a detailed mechanistic explanation of\nhow this algorithm is implemented within the network architecture.\nSubtraction-only models have lower accuracy. With the initialized mixed models,\nthrough parameter transfer experiments, we explore how multitask learning\ndynamics evolve, revealing that some features originally specialized for\naddition become polysemantic, serving both operations, and boosting subtraction\naccuracy. We explain the mixed algorithm mechanically. Finally, we introduce a\nreusable library of mechanistic interpretability tools to define, locate, and\nvisualize these algorithmic circuits across multiple models.\n","authors":["Philip Quirke","Clement Neo","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.02619v9.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.18472v3","updated":"2025-02-14T04:30:10Z","published":"2024-09-27T06:18:55Z","title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological\n  and Multilingual Knowledge Base","summary":"  URIEL is a knowledge base offering geographical, phylogenetic, and\ntypological vector representations for 7970 languages. It includes distance\nmeasures between these vectors for 4005 languages, which are accessible via the\nlang2vec tool. Despite being frequently cited, URIEL is limited in terms of\nlinguistic inclusion and overall usability. To tackle these challenges, we\nintroduce URIEL+, an enhanced version of URIEL and lang2vec that addresses\nthese limitations. In addition to expanding typological feature coverage for\n2898 languages, URIEL+ improves the user experience with robust, customizable\ndistance calculations to better suit the needs of users. These upgrades also\noffer competitive performance on downstream tasks and provide distances that\nbetter align with linguistic distance studies.\n","authors":["Aditya Khan","Mason Shipton","David Anugraha","Kaiyao Duan","Phuong H. Hoang","Eric Khiu","A. Seza Doƒüru√∂z","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2409.18472v3.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2304.00228v3","updated":"2025-02-14T04:00:08Z","published":"2023-04-01T05:04:06Z","title":"Accuracy and Political Bias of News Source Credibility Ratings by Large\n  Language Models","summary":"  Search engines increasingly leverage large language models (LLMs) to generate\ndirect answers, and AI chatbots now access the Internet for fresh data. As\ninformation curators for billions of users, LLMs must assess the accuracy and\nreliability of different sources. This paper audits nine widely used LLMs from\nthree leading providers -- OpenAI, Google, and Meta -- to evaluate their\nability to discern credible and high-quality information sources from\nlow-credibility ones. We find that while LLMs can rate most tested news\noutlets, larger models more frequently refuse to provide ratings due to\ninsufficient information, whereas smaller models are more prone to making\nerrors in their ratings. For sources where ratings are provided, LLMs exhibit a\nhigh level of agreement among themselves (average Spearman's $\\rho = 0.79$),\nbut their ratings align only moderately with human expert evaluations (average\n$\\rho = 0.50$). Analyzing news sources with different political leanings in the\nUS, we observe a liberal bias in credibility ratings yielded by all LLMs in\ndefault configurations. Additionally, assigning partisan roles to LLMs\nconsistently induces strong politically congruent bias in their ratings. These\nfindings have important implications for the use of LLMs in curating news and\npolitical information.\n","authors":["Kai-Cheng Yang","Filippo Menczer"],"pdf_url":"https://arxiv.org/pdf/2304.00228v3.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09573v2","updated":"2025-02-14T03:31:39Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15140v2","updated":"2025-02-14T02:57:30Z","published":"2025-01-25T08:52:43Z","title":"Analyzing and Boosting the Power of Fine-Grained Visual Recognition for\n  Multi-modal Large Language Models","summary":"  Multi-modal large language models (MLLMs) have shown remarkable abilities in\nvarious visual understanding tasks. However, MLLMs still struggle with\nfine-grained visual recognition (FGVR), which aims to identify\nsubordinate-level categories from images. This can negatively impact more\nadvanced capabilities of MLLMs, such as object-centric visual question\nanswering and reasoning. In our study, we revisit three quintessential\ncapabilities of MLLMs for FGVR, including object information extraction,\ncategory knowledge reserve, object-category alignment, and position of the root\ncause as a misalignment problem. To address this issue, we present Finedefics,\nan MLLM that enhances the model's FGVR capability by incorporating informative\nattribute descriptions of objects into the training phase. We employ\ncontrastive learning on object-attribute pairs and attribute-category pairs\nsimultaneously and use examples from similar but incorrect categories as hard\nnegatives, naturally bringing representations of visual objects and category\nnames closer. Extensive evaluations across multiple popular FGVR datasets\ndemonstrate that Finedefics outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code is available at\nhttps://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.\n","authors":["Hulingxiao He","Geng Li","Zijun Geng","Jinglin Xu","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2501.15140v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09870v1","updated":"2025-02-14T02:43:46Z","published":"2025-02-14T02:43:46Z","title":"A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism\n  of Language Technologies","summary":"  Recent attention to anthropomorphism -- the attribution of human-like\nqualities to non-human objects or entities -- of language technologies like\nLLMs has sparked renewed discussions about potential negative impacts of\nanthropomorphism. To productively discuss the impacts of this anthropomorphism\nand in what contexts it is appropriate, we need a shared vocabulary for the\nvast variety of ways that language can be anthropomorphic. In this work, we\ndraw on existing literature and analyze empirical cases of user interactions\nwith language technologies to develop a taxonomy of textual expressions that\ncan contribute to anthropomorphism. We highlight challenges and tensions\ninvolved in understanding linguistic anthropomorphism, such as how all language\nis fundamentally human and how efforts to characterize and shift perceptions of\nhumanness in machines can also dehumanize certain humans. We discuss ways that\nour taxonomy supports more precise and effective discussions of and decisions\nabout anthropomorphism of language technologies.\n","authors":["Alicia DeVrio","Myra Cheng","Lisa Egede","Alexandra Olteanu","Su Lin Blodgett"],"pdf_url":"https://arxiv.org/pdf/2502.09870v1.pdf","comment":"18 pages, 1 figure, to appear at CHI 2025"},{"id":"http://arxiv.org/abs/2409.19471v2","updated":"2025-02-14T02:40:55Z","published":"2024-09-28T22:33:44Z","title":"SELP: Generating Safe and Efficient Task Plans for Robot Agents with\n  Large Language Models","summary":"  Despite significant advancements in large language models (LLMs) that enhance\nrobot agents' understanding and execution of natural language (NL) commands,\nensuring the agents adhere to user-specified constraints remains challenging,\nparticularly for complex commands and long-horizon tasks. To address this\nchallenge, we present three key insights, equivalence voting, constrained\ndecoding, and domain-specific fine-tuning, which significantly enhance LLM\nplanners' capability in handling complex tasks. Equivalence voting ensures\nconsistency by generating and sampling multiple Linear Temporal Logic (LTL)\nformulas from NL commands, grouping equivalent LTL formulas, and selecting the\nmajority group of formulas as the final LTL formula. Constrained decoding then\nuses the generated LTL formula to enforce the autoregressive inference of\nplans, ensuring the generated plans conform to the LTL. Domain-specific\nfine-tuning customizes LLMs to produce safe and efficient plans within specific\ntask domains. Our approach, Safe Efficient LLM Planner (SELP), combines these\ninsights to create LLM planners to generate plans adhering to user commands\nwith high confidence. We demonstrate the effectiveness and generalizability of\nSELP across different robot agents and tasks, including drone navigation and\nrobot manipulation. For drone navigation tasks, SELP outperforms\nstate-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks\nconforming to NL commands) and by 19.8% in plan efficiency. For robot\nmanipulation tasks, SELP achieves 20.4% improvement in safety rate. Our\ndatasets for evaluating NL-to-LTL and robot task planning will be released in\ngithub.com/lt-asset/selp.\n","authors":["Yi Wu","Zikang Xiong","Yiran Hu","Shreyash S. Iyengar","Nan Jiang","Aniket Bera","Lin Tan","Suresh Jagannathan"],"pdf_url":"https://arxiv.org/pdf/2409.19471v2.pdf","comment":"This paper has been accepted for presentation at the 2025 IEEE\n  International Conference on Robotics and Automation (ICRA), May 19-23, 2025,\n  Atlanta, USA, and for inclusion in the conference proceeding"},{"id":"http://arxiv.org/abs/2502.09863v1","updated":"2025-02-14T02:16:48Z","published":"2025-02-14T02:16:48Z","title":"Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence\n  of Analogical Reasoning","summary":"  The remarkable success of large language models relies on their ability to\nimplicitly learn structured latent representations from the pretraining corpus.\nAs a simpler surrogate for representation learning in language modeling, we\nstudy a class of solvable contrastive self-supervised algorithms which we term\nquadratic word embedding models. These models resemble the word2vec algorithm\nand perform similarly on downstream tasks. Our main contributions are\nanalytical solutions for both the training dynamics (under certain\nhyperparameter choices) and the final word embeddings, given in terms of only\nthe corpus statistics. Our solutions reveal that these models learn orthogonal\nlinear subspaces one at a time, each one incrementing the effective rank of the\nembeddings until model capacity is saturated. Training on WikiText, we find\nthat the top subspaces represent interpretable concepts. Finally, we use our\ndynamical theory to predict how and when models acquire the ability to complete\nanalogies.\n","authors":["Dhruva Karkada","James B. Simon","Yasaman Bahri","Michael R. DeWeese"],"pdf_url":"https://arxiv.org/pdf/2502.09863v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.09858v1","updated":"2025-02-14T01:46:00Z","published":"2025-02-14T01:46:00Z","title":"Automated Hypothesis Validation with Agentic Sequential Falsifications","summary":"  Hypotheses are central to information acquisition, decision-making, and\ndiscovery. However, many real-world hypotheses are abstract, high-level\nstatements that are difficult to validate directly. This challenge is further\nintensified by the rise of hypothesis generation from Large Language Models\n(LLMs), which are prone to hallucination and produce hypotheses in volumes that\nmake manual validation impractical. Here we propose Popper, an agentic\nframework for rigorous automated validation of free-form hypotheses. Guided by\nKarl Popper's principle of falsification, Popper validates a hypothesis using\nLLM agents that design and execute falsification experiments targeting its\nmeasurable implications. A novel sequential testing framework ensures strict\nType-I error control while actively gathering evidence from diverse\nobservations, whether drawn from existing data or newly conducted procedures.\nWe demonstrate Popper on six domains including biology, economics, and\nsociology. Popper delivers robust error control, high power, and scalability.\nFurthermore, compared to human scientists, Popper achieved comparable\nperformance in validating complex biological hypotheses while reducing time by\n10 folds, providing a scalable, rigorous solution for hypothesis validation.\n","authors":["Kexin Huang","Ying Jin","Ryan Li","Michael Y. Li","Emmanuel Cand√®s","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2502.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09854v1","updated":"2025-02-14T01:39:45Z","published":"2025-02-14T01:39:45Z","title":"Efficient Multitask Learning in Small Language Models Through\n  Upside-Down Reinforcement Learning","summary":"  In this work, we demonstrate that small language models (SLMs), specifically\na 100M parameter GPT-2 model, can achieve competitive performance in multitask\nprompt generation tasks while requiring only a fraction of the computational\nresources needed by large language models (LLMs). Through a novel combination\nof upside-down reinforcement learning and synthetic data distillation from a\npowerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5%\nof state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite\nbeing up to 80 times smaller, making it highly suitable for\nresource-constrained and real-time applications. This study highlights the\npotential of SLMs as efficient multitask learners in multimodal settings,\nproviding a promising alternative to LLMs for scalable, low-latency\ndeployments.\n","authors":["Yu-Chen Lin","Sanat Sharma","Hari Manikandan","Jayant Kumar","Tracy Holloway King","Jing Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.09854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05584v5","updated":"2025-02-14T01:21:57Z","published":"2024-10-08T00:52:03Z","title":"Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?","summary":"  Reward Models (RMs) are crucial for aligning language models with human\npreferences. Currently, the evaluation of RMs depends on measuring accuracy\nagainst a validation set of manually annotated preference data. Although this\nmethod is straightforward and widely adopted, the relationship between RM\naccuracy and downstream policy performance remains under-explored. In this\nwork, we conduct experiments in a synthetic setting to investigate how\ndifferences in RM measured by accuracy translate into gaps in optimized policy\nperformance. Our findings reveal that while there is a weak positive\ncorrelation between accuracy and downstream performance, policies optimized\ntowards RMs with similar accuracy can exhibit quite different performance.\nMoreover, we discover that the way of measuring accuracy significantly impacts\nits ability to predict the final policy performance. Through the lens of the\nRegressional Goodhart effect, we recognize that accuracy, when used for\nmeasuring RM quality, can fail to fully capture the potential RM\noveroptimization. This underscores the inadequacy of relying solely on accuracy\nto reflect their impact on policy optimization.\n","authors":["Xueru Wen","Jie Lou","Yaojie Lu","Hongyu Lin","Xing Yu","Xinyu Lu","Ben He","Xianpei Han","Debing Zhang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05584v5.pdf","comment":"Accepted at ICLR2025 Spotlight"},{"id":"http://arxiv.org/abs/2502.03824v3","updated":"2025-02-14T01:05:29Z","published":"2025-02-06T07:19:59Z","title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","summary":"  LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.\n","authors":["Minsang Kim","Seungjun Baek"],"pdf_url":"https://arxiv.org/pdf/2502.03824v3.pdf","comment":"the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted"},{"id":"http://arxiv.org/abs/2501.12746v4","updated":"2025-02-14T01:02:04Z","published":"2025-01-22T09:27:11Z","title":"EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering","summary":"  When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted analysis to provide high-quality answers. Current LLM-based\nquestion answering methods lack a detailed definition and learning process for\nevidence analysis, leading to the risk of error propagation and hallucinations\nwhile using evidence. Although increasing the parameter size of LLMs can\nalleviate these issues, it also presents challenges in training and deployment\nwith limited resources. In this study, we propose EvidenceMap, which aims to\nenable a tiny pre-trained language model to explicitly learn multiple aspects\nof biomedical evidence, including supportive evaluation, logical correlation\nand content summarization, thereby latently guiding a small generative model\n(around 3B parameters) to provide textual responses. Experimental results\ndemonstrate that our method, learning evidence analysis by fine-tuning a model\nwith only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and\n5.7% in reference-based quality and accuracy, respectively.\n","authors":["Chang Zong","Jian Wan","Siliang Tang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.12746v4.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08648v2","updated":"2025-02-14T00:32:56Z","published":"2025-01-15T08:24:03Z","title":"MAGNET: Augmenting Generative Decoders with Representation Learning and\n  Infilling Capabilities","summary":"  While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning). This separation overlooks the opportunity for\ndeveloping a more versatile language model and for these objectives to\ncomplement each other. In this work, we propose MAGNET, a method for adapting\ndecoder-only LLMs to generate robust representations and infill missing text\nspans. MAGNET employs three self-supervised training objectives and introduces\nan attention mechanism that combines bidirectional and causal attention,\nenabling unified training across all objectives. Our results demonstrate that\nLLMs adapted with MAGNET (1) surpass strong text encoders on token-level and\nsentence-level representation learning tasks, (2) generate contextually\nappropriate text infills by leveraging past and future contexts, (3) perform\nopen-ended text generation without excessive repetition of words or phrases,\nand (4) preserve the knowledge and reasoning capability gained by the LLM\nduring pretraining.\n","authors":["Savya Khosla","Aditi Tiwari","Kushal Kafle","Simon Jenni","Handong Zhao","John Collomosse","Jing Shi"],"pdf_url":"https://arxiv.org/pdf/2501.08648v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.10392v1","updated":"2025-02-14T18:59:59Z","published":"2025-02-14T18:59:59Z","title":"Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding","summary":"  In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.\n","authors":["Wenxuan Guo","Xiuwei Xu","Ziwei Wang","Jianjiang Feng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2502.10392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10391v1","updated":"2025-02-14T18:59:51Z","published":"2025-02-14T18:59:51Z","title":"MM-RLHF: The Next Step Forward in Multimodal LLM Alignment","summary":"  Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.\n","authors":["Yi-Fan Zhang","Tao Yu","Haochen Tian","Chaoyou Fu","Peiyan Li","Jianshu Zeng","Wulin Xie","Yang Shi","Huanyu Zhang","Junkang Wu","Xue Wang","Yibo Hu","Bin Wen","Fan Yang","Zhang Zhang","Tingting Gao","Di Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2502.10391v1.pdf","comment":"Project Page: https://mm-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2502.10389v1","updated":"2025-02-14T18:59:36Z","published":"2025-02-14T18:59:36Z","title":"Region-Adaptive Sampling for Diffusion Transformers","summary":"  Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.\n","authors":["Ziming Liu","Yifan Yang","Chengruidong Zhang","Yiqi Zhang","Lili Qiu","Yang You","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10385v1","updated":"2025-02-14T18:58:04Z","published":"2025-02-14T18:58:04Z","title":"Simplifying DINO via Coding Rate Regularization","summary":"  DINO and DINOv2 are two model families being widely used to learn\nrepresentations from unlabeled imagery data at large scales. Their learned\nrepresentations often enable state-of-the-art performance for downstream tasks,\nsuch as image classification and segmentation. However, they employ many\nempirically motivated design choices and their training pipelines are highly\ncomplex and unstable -- many hyperparameters need to be carefully tuned to\nensure that the representations do not collapse -- which poses considerable\ndifficulty to improving them or adapting them to new domains. In this work, we\nposit that we can remove most such-motivated idiosyncrasies in the pre-training\npipelines, and only need to add an explicit coding rate term in the loss\nfunction to avoid collapse of the representations. As a result, we obtain\nhighly simplified variants of the DINO and DINOv2 which we call SimDINO and\nSimDINOv2, respectively. Remarkably, these simplified models are more robust to\ndifferent design choices, such as network architecture and hyperparameters, and\nthey learn even higher-quality representations, measured by performance on\ndownstream tasks, offering a Pareto improvement over the corresponding DINO and\nDINOv2 models. This work highlights the potential of using simplifying design\nprinciples to improve the empirical practice of deep learning.\n","authors":["Ziyang Wu","Jingyuan Zhang","Druv Pai","XuDong Wang","Chandan Singh","Jianwei Yang","Jianfeng Gao","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2502.10385v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.10377v1","updated":"2025-02-14T18:54:21Z","published":"2025-02-14T18:54:21Z","title":"ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences","summary":"  We introduce ReStyle3D, a novel framework for scene-level appearance transfer\nfrom a single style image to a real-world scene represented by multiple views.\nThe method combines explicit semantic correspondences with multi-view\nconsistency to achieve precise and coherent stylization. Unlike conventional\nstylization methods that apply a reference style globally, ReStyle3D uses\nopen-vocabulary segmentation to establish dense, instance-level correspondences\nbetween the style and real-world images. This ensures that each object is\nstylized with semantically matched textures. It first transfers the style to a\nsingle view using a training-free semantic-attention mechanism in a diffusion\nmodel. It then lifts the stylization to additional views via a learned\nwarp-and-refine network guided by monocular depth and pixel-wise\ncorrespondences. Experiments show that ReStyle3D consistently outperforms prior\nmethods in structure preservation, perceptual style similarity, and multi-view\ncoherence. User studies further validate its ability to produce\nphoto-realistic, semantically faithful results. Our code, pretrained models,\nand dataset will be publicly released, to support new applications in interior\ndesign, virtual staging, and 3D-consistent stylization.\n","authors":["Liyuan Zhu","Shengqu Cai","Shengyu Huang","Gordon Wetzstein","Naji Khosravan","Iro Armeni"],"pdf_url":"https://arxiv.org/pdf/2502.10377v1.pdf","comment":"Project page: https://restyle3d.github.io/"},{"id":"http://arxiv.org/abs/2502.00700v2","updated":"2025-02-14T18:30:07Z","published":"2025-02-02T07:15:51Z","title":"S2CFormer: Reorienting Learned Image Compression from Spatial\n  Interaction to Channel Aggregation","summary":"  Transformers have achieved significant success in learned image compression\n(LIC), with Swin Transformers emerging as the mainstream choice for nonlinear\ntransforms. A common belief is that their sophisticated spatial operations\ncontribute most to their efficacy. However, the crucial role of the\nfeed-forward network (FFN) based Channel Aggregation module within the\ntransformer architecture has been largely overlooked, and the over-design of\nspatial operations leads to a suboptimal trade-off between decoding latency and\nR-D performance. In this paper, we reevaluate the key factors behind the\ncompetence of transformers in LIC. By replacing spatial operations with\nidentity mapping, we are surprised to find that channel operations alone can\napproach the R-D performance of the leading methods. This solid lower bound of\nperformance emphasizes that the presence of channel aggregation is more\nessential for the LIC model to achieve competitive performance, while the\npreviously complex spatial interactions are partly redundant. Based on this\ninsight, we initiate the \"S2CFormer\" paradigm, a general architecture that\nreorients the focus of LIC from Spatial Interaction to Channel Aggregation. We\npresent two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each\none incorporates a simple operator for spatial interaction and serves as\nnonlinear transform blocks for our LIC models. Both models demonstrate\nstate-of-the-art (SOTA) R-D performance and significantly faster decoding\nspeed. These results also motivate further exploration of advanced FFN\nstructures to enhance the R-D performance while maintaining model efficiency.\nWith these foundations, we introduce S2C-Hybrid, an enhanced LIC model that\ncombines the strengths of different S2CFormer instantiations. This model\noutperforms all the existing methods on several datasets, setting a new\nbenchmark for efficient and high-performance LIC.\n","authors":["Yunuo Chen","Qian Li","Bing He","Donghui Feng","Ronghua Wu","Qi Wang","Li Song","Guo Lu","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17331v4","updated":"2025-02-14T18:09:50Z","published":"2023-11-29T03:10:42Z","title":"Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering","summary":"  Recently, to comprehensively improve Vision Language Models (VLMs) for Visual\nQuestion Answering (VQA), several methods have been proposed to further\nreinforce the inference capabilities of VLMs to independently tackle VQA tasks\nrather than some methods that only utilize VLMs as aids to Large Language\nModels (LLMs). However, these methods ignore the rich common-sense knowledge\ninside the given VQA image sampled from the real world. Thus, they cannot fully\nuse the powerful VLM for the given VQA question to achieve optimal performance.\nAttempt to overcome this limitation and inspired by the human top-down\nreasoning process, i.e., systematically exploring relevant issues to derive a\ncomprehensive answer, this work introduces a novel, explainable multi-agent\ncollaboration framework by leveraging the expansive knowledge of Large Language\nModels (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our\nframework comprises three agents, i.e., Responder, Seeker, and Integrator, to\ncollaboratively answer the given VQA question by seeking its relevant issues\nand generating the final answer in such a top-down reasoning process. The\nVLM-based Responder agent generates the answer candidates for the question and\nresponds to other relevant issues. The Seeker agent, primarily based on LLM,\nidentifies relevant issues related to the question to inform the Responder\nagent and constructs a Multi-View Knowledge Base (MVKB) for the given visual\nscene by leveraging the build-in world knowledge of LLM. The Integrator agent\ncombines knowledge from the Seeker agent and the Responder agent to produce the\nfinal VQA answer. Extensive and comprehensive evaluations on diverse VQA\ndatasets with a variety of VLMs demonstrate the superior performance and\ninterpretability of our framework over the baseline method in the zero-shot\nsetting without extra training cost.\n","authors":["Zeqing Wang","Wentao Wan","Qiqing Lao","Runmeng Chen","Minjie Lang","Xiao Wang","Keze Wang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2311.17331v4.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.07701v2","updated":"2025-02-14T18:02:38Z","published":"2025-02-11T16:58:15Z","title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute","summary":"  In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.\n","authors":["Hongwei Yi","Shitong Shao","Tian Ye","Jiantong Zhao","Qingyu Yin","Michael Lingelbach","Li Yuan","Yonghong Tian","Enze Xie","Daquan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07701v2.pdf","comment":"Serious modification needed."},{"id":"http://arxiv.org/abs/2502.10334v1","updated":"2025-02-14T17:47:18Z","published":"2025-02-14T17:47:18Z","title":"Ocular Disease Classification Using CNN with Deep Convolutional\n  Generative Adversarial Network","summary":"  The Convolutional Neural Network (CNN) has shown impressive performance in\nimage classification because of its strong learning capabilities. However, it\ndemands a substantial and balanced dataset for effective training. Otherwise,\nnetworks frequently exhibit over fitting and struggle to generalize to new\nexamples. Publicly available dataset of fundus images of ocular disease is\ninsufficient to train any classification model to achieve satisfactory\naccuracy. So, we propose Generative Adversarial Network(GAN) based data\ngeneration technique to synthesize dataset for training CNN based\nclassification model and later use original disease containing ocular images to\ntest the model. During testing the model classification accuracy with the\noriginal ocular image, the model achieves an accuracy rate of 78.6% for myopia,\n88.6% for glaucoma, and 84.6% for cataract, with an overall classification\naccuracy of 84.6%.\n","authors":["Arun Kunwar","Dibakar Raj Pant","Jukka Heikkonen","Rajeev Kanth"],"pdf_url":"https://arxiv.org/pdf/2502.10334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10008v2","updated":"2025-02-14T17:28:44Z","published":"2024-05-16T11:49:08Z","title":"Solving the enigma: Enhancing faithfulness and comprehensibility in\n  explanations of deep networks","summary":"  The accelerated progress of artificial intelligence (AI) has popularized deep\nlearning models across various domains, yet their inherent opacity poses\nchallenges, particularly in critical fields like healthcare, medicine, and the\ngeosciences. Explainable AI (XAI) has emerged to shed light on these 'black\nbox' models, aiding in deciphering their decision-making processes. However,\ndifferent XAI methods often produce significantly different explanations,\nleading to high inter-method variability that increases uncertainty and\nundermines trust in deep networks' predictions. In this study, we address this\nchallenge by introducing a novel framework designed to enhance the\nexplainability of deep networks through a dual focus on maximizing both\naccuracy and comprehensibility in the explanations. Our framework integrates\noutputs from multiple established XAI methods and leverages a non-linear neural\nnetwork model, termed the 'explanation optimizer,' to construct a unified,\noptimal explanation. The optimizer evaluates explanations using two key\nmetrics: faithfulness (accuracy in reflecting the network's decisions) and\ncomplexity (comprehensibility). By balancing these, it provides accurate and\naccessible explanations, addressing a key XAI limitation. Experiments on\nmulti-class and binary classification in 2D object and 3D neuroscience imaging\nconfirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63%\nhigher than the best XAI methods in 3D and 2D tasks, respectively, while also\nreducing complexity for better understanding. These results demonstrate that\noptimal explanations based on specific quality criteria are achievable,\noffering a solution to the issue of inter-method variability in the current XAI\nliterature and supporting more trustworthy deep network predictions\n","authors":["Michail Mamalakis","Antonios Mamalakis","Ingrid Agartz","Lynn Egeland M√∏rch-Johnsen","Graham Murray","John Suckling","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2405.10008v2.pdf","comment":"keywords: XAI, neuroscience, brain, 3D, 2D, computer vision,\n  classification"},{"id":"http://arxiv.org/abs/2502.07516v2","updated":"2025-02-14T17:24:56Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study presents the first systematic attempt to\nidentify prompts and text tokens in MIMIC-CXR that contribute the most to\ntraining data memorization. Our analysis reveals two unexpected findings: (1)\nprompts containing traces of de-identification procedures (markers introduced\nto hide Protected Health Information) are the most memorized, and (2) among all\ntokens, de-identification markers contribute the most towards memorization.\nThis highlights a broader issue with the standard anonymization practices and\nT2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time\nmemorization mitigation strategies are ineffective and fail to sufficiently\nreduce the model's reliance on memorized text tokens. On this front, we propose\nactionable strategies for different stakeholders to enhance privacy and improve\nthe reliability of generative models in medical imaging. Finally, our results\nprovide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset. The anonymized code is available at\nhttps://anonymous.4open.science/r/diffusion_memorization-8011/\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10310v1","updated":"2025-02-14T17:13:52Z","published":"2025-02-14T17:13:52Z","title":"Object Detection and Tracking","summary":"  Efficient and accurate object detection is an important topic in the\ndevelopment of computer vision systems. With the advent of deep learning\ntechniques, the accuracy of object detection has increased significantly. The\nproject aims to integrate a modern technique for object detection with the aim\nof achieving high accuracy with real-time performance. The reliance on other\ncomputer vision algorithms in many object identification systems, which results\nin poor and ineffective performance, is a significant obstacle. In this\nresearch, we solve the end-to-end object detection problem entirely using deep\nlearning techniques. The network is trained using the most difficult publicly\navailable dataset, which is used for an annual item detection challenge.\nApplications that need object detection can benefit the system's quick and\nprecise finding.\n","authors":["Md Pranto","Omar Faruk"],"pdf_url":"https://arxiv.org/pdf/2502.10310v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.10307v1","updated":"2025-02-14T17:10:17Z","published":"2025-02-14T17:10:17Z","title":"SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer\n  learning using Foundation Models","summary":"  Traditional solar forecasting models are based on several years of\nsite-specific historical irradiance data, often spanning five or more years,\nwhich are unavailable for newer photovoltaic farms. As renewable energy is\nhighly intermittent, building accurate solar irradiance forecasting systems is\nessential for efficient grid management and enabling the ongoing proliferation\nof solar energy, which is crucial to achieve the United Nations' net zero\ngoals. In this work, we propose SPIRIT, a novel approach leveraging foundation\nmodels for solar irradiance forecasting, making it applicable to newer solar\ninstallations. Our approach outperforms state-of-the-art models in zero-shot\ntransfer learning by about 70%, enabling effective performance at new locations\nwithout relying on any historical data. Further improvements in performance are\nachieved through fine-tuning, as more location-specific data becomes available.\nThese findings are supported by statistical significance, further validating\nour approach. SPIRIT represents a pivotal step towards rapid, scalable, and\nadaptable solar forecasting solutions, advancing the integration of renewable\nenergy into global power systems.\n","authors":["Aditya Mishra","Ravindra T","Srinivasan Iyengar","Shivkumar Kalyanaraman","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2502.10307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10294v1","updated":"2025-02-14T16:56:24Z","published":"2025-02-14T16:56:24Z","title":"QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for\n  Scribble-Supervised Segmentation of Medical Images","summary":"  The deployment of advanced deep learning models for medical image\nsegmentation is often constrained by the requirement for extensively annotated\ndatasets. Weakly-supervised learning, which allows less precise labels, has\nbecome a promising solution to this challenge. Building on this approach, we\npropose QMaxViT-Unet+, a novel framework for scribble-supervised medical image\nsegmentation. This framework is built on the U-Net architecture, with the\nencoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks.\nThese blocks enhance the model's ability to learn local and global features\nefficiently. Additionally, our approach integrates a query-based Transformer\ndecoder to refine features and an edge enhancement module to compensate for the\nlimited boundary information in the scribble label. We evaluate the proposed\nQMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal\npolyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation\nmetrics include the Dice similarity coefficient (DSC) and the 95th percentile\nof Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+\nachieves 89.1\\% DSC and 1.316mm HD95 on ACDC, 88.4\\% DSC and 2.226mm HD95 on\nMS-CMRSeg, 71.4\\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\\% DSC and 50.122mm\nHD95 on BUSI. These results demonstrate that our method outperforms existing\napproaches in terms of accuracy, robustness, and efficiency while remaining\ncompetitive with fully-supervised learning approaches. This makes it ideal for\nmedical image analysis, where high-quality annotations are often scarce and\nrequire significant effort and expense. The code is available at:\nhttps://github.com/anpc849/QMaxViT-Unet\n","authors":["Thien B. Nguyen-Tat","Hoang-An Vo","Phuoc-Sang Dang"],"pdf_url":"https://arxiv.org/pdf/2502.10294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10277v1","updated":"2025-02-14T16:34:21Z","published":"2025-02-14T16:34:21Z","title":"Artificial Intelligence to Assess Dental Findings from Panoramic\n  Radiographs -- A Multinational Study","summary":"  Dental panoramic radiographs (DPRs) are widely used in clinical practice for\ncomprehensive oral assessment but present challenges due to overlapping\nstructures and time constraints in interpretation.\n  This study aimed to establish a solid baseline for the AI-automated\nassessment of findings in DPRs by developing, evaluating an AI system, and\ncomparing its performance with that of human readers across multinational data\nsets.\n  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and\nTaiwan), focusing on 8 types of dental findings. The AI system combined object\ndetection and semantic segmentation techniques for per-tooth finding\nidentification. Performance metrics included sensitivity, specificity, and area\nunder the receiver operating characteristic curve (AUC-ROC). AI\ngeneralizability was tested across data sets, and performance was compared with\nhuman dental practitioners.\n  The AI system demonstrated comparable or superior performance to human\nreaders, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for\nidentifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008)\nsensitivity for identifying missing teeth. The AI achieved a macro-averaged\nAUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with\nthe reference were comparable to inter-human agreements in 7 of 8 findings\nexcept for caries (p = .024). The AI system demonstrated robust generalization\nacross diverse imaging and demographic settings and processed images 79 times\nfaster (95% CI: 75-82) than human readers.\n  The AI system effectively assessed findings in DPRs, achieving performance on\npar with or better than human experts while significantly reducing\ninterpretation time. These results highlight the potential for integrating AI\ninto clinical workflows to improve diagnostic efficiency and accuracy, and\npatient management.\n","authors":["Yin-Chih Chelsea Wang","Tsao-Lun Chen","Shankeeth Vinayahalingam","Tai-Hsien Wu","Chu Wei Chang","Hsuan Hao Chang","Hung-Jen Wei","Mu-Hsiung Chen","Ching-Chang Ko","David Anssari Moin","Bram van Ginneken","Tong Xi","Hsiao-Cheng Tsai","Min-Huey Chen","Tzu-Ming Harry Hsu","Hye Chou"],"pdf_url":"https://arxiv.org/pdf/2502.10277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10273v1","updated":"2025-02-14T16:31:43Z","published":"2025-02-14T16:31:43Z","title":"Probing Perceptual Constancy in Large Vision Language Models","summary":"  Perceptual constancy is the ability to maintain stable perceptions of objects\ndespite changes in sensory input, such as variations in distance, angle, or\nlighting. This ability is crucial for recognizing visual information in a\ndynamic world, making it essential for Vision-Language Models (VLMs). However,\nwhether VLMs are currently and theoretically capable of mastering this ability\nremains underexplored. In this study, we evaluated 33 VLMs using 253\nexperiments across three domains: color, size, and shape constancy. The\nexperiments included single-image and video adaptations of classic cognitive\ntasks, along with novel tasks in in-the-wild conditions, to evaluate the\nmodels' recognition of object properties under varying conditions. We found\nsignificant variability in VLM performance, with models performance in shape\nconstancy clearly dissociated from that of color and size constancy.\n","authors":["Haoran Sun","Suyang Yu","Yijiang Li","Qingying Gao","Haiyun Lyu","Hokin Deng","Dezhi Luo"],"pdf_url":"https://arxiv.org/pdf/2502.10273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10259v1","updated":"2025-02-14T16:12:14Z","published":"2025-02-14T16:12:14Z","title":"MITO: Enabling Non-Line-of-Sight Perception using Millimeter-waves\n  through Real-World Datasets and Simulation Tools","summary":"  We present MITO, the first dataset of multi-spectral millimeter-wave (mmWave)\nimages of everyday objects. Unlike visible light, mmWave signals can image\nthrough everyday occlusions (e.g., cardboard boxes, fabric, plastic). However,\ndue to the dearth of publicly-available mmWave images and the interdisciplinary\nchallenges in collecting and processing mmWave signals, it remains difficult\ntoday for computer vision researchers to develop mmWave-based non-line-of-sight\nperception algorithms and models.\n  To overcome these challenges, we introduce a real-world dataset and\nopen-source simulation tool for mmWave imaging. The dataset is acquired using a\nUR5 robotic arm with two mmWave radars operating at different frequencies and\nan RGB-D camera. Through a signal processing pipeline, we capture and create\nover 580 real-world 3D mmWave images from over 76 different objects in the YCB\ndataset, a standard dataset for robotics manipulation. We provide real-world\nmmWave images in line-of-sight and non-line-of-sight, as well as RGB-D images\nand ground truth segmentation masks. We also develop an open-source simulation\ntool that can be used to generate synthetic mmWave images for any 3D triangle\nmesh, which achieves a median F-Score of 94% when compared to real-world mmWave\nimages.\n  We show the usefulness of this dataset and simulation tool in multiple CV\ntasks in non-line-of-sight. First, we perform object segmentation for mmWave\nimages using the segment anything model (SAM), and achieve a median precision\nand recall of 92.6% and 64%. Second, we train a classifier that can recognize\nobjects in non-line-of-sight. It is trained on synthetic images and can\nclassify real-world images with 85% accuracy.\n  We believe MITO will be a valuable resource for computer vision researchers\nin developing non-line-of-sight perception, similar to how early camera-based\ndatasets shaped the field.\n","authors":["Laura Dodds","Tara Boroushaki","Fadel Adib"],"pdf_url":"https://arxiv.org/pdf/2502.10259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10258v1","updated":"2025-02-14T16:11:57Z","published":"2025-02-14T16:11:57Z","title":"PromptArtisan: Multi-instruction Image Editing in Single Pass with\n  Complete Attention Control","summary":"  We present PromptArtisan, a groundbreaking approach to multi-instruction\nimage editing that achieves remarkable results in a single pass, eliminating\nthe need for time-consuming iterative refinement. Our method empowers users to\nprovide multiple editing instructions, each associated with a specific mask\nwithin the image. This flexibility allows for complex edits involving mask\nintersections or overlaps, enabling the realization of intricate and nuanced\nimage transformations. PromptArtisan leverages a pre-trained InstructPix2Pix\nmodel in conjunction with a novel Complete Attention Control Mechanism (CACM).\nThis mechanism ensures precise adherence to user instructions, granting\nfine-grained control over the editing process. Furthermore, our approach is\nzero-shot, requiring no additional training, and boasts improved processing\ncomplexity compared to traditional iterative methods. By seamlessly integrating\nmulti-instruction capabilities, single-pass efficiency, and complete attention\ncontrol, PromptArtisan unlocks new possibilities for creative and efficient\nimage editing workflows, catering to both novice and expert users alike.\n","authors":["Kunal Swami","Raghu Chittersu","Pranav Adlinge","Rajeev Irny","Shashavali Doodekula","Alok Shukla"],"pdf_url":"https://arxiv.org/pdf/2502.10258v1.pdf","comment":"Accepted in ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.10250v1","updated":"2025-02-14T15:59:33Z","published":"2025-02-14T15:59:33Z","title":"VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models","summary":"  Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.\n","authors":["Gokul Karthik Kumar","Iheb Chaabane","Kebin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.10250v1.pdf","comment":"Accepted at PAKDD 2025"},{"id":"http://arxiv.org/abs/2409.04796v2","updated":"2025-02-14T15:58:39Z","published":"2024-09-07T11:24:52Z","title":"Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution\n  Detection","summary":"  Out-of-Distribution (OOD) detection, aiming to distinguish outliers from\nknown categories, has gained prominence in practical scenarios. Recently, the\nadvent of vision-language models (VLM) has heightened interest in enhancing OOD\ndetection for VLM through few-shot tuning. However, existing methods mainly\nfocus on optimizing global prompts, ignoring refined utilization of local\ninformation with regard to outliers. Motivated by this, we freeze global\nprompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to\nemphasize regional enhancement with local prompts. Our method comprises two\nintegral components: global prompt guided negative augmentation and local\nprompt enhanced regional regularization. The former utilizes frozen, coarse\nglobal prompts as guiding cues to incorporate negative augmentation, thereby\nleveraging local outlier knowledge. The latter employs trainable local prompts\nand a regional regularization to capture local information effectively, aiding\nin outlier identification. We also propose regional-related metric to empower\nthe enrichment of OOD detection. Moreover, since our approach explores\nenhancing local prompts only, it can be seamlessly integrated with trained\nglobal prompts during inference to boost the performance. Comprehensive\nexperiments demonstrate the effectiveness and potential of our method. Notably,\nour method reduces average FPR95 by 5.17% against state-of-the-art method in\n4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot\nresults of previous methods. Code is released at\nhttps://github.com/AuroraZengfh/Local-Prompt.\n","authors":["Fanhu Zeng","Zhen Cheng","Fei Zhu","Hongxin Wei","Xu-Yao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04796v2.pdf","comment":"Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025). Code is available at\n  https://github.com/AuroraZengfh/Local-Prompt"},{"id":"http://arxiv.org/abs/2502.10248v1","updated":"2025-02-14T15:58:10Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v1.pdf","comment":"35 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.10214v1","updated":"2025-02-14T15:08:37Z","published":"2025-02-14T15:08:37Z","title":"Mapping bathymetry of inland water bodies on the North Slope of Alaska\n  with Landsat using Random Forest","summary":"  The North Slope of Alaska is dominated by small waterbodies that provide\ncritical ecosystem services for local population and wildlife. Detailed\ninformation on the depth of the waterbodies is scarce due to the challenges\nwith collecting such information. In this work we have trained a machine\nlearning (Random Forest Regressor) model to predict depth from multispectral\nLandsat data in waterbodies across the North Slope of Alaska. The greatest\nchallenge is the scarcity of in situ data, which is expensive and difficult to\nobtain, to train the model. We overcame this challenge by using modeled depth\npredictions from a prior study as synthetic training data to provide a more\ndiverse training data pool for the Random Forest. The final Random Forest model\nwas more robust than models trained directly on the in situ data and when\napplied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall\n$r^{2}$ value of 0.76 on validation. The final map has been made available\nthrough the Oak Ridge National Laboratory Distribute Active Archive Center\n(ORNL-DAAC). This map represents a first of its kind regional assessment of\nwaterbody depth with per pixel estimates of depth for the entire North Slope of\nAlaska.\n","authors":["Mark L. Carroll","Margaret R. Wooten","Claire E. Simpson","Caleb S. Spradlin","Melanie J. Frost","Mariana Blanco-Rojas","Zachary W. Williams","Jordan A. Caraballo-Vega","Christopher S. R. Neigh"],"pdf_url":"https://arxiv.org/pdf/2502.10214v1.pdf","comment":"24 Pages, 6 Figures, 1 Table. This article is a US Government work.\n  Landsat data from the US Geological Survey Earth Explorer system:\n  https://earthexplorer.usgs.gov. Sonar training measurements:\n  https://doi.org/10.18739/A2JD4PP1H. Output maps from the Oak Ridge National\n  Laboratory Distribute Active Archive Center (ORNL-DAAC):\n  https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=2243"},{"id":"http://arxiv.org/abs/2501.14679v4","updated":"2025-02-14T14:55:40Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04634v2","updated":"2025-02-14T14:52:51Z","published":"2024-10-06T21:42:53Z","title":"Is What You Ask For What You Get? Investigating Concept Associations in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models are increasingly used in impactful real-life\napplications. As such, there is a growing need to audit these models to ensure\nthat they generate desirable, task-appropriate images. However, systematically\ninspecting the associations between prompts and generated content in a\nhuman-understandable way remains challenging. To address this, we propose\nConcept2Concept, a framework where we characterize conditional distributions of\nvision language models using interpretable concepts and metrics that can be\ndefined in terms of these concepts. This characterization allows us to use our\nframework to audit models and prompt-datasets. To demonstrate, we investigate\nseveral case studies of conditional distributions of prompts, such as\nuser-defined distributions or empirical, real-world distributions. Lastly, we\nimplement Concept2Concept as an open-source interactive visualization tool to\nfacilitate use by non-technical end-users. A demo is available at\nhttps://tinyurl.com/Concept2ConceptDemo.\n","authors":["Salma Abdel Magid","Weiwei Pan","Simon Warchol","Grace Guo","Junsik Kim","Mahia Rahman","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22099v4","updated":"2025-02-14T14:46:03Z","published":"2024-10-29T14:53:10Z","title":"TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds","summary":"  Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet.\n","authors":["Yui Lo","Yuqian Chen","Dongnan Liu","Jon Haitz Legarreta","Leo Zekelman","Fan Zhang","Jarrett Rushmore","Yogesh Rathi","Nikos Makris","Alexandra J. Golby","Weidong Cai","Lauren J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2410.22099v4.pdf","comment":"10 pages, 2 figures, 4 tables. This work has been accepted to 2025\n  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2502.10195v1","updated":"2025-02-14T14:39:24Z","published":"2025-02-14T14:39:24Z","title":"Exploring the Camera Bias of Person Re-identification","summary":"  We empirically investigate the camera bias of person re-identification (ReID)\nmodels. Previously, camera-aware methods have been proposed to address this\nissue, but they are largely confined to training domains of the models. We\nmeasure the camera bias of ReID models on unseen domains and reveal that camera\nbias becomes more pronounced under data distribution shifts. As a debiasing\nmethod for unseen domain data, we revisit feature normalization on embedding\nvectors. While the normalization has been used as a straightforward solution,\nits underlying causes and broader applicability remain unexplored. We analyze\nwhy this simple method is effective at reducing bias and show that it can be\napplied to detailed bias factors such as low-level image properties and body\nangle. Furthermore, we validate its generalizability across various models and\nbenchmarks, highlighting its potential as a simple yet effective test-time\npostprocessing method for ReID. In addition, we explore the inherent risk of\ncamera bias in unsupervised learning of ReID models. The unsupervised models\nremain highly biased towards camera labels even for seen domain data,\nindicating substantial room for improvement. Based on observations of the\nnegative impact of camera-biased pseudo labels on training, we suggest simple\ntraining strategies to mitigate the bias. By applying these strategies to\nexisting unsupervised learning algorithms, we show that significant performance\nimprovements can be achieved with minor modifications.\n","authors":["Myungseo Song","Jin-Woo Park","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2502.10195v1.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2502.05606v2","updated":"2025-02-14T14:17:12Z","published":"2025-02-08T15:25:03Z","title":"FreeBlend: Advancing Concept Blending with Staged Feedback-Driven\n  Interpolation Diffusion","summary":"  Concept blending is a promising yet underexplored area in generative models.\nWhile recent approaches, such as embedding mixing and latent modification based\non structural sketches, have been proposed, they often suffer from incompatible\nsemantic information and discrepancies in shape and appearance. In this work,\nwe introduce FreeBlend, an effective, training-free framework designed to\naddress these challenges. To mitigate cross-modal loss and enhance feature\ndetail, we leverage transferred image embeddings as conditional inputs. The\nframework employs a stepwise increasing interpolation strategy between latents,\nprogressively adjusting the blending ratio to seamlessly integrate auxiliary\nfeatures. Additionally, we introduce a feedback-driven mechanism that updates\nthe auxiliary latents in reverse order, facilitating global blending and\npreventing rigid or unnatural outputs. Extensive experiments demonstrate that\nour method significantly improves both the semantic coherence and visual\nquality of blended images, yielding compelling and coherent results.\n","authors":["Yufan Zhou","Haoyu Shen","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05606v2.pdf","comment":"Webpage: https://petershen-csworld.github.io/FreeBlend. -- updated"},{"id":"http://arxiv.org/abs/2502.10162v1","updated":"2025-02-14T13:46:14Z","published":"2025-02-14T13:46:14Z","title":"Revisiting Generalization Power of a DNN in Terms of Symbolic\n  Interactions","summary":"  This paper aims to analyze the generalization power of deep neural networks\n(DNNs) from the perspective of interactions. Unlike previous analysis of a\nDNN's generalization power in a highdimensional feature space, we find that the\ngeneralization power of a DNN can be explained as the generalization power of\nthe interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a\nspindle-shaped distribution. Furthermore, our theory can effectively\ndisentangle these two types of interactions from a DNN. We have verified that\nour theory can well match real interactions in a DNN in experiments.\n","authors":["Lei Cheng","Junpeng Zhang","Qihan Ren","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10162v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.19198"},{"id":"http://arxiv.org/abs/2502.10156v1","updated":"2025-02-14T13:36:00Z","published":"2025-02-14T13:36:00Z","title":"MonoForce: Learnable Image-conditioned Physics Engine","summary":"  We propose a novel model for the prediction of robot trajectories on rough\noffroad terrain from the onboard camera images. This model enforces the laws of\nclassical mechanics through a physics-aware neural symbolic layer while\npreserving the ability to learn from large-scale data as it is end-to-end\ndifferentiable. The proposed hybrid model integrates a black-box component that\npredicts robot-terrain interaction forces with a neural-symbolic layer. This\nlayer includes a differentiable physics engine that computes the robot's\ntrajectory by querying these forces at the points of contact with the terrain.\nAs the proposed architecture comprises substantial geometrical and physics\npriors, the resulting model can also be seen as a learnable physics engine\nconditioned on real images that delivers $10^4$ trajectories per second. We\nargue and empirically demonstrate that this architecture reduces the\nsim-to-real gap and mitigates out-of-distribution sensitivity. The\ndifferentiability, in conjunction with the rapid simulation speed, makes the\nmodel well-suited for various applications including model predictive control,\ntrajectory shooting, supervised and reinforcement learning or SLAM. The codes\nand data are publicly available.\n","authors":["Ruslan Agishev","Karel Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2502.10156v1.pdf","comment":"Submitted to IEEE Transactions on Robotics (T-RO), 2025. Code:\n  https://github.com/ctu-vras/monoforce"},{"id":"http://arxiv.org/abs/2412.10853v2","updated":"2025-02-14T13:33:14Z","published":"2024-12-14T14:54:44Z","title":"SEW: Self-calibration Enhanced Whole Slide Pathology Image Analysis","summary":"  Pathology images are considered the ``gold standard\" for cancer diagnosis and\ntreatment, with gigapixel images providing extensive tissue and cellular\ninformation. Existing methods fail to simultaneously extract global structural\nand local detail features for comprehensive pathology image analysis\nefficiently. To address these limitations, we propose a self-calibration\nenhanced framework for whole slide pathology image analysis, comprising three\ncomponents: a global branch, a focus predictor, and a detailed branch. The\nglobal branch initially classifies using the pathological thumbnail, while the\nfocus predictor identifies relevant regions for classification based on the\nlast layer features of the global branch. The detailed extraction branch then\nassesses whether the magnified regions correspond to the lesion area. Finally,\na feature consistency constraint between the global and detail branches ensures\nthat the global branch focuses on the appropriate region and extracts\nsufficient discriminative features for final identification. These focused\ndiscriminative features prove invaluable for uncovering novel prognostic tumor\nmarkers from the perspective of feature cluster uniqueness and tissue spatial\ndistribution. Extensive experiment results demonstrate that the proposed\nframework can rapidly deliver accurate and explainable results for pathological\ngrading and prognosis tasks.\n","authors":["Haoming Luo","Xiaotian Yu","Shengxuming Zhang","Jiabin Xia","Yang Jian","Yuning Sun","Liang Xue","Mingli Song","Jing Zhang","Xiuming Zhang","Zunlei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10145v1","updated":"2025-02-14T13:15:21Z","published":"2025-02-14T13:15:21Z","title":"Interpretable Concept-based Deep Learning Framework for Multimodal Human\n  Behavior Modeling","summary":"  In the contemporary era of intelligent connectivity, Affective Computing\n(AC), which enables systems to recognize, interpret, and respond to human\nbehavior states, has become an integrated part of many AI systems. As one of\nthe most critical components of responsible AI and trustworthiness in all\nhuman-centered systems, explainability has been a major concern in AC.\nParticularly, the recently released EU General Data Protection Regulation\nrequires any high-risk AI systems to be sufficiently interpretable, including\nbiometric-based systems and emotion recognition systems widely used in the\naffective computing field. Existing explainable methods often compromise\nbetween interpretability and performance. Most of them focus only on\nhighlighting key network parameters without offering meaningful,\ndomain-specific explanations to the stakeholders. Additionally, they also face\nchallenges in effectively co-learning and explaining insights from multimodal\ndata sources. To address these limitations, we propose a novel and\ngeneralizable framework, namely the Attention-Guided Concept Model (AGCM),\nwhich provides learnable conceptual explanations by identifying what concepts\nthat lead to the predictions and where they are observed. AGCM is extendable to\nany spatial and temporal signals through multimodal concept alignment and\nco-learning, empowering stakeholders with deeper insights into the model's\ndecision-making process. We validate the efficiency of AGCM on well-established\nFacial Expression Recognition benchmark datasets while also demonstrating its\ngeneralizability on more complex real-world human behavior understanding\napplications.\n","authors":["Xinyu Li","Marwa Mahmoud"],"pdf_url":"https://arxiv.org/pdf/2502.10145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10127v1","updated":"2025-02-14T12:56:10Z","published":"2025-02-14T12:56:10Z","title":"Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph\n  Generation","summary":"  High-Definition (HD) maps play a crucial role in autonomous vehicle\nnavigation, complementing onboard perception sensors for improved accuracy and\nsafety. Traditional HD map generation relies on dedicated mapping vehicles,\nwhich are costly and fail to capture real-time infrastructure changes. This\npaper presents HDMapLaneNet, a novel framework leveraging V2X communication and\nScene Graph Generation to collaboratively construct a localized geometric layer\nof HD maps. The approach extracts lane centerlines from front-facing camera\nimages, represents them as graphs, and transmits the data for global\naggregation to the cloud via V2X. Preliminary results on the nuScenes dataset\ndemonstrate superior association prediction performance compared to a\nstate-of-the-art method.\n","authors":["Gamal Elghazaly","Raphael Frank"],"pdf_url":"https://arxiv.org/pdf/2502.10127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10120v1","updated":"2025-02-14T12:40:37Z","published":"2025-02-14T12:40:37Z","title":"Compress image to patches for Vision Transformer","summary":"  The Vision Transformer (ViT) has made significant strides in the field of\ncomputer vision. However, as the depth of the model and the resolution of the\ninput images increase, the computational cost associated with training and\nrunning ViT models has surged dramatically.This paper proposes a hybrid model\nbased on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a\nmodule called CI2P, which utilizes the CompressAI encoder to compress images\nand subsequently generates a sequence of patches through a series of\nconvolutions. CI2P can replace the Patch Embedding component in the ViT model,\nenabling seamless integration into existing ViT models.Compared to ViT-B/16,\nCI2P-ViT has the number of patches input to the self-attention layer reduced to\na quarter of the original.This design not only significantly reduces the\ncomputational cost of the ViT model but also effectively enhances the model's\naccuracy by introducing the inductive bias properties of CNN.The ViT model's\nprecision is markedly enhanced.When trained from the ground up on the\nAnimals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing\na 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's\ncomputational operations, measured in floating-point operations per second\n(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in\ntraining velocity on identical hardware configurations.\n","authors":["Xinfeng Zhao","Yaoru Sun"],"pdf_url":"https://arxiv.org/pdf/2502.10120v1.pdf","comment":"15 pages,5 figures"},{"id":"http://arxiv.org/abs/2502.10118v1","updated":"2025-02-14T12:33:19Z","published":"2025-02-14T12:33:19Z","title":"Image Embedding Sampling Method for Diverse Captioning","summary":"  Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, smaller VLMs\nprioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.\n","authors":["Sania Waheed","Na Min An"],"pdf_url":"https://arxiv.org/pdf/2502.10118v1.pdf","comment":"15 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.14695v2","updated":"2025-02-14T12:08:20Z","published":"2024-11-22T03:05:06Z","title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","summary":"  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n","authors":["Hao Chen","Francois Bremond","Nicu Sebe","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14695v2.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2502.02867v2","updated":"2025-02-14T11:57:25Z","published":"2025-02-05T03:52:36Z","title":"Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation\n  Learning with Visual Observations","summary":"  Imitation learning (IL) enables agents to mimic expert behavior without\nreward signals but faces challenges in cross-domain scenarios with\nhigh-dimensional, noisy, and incomplete visual observations. To address this,\nwe propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning\n(DIFF-IL), a novel IL method that extracts domain-invariant features from\nindividual frames and adapts them into sequences to isolate and replicate\nexpert behaviors. We also introduce a frame-wise time labeling technique to\nsegment expert behaviors by timesteps and assign rewards aligned with temporal\ncontexts, enhancing task performance. Experiments across diverse visual\nenvironments demonstrate the effectiveness of DIFF-IL in addressing complex\nvisual tasks.\n","authors":["Minung Kim","Kawon Lee","Jungmo Kim","Sungho Choi","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02867v2.pdf","comment":"8 pages main, 19 pages appendix with reference. Submitted to ICML\n  2025"},{"id":"http://arxiv.org/abs/2405.11345v3","updated":"2025-02-14T11:55:30Z","published":"2024-05-18T17:28:35Z","title":"City-Scale Multi-Camera Vehicle Tracking System with Improved\n  Self-Supervised Camera Link Model","summary":"  Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms\nthe basis for numerous future city-wide systems (e.g. traffic management, crash\ndetection, etc.). However, the challenge of matching vehicle trajectories\nacross different cameras based solely on feature extraction poses significant\ndifficulties. This article introduces an innovative multi-camera vehicle\ntracking system that utilizes a self-supervised camera link model. In contrast\nto related works that rely on manual spatial-temporal annotations, our model\nautomatically extracts crucial multi-camera relationships for vehicle matching.\nThe camera link is established through a pre-matching process that evaluates\nfeature similarities, pair numbers, and time variance for high-quality tracks.\nThis process calculates the probability of spatial linkage for all camera\ncombinations, selecting the highest scoring pairs to create camera links. Our\napproach significantly improves deployment times by eliminating the need for\nhuman annotation, offering substantial improvements in efficiency and\ncost-effectiveness when it comes to real-world application. This pairing\nprocess supports cross camera matching by setting spatial-temporal constraints,\nreducing the searching space for potential vehicle matches. According to our\nexperimental results, the proposed method achieves a new state-of-the-art among\nautomatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1\nScore.\n","authors":["Yuqiang Lin","Sam Lockyer","Nic Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.11345v3.pdf","comment":"Upload the revised manuscript with the publisher's requirement"},{"id":"http://arxiv.org/abs/2408.10919v4","updated":"2025-02-14T11:12:02Z","published":"2024-08-20T15:04:14Z","title":"CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network","summary":"  In recent years, Wi-Fi sensing has garnered significant attention due to its\nnumerous benefits, such as privacy protection, low cost, and penetration\nability. Extensive research has been conducted in this field, focusing on areas\nsuch as gesture recognition, people identification, and fall detection.\nHowever, many data-driven methods encounter challenges related to domain shift,\nwhere the model fails to perform well in environments different from the\ntraining data. One major factor contributing to this issue is the limited\navailability of Wi-Fi sensing datasets, which makes models learn excessive\nirrelevant information and over-fit to the training set. Unfortunately,\ncollecting large-scale Wi-Fi sensing datasets across diverse scenarios is a\nchallenging task. To address this problem, we propose CrossFi, a siamese\nnetwork-based approach that excels in both in-domain scenario and cross-domain\nscenario, including few-shot, zero-shot scenarios, and even works in few-shot\nnew-class scenario where testing set contains new categories. The core\ncomponent of CrossFi is a sample-similarity calculation network called CSi-Net,\nwhich improves the structure of the siamese network by using an attention\nmechanism to capture similarity information, instead of simply calculating the\ndistance or cosine similarity. Based on it, we develop an extra Weight-Net that\ncan generate a template for each class, so that our CrossFi can work in\ndifferent scenarios. Experimental results demonstrate that our CrossFi achieves\nstate-of-the-art performance across various scenarios. In gesture recognition\ntask, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%\nin one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,\nand 84.75% in one-shot new-class scenario. The code for our model is publicly\navailable at https://github.com/RS2002/CrossFi.\n","authors":["Zijian Zhao","Tingwei Chen","Zhijie Cai","Xiaoyang Li","Hang Li","Qimei Chen","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.10919v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10360v5","updated":"2025-02-14T10:53:27Z","published":"2024-08-19T18:56:24Z","title":"HaSPeR: An Image Repository for Hand Shadow Puppet Recognition","summary":"  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of\ntheatrical art and storytelling where hand shadows are projected onto flat\nsurfaces to create illusions of living creatures. The skilled performers create\nthese silhouettes by hand positioning, finger movements, and dexterous gestures\nto resemble shadows of animals and objects. Due to the lack of practitioners\nand a seismic shift in people's entertainment standards, this art form is on\nthe verge of extinction. To facilitate its preservation and proliferate it to a\nwider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset\nconsisting of 15,000 images of hand shadow puppets across 15 classes extracted\nfrom both professional and amateur hand shadow puppeteer clips. We provide a\ndetailed statistical analysis of the dataset and employ a range of pretrained\nimage classification models to establish baselines. Our findings show a\nsubstantial performance superiority of skip-connected convolutional models over\nattention-based transformer architectures. We also find that lightweight\nmodels, such as MobileNetV2, suited for mobile applications and embedded\ndevices, perform comparatively well. We surmise that such low-latency\narchitectures can be useful in developing ombromanie teaching tools, and we\ncreate a prototype application to explore this surmission. Keeping the\nbest-performing model ResNet34 under the limelight, we conduct comprehensive\nfeature-spatial, explainability, and error analyses to gain insights into its\ndecision-making process. To the best of our knowledge, this is the first\ndocumented dataset and research endeavor to preserve this dying art for future\ngenerations, with computer vision approaches. Our code and data will be\npublicly available.\n","authors":["Syed Rifat Raiyan","Zibran Zarif Amio","Sabbir Ahmed"],"pdf_url":"https://arxiv.org/pdf/2408.10360v5.pdf","comment":"Submitted to Machine Vision and Applications, 13 pages, 105 figures,\n  2 tables"},{"id":"http://arxiv.org/abs/2502.10064v1","updated":"2025-02-14T10:41:42Z","published":"2025-02-14T10:41:42Z","title":"Hands-off Image Editing: Language-guided Editing without any\n  Task-specific Labeling, Masking or even Training","summary":"  Instruction-guided image editing consists in taking an image and an\ninstruction and deliverring that image altered according to that instruction.\nState-of-the-art approaches to this task suffer from the typical scaling up and\ndomain adaptation hindrances related to supervision as they eventually resort\nto some kind of task-specific labelling, masking or training. We propose a\nnovel approach that does without any such task-specific supervision and offers\nthus a better potential for improvement. Its assessment demonstrates that it is\nhighly effective, achieving very competitive performance.\n","authors":["Rodrigo Santos","Ant√≥nio Branco","Jo√£o Silva","Jo√£o Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2502.10064v1.pdf","comment":"Published in COLING 2025"},{"id":"http://arxiv.org/abs/2502.10060v1","updated":"2025-02-14T10:26:14Z","published":"2025-02-14T10:26:14Z","title":"DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery","summary":"  Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation.\n","authors":["Utkarsh Mall","Cheng Perng Phoo","Mia Chiquier","Bharath Hariharan","Kavita Bala","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2502.10060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10059v1","updated":"2025-02-14T10:21:49Z","published":"2025-02-14T10:21:49Z","title":"RealCam-I2V: Real-World Image-to-Video Generation with Interactive\n  Complex Camera Control","summary":"  Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to absolute values, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain\ndynamic, coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nWe will release our absolute-scale annotation, codes, and all checkpoints.\nPlease see dynamic results in https://zgctroy.github.io/RealCam-I2V.\n","authors":["Teng Li","Guangcong Zheng","Rui Jiang"," Shuigenzhan","Tao Wu","Yehao Lu","Yining Lin","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2502.10059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17555v2","updated":"2025-02-14T10:12:02Z","published":"2023-06-30T11:15:26Z","title":"Why does my medical AI look at pictures of birds? Exploring the efficacy\n  of transfer learning across domain boundaries","summary":"  It is an open secret that ImageNet is treated as the panacea of pretraining.\nParticularly in medical machine learning, models not trained from scratch are\noften finetuned based on ImageNet-pretrained models. We posit that pretraining\non data from the domain of the downstream task should almost always be\npreferred instead. We leverage RadNet-12M, a dataset containing more than 12\nmillion computed tomography (CT) image slices, to explore the efficacy of\nself-supervised pretraining on medical and natural images. Our experiments\ncover intra- and cross-domain transfer scenarios, varying data scales,\nfinetuning vs. linear evaluation, and feature space analysis. We observe that\nintra-domain transfer compares favorably to cross-domain transfer, achieving\ncomparable or improved performance (0.44% - 2.07% performance increase using\nRadNet pretraining, depending on the experiment) and demonstrate the existence\nof a domain boundary-related generalization gap and domain-specific learned\nfeatures.\n","authors":["Frederic Jonske","Moon Kim","Enrico Nasca","Janis Evers","Johannes Haubold","Ren√© Hosch","Felix Nensa","Michael Kamp","Constantin Seibold","Jan Egger","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2306.17555v2.pdf","comment":"Code available from\n  https://github.com/TIO-IKIM/Transfer-learning-across-domain-boundaries/ -\n  Paper, code, and contents are subject to the CC-BY-NC 4.0 license"},{"id":"http://arxiv.org/abs/2502.10054v1","updated":"2025-02-14T10:02:38Z","published":"2025-02-14T10:02:38Z","title":"Towards Polyp Counting In Full-Procedure Colonoscopy Videos","summary":"  Automated colonoscopy reporting holds great potential for enhancing quality\ncontrol and improving cost-effectiveness of colonoscopy procedures. A major\nchallenge lies in the automated identification, tracking, and re-association\n(ReID) of polyps tracklets across full-procedure colonoscopy videos. This is\nessential for precise polyp counting and enables automated computation of key\nquality metrics, such as Adenoma Detection Rate (ADR) and Polyps Per\nColonoscopy (PPC). However, polyp ReID is challenging due to variations in\npolyp appearance, frequent disappearance from the field of view, and\nocclusions. In this work, we leverage the REAL-Colon dataset, the first\nopen-access dataset providing full-procedure videos, to define tasks, data\nsplits and metrics for the problem of automatically count polyps in\nfull-procedure videos, establishing an open-access framework. We re-implement\npreviously proposed SimCLR-based methods for learning representations of polyp\ntracklets, both single-frame and multi-view, and adapt them to the polyp\ncounting task. We then propose an Affinity Propagation-based clustering method\nto further improve ReID based on these learned representations, ultimately\nenhancing polyp counting. Our approach achieves state-of-the-art performance,\nwith a polyp fragmentation rate of 6.30 and a false positive rate (FPR) below\n5% on the REAL-Colon dataset. We release code at\nhttps://github.com/lparolari/towards-polyp-counting.\n","authors":["Luca Parolari","Andrea Cherubini","Lamberto Ballan","Carlo Biffi"],"pdf_url":"https://arxiv.org/pdf/2502.10054v1.pdf","comment":"Accepted to ISBI 2025"},{"id":"http://arxiv.org/abs/2406.04344v3","updated":"2025-02-14T09:51:46Z","published":"2024-06-06T17:59:56Z","title":"Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models","summary":"  Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.\n","authors":["Tim Z. Xiao","Robert Bamler","Bernhard Sch√∂lkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.04344v3.pdf","comment":"Published in Transactions on Machine Learning Research (116 pages, 32\n  figures, v3: refined the paper structure and added more empirical results)"},{"id":"http://arxiv.org/abs/2502.10046v1","updated":"2025-02-14T09:46:43Z","published":"2025-02-14T09:46:43Z","title":"ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in\n  Arbitrary Virtual Environments","summary":"  Creating lifelike virtual agents capable of interacting with their\nenvironments is a longstanding goal in computer graphics. This paper addresses\nthe challenge of generating natural head rotations, a critical aspect of\nbelievable agent behavior for visual information gathering and dynamic\nresponses to environmental cues. Although earlier methods have made significant\nstrides, many rely on data-driven or saliency-based approaches, which often\nunderperform in diverse settings and fail to capture deeper cognitive factors\nsuch as risk assessment, information seeking, and contextual prioritization.\nConsequently, generated behaviors can appear rigid or overlook critical scene\nelements, thereby diminishing the sense of realism. In this paper, we propose\n\\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head\nMovement \\textbf{C}ontrol framework, which exploits the common-sense knowledge\nand reasoning capabilities of large-scale models, including Vision-Language\nModels (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling\nevery cognitive mechanism, ViRAC leverages the biases and patterns internalized\nby these models from extensive training, thus emulating human-like perceptual\nprocesses without hand-tuned heuristics. Experimental results in multiple\nscenarios reveal that ViRAC produces more natural and context-aware head\nrotations than recent state-of-the-art techniques. Quantitative evaluations\nshow a closer alignment with real human head-movement data, while user studies\nconfirm improved realism and cognitive plausibility.\n","authors":["Juyeong Hwang","Seong-Eun Hong","Hyeongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2502.10046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10028v1","updated":"2025-02-14T09:13:57Z","published":"2025-02-14T09:13:57Z","title":"ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow\n  for Robotic Manipulation","summary":"  Language-conditioned manipulation is a vital but challenging robotic task due\nto the high-level abstraction of language. To address this, researchers have\nsought improved goal representations derived from natural language. In this\npaper, we highlight 3D flow - representing the motion trend of 3D particles\nwithin a scene - as an effective bridge between language-based future image\ngeneration and fine-grained action prediction. To this end, we develop\nManiTrend, a unified framework that models the dynamics of 3D particles, vision\nobservations and manipulation actions with a causal transformer. Within this\nframework, features for 3D flow prediction serve as additional conditions for\nfuture image generation and action prediction, alleviating the complexity of\npixel-wise spatiotemporal modeling and providing seamless action guidance.\nFurthermore, 3D flow can substitute missing or heterogeneous action labels\nduring large-scale pretraining on cross-embodiment demonstrations. Experiments\non two comprehensive benchmarks demonstrate that our method achieves\nstate-of-the-art performance with high efficiency. Our code and model\ncheckpoints will be available upon acceptance.\n","authors":["Yuxin He","Qiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.10028v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.07360v2","updated":"2025-02-14T09:13:09Z","published":"2025-02-11T08:30:25Z","title":"Supervised contrastive learning for cell stage classification of animal\n  embryos","summary":"  Video microscopy, when combined with machine learning, offers a promising\napproach for studying the early development of in vitro produced (IVP) embryos.\nHowever, manually annotating developmental events, and more specifically cell\ndivisions, is time-consuming for a biologist and cannot scale up for practical\napplications. We aim to automatically classify the cell stages of embryos from\n2D time-lapse microscopy videos with a deep learning approach. We focus on the\nanalysis of bovine embryonic development using video microscopy, as we are\nprimarily interested in the application of cattle breeding, and we have created\na Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1)\nlow-quality images and bovine dark cells that make the identification of cell\nstages difficult, (2) class ambiguity at the boundaries of developmental\nstages, and (3) imbalanced data distribution. To address these challenges, we\nintroduce CLEmbryo, a novel method that leverages supervised contrastive\nlearning combined with focal loss for training, and the lightweight 3D neural\nnetwork CSN-50 as an encoder. We also show that our method generalizes well.\nCLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset\nand the publicly available NYU Mouse Embryos dataset.\n","authors":["Yasmine Hachani","Patrick Bouthemy","Elisa Fromont","Sylvie Ruffini","Ludivine Laffont","Alline de Paula Reis"],"pdf_url":"https://arxiv.org/pdf/2502.07360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01163v2","updated":"2025-02-14T08:38:03Z","published":"2024-07-01T10:33:44Z","title":"Benchmarking Predictive Coding Networks -- Made Simple","summary":"  In this work, we tackle the problems of efficiency and scalability for\npredictive coding networks (PCNs) in machine learning. To do so, we propose a\nlibrary, called PCX, that focuses on performance and simplicity, and use it to\nimplement a large set of standard benchmarks for the community to use for their\nexperiments. As most works in the field propose their own tasks and\narchitectures, do not compare one against each other, and focus on small-scale\ntasks, a simple and fast open-source library and a comprehensive set of\nbenchmarks would address all these concerns. Then, we perform extensive tests\non such benchmarks using both existing algorithms for PCNs, as well as\nadaptations of other methods popular in the bio-plausible deep learning\ncommunity. All this has allowed us to (i) test architectures much larger than\ncommonly used in the literature, on more complex datasets; (ii)~reach new\nstate-of-the-art results in all of the tasks and datasets provided;\n(iii)~clearly highlight what the current limitations of PCNs are, allowing us\nto state important future research directions. With the hope of galvanizing\ncommunity efforts towards one of the main open problems in the field,\nscalability, we release code, tests, and benchmarks. Link to the library:\nhttps://github.com/liukidar/pcx\n","authors":["Luca Pinchetti","Chang Qi","Oleh Lokshyn","Gaspard Olivers","Cornelius Emde","Mufeng Tang","Amine M'Charrak","Simon Frieder","Bayar Menzat","Rafal Bogacz","Thomas Lukasiewicz","Tommaso Salvatori"],"pdf_url":"https://arxiv.org/pdf/2407.01163v2.pdf","comment":"34 pages, 26 figures"},{"id":"http://arxiv.org/abs/2502.09993v1","updated":"2025-02-14T08:24:38Z","published":"2025-02-14T08:24:38Z","title":"Navigating Label Ambiguity for Facial Expression Recognition in the Wild","summary":"  Facial expression recognition (FER) remains a challenging task due to label\nambiguity caused by the subjective nature of facial expressions and noisy\nsamples. Additionally, class imbalance, which is common in real-world datasets,\nfurther complicates FER. Although many studies have shown impressive\nimprovements, they typically address only one of these issues, leading to\nsuboptimal results. To tackle both challenges simultaneously, we propose a\nnovel framework called Navigating Label Ambiguity (NLA), which is robust under\nreal-world conditions. The motivation behind NLA is that dynamically estimating\nand emphasizing ambiguous samples at each iteration helps mitigate noise and\nclass imbalance by reducing the model's bias toward majority classes. To\nachieve this, NLA consists of two main components: Noise-aware Adaptive\nWeighting (NAW) and consistency regularization. Specifically, NAW adaptively\nassigns higher importance to ambiguous samples and lower importance to noisy\nones, based on the correlation between the intermediate prediction scores for\nthe ground truth and the nearest negative. Moreover, we incorporate a\nregularization term to ensure consistent latent distributions. Consequently,\nNLA enables the model to progressively focus on more challenging ambiguous\nsamples, which primarily belong to the minority class, in the later stages of\ntraining. Extensive experiments demonstrate that NLA outperforms existing\nmethods in both overall and mean accuracy, confirming its robustness against\nnoise and class imbalance. To the best of our knowledge, this is the first\nframework to address both problems simultaneously.\n","authors":["JunGyu Lee","Yeji Choi","Haksub Kim","Ig-Jae Kim","Gi Pyo Nam"],"pdf_url":"https://arxiv.org/pdf/2502.09993v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2502.09990v1","updated":"2025-02-14T08:22:51Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09980v1","updated":"2025-02-14T08:05:41Z","published":"2025-02-14T08:05:41Z","title":"V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multi-Modal Large Language Models","summary":"  Current autonomous driving vehicles rely mainly on their individual sensors\nto understand surrounding scenes and plan for future trajectories, which can be\nunreliable when the sensors are malfunctioning or occluded. To address this\nproblem, cooperative perception methods via vehicle-to-vehicle (V2V)\ncommunication have been proposed, but they have tended to focus on detection\nand tracking. How those approaches contribute to overall cooperative planning\nperformance is still under-explored. Inspired by recent progress using Large\nLanguage Models (LLMs) to build autonomous driving systems, we propose a novel\nproblem setting that integrates an LLM into cooperative autonomous driving,\nwith the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and\nbenchmark. We also propose our baseline method Vehicle-to-Vehicle Large\nLanguage Model (V2V-LLM), which uses an LLM to fuse perception information from\nmultiple connected autonomous vehicles (CAVs) and answer driving-related\nquestions: grounding, notable object identification, and planning. Experimental\nresults show that our proposed V2V-LLM can be a promising unified model\narchitecture for performing various tasks in cooperative autonomous driving,\nand outperforms other baseline methods that use different fusion approaches.\nOur work also creates a new research direction that can improve the safety of\nfuture autonomous driving systems. Our project website:\nhttps://eddyhkchiu.github.io/v2vllm.github.io/ .\n","authors":["Hsu-kuang Chiu","Ryo Hachiuma","Chien-Yi Wang","Stephen F. Smith","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09971v1","updated":"2025-02-14T07:56:21Z","published":"2025-02-14T07:56:21Z","title":"Conditional Latent Coding with Learnable Synthesized Reference for Deep\n  Image Compression","summary":"  In this paper, we study how to synthesize a dynamic reference from an\nexternal dictionary to perform conditional coding of the input image in the\nlatent domain and how to learn the conditional latent synthesis and coding\nmodules in an end-to-end manner. Our approach begins by constructing a\nuniversal image feature dictionary using a multi-stage approach involving\nmodified spatial pyramid pooling, dimension reduction, and multi-scale feature\nclustering. For each input image, we learn to synthesize a conditioning latent\nby selecting and synthesizing relevant features from the dictionary, which\nsignificantly enhances the model's capability in capturing and exploring image\nsource correlation. This conditional latent synthesis involves a\ncorrelation-based feature matching and alignment strategy, comprising a\nConditional Latent Matching (CLM) module and a Conditional Latent Synthesis\n(CLS) module. The synthesized latent is then used to guide the encoding\nprocess, allowing for more efficient compression by exploiting the correlation\nbetween the input image and the reference dictionary. According to our\ntheoretical analysis, the proposed conditional latent coding (CLC) method is\nrobust to perturbations in the external dictionary samples and the selected\nconditioning latent, with an error bound that scales logarithmically with the\ndictionary size, ensuring stability even with large and diverse dictionaries.\nExperimental results on benchmark datasets show that our new method improves\nthe coding performance by a large margin (up to 1.2 dB) with a very small\noverhead of approximately 0.5\\% bits per pixel. Our code is publicly available\nat https://github.com/ydchen0806/CLC.\n","authors":["Siqi Wu","Yinda Chen","Dong Liu","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2502.09971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09967v1","updated":"2025-02-14T07:49:06Z","published":"2025-02-14T07:49:06Z","title":"VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly\n  Supervised Group Activity Recognition","summary":"  Existing weakly supervised group activity recognition methods rely on object\ndetectors or attention mechanisms to capture key areas automatically. However,\nthey overlook the semantic information associated with captured areas, which\nmay adversely affect the recognition performance. In this paper, we propose a\nnovel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM)\nwhich effectively captures the locations of individual actions and integrates\nthem with action semantics for weakly supervised group activity recognition.It\ngenerates individual action prototypes from training set as visual conceptual\nknowledge to bridge action semantics and visual representations. Guided by this\nknowledge, VicKAM produces action maps that indicate the likelihood of each\naction occurring at various locations, based on image correlation theorem. It\nfurther augments individual action maps using group activity related\nstatistical information, representing individual action distribution under\ndifferent group activities, to establish connections between action maps and\nspecific group activities. The augmented action map is incorporated with action\nsemantic representations for group activity recognition.Extensive experiments\non two public benchmarks, the Volleyball and the NBA datasets, demonstrate the\neffectiveness of our proposed method, even in cases of limited training data.\nThe code will be released later.\n","authors":["Zhuming Wang","Yihao Zheng","Jiarui Li","Yaofei Wu","Yan Huang","Zun Li","Lifang Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09963v1","updated":"2025-02-14T07:41:47Z","published":"2025-02-14T07:41:47Z","title":"Generating on Generated: An Approach Towards Self-Evolving Diffusion\n  Models","summary":"  Recursive Self-Improvement (RSI) enables intelligence systems to autonomously\nrefine their capabilities. This paper explores the application of RSI in\ntext-to-image diffusion models, addressing the challenge of training collapse\ncaused by synthetic data. We identify two key factors contributing to this\ncollapse: the lack of perceptual alignment and the accumulation of generative\nhallucinations. To mitigate these issues, we propose three strategies: (1) a\nprompt construction and filtering pipeline designed to facilitate the\ngeneration of perceptual aligned data, (2) a preference sampling method to\nidentify human-preferred samples and filter out generative hallucinations, and\n(3) a distribution-based weighting scheme to penalize selected samples with\nhallucinatory errors. Our extensive experiments validate the effectiveness of\nthese approaches.\n","authors":["Xulu Zhang","Xiaoyong Wei","Jinlin Wu","Jiaxin Wu","Zhaoxiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2502.09963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00625v4","updated":"2025-02-14T07:34:39Z","published":"2024-06-02T06:08:26Z","title":"SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection","summary":"  Visual anomaly detection is vital in real-world applications, such as\nindustrial defect detection and medical diagnosis. However, most existing\nmethods focus on local structural anomalies and fail to detect higher-level\nfunctional anomalies under logical conditions. Although recent studies have\nexplored logical anomaly detection, they can only address simple anomalies like\nmissing or addition and show poor generalizability due to being heavily\ndata-driven. To fill this gap, we propose SAM-LAD, a zero-shot, plug-and-play\nframework for logical anomaly detection in any scene. First, we obtain a query\nimage's feature map using a pre-trained backbone. Simultaneously, we retrieve\nthe reference images and their corresponding feature maps via the nearest\nneighbor search of the query image. Then, we introduce the Segment Anything\nModel (SAM) to obtain object masks of the query and reference images. Each\nobject mask is multiplied with the entire image's feature map to obtain object\nfeature maps. Next, an Object Matching Model (OMM) is proposed to match objects\nin the query and reference images. To facilitate object matching, we further\npropose a Dynamic Channel Graph Attention (DCGA) module, treating each object\nas a keypoint and converting its feature maps into feature vectors. Finally,\nbased on the object matching relations, an Anomaly Measurement Model (AMM) is\nproposed to detect objects with logical anomalies. Structural anomalies in the\nobjects can also be detected. We validate our proposed SAM-LAD using various\nbenchmarks, including industrial datasets (MVTec Loco AD, MVTec AD), and the\nlogical dataset (DigitAnatomy). Extensive experimental results demonstrate that\nSAM-LAD outperforms existing SoTA methods, particularly in detecting logical\nanomalies.\n","authors":["Yun Peng","Xiao Lin","Nachuan Ma","Jiayuan Du","Chuangwei Liu","Chengju Liu","Qijun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.00625v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.05768 by other authors"},{"id":"http://arxiv.org/abs/2502.09952v1","updated":"2025-02-14T07:12:19Z","published":"2025-02-14T07:12:19Z","title":"Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe","summary":"  China's Chang'e 5 mission has been a remarkable success, with the chang'e 5\nlander traveling on the Oceanus Procellarum to collect images of the lunar\nsurface. Over the past half century, people have brought back some lunar rock\nsamples, but its quantity does not meet the need for research. Under current\ncircumstances, people still mainly rely on the analysis of rocks on the lunar\nsurface through the detection of lunar rover. The Oceanus Procellarum, chosen\nby Chang'e 5 mission, contains various kind of rock species. Therefore, we\nfirst applied to the National Astronomical Observatories of the China under the\nChinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of\nthe lunar surface image, and established a lunar surface rock image data set\nCE5ROCK. The data set contains 100 images, which randomly divided into\ntraining, validation and test set. Experimental results show that the\nidentification accuracy testing on convolutional neural network (CNN) models\nlike AlexNet or MobileNet is about to 40.0%. In order to make full use of the\nglobal information in Moon images, this paper proposes the MRNet (MoonRockNet)\nnetwork architecture. The encoding structure of the network uses VGG16 for\nfeature extraction, and the decoding part adds dilated convolution and commonly\nused U-Net structure on the original VGG16 decoding structure, which is more\nconducive to identify more refined but more sparsely distributed types of lunar\nrocks. We have conducted extensive experiments on the established CE5ROCK data\nset, and the experimental results show that MRNet can achieve more accurate\nrock type identification, and outperform other existing mainstream algorithms\nin the identification performance.\n","authors":["Jin Cui","Yifei Zou","Siyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09952v1.pdf","comment":"Published at the 8th International Conference on Advances in\n  Machinery, Material Science and Engineering Application (MMSE 2022)"},{"id":"http://arxiv.org/abs/2411.19564v2","updated":"2025-02-14T06:44:21Z","published":"2024-11-29T09:19:57Z","title":"A Comprehensive Framework for Automated Segmentation of Perivascular\n  Spaces in Brain MRI with the nnU-Net","summary":"  Background: Enlargement of perivascular spaces (PVS) is common in\nneurodegenerative disorders including cerebral small vessel disease,\nAlzheimer's disease, and Parkinson's disease. PVS enlargement may indicate\nimpaired clearance pathways and there is a need for reliable PVS detection\nmethods which are currently lacking. Aim: To optimise a widely used deep\nlearning model, the no-new-UNet (nnU-Net), for PVS segmentation. Methods: In 30\nhealthy participants (mean$\\pm$SD age: 50$\\pm$18.9 years; 13 females),\nT1-weighted MRI images were acquired using three different protocols on three\nMRI scanners (3T Siemens Tim Trio, 3T Philips Achieva, and 7T Siemens\nMagnetom). PVS were manually segmented across ten axial slices in each\nparticipant. Segmentations were completed using a sparse annotation strategy.\nIn total, 11 models were compared using various strategies for image handling,\npreprocessing and semi-supervised learning with pseudo-labels. Model\nperformance was evaluated using 5-fold cross validation (5FCV). The main\nperformance metric was the Dice Similarity Coefficient (DSC). Results: The\nvoxel-spacing agnostic model (mean$\\pm$SD DSC=64.3$\\pm$3.3%) outperformed\nmodels which resampled images to a common resolution (DSC=40.5-55%). Model\nperformance improved substantially following iterative label cleaning\n(DSC=85.7$\\pm$1.2%). Semi-supervised learning with pseudo-labels (n=12,740)\nfrom 18 additional datasets improved the agreement between raw and predicted\nPVS cluster counts (Lin's concordance correlation coefficient=0.89,\n95%CI=0.82-0.94). We extended the model to enable PVS segmentation in the\nmidbrain (DSC=64.3$\\pm$6.5%) and hippocampus (DSC=67.8$\\pm$5%). Conclusions:\nOur deep learning models provide a robust and holistic framework for the\nautomated quantification of PVS in brain MRI.\n","authors":["William Pham","Alexander Jarema","Donggyu Rim","Zhibin Chen","Mohamed S. H. Khlif","Vaughan G. Macefield","Luke A. Henderson","Amy Brodtmann"],"pdf_url":"https://arxiv.org/pdf/2411.19564v2.pdf","comment":"46 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.09858v2","updated":"2025-02-14T06:42:11Z","published":"2024-11-15T00:37:29Z","title":"One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with\n  Semantic-Aware Views for Efficient Visual Representation","summary":"  This paper proposes a scalable and straightforward pre-training paradigm for\nefficient visual conceptual representation called occluded image contrastive\nlearning (OCL). Our OCL approach is simple: we randomly mask patches to\ngenerate different views within an image and contrast them among a mini-batch\nof images. The core idea behind OCL consists of two designs. First, masked\ntokens have the potential to significantly diminish the conceptual redundancy\ninherent in images, and create distinct views with substantial fine-grained\ndifferences on the semantic concept level instead of the instance level.\nSecond, contrastive learning is adept at extracting high-level semantic\nconceptual features during the pre-training, circumventing the high-frequency\ninterference and additional costs associated with image reconstruction.\nImportantly, OCL learns highly semantic conceptual representations efficiently\nwithout relying on hand-crafted data augmentations or additional auxiliary\nmodules. Empirically, OCL demonstrates high scalability with Vision\nTransformers, as the ViT-L/16 can complete pre-training in 133 hours using only\n4 A100 GPUs, achieving 85.8\\% accuracy in downstream fine-tuning tasks. Code is\navailable at https://anonymous.4open.science/r/OLRS/.\n","authors":["Xiaoyu Yang","Lijian Xu","Hongsheng Li","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.09858v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.09941v1","updated":"2025-02-14T06:35:44Z","published":"2025-02-14T06:35:44Z","title":"A Lightweight and Effective Image Tampering Localization Network with\n  Vision Mamba","summary":"  Current image tampering localization methods primarily rely on Convolutional\nNeural Networks (CNNs) and Transformers. While CNNs suffer from limited local\nreceptive fields, Transformers offer global context modeling at the expense of\nquadratic computational complexity. Recently, the state space model Mamba has\nemerged as a competitive alternative, enabling linear-complexity global\ndependency modeling. Inspired by it, we propose a lightweight and effective\nFORensic network based on vision MAmba (ForMa) for blind image tampering\nlocalization. Firstly, ForMa captures multi-scale global features that achieves\nefficient global dependency modeling through linear complexity. Then the\npixel-wise localization map is generated by a lightweight decoder, which\nemploys a parameter-free pixel shuffle layer for upsampling. Additionally, a\nnoise-assisted decoding strategy is proposed to integrate complementary\nmanipulation traces from tampered images, boosting decoder sensitivity to\nforgery cues. Experimental results on 10 standard datasets demonstrate that\nForMa achieves state-of-the-art generalization ability and robustness, while\nmaintaining the lowest computational complexity. Code is available at\nhttps://github.com/multimediaFor/ForMa.\n","authors":["Kun Guo","Gang Cao","Zijie Lou","Xianglin Huang","Jiaoyun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11214v2","updated":"2025-02-14T06:25:16Z","published":"2024-12-15T15:10:53Z","title":"Image Forgery Localization with State Space Models","summary":"  Pixel dependency modeling from tampered images is pivotal for image forgery\nlocalization. Current approaches predominantly rely on Convolutional Neural\nNetworks (CNNs) or Transformer-based models, which often either lack sufficient\nreceptive fields or entail significant computational overheads. Recently, State\nSpace Models (SSMs), exemplified by Mamba, have emerged as a promising\napproach. They not only excel in modeling long-range interactions but also\nmaintain a linear computational complexity. In this paper, we propose LoMa, a\nnovel image forgery localization method that leverages the selective SSMs.\nSpecifically, LoMa initially employs atrous selective scan to traverse the\nspatial domain and convert the tampered image into ordered patch sequences, and\nsubsequently applies multi-directional state space modeling. In addition, an\nauxiliary convolutional branch is introduced to enhance local feature\nextraction. Extensive experimental results validate the superiority of LoMa\nover CNN-based and Transformer-based state-of-the-arts. To our best knowledge,\nthis is the first image forgery localization model constructed based on the\nSSM-based model. We aim to establish a baseline and provide valuable insights\nfor the future development of more efficient and effective SSM-based forgery\nlocalization models. Code is available at\nhttps://github.com/multimediaFor/LoMa.\n","authors":["Zijie Lou","Gang Cao","Kun Guo","Shaowei Weng","Lifang Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09939v1","updated":"2025-02-14T06:24:04Z","published":"2025-02-14T06:24:04Z","title":"Temporal Scale and Shift Invariant Automatic Event Recognition using the\n  Mellin Transform","summary":"  The Spatio-temporal holographic correlator combines the traditional 2D\noptical image correlation techniques with inhomogeneously broadened arrays of\ncold atoms to achieve 3D time-space correlation to realize automatic event\nrecognition at an ultra-high speed. Here we propose a method to realize such\nevent recognition for videos running at different speeds. With this method, we\ncan highly improve recognition accuracy and filter almost all the unwanted\nevents in the video database.\n","authors":["Xi Shen","Julian Gamboa","Tabassom Hamidfar","Shamima A. Mitu","Selim M. Shahriar"],"pdf_url":"https://arxiv.org/pdf/2502.09939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08226v2","updated":"2025-02-14T06:23:57Z","published":"2025-02-12T09:12:30Z","title":"TRISHUL: Towards Region Identification and Screen Hierarchy\n  Understanding for Large VLM based GUI Agents","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have enabled the\ndevelopment of LVLM-based Graphical User Interface (GUI) agents under various\nparadigms. Training-based approaches, such as CogAgent and SeeClick, struggle\nwith cross-dataset and cross-platform generalization due to their reliance on\ndataset-specific training. Generalist LVLMs, such as GPT-4V, employ\nSet-of-Marks (SoM) for action grounding, but obtaining SoM labels requires\nmetadata like HTML source, which is not consistently available across\nplatforms. Moreover, existing methods often specialize in singular GUI tasks\nrather than achieving comprehensive GUI understanding. To address these\nlimitations, we introduce TRISHUL, a novel, training-free agentic framework\nthat enhances generalist LVLMs for holistic GUI comprehension. Unlike prior\nworks that focus on either action grounding (mapping instructions to GUI\nelements) or GUI referring (describing GUI elements given a location), TRISHUL\nseamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen\nParsing (HSP) and the Spatially Enhanced Element Description (SEED) module,\nwhich work synergistically to provide multi-granular, spatially, and\nsemantically enriched representations of GUI elements. Our results demonstrate\nTRISHUL's superior performance in action grounding across the ScreenSpot,\nVisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,\nTRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new\nstandard for robust and adaptable GUI comprehension.\n","authors":["Kunal Singh","Shreyas Singh","Mukund Khanna"],"pdf_url":"https://arxiv.org/pdf/2502.08226v2.pdf","comment":"8 pages 5 figures"},{"id":"http://arxiv.org/abs/2405.05538v2","updated":"2025-02-14T06:16:50Z","published":"2024-05-09T04:36:04Z","title":"A Survey on Personalized Content Synthesis with Diffusion Models","summary":"  Recent advancements in generative models have significantly impacted content\ncreation, leading to the emergence of Personalized Content Synthesis (PCS).\nWith a small set of user-provided examples, PCS aims to customize the subject\nof interest to specific user-defined prompts. Over the past two years, more\nthan 150 methods have been proposed. However, existing surveys mainly focus on\ntext-to-image generation, with few providing up-to-date summaries on PCS. This\npaper offers a comprehensive survey of PCS, with a particular focus on the\ndiffusion models. Specifically, we introduce the generic frameworks of PCS\nresearch, which can be broadly classified into optimization-based and\nlearning-based approaches. We further categorize and analyze these\nmethodologies, discussing their strengths, limitations, and key techniques.\nAdditionally, we delve into specialized tasks within the field, such as\npersonalized object generation, face synthesis, and style personalization,\nhighlighting their unique challenges and innovations. Despite encouraging\nprogress, we also present an analysis of the challenges such as overfitting and\nthe trade-off between subject fidelity and text alignment. Through this\ndetailed overview and analysis, we propose future directions to advance the\ndevelopment of PCS.\n","authors":["Xulu Zhang","Xiaoyong Wei","Wengyu Zhang","Jinlin Wu","Zhaoxiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2405.05538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09935v1","updated":"2025-02-14T06:11:23Z","published":"2025-02-14T06:11:23Z","title":"Precise Parameter Localization for Textual Generation in Diffusion\n  Models","summary":"  Novel diffusion models can synthesize photo-realistic images with integrated\nhigh-quality text. Surprisingly, we demonstrate through attention activation\npatching that only less than 1% of diffusion models' parameters, all contained\nin attention layers, influence the generation of textual content within the\nimages. Building on this observation, we improve textual generation efficiency\nand performance by targeting cross and joint attention layers of diffusion\nmodels. We introduce several applications that benefit from localizing the\nlayers responsible for textual content generation. We first show that a\nLoRA-based fine-tuning solely of the localized layers enhances, even more, the\ngeneral text-generation capabilities of large diffusion models while preserving\nthe quality and diversity of the diffusion models' generations. Then, we\ndemonstrate how we can use the localized layers to edit textual content in\ngenerated images. Finally, we extend this idea to the practical use case of\npreventing the generation of toxic text in a cost-free manner. In contrast to\nprior work, our localization approach is broadly applicable across various\ndiffusion model architectures, including U-Net (e.g., LDM and SDXL) and\ntransformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing\ndiverse text encoders (e.g., from CLIP to the large language models like T5).\nProject page available at https://t2i-text-loc.github.io/.\n","authors":["≈Åukasz Staniszewski","Bartosz Cywi≈Ñski","Franziska Boenisch","Kamil Deja","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2502.09935v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09932v1","updated":"2025-02-14T06:02:59Z","published":"2025-02-14T06:02:59Z","title":"AffectSRNet : Facial Emotion-Aware Super-Resolution Network","summary":"  Facial expression recognition (FER) systems in low-resolution settings face\nsignificant challenges in accurately identifying expressions due to the loss of\nfine-grained facial details. This limitation is especially problematic for\napplications like surveillance and mobile communications, where low image\nresolution is common and can compromise recognition accuracy. Traditional\nsingle-image face super-resolution (FSR) techniques, however, often fail to\npreserve the emotional intent of expressions, introducing distortions that\nobscure the original affective content. Given the inherently ill-posed nature\nof single-image super-resolution, a targeted approach is required to balance\nimage quality enhancement with emotion retention. In this paper, we propose\nAffectSRNet, a novel emotion-aware super-resolution framework that reconstructs\nhigh-quality facial images from low-resolution inputs while maintaining the\nintensity and fidelity of facial expressions. Our method effectively bridges\nthe gap between image resolution and expression accuracy by employing an\nexpression-preserving loss function, specifically tailored for FER\napplications. Additionally, we introduce a new metric to assess emotion\npreservation in super-resolved images, providing a more nuanced evaluation of\nFER system performance in low-resolution scenarios. Experimental results on\nstandard datasets, including CelebA, FFHQ, and Helen, demonstrate that\nAffectSRNet outperforms existing FSR approaches in both visual quality and\nemotion fidelity, highlighting its potential for integration into practical FER\napplications. This work not only improves image clarity but also ensures that\nemotion-driven applications retain their core functionality in suboptimal\nresolution environments, paving the way for broader adoption in FER systems.\n","authors":["Syed Sameen Ahmad Rizvi","Soham Kumar","Aryan Seth","Pratik Narang"],"pdf_url":"https://arxiv.org/pdf/2502.09932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09931v1","updated":"2025-02-14T05:54:13Z","published":"2025-02-14T05:54:13Z","title":"TransGUNet: Transformer Meets Graph-based Skip Connection for Medical\n  Image Segmentation","summary":"  Skip connection engineering is primarily employed to address the semantic gap\nbetween the encoder and decoder, while also integrating global dependencies to\nunderstand the relationships among complex anatomical structures in medical\nimage segmentation. Although several models have proposed transformer-based\napproaches to incorporate global dependencies within skip connections, they\noften face limitations in capturing detailed local features with high\ncomputational complexity. In contrast, graph neural networks (GNNs) exploit\ngraph structures to effectively capture local and global features. Leveraging\nthese properties, we introduce an attentional cross-scale graph neural network\n(ACS-GNN), which enhances the skip connection framework by converting\ncross-scale feature maps into a graph structure and capturing complex\nanatomical structures through node attention. Additionally, we observed that\ndeep learning models often produce uninformative feature maps, which degrades\nthe quality of spatial attention maps. To address this problem, we integrated\nentropy-driven feature selection (EFS) with spatial attention, calculating an\nentropy score for each channel and filtering out high-entropy feature maps. Our\ninnovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial\nattentio} to effectively enhance domain generalizability across various\nmodalities by leveraging GNNs alongside a reliable spatial attention map,\nensuring more robust features within the skip connection. Through comprehensive\nexperiments and analysis, TransGUNet achieved superior segmentation performance\non six seen and eight unseen datasets, demonstrating significantly higher\nefficiency compared to previous methods.\n","authors":["Ju-Hyeon Nam","Nur Suriza Syazwany","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2502.09931v1.pdf","comment":"24 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.09928v1","updated":"2025-02-14T05:41:33Z","published":"2025-02-14T05:41:33Z","title":"Deep Tree Tensor Networks for Image Recognition","summary":"  Originating in quantum physics, tensor networks (TNs) have been widely\nadopted as exponential machines and parameter decomposers for recognition\ntasks. Typical TN models, such as Matrix Product States (MPS), have not yet\nachieved successful application in natural image processing. When employed,\nthey primarily serve to compress parameters within off-the-shelf networks, thus\nlosing their distinctive capability to enhance exponential-order feature\ninteractions. This paper introduces a novel architecture named\n\\textit{\\textbf{D}eep \\textbf{T}ree \\textbf{T}ensor \\textbf{N}etwork} (DTTN),\nwhich captures $2^L$-order multiplicative interactions across features through\nmultilinear operations, while essentially unfolding into a \\emph{tree}-like TN\ntopology with the parameter-sharing property. DTTN is stacked with multiple\nantisymmetric interacting modules (AIMs), and this design facilitates efficient\nimplementation. Moreover, we theoretically reveal the equivalency among\nquantum-inspired TN models and polynomial and multilinear networks under\ncertain conditions, and we believe that DTTN can inspire more interpretable\nstudies in this field. We evaluate the proposed model against a series of\nbenchmarks and achieve excellent performance compared to its peers and\ncutting-edge architectures. Our code will soon be publicly available.\n","authors":["Chang Nie","Junfang Chen","Yajie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09927v1","updated":"2025-02-14T05:36:32Z","published":"2025-02-14T05:36:32Z","title":"Granite Vision: a lightweight, open-source multimodal model for\n  enterprise Intelligence","summary":"  We introduce Granite Vision, a lightweight large language model with vision\ncapabilities, specifically designed to excel in enterprise use cases,\nparticularly in visual document understanding. Our model is trained on a\ncomprehensive instruction-following dataset, including document-related tasks,\nsuch as content extraction from tables, charts, diagrams, sketches, and\ninfographics, as well as general image tasks. The architecture of Granite\nVision is centered around visual modality alignment with a decoder-only, 2\nbillion parameter Granite large language model. Additionally, we introduce a\ndedicated safety classification approach in test-time that leverages a sparse\nset of attention vectors to identify potential harmful inputs. Despite its\nlightweight architecture, Granite Vision achieves strong results in standard\nbenchmarks related to visual document understanding, as well as on the LiveXiv\nbenchmark, which is designed to avoid test set contamination by using a\nconstantly updated corpus of recently published Arxiv papers. We are releasing\nthe model under the Apache-2 license, allowing for both research and commercial\nuse, while offering complete visibility into the training data and other\nrelevant details. See https://huggingface.co/ibm-granite/ for model weights.\n","authors":[" Granite Vision Team","Leonid Karlinsky","Assaf Arbelle","Abraham Daniels","Ahmed Nassar","Amit Alfassi","Bo Wu","Eli Schwartz","Dhiraj Joshi","Jovana Kondic","Nimrod Shabtay","Pengyuan Li","Roei Herzig","Shafiq Abedin","Shaked Perek","Sivan Harary","Udi Barzelay","Adi Raz Goldfarb","Aude Oliva","Ben Wieles","Bishwaranjan Bhattacharjee","Brandon Huang","Christoph Auer","Dan Gutfreund","David Beymer","David Wood","Hilde Kuehne","Jacob Hansen","Joseph Shtok","Ken Wong","Luis Angel Bathen","Mayank Mishra","Maksym Lysak","Michele Dolfi","Mikhail Yurochkin","Nikolaos Livathinos","Nimrod Harel","Ophir Azulai","Oshri Naparstek","Rafael Teixeira de Lima","Rameswar Panda","Sivan Doveh","Shubham Gupta","Subhro Das","Syed Zawad","Yusik Kim","Zexue He","Alexander Brooks","Gabe Goodhart","Anita Govindjee","Derek Leist","Ibrahim Ibrahim","Aya Soffer","David Cox","Kate Soule","Luis Lastras","Nirmit Desai","Shila Ofek-koifman","Sriram Raghavan","Tanveer Syeda-Mahmood","Peter Staar","Tal Drory","Rogerio Feris"],"pdf_url":"https://arxiv.org/pdf/2502.09927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08093v3","updated":"2025-02-14T05:33:21Z","published":"2024-08-15T11:36:18Z","title":"When Video Coding Meets Multimodal Large Language Models: A Unified\n  Paradigm for Video Coding","summary":"  Existing codecs are designed to eliminate intrinsic redundancies to create a\ncompact representation for compression. However, strong external priors from\nMultimodal Large Language Models (MLLMs) have not been explicitly explored in\nvideo compression. Herein, we introduce a unified paradigm for Cross-Modality\nVideo Coding (CMVC), which is a pioneering approach to explore multimodality\nrepresentation and video generative models in video coding. Specifically, on\nthe encoder side, we disentangle a video into spatial content and motion\ncomponents, which are subsequently transformed into distinct modalities to\nachieve very compact representation by leveraging MLLMs. During decoding,\npreviously encoded components and video generation models are leveraged to\ncreate multiple encoding-decoding modes that optimize video reconstruction\nquality for specific decoding requirements, including Text-Text-to-Video (TT2V)\nmode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)\nmode to achieve superb perceptual consistency. In addition, we propose an\nefficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)\ntuning to guarantee perceptual quality, which allows the generated motion cues\nto behave smoothly. Experiments on benchmarks indicate that TT2V achieves\neffective semantic reconstruction, while IT2V exhibits competitive perceptual\nconsistency. These results highlight potential directions for future research\nin video coding.\n","authors":["Pingping Zhang","Jinlong Li","Kecheng Chen","Meng Wang","Long Xu","Haoliang Li","Nicu Sebe","Sam Kwong","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09925v1","updated":"2025-02-14T05:32:46Z","published":"2025-02-14T05:32:46Z","title":"TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of\n  Thousands Vision Task Types","summary":"  Multimodal visual language models are gaining prominence in open-world\napplications, driven by advancements in model architectures, training\ntechniques, and high-quality data. However, their performance is often limited\nby insufficient task-specific data, leading to poor generalization and biased\noutputs. Existing efforts to increase task diversity in fine-tuning datasets\nare hindered by the labor-intensive process of manual task labeling, which\ntypically produces only a few hundred task types. To address this, we propose\nTaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising\n19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o\nto enrich task diversity by expanding from a small set of manually defined\ntasks, with CLIP and GPT-4o filtering those that best match open-source images,\nand generating relevant question-answer pairs. Multiple models are employed to\nensure sample quality. This automated process enhances both task diversity and\ndata quality, reducing manual intervention. Incorporating TaskGalaxy into\nLLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance\nimprovements across 16 benchmarks, demonstrating the critical importance of\ntask diversity. TaskGalaxy is publicly released at\nhttps://github.com/Kwai-YuanQi/TaskGalaxy.\n","authors":["Jiankang Chen","Tianke Zhang","Changyi Liu","Haojie Ding","Yaya Shi","Feng Cheng","Huihui Xiao","Bin Wen","Fan Yang","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09923v1","updated":"2025-02-14T05:23:56Z","published":"2025-02-14T05:23:56Z","title":"Self-Consistent Model-based Adaptation for Visual Reinforcement Learning","summary":"  Visual reinforcement learning agents typically face serious performance\ndeclines in real-world applications caused by visual distractions. Existing\nmethods rely on fine-tuning the policy's representations with hand-crafted\naugmentations. In this work, we propose Self-Consistent Model-based Adaptation\n(SCMA), a novel method that fosters robust adaptation without modifying the\npolicy. By transferring cluttered observations to clean ones with a denoising\nmodel, SCMA can mitigate distractions for various policies as a plug-and-play\nenhancement. To optimize the denoising model in an unsupervised manner, we\nderive an unsupervised distribution matching objective with a theoretical\nanalysis of its optimality. We further present a practical algorithm to\noptimize the objective by estimating the distribution of clean observations\nwith a pre-trained world model. Extensive experiments on multiple visual\ngeneralization benchmarks and real robot data demonstrate that SCMA effectively\nboosts performance across various distractions and exhibits better sample\nefficiency.\n","authors":["Xinning Zhou","Chengyang Ying","Yao Feng","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.09923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09906v1","updated":"2025-02-14T04:29:17Z","published":"2025-02-14T04:29:17Z","title":"Insect-Foundation: A Foundation Model and Large Multimodal Dataset for\n  Vision-Language Insect Understanding","summary":"  Multimodal conversational generative AI has shown impressive capabilities in\nvarious vision and language understanding through learning massive text-image\ndata. However, current conversational models still lack knowledge about visual\ninsects since they are often trained on the general knowledge of\nvision-language data. Meanwhile, understanding insects is a fundamental problem\nin precision agriculture, helping to promote sustainable development in\nagriculture. Therefore, this paper proposes a novel multimodal conversational\nmodel, Insect-LLaVA, to promote visual understanding in insect-domain\nknowledge. In particular, we first introduce a new large-scale Multimodal\nInsect Dataset with Visual Insect Instruction Data that enables the capability\nof learning the multimodal foundation models. Our proposed dataset enables\nconversational models to comprehend the visual and semantic features of the\ninsects. Second, we propose a new Insect-LLaVA model, a new general Large\nLanguage and Vision Assistant in Visual Insect Understanding. Then, to enhance\nthe capability of learning insect features, we develop an Insect Foundation\nModel by introducing a new micro-feature self-supervised learning with a\nPatch-wise Relevant Attention mechanism to capture the subtle differences among\ninsect images. We also present Description Consistency loss to improve\nmicro-feature learning via text descriptions. The experimental results\nevaluated on our new Visual Insect Question Answering benchmarks illustrate the\neffective performance of our proposed approach in visual insect understanding\nand achieve State-of-the-Art performance on standard benchmarks of\ninsect-related tasks.\n","authors":["Thanh-Dat Truong","Hoang-Quan Nguyen","Xuan-Bac Nguyen","Ashley Dowling","Xin Li","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2502.09906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17241v2","updated":"2025-02-14T04:03:10Z","published":"2024-12-23T03:22:44Z","title":"QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level\n  Feature Distribution for Medical Image Segmentation","summary":"  Medical image segmentation plays a crucial role in assisting healthcare\nprofessionals with accurate diagnoses and enabling automated diagnostic\nprocesses. Traditional convolutional neural networks (CNNs) often struggle with\ncapturing long-range dependencies, while transformer-based architectures,\ndespite their effectiveness, come with increased computational complexity.\nRecent efforts have focused on combining CNNs and transformers to balance\nperformance and efficiency, but existing approaches still face challenges in\nachieving high segmentation accuracy while maintaining low computational costs.\nFurthermore, many methods underutilize the CNN encoder's capability to capture\nlocal spatial information, concentrating primarily on mitigating long-range\ndependency issues. To address these limitations, we propose QTSeg, a novel\narchitecture for medical image segmentation that effectively integrates local\nand global information. QTSeg features a dual-mix attention decoder designed to\nenhance segmentation performance through: (1) a cross-attention mechanism for\nimproved feature alignment, (2) a spatial attention module to capture\nlong-range dependencies, and (3) a channel attention block to learn\ninter-channel relationships. Additionally, we introduce a multi-level feature\ndistribution module, which adaptively balances feature propagation between the\nencoder and decoder, further boosting performance. Extensive experiments on\nfive publicly available datasets covering diverse segmentation tasks, including\nlesion, polyp, breast cancer, cell, and retinal vessel segmentation,\ndemonstrate that QTSeg outperforms state-of-the-art methods across multiple\nevaluation metrics while maintaining lower computational costs. Our\nimplementation can be found at: https://github.com/tpnam0901/QTSeg (v1.0.0)\n","authors":["Phuong-Nam Tran","Nhat Truong Pham","Duc Ngoc Minh Dang","Eui-Nam Huh","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2412.17241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13198v2","updated":"2025-02-14T03:48:39Z","published":"2024-11-20T10:58:47Z","title":"Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature\n  Learning in Chest CT Segmentation","summary":"  In the field of medical image segmentation, challenges such as indistinct\nlesion features, ambiguous boundaries,and multi-scale characteristics have long\nrevailed. This paper proposes an improved method named Intensity-Spatial Dual\nMasked AutoEncoder (ISD-MAE). Based on the tissue-contrast semi-masked\nautoencoder, a Masked AutoEncoder (MAE) branch is introduced to perform\nintensity masking and spatial masking operations on chest CT images for\nmulti-scale feature learning and segmentation tasks. The model utilizes a\ndual-branch structure and contrastive learning to enhance the ability to learn\ntissue features and boundary details. Experiments are conducted on multiple 2D\nand 3D datasets. The results show that ISD-MAE significantly outperforms other\nmethods in 2D pneumonia and mediastinal tumor segmentation tasks. For example,\nthe Dice score reaches 90.10% on the COVID19 LESION dataset, and the\nperformance is relatively stable. However, there is still room for improvement\non 3D datasets. In response to this, improvement directions are proposed,\nincluding optimizing the loss function, using enhanced 3D convolution blocks,\nand processing datasets from multiple perspectives.Our code is available\nat:https://github.com/prowontheus/ISD-MAE.\n","authors":["Yuexing Ding","Jun Wang","Hongbing Lyu"],"pdf_url":"https://arxiv.org/pdf/2411.13198v2.pdf","comment":"During further verification, we found that due to operational errors,\n  a small number of images in the dataset used for training appeared in the\n  validation set, which led to inaccurate main conclusions. We are correcting\n  these problems and plan to withdraw this paper."},{"id":"http://arxiv.org/abs/2407.20908v2","updated":"2025-02-14T03:44:24Z","published":"2024-07-30T15:33:58Z","title":"Dynamic Scene Understanding through Object-Centric Voxelization and\n  Neural Rendering","summary":"  Learning object-centric representations from unsupervised videos is\nchallenging. Unlike most previous approaches that focus on decomposing 2D\nimages, we present a 3D generative model named DynaVol-S for dynamic scenes\nthat enables object-centric learning within a differentiable volume rendering\nframework. The key idea is to perform object-centric voxelization to capture\nthe 3D nature of the scene, which infers per-object occupancy probabilities at\nindividual spatial locations. These voxel features evolve through a\ncanonical-space deformation function and are optimized in an inverse rendering\npipeline with a compositional NeRF. Additionally, our approach integrates 2D\nsemantic features to create 3D semantic grids, representing the scene through\nmultiple disentangled voxel grids. DynaVol-S significantly outperforms existing\nmodels in both novel view synthesis and unsupervised decomposition tasks for\ndynamic scenes. By jointly considering geometric structures and semantic\nfeatures, it effectively addresses challenging real-world scenarios involving\ncomplex object interactions. Furthermore, once trained, the explicitly\nmeaningful voxel features enable additional capabilities that 2D scene\ndecomposition methods cannot achieve, such as novel scene generation through\nediting geometric shapes or manipulating the motion trajectories of objects.\n","authors":["Yanpeng Zhao","Yiwei Hao","Siyu Gao","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.20908v2.pdf","comment":"Accepted by TPAMI2025"},{"id":"http://arxiv.org/abs/2502.09893v1","updated":"2025-02-14T03:33:04Z","published":"2025-02-14T03:33:04Z","title":"Dynamic-Computed Tomography Angiography for Cerebral Vessel Templates\n  and Segmentation","summary":"  Background: Computed Tomography Angiography (CTA) is crucial for\ncerebrovascular disease diagnosis. Dynamic CTA is a type of imaging that\ncaptures temporal information about the We aim to develop and evaluate two\nsegmentation techniques to segment vessels directly on CTA images: (1) creating\nand registering population-averaged vessel atlases and (2) using deep learning\n(DL). Methods: We retrieved 4D-CT of the head from our institutional research\ndatabase, with bone and soft tissue subtracted from post-contrast images. An\nAdvanced Normalization Tools pipeline was used to create angiographic atlases\nfrom 25 patients. Then, atlas-driven ROIs were identified by a CT attenuation\nthreshold to generate segmentation of the arteries and veins using non-linear\nregistration. To create DL vessel segmentations, arterial and venous structures\nwere segmented using the MRA vessel segmentation tool, iCafe, in 29 patients.\nThese were then used to train a DL model, with bone-in CT images as input.\nMultiple phase images in the 4D-CT were used to increase the training and\nvalidation dataset. Both segmentation approaches were evaluated on a test 4D-CT\ndataset of 11 patients which were also processed by iCafe and validated by a\nneuroradiologist. Specifically, branch-wise segmentation accuracy was\nquantified with 20 labels for arteries and one for veins. DL outperformed the\natlas-based segmentation models for arteries (average modified dice coefficient\n(amDC) 0.856 vs. 0.324) and veins (amDC 0.743 vs. 0.495) overall. For ICAs,\nvertebral and basilar arteries, DL and atlas -based segmentation had an amDC of\n0.913 and 0.402, respectively. The amDC for MCA-M1, PCA-P1, and ACA-A1 segments\nwere 0.932 and 0.474, respectively. Conclusion: Angiographic CT templates are\ndeveloped for the first time in literature. Using 4D-CTA enables the use of\ntools like iCafe, lessening the burden of manual annotation.\n","authors":["Shrikanth Yadav","Jisoo Kim","Geoffrey Young","Lei Qin"],"pdf_url":"https://arxiv.org/pdf/2502.09893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09573v2","updated":"2025-02-14T03:31:39Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09954v3","updated":"2025-02-14T03:03:41Z","published":"2024-12-13T08:24:12Z","title":"$\\textrm{A}^{\\textrm{2}}$RNet: Adversarial Attack Resilient Network for\n  Robust Infrared and Visible Image Fusion","summary":"  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n","authors":["Jiawei Li","Hongwei Yu","Jiansheng Chen","Xinlong Ding","Jinlong Wang","Jinyuan Liu","Bochao Zou","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.09954v3.pdf","comment":"9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2501.15140v2","updated":"2025-02-14T02:57:30Z","published":"2025-01-25T08:52:43Z","title":"Analyzing and Boosting the Power of Fine-Grained Visual Recognition for\n  Multi-modal Large Language Models","summary":"  Multi-modal large language models (MLLMs) have shown remarkable abilities in\nvarious visual understanding tasks. However, MLLMs still struggle with\nfine-grained visual recognition (FGVR), which aims to identify\nsubordinate-level categories from images. This can negatively impact more\nadvanced capabilities of MLLMs, such as object-centric visual question\nanswering and reasoning. In our study, we revisit three quintessential\ncapabilities of MLLMs for FGVR, including object information extraction,\ncategory knowledge reserve, object-category alignment, and position of the root\ncause as a misalignment problem. To address this issue, we present Finedefics,\nan MLLM that enhances the model's FGVR capability by incorporating informative\nattribute descriptions of objects into the training phase. We employ\ncontrastive learning on object-attribute pairs and attribute-category pairs\nsimultaneously and use examples from similar but incorrect categories as hard\nnegatives, naturally bringing representations of visual objects and category\nnames closer. Extensive evaluations across multiple popular FGVR datasets\ndemonstrate that Finedefics outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code is available at\nhttps://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.\n","authors":["Hulingxiao He","Geng Li","Zijun Geng","Jinglin Xu","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2501.15140v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09874v1","updated":"2025-02-14T02:51:25Z","published":"2025-02-14T02:51:25Z","title":"FrGNet: A fourier-guided weakly-supervised framework for nuclear\n  instance segmentation","summary":"  Nuclear instance segmentation has played a critical role in pathology image\nanalysis. The main challenges arise from the difficulty in accurately\nsegmenting instances and the high cost of precise mask-level annotations for\nfully-supervised training.In this work, we propose a fourier guidance framework\nfor solving the weakly-supervised nuclear instance segmentation problem. In\nthis framework, we construct a fourier guidance module to fuse the priori\ninformation into the training process of the model, which facilitates the model\nto capture the relevant features of the nuclear.Meanwhile, in order to further\nimprove the model's ability to represent the features of nuclear, we propose\nthe guide-based instance level contrastive module. This module makes full use\nof the framework's own properties and guide information to effectively enhance\nthe representation features of nuclear. We show on two public datasets that our\nmodel can outperform current SOTA methods under fully-supervised design, and in\nweakly-supervised experiments, with only a small amount of labeling our model\nstill maintains close to the performance under full supervision.In addition, we\nalso perform generalization experiments on a private dataset, and without any\nlabeling, our model is able to segment nuclear images that have not been seen\nduring training quite effectively. As open science, all codes and pre-trained\nmodels are available at https://github.com/LQY404/FrGNet.\n","authors":["Peng Ling"],"pdf_url":"https://arxiv.org/pdf/2502.09874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09873v1","updated":"2025-02-14T02:46:27Z","published":"2025-02-14T02:46:27Z","title":"Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal","summary":"  Diffusion models have demonstrated remarkable success in image restoration\ntasks. However, their multi-step denoising process introduces significant\ncomputational overhead, limiting their practical deployment. Furthermore,\nexisting methods struggle to effectively remove severe JPEG artifact,\nespecially in highly compressed images. To address these challenges, we propose\nCODiff, a compression-aware one-step diffusion model for JPEG artifact removal.\nThe core of CODiff is the compression-aware visual embedder (CaVE), which\nextracts and leverages JPEG compression priors to guide the diffusion model. We\npropose a dual learning strategy that combines explicit and implicit learning.\nSpecifically, explicit learning enforces a quality prediction objective to\ndifferentiate low-quality images with different compression levels. Implicit\nlearning employs a reconstruction objective that enhances the model's\ngeneralization. This dual learning allows for a deeper and more comprehensive\nunderstanding of JPEG compression. Experimental results demonstrate that CODiff\nsurpasses recent leading methods in both quantitative and visual quality\nmetrics. The code and models will be released at\nhttps://github.com/jp-guo/CODiff.\n","authors":["Jinpei Guo","Zheng Chen","Wenbo Li","Yong Guo","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09872v1","updated":"2025-02-14T02:45:16Z","published":"2025-02-14T02:45:16Z","title":"Learning to Calibrate for Reliable Visual Fire Detection","summary":"  Fire is characterized by its sudden onset and destructive power, making early\nfire detection crucial for ensuring human safety and protecting property. With\nthe advancement of deep learning, the application of computer vision in fire\ndetection has significantly improved. However, deep learning models often\nexhibit a tendency toward overconfidence, and most existing works focus\nprimarily on enhancing classification performance, with limited attention given\nto uncertainty modeling. To address this issue, we propose transforming the\nExpected Calibration Error (ECE), a metric for measuring uncertainty, into a\ndifferentiable ECE loss function. This loss is then combined with the\ncross-entropy loss to guide the training process of multi-class fire detection\nmodels. Additionally, to achieve a good balance between classification accuracy\nand reliable decision, we introduce a curriculum learning-based approach that\ndynamically adjusts the weight of the ECE loss during training. Extensive\nexperiments are conducted on two widely used multi-class fire detection\ndatasets, DFAN and EdgeFireSmoke, validating the effectiveness of our\nuncertainty modeling method.\n","authors":["Ziqi Zhang","Xiuzhuang Zhou","Xiangyang Gong"],"pdf_url":"https://arxiv.org/pdf/2502.09872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04794v2","updated":"2025-02-14T01:14:15Z","published":"2025-02-07T09:57:03Z","title":"MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of\n  Fever of Unknown Origin","summary":"  Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is\nintroduced as a multimodal framework inspired by real-world diagnostic\nprocesses. It uses pretrained models such as DINOv2, Vision Transformer, and\nResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into\nlow-dimensional, semantically meaningful features. A learnable\nself-attention-based fusion network then integrates these imaging features with\nclinical data for classification. Using 416 FUO patient cases from Sichuan\nUniversity West China Hospital from 2017 to 2023, the multimodal fusion\nclassification network MFCN achieved macro-AUROC scores ranging from 0.8654 to\n0.9291 across seven tasks, outperforming conventional machine learning and\nsingle-modality deep learning methods. Ablation studies and five-fold\ncross-validation further validated its effectiveness. By combining the\nstrengths of pretrained large models and deep learning, MedMimic offers a\npromising solution for disease classification.\n","authors":["Minrui Chen","Yi Zhou","Huidong Jiang","Yuhan Zhu","Guanjie Zou","Minqi Chen","Rong Tian","Hiroto Saigo"],"pdf_url":"https://arxiv.org/pdf/2502.04794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09838v1","updated":"2025-02-14T00:42:36Z","published":"2025-02-14T00:42:36Z","title":"HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation","summary":"  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n","authors":["Tianwei Lin","Wenqiao Zhang","Sijing Li","Yuqian Yuan","Binhe Yu","Haoyuan Li","Wanggui He","Hao Jiang","Mengze Li","Xiaohui Song","Siliang Tang","Jun Xiao","Hui Lin","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2502.09838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11793v3","updated":"2025-02-14T00:33:29Z","published":"2023-04-24T02:36:25Z","title":"Coevolution of Camouflage","summary":"  Camouflage in nature seems to arise from competition between predator and\nprey. To survive, predators must find prey, and prey must avoid being found.\nThis work simulates an abstract model of that adversarial relationship. It\nlooks at crypsis through evolving prey camouflage patterns (as color textures)\nin competition with evolving predator vision. During their \"lifetime\" predators\nlearn to better locate camouflaged prey. The environment for this 2D simulation\nis provided by a set of photographs, typically of natural scenes. This model is\nbased on two evolving populations, one of prey and another of predators. Mutual\nconflict between these populations can produce both effective prey camouflage\nand predators skilled at \"breaking\" camouflage. The result is an open source\nartificial life model to help study camouflage in nature, and the perceptual\nphenomenon of camouflage more generally.\n","authors":["Craig Reynolds"],"pdf_url":"https://arxiv.org/pdf/2304.11793v3.pdf","comment":"20 pages, 21 figures"},{"id":"http://arxiv.org/abs/2305.06110v4","updated":"2025-02-14T00:24:07Z","published":"2023-05-10T12:54:02Z","title":"Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification\n  with Snoring Usecase","summary":"  This paper proposes an atomic behaviour intervention strategy using Pavlok\ndevice. Pavlok utilises beeps, vibration and shocks as a mode of aversion\ntechnique to help individuals with behaviour modification. While the device can\nbe useful in certain periodic daily life situations, like alarms and exercise\nnotifications, the device relies on manual operations that limit its usage. To\nautomate behaviour modification, we propose a framework that first detects\ntargeted behaviours through a lightweight deep learning model and subsequently\nnudges the user through Pavlok. Our proposed solution is implemented and\nverified in the context of snoring, which captures audio from the environment\nfollowing a prediction of whether the audio content is a snore or not using a\n1D convolutional neural network. Based on the prediction, we use Pavlok to\nnudge users for preventive measures, such as a change in sleeping posture. We\nbelieve that this simple solution can help people change their atomic habits,\nwhich may lead to long-term health benefits. Our proposed real-time,\nlightweight model (99.8% fewer parameters over SOTA; 1,278,049 --> 1337)\nachieves SOTA performance (test accuracy of 0.99) on a public benchmark. The\ncode and model are publicly available at\nhttps://github.com/hasan-rakibul/pavlok-nudge-snore.\n","authors":["Md Rakibul Hasan","Shreya Ghosh","Pradyumna Agrawal","Zhixi Cai","Abhinav Dhall","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2305.06110v4.pdf","comment":"Md Rakibul Hasan and Shreya Ghosh are co-first authors"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.10291v2","updated":"2025-02-14T17:37:35Z","published":"2024-06-13T03:26:30Z","title":"ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents","summary":"  Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.\n","authors":["Hao Kang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.10291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10284v1","updated":"2025-02-14T16:42:54Z","published":"2025-02-14T16:42:54Z","title":"A Hybrid Cross-Stage Coordination Pre-ranking Model for Online\n  Recommendation Systems","summary":"  Large-scale recommendation systems often adopt cascading architecture\nconsisting of retrieval, pre-ranking, ranking, and re-ranking stages. With\nstrict latency requirements, pre-ranking utilizes lightweight models to perform\na preliminary selection from massive retrieved candidates. However, recent\nworks focus solely on improving consistency with ranking, relying exclusively\non downstream stages. Since downstream input is derived from the pre-ranking\noutput, they will exacerbate the sample selection bias (SSB) issue and Matthew\neffect, leading to sub-optimal results. To address the limitation, we propose a\nnovel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate\ninformation from upstream (retrieval) and downstream (ranking, re-ranking)\nstages. Specifically, cross-stage coordination refers to the pre-ranking's\nadaptability to the entire stream and the role of serving as a more effective\nbridge between upstream and downstream. HCCP consists of Hybrid Sample\nConstruction and Hybrid Objective Optimization. Hybrid sample construction\ncaptures multi-level unexposed data from the entire stream and rearranges them\nto become the optimal guiding \"ground truth\" for pre-ranking learning. Hybrid\nobjective optimization contains the joint optimization of consistency and\nlong-tail precision through our proposed Margin InfoNCE loss. It is\nspecifically designed to learn from such hybrid unexposed samples, improving\nthe overall performance and mitigating the SSB issue. The appendix describes a\nproof of the efficacy of the proposed loss in selecting potential positives.\nExtensive offline and online experiments indicate that HCCP outperforms SOTA\nmethods by improving cross-stage coordination. It contributes up to 14.9% UCVR\nand 1.3% UCTR in the JD E-commerce recommendation system. Concerning code\nprivacy, we provide a pseudocode for reference.\n","authors":["Binglei Zhao","Houying Qi","Guang Xu","Mian Ma","Xiwei Zhao","Feng Mei","Sulong Xu","Jinghe Hu"],"pdf_url":"https://arxiv.org/pdf/2502.10284v1.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.10230v1","updated":"2025-02-14T15:34:36Z","published":"2025-02-14T15:34:36Z","title":"ProReco: A Process Discovery Recommender System","summary":"  Process discovery aims to automatically derive process models from historical\nexecution data (event logs). While various process discovery algorithms have\nbeen proposed in the last 25 years, there is no consensus on a dominating\ndiscovery algorithm. Selecting the most suitable discovery algorithm remains a\nchallenge due to competing quality measures and diverse user requirements.\nManually selecting the most suitable process discovery algorithm from a range\nof options for a given event log is a time-consuming and error-prone task. This\npaper introduces ProReco, a Process discovery Recommender system designed to\nrecommend the most appropriate algorithm based on user preferences and event\nlog characteristics. ProReco incorporates state-of-the-art discovery\nalgorithms, extends the feature pools from previous work, and utilizes\neXplainable AI (XAI) techniques to provide explanations for its\nrecommendations.\n","authors":["Tsung-Hao Huang","Tarek Junied","Marco Pegoraro","Wil M. P. van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2502.10230v1.pdf","comment":"8 pages, 5 figures, 9 references"},{"id":"http://arxiv.org/abs/2502.08277v2","updated":"2025-02-14T15:33:27Z","published":"2025-02-12T10:31:45Z","title":"ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling","summary":"  Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space.\n","authors":["Wei Cheng","Yucheng Lu","Boyang Xia","Jiangxia Cao","Kuan Xu","Mingxing Wen","Wei Jiang","Jiaming Zhang","Zhaojie Liu","Liyin Hong","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08277v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.02844v3","updated":"2025-02-14T15:32:00Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08346v2","updated":"2025-02-14T15:25:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10157v1","updated":"2025-02-14T13:36:20Z","published":"2025-02-14T13:36:20Z","title":"SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation","summary":"  We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency.\n","authors":["Lei Huang","Hao Guo","Linzhi Peng","Long Zhang","Xiaoteng Wang","Daoyuan Wang","Shichao Wang","Jinpeng Wang","Lei Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10151v1","updated":"2025-02-14T13:25:29Z","published":"2025-02-14T13:25:29Z","title":"Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay","summary":"  Centralized search engines are key for the Internet, but lead to undesirable\nconcentration of power. Decentralized alternatives fail to offer equal document\nretrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come\nclose to the performance of centralized solutions when the semantics of\ndocuments are properly captured. This work uses embeddings from Large Language\nModels to capture semantics and fulfill the promise of Semantic Overlay\nNetworks. Our proposed algorithm, called Semantica, constructs a prefix tree\n(trie) utilizing document embeddings calculated by a language model. Users\nconnect to each other based on the embeddings of their documents, ensuring that\nsemantically similar users are directly linked. Thereby, this construction\nmakes it more likely for user searches to be answered by the users that they\nare directly connected to, or by the users they are close to in the network\nconnection graph. The implementation of our algorithm also accommodates the\nsemantic diversity of individual users by spawning \"clone\" user identifiers in\nthe tree. Our experiments use emulation with a real-world workload to show\nSemantica's ability to identify and connect to similar users quickly. Semantica\nfinds up to ten times more semantically similar users than current\nstate-of-the-art approaches. At the same time, Semantica can retrieve more than\ntwo times the number of relevant documents given the same network load. We also\nmake our code publicly available to facilitate further research in the area.\n","authors":["Petru Neague","Quinten Stokkink","Naman Goel","Johan Pouwelse"],"pdf_url":"https://arxiv.org/pdf/2502.10151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10050v1","updated":"2025-02-14T09:57:07Z","published":"2025-02-14T09:57:07Z","title":"A Survey on LLM-powered Agents for Recommender Systems","summary":"  Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field.\n","authors":["Qiyao Peng","Hongtao Liu","Hua Huang","Qing Yang","Minglai Shao"],"pdf_url":"https://arxiv.org/pdf/2502.10050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08845v2","updated":"2025-02-14T09:35:58Z","published":"2025-02-12T23:32:09Z","title":"Optimal Dataset Size for Recommender Systems: Evaluating Algorithms'\n  Performance via Downsampling","summary":"  This thesis investigates dataset downsampling as a strategy to optimize\nenergy efficiency in recommender systems while maintaining competitive\nperformance. With increasing dataset sizes posing computational and\nenvironmental challenges, this study explores the trade-offs between energy\nefficiency and recommendation quality in Green Recommender Systems, which aim\nto reduce environmental impact. By applying two downsampling approaches to\nseven datasets, 12 algorithms, and two levels of core pruning, the research\ndemonstrates significant reductions in runtime and carbon emissions. For\nexample, a 30% downsampling portion can reduce runtime by 52% compared to the\nfull dataset, leading to a carbon emission reduction of up to 51.02 KgCO2e\nduring the training of a single algorithm on a single dataset. The analysis\nreveals that algorithm performance under different downsampling portions\ndepends on factors like dataset characteristics, algorithm complexity, and the\nspecific downsampling configuration (scenario dependent). Some algorithms,\nwhich showed lower nDCG@10 scores compared to higher-performing ones, exhibited\nlower sensitivity to the amount of training data, offering greater potential\nfor efficiency in lower downsampling portions. On average, these algorithms\nretained 81% of full-size performance using only 50% of the training set. In\ncertain downsampling configurations, where more users were progressively\nincluded while keeping the test set size fixed, they even showed higher nDCG@10\nscores than when using the full dataset. These findings highlight the\nfeasibility of balancing sustainability and effectiveness, providing insights\nfor designing energy-efficient recommender systems and promoting sustainable AI\npractices.\n","authors":["Ardalan Arabzadeh"],"pdf_url":"https://arxiv.org/pdf/2502.08845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09956v1","updated":"2025-02-14T07:28:08Z","published":"2025-02-14T07:28:08Z","title":"KGGen: Extracting Knowledge Graphs from Plain Text with Language Models","summary":"  Recent interest in building foundation models for KGs has highlighted a\nfundamental challenge: knowledge-graph data is relatively scarce. The\nbest-known KGs are primarily human-labeled, created by pattern-matching, or\nextracted using early NLP techniques. While human-generated KGs are in short\nsupply, automatically extracted KGs are of questionable quality. We present a\nsolution to this data scarcity problem in the form of a text-to-KG generator\n(KGGen), a package that uses language models to create high-quality graphs from\nplaintext. Unlike other KG extractors, KGGen clusters related entities to\nreduce sparsity in extracted KGs. KGGen is available as a Python library\n(\\texttt{pip install kg-gen}), making it accessible to everyone. Along with\nKGGen, we release the first benchmark, Measure of of Information in Nodes and\nEdges (MINE), that tests an extractor's ability to produce a useful KG from\nplain text. We benchmark our new tool against existing extractors and\ndemonstrate far superior performance.\n","authors":["Belinda Mo","Kyssen Yu","Joshua Kazdan","Proud Mpala","Lisa Yu","Chris Cundy","Charilaos Kanatsoulis","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2502.09956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00131v2","updated":"2025-02-14T05:05:02Z","published":"2025-01-31T19:28:26Z","title":"Middleman Bias in Advertising: Aligning Relevance of Keyphrase\n  Recommendations with Search","summary":"  E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). Keyphrases\nmust be pertinent to items; otherwise, it can result in seller dissatisfaction\nand poor targeting -- towards that end relevance filters are employed. In this\nwork, we describe the shortcomings of training relevance filter models on\nbiased click/sales signals. We re-conceptualize advertiser keyphrase relevance\nas interaction between two dynamical systems -- Advertising which produces the\nkeyphrases and Search which acts as a middleman to reach buyers. We discuss the\nbias of search relevance systems (middleman bias) and the need to align\nadvertiser keyphrases with search relevance signals. We also compare the\nperformance of cross encoders and bi-encoders in modeling this alignment and\nthe scalability of such a solution for sellers at eBay.\n","authors":["Soumik Dey","Wei Zhang","Hansi Wu","Bingfeng Dong","Binbin Li"],"pdf_url":"https://arxiv.org/pdf/2502.00131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00228v3","updated":"2025-02-14T04:00:08Z","published":"2023-04-01T05:04:06Z","title":"Accuracy and Political Bias of News Source Credibility Ratings by Large\n  Language Models","summary":"  Search engines increasingly leverage large language models (LLMs) to generate\ndirect answers, and AI chatbots now access the Internet for fresh data. As\ninformation curators for billions of users, LLMs must assess the accuracy and\nreliability of different sources. This paper audits nine widely used LLMs from\nthree leading providers -- OpenAI, Google, and Meta -- to evaluate their\nability to discern credible and high-quality information sources from\nlow-credibility ones. We find that while LLMs can rate most tested news\noutlets, larger models more frequently refuse to provide ratings due to\ninsufficient information, whereas smaller models are more prone to making\nerrors in their ratings. For sources where ratings are provided, LLMs exhibit a\nhigh level of agreement among themselves (average Spearman's $\\rho = 0.79$),\nbut their ratings align only moderately with human expert evaluations (average\n$\\rho = 0.50$). Analyzing news sources with different political leanings in the\nUS, we observe a liberal bias in credibility ratings yielded by all LLMs in\ndefault configurations. Additionally, assigning partisan roles to LLMs\nconsistently induces strong politically congruent bias in their ratings. These\nfindings have important implications for the use of LLMs in curating news and\npolitical information.\n","authors":["Kai-Cheng Yang","Filippo Menczer"],"pdf_url":"https://arxiv.org/pdf/2304.00228v3.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09891v1","updated":"2025-02-14T03:28:36Z","published":"2025-02-14T03:28:36Z","title":"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has proven effective in integrating\nexternal knowledge into large language models (LLMs) for question-answer (QA)\ntasks. The state-of-the-art RAG approaches often use the graph data as the\nexternal data since they capture the rich semantic information and link\nrelationships between entities. However, existing graph-based RAG approaches\ncannot accurately identify the relevant information from the graph and also\nconsume large numbers of tokens in the online retrieval process. To address\nthese issues, we introduce a novel graph-based RAG approach, called Attributed\nCommunity-based Hierarchical RAG (ArchRAG), by augmenting the question using\nattributed communities, and also introducing a novel LLM-based hierarchical\nclustering method. To retrieve the most relevant information from the graph for\nthe question, we build a novel hierarchical index structure for the attributed\ncommunities and develop an effective online retrieval method. Experimental\nresults demonstrate that ArchRAG outperforms existing methods in terms of both\naccuracy and token cost.\n","authors":["Shu Wang","Yixiang Fang","Yingli Zhou","Xilin Liu","Yuchi Ma"],"pdf_url":"https://arxiv.org/pdf/2502.09891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09888v1","updated":"2025-02-14T03:25:09Z","published":"2025-02-14T03:25:09Z","title":"An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law","summary":"  The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.\n","authors":["Songpei Xu","Shijia Wang","Da Guo","Xianwen Guo","Qiang Xiao","Fangjian Li","Chuanjiang Luo"],"pdf_url":"https://arxiv.org/pdf/2502.09888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14482v3","updated":"2025-02-14T20:58:41Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context understanding\nand retrieval-augmented generation (RAG) capabilities. These two capabilities\nare complementary to each other and essential for LLMs to process large volumes\nof information that cannot fit into a single prompt. We present a detailed\ncontinued training recipe to extend the context window of Llama3-70B-base from\n8K to 128K tokens, along with a three-stage instruction tuning process to\nenhance the model's instruction-following, RAG performance, and long-context\nunderstanding capabilities. Our results demonstrate that the\nLlama3-ChatQA-2-70B model outperforms most existing state-of-the-art models,\nincluding GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and\nLlama3.1-70B-Instruct, on ultra-long tasks beyond 100K tokens, as well as on\nthe RAG benchmark using only a 4K context window, showing the strong long\ncontext capability across varying sequence lengths. We further provide\nextensive comparisons between direct long-context and RAG solutions using the\nsame state-of-the-art long-context LLMs. Interestingly, we find that the\nperformance of strong long-context LLMs using RAG improves when retrieving a\nlarger number of chunks. With a large set of top-k chunks, RAG consistently\noutperforms direct long-context solution using the same state-of-the-art\nlong-context models (e.g., Llama3-ChatQA-2-70B and Qwen2-72B-Instruct) on both\n32K and 128K benchmarks. We open-source the model weights, training data, and\nthe evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Chejian Xu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v3.pdf","comment":"Accepted at ICLR 2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.10392v1","updated":"2025-02-14T18:59:59Z","published":"2025-02-14T18:59:59Z","title":"Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding","summary":"  In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.\n","authors":["Wenxuan Guo","Xiuwei Xu","Ziwei Wang","Jianjiang Feng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2502.10392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10390v1","updated":"2025-02-14T18:59:40Z","published":"2025-02-14T18:59:40Z","title":"(How) Can Transformers Predict Pseudo-Random Numbers?","summary":"  Transformers excel at discovering patterns in sequential data, yet their\nfundamental limitations and learning mechanisms remain crucial topics of\ninvestigation. In this paper, we study the ability of Transformers to learn\npseudo-random number sequences from linear congruential generators (LCGs),\ndefined by the recurrence relation $x_{t+1} = a x_t + c \\;\\mathrm{mod}\\; m$.\nOur analysis reveals that with sufficient architectural capacity and training\ndata variety, Transformers can perform in-context prediction of LCG sequences\nwith unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding\nlayers and attention patterns, we uncover how Transformers develop algorithmic\nstructures to learn these sequences in two scenarios of increasing complexity.\nFirst, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but\nfixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our\nanalysis reveals that models learn to factorize the modulus and utilize\ndigit-wise number representations to make sequential predictions. In the\nsecond, more challenging scenario of unseen moduli, we show that Transformers\ncan generalize to unseen moduli up to $m_{\\text{test}} = 2^{16}$. In this case,\nthe model employs a two-step strategy: first estimating the unknown modulus\nfrom the context, then utilizing prime factorizations to generate predictions.\nFor this task, we observe a sharp transition in the accuracy at a critical\ndepth $=3$. We also find that the number of in-context sequence elements needed\nto reach high accuracy scales sublinearly with the modulus.\n","authors":["Tao Tao","Darshil Doshi","Dayal Singh Kalra","Tianyu He","Maissam Barkeshli"],"pdf_url":"https://arxiv.org/pdf/2502.10390v1.pdf","comment":"10+16 pages, 12+20 figures"},{"id":"http://arxiv.org/abs/2502.10381v1","updated":"2025-02-14T18:57:16Z","published":"2025-02-14T18:57:16Z","title":"Balancing the Scales: A Theoretical and Algorithmic Framework for\n  Learning from Imbalanced Data","summary":"  Class imbalance remains a major challenge in machine learning, especially in\nmulti-class problems with long-tailed distributions. Existing methods, such as\ndata resampling, cost-sensitive techniques, and logistic loss modifications,\nthough popular and often effective, lack solid theoretical foundations. As an\nexample, we demonstrate that cost-sensitive methods are not Bayes consistent.\nThis paper introduces a novel theoretical framework for analyzing\ngeneralization in imbalanced classification. We propose a new class-imbalanced\nmargin loss function for both binary and multi-class settings, prove its strong\n$H$-consistency, and derive corresponding learning guarantees based on\nempirical loss and a new notion of class-sensitive Rademacher complexity.\nLeveraging these theoretical results, we devise novel and general learning\nalgorithms, IMMAX (Imbalanced Margin Maximization), which incorporate\nconfidence margins and are applicable to various hypothesis sets. While our\nfocus is theoretical, we also present extensive empirical results demonstrating\nthe effectiveness of our algorithms compared to existing baselines.\n","authors":["Corinna Cortes","Anqi Mao","Mehryar Mohri","Yutao Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.10381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08008v2","updated":"2025-02-14T18:52:34Z","published":"2025-02-11T23:07:14Z","title":"An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models","summary":"  Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.\n","authors":["Kasra Ahmadi","Rouzbeh Behnia","Reza Ebrahimi","Mehran Mozaffari Kermani","Jeremiah Birrell","Jason Pacheco","Attila A Yavuz"],"pdf_url":"https://arxiv.org/pdf/2502.08008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10373v1","updated":"2025-02-14T18:51:40Z","published":"2025-02-14T18:51:40Z","title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation\n  Models","summary":"  Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.\n","authors":["William Chen","Jinchuan Tian","Yifan Peng","Brian Yan","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.10373v1.pdf","comment":"23 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.10365v1","updated":"2025-02-14T18:43:22Z","published":"2025-02-14T18:43:22Z","title":"AffinityFlow: Guided Flows for Antibody Affinity Maturation","summary":"  Antibodies are widely used as therapeutics, but their development requires\ncostly affinity maturation, involving iterative mutations to enhance binding\naffinity.This paper explores a sequence-only scenario for affinity maturation,\nusing solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold\nwithin flow matching to generate diverse protein structures, enabling a\nsequence-conditioned generative model of structure. Building on this, we\npropose an alternating optimization framework that (1) fixes the sequence to\nguide structure generation toward high binding affinity using a structure-based\naffinity predictor, then (2) applies inverse folding to create sequence\nmutations, refined by a sequence-based affinity predictor for post selection.\nTo address this, we develop a co-teaching module that incorporates valuable\ninformation from noisy biophysical energies into predictor refinement. The\nsequence-based predictor selects consensus samples to teach the structure-based\npredictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art\nperformance in affinity maturation experiments. We plan to open-source our code\nafter acceptance.\n","authors":["Can Chen","Karla-Luise Herpoldt","Chenchao Zhao","Zichen Wang","Marcus Collins","Shang Shang","Ron Benson"],"pdf_url":"https://arxiv.org/pdf/2502.10365v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.10363v1","updated":"2025-02-14T18:42:42Z","published":"2025-02-14T18:42:42Z","title":"BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds","summary":"  Traversing risky terrains with sparse footholds poses a significant challenge\nfor humanoid robots, requiring precise foot placements and stable locomotion.\nExisting approaches designed for quadrupedal robots often fail to generalize to\nhumanoid robots due to differences in foot geometry and unstable morphology,\nwhile learning-based approaches for humanoid locomotion still face great\nchallenges on complex terrains due to sparse foothold reward signals and\ninefficient learning processes. To address these challenges, we introduce\nBeamDojo, a reinforcement learning (RL) framework designed for enabling agile\nhumanoid locomotion on sparse footholds. BeamDojo begins by introducing a\nsampling-based foothold reward tailored for polygonal feet, along with a double\ncritic to balancing the learning process between dense locomotion rewards and\nsparse foothold rewards. To encourage sufficient trail-and-error exploration,\nBeamDojo incorporates a two-stage RL approach: the first stage relaxes the\nterrain dynamics by training the humanoid on flat terrain while providing it\nwith task terrain perceptive observations, and the second stage fine-tunes the\npolicy on the actual task terrain. Moreover, we implement a onboard LiDAR-based\nelevation map to enable real-world deployment. Extensive simulation and\nreal-world experiments demonstrate that BeamDojo achieves efficient learning in\nsimulation and enables agile locomotion with precise foot placement on sparse\nfootholds in the real world, maintaining a high success rate even under\nsignificant external disturbances.\n","authors":["Huayi Wang","Zirui Wang","Junli Ren","Qingwei Ben","Tao Huang","Weinan Zhang","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.10363v1.pdf","comment":"Project website: https://why618188.github.io/beamdojo"},{"id":"http://arxiv.org/abs/2502.10361v1","updated":"2025-02-14T18:42:07Z","published":"2025-02-14T18:42:07Z","title":"Enhancing Multilingual LLM Pretraining with Model-Based Data Selection","summary":"  Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets.\n","authors":["Bettina Messmer","Vinko Sabolƒçec","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2502.10361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10359v1","updated":"2025-02-14T18:41:53Z","published":"2025-02-14T18:41:53Z","title":"Proper Learnability and the Role of Unlabeled Data","summary":"  Proper learning refers to the setting in which learners must emit predictors\nin the underlying hypothesis class $H$, and often leads to learners with simple\nalgorithmic forms (e.g. empirical risk minimization (ERM), structural risk\nminimization (SRM)). The limitation of proper learning, however, is that there\nexist problems which can only be learned improperly, e.g. in multiclass\nclassification. Thus, we ask: Under what assumptions on the hypothesis class or\nthe information provided to the learner is a problem properly learnable? We\nfirst demonstrate that when the unlabeled data distribution is given, there\nalways exists an optimal proper learner governed by distributional\nregularization, a randomized generalization of regularization. We refer to this\nsetting as the distribution-fixed PAC model, and continue to evaluate the\nlearner on its worst-case performance over all distributions. Our result holds\nfor all metric loss functions and any finite learning problem (with no\ndependence on its size). Further, we demonstrate that sample complexities in\nthe distribution-fixed PAC model can shrink by only a logarithmic factor from\nthe classic PAC model, strongly refuting the role of unlabeled data in PAC\nlearning (from a worst-case perspective).\n  We complement this with impossibility results which obstruct any\ncharacterization of proper learnability in the realizable PAC model. First, we\nobserve that there are problems whose proper learnability is logically\nundecidable, i.e., independent of the ZFC axioms. We then show that proper\nlearnability is not a monotone property of the underlying hypothesis class, and\nthat it is not a local property (in a precise sense). Our impossibility results\nall hold even for the fundamental setting of multiclass classification, and go\nthrough a reduction of EMX learning (Ben-David et al., 2019) to proper\nclassification which may be of independent interest.\n","authors":["Julian Asilis","Siddartha Devic","Shaddin Dughmi","Vatsal Sharan","Shang-Hua Teng"],"pdf_url":"https://arxiv.org/pdf/2502.10359v1.pdf","comment":"ALT 2025, 22 pages"},{"id":"http://arxiv.org/abs/2502.10357v1","updated":"2025-02-14T18:39:18Z","published":"2025-02-14T18:39:18Z","title":"Learning Euler Factors of Elliptic Curves","summary":"  We apply transformer models and feedforward neural networks to predict\nFrobenius traces $a_p$ from elliptic curves given other traces $a_q$. We train\nfurther models to predict $a_p \\bmod 2$ from $a_q \\bmod 2$, and cross-analysis\nsuch as $a_p \\bmod 2$ from $a_q$. Our experiments reveal that these models\nachieve high accuracy, even in the absence of explicit number-theoretic tools\nlike functional equations of $L$-functions. We also present partial\ninterpretability findings.\n","authors":["Angelica Babei","Fran√ßois Charton","Edgar Costa","Xiaoyu Huang","Kyu-Hwan Lee","David Lowry-Duda","Ashvni Narayanan","Alexey Pozdnyakov"],"pdf_url":"https://arxiv.org/pdf/2502.10357v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2411.00843v2","updated":"2025-02-14T18:35:03Z","published":"2024-10-30T04:20:10Z","title":"The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation","summary":"  Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.\n","authors":["Reza Moravej","Saurabh Bodhe","Zhanguang Zhang","Didier Chetelat","Dimitrios Tsaras","Yingxue Zhang","Hui-Ling Zhen","Jianye Hao","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.00843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10354v1","updated":"2025-02-14T18:32:22Z","published":"2025-02-14T18:32:22Z","title":"Dimension-free Score Matching and Time Bootstrapping for Diffusion\n  Models","summary":"  Diffusion models generate samples by estimating the score function of the\ntarget distribution at various noise levels. The model is trained using samples\ndrawn from the target distribution, progressively adding noise. In this work,\nwe establish the first (nearly) dimension-free sample complexity bounds for\nlearning these score functions, achieving a double exponential improvement in\ndimension over prior results. A key aspect of our analysis is the use of a\nsingle function approximator to jointly estimate scores across noise levels, a\ncritical feature of diffusion models in practice which enables generalization\nacross timesteps. Our analysis introduces a novel martingale-based error\ndecomposition and sharp variance bounds, enabling efficient learning from\ndependent data generated by Markov processes, which may be of independent\ninterest. Building on these insights, we propose Bootstrapped Score Matching\n(BSM), a variance reduction technique that utilizes previously learned scores\nto improve accuracy at higher noise levels. These results provide crucial\ninsights into the efficiency and effectiveness of diffusion models for\ngenerative modeling.\n","authors":["Syamantak Kumar","Dheeraj Nagaraj","Purnamrita Sarkar"],"pdf_url":"https://arxiv.org/pdf/2502.10354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10353v1","updated":"2025-02-14T18:32:11Z","published":"2025-02-14T18:32:11Z","title":"Assortment Optimization for Patient-Provider Matching","summary":"  Rising provider turnover forces healthcare administrators to frequently\nrematch patients to available providers, which can be cumbersome and\nlabor-intensive. To reduce the burden of rematching, we study algorithms for\nmatching patients and providers through assortment optimization. We develop a\npatient-provider matching model in which we simultaneously offer each patient a\nmenu of providers, and patients subsequently respond and select providers. By\noffering assortments upfront, administrators can balance logistical ease and\npatient autonomy. We study policies for assortment optimization and\ncharacterize their performance under different problem settings. We demonstrate\nthat the selection of assortment policy is highly dependent on problem\nspecifics and, in particular, on a patient's willingness to match and the ratio\nbetween patients and providers. On real-world data, we show that our best\npolicy can improve match quality by 13% over a greedy solution by tailoring\nassortment sizes based on patient characteristics. We conclude with\nrecommendations for running a real-world patient-provider matching system\ninspired by our results.\n","authors":["Naveen Raman","Holly Wiberg"],"pdf_url":"https://arxiv.org/pdf/2502.10353v1.pdf","comment":"36 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2405.19272v4","updated":"2025-02-14T18:24:55Z","published":"2024-05-29T17:03:31Z","title":"Differentially Private Clustered Federated Learning","summary":"  Federated learning (FL), which is a decentralized machine learning (ML)\napproach, often incorporates differential privacy (DP) to provide rigorous data\nprivacy guarantees. Previous works attempted to address high structured data\nheterogeneity in vanilla FL settings through clustering clients (a.k.a\nclustered FL), but these methods remain sensitive and prone to errors, further\nexacerbated by the DP noise. This vulnerability makes the previous methods\ninappropriate for differentially private FL (DPFL) settings with structured\ndata heterogeneity. To address this gap, we propose an algorithm for\ndifferentially private clustered FL, which is robust to the DP noise in the\nsystem and identifies the underlying clients' clusters correctly. To this end,\nwe propose to cluster clients based on both their model updates and training\nloss values. Furthermore, for clustering clients' model updates at the end of\nthe first round, our proposed approach addresses the server's uncertainties by\nemploying large batch sizes as well as Gaussian Mixture Models (GMM) to reduce\nthe impact of DP and stochastic noise and avoid potential clustering errors.\nThis idea is efficient especially in privacy-sensitive scenarios with more DP\nnoise. We provide theoretical analysis to justify our approach and evaluate it\nacross diverse data distributions and privacy budgets. Our experimental results\nshow its effectiveness in addressing large structured data heterogeneity in\nDPFL.\n","authors":["Saber Malekmohammadi","Afaf Taik","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2405.19272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07964v2","updated":"2025-02-14T18:24:52Z","published":"2025-02-11T21:21:28Z","title":"New tools for comparing classical and neural ODE models for tumor growth","summary":"  A new computational tool TumorGrowth$.$jl for modeling tumor growth is\nintroduced. The tool allows the comparison of standard textbook models, such as\nGeneral Bertalanffy and Gompertz, with some newer models, including, for the\nfirst time, neural ODE models. As an application, we revisit a human meta-study\nof non-small cell lung cancer and bladder cancer lesions, in patients\nundergoing two different treatment options, to determine if previously reported\nperformance differences are statistically significant, and if newer, more\ncomplex models perform any better. In a population of examples with at least\nfour time-volume measurements available for calibration, and an average of\nabout 6.3, our main conclusion is that the General Bertalanffy model has\nsuperior performance, on average. However, where more measurements are\navailable, we argue that more complex models, capable of capturing rebound and\nrelapse behavior, may be better choices.\n","authors":["Anthony D. Blaom","Samuel Okon"],"pdf_url":"https://arxiv.org/pdf/2502.07964v2.pdf","comment":"9 pages, 2 figures. Related software is archived at\n  https://github.com/ablaom/TumorGrowth.jl"},{"id":"http://arxiv.org/abs/2406.03519v4","updated":"2025-02-14T18:16:23Z","published":"2024-06-05T17:41:42Z","title":"Noise-Aware Algorithm for Heterogeneous Differentially Private Federated\n  Learning","summary":"  High utility and rigorous data privacy are of the main goals of a federated\nlearning (FL) system, which learns a model from the data distributed among some\nclients. The latter has been tried to achieve by using differential privacy in\nFL (DPFL). There is often heterogeneity in clients privacy requirements, and\nexisting DPFL works either assume uniform privacy requirements for clients or\nare not applicable when server is not fully trusted (our setting). Furthermore,\nthere is often heterogeneity in batch and/or dataset size of clients, which as\nshown, results in extra variation in the DP noise level across clients model\nupdates. With these sources of heterogeneity, straightforward aggregation\nstrategies, e.g., assigning clients aggregation weights proportional to their\nprivacy parameters will lead to lower utility. We propose Robust-HDP, which\nefficiently estimates the true noise level in clients model updates and reduces\nthe noise-level in the aggregated model updates considerably. Robust-HDP\nimproves utility and convergence speed, while being safe to the clients that\nmay maliciously send falsified privacy parameter to server. Extensive\nexperimental results on multiple datasets and our theoretical analysis confirm\nthe effectiveness of Robust-HDP. Our code can be found here.\n","authors":["Saber Malekmohammadi","Yaoliang Yu","Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2406.03519v4.pdf","comment":"Proceedings of the 41 st International Conference on Machine\n  Learning, Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2410.13502v3","updated":"2025-02-14T18:15:01Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Sch√∂lkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10339v1","updated":"2025-02-14T17:59:58Z","published":"2025-02-14T17:59:58Z","title":"STAR: Spectral Truncation and Rescale for Model Merging","summary":"  Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.\n","authors":["Yu-Ang Lee","Ching-Yun Ko","Tejaswini Pedapati","I-Hsin Chung","Mi-Yen Yeh","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.10339v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.10335v1","updated":"2025-02-14T17:50:00Z","published":"2025-02-14T17:50:00Z","title":"Studying number theory with deep learning: a case study with the\n  M√∂bius and squarefree indicator functions","summary":"  Building on work of Charton, we train small transformer models to calculate\nthe M\\\"obius function $\\mu(n)$ and the squarefree indicator function\n$\\mu^2(n)$. The models attain nontrivial predictive power. We then iteratively\ntrain additional models to understand how the model functions, ultimately\nfinding a theoretical explanation.\n","authors":["David Lowry-Duda"],"pdf_url":"https://arxiv.org/pdf/2502.10335v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.09638v2","updated":"2025-02-14T17:49:54Z","published":"2024-06-14T00:07:52Z","title":"RASPNet: A Benchmark Dataset for Radar Adaptive Signal Processing\n  Applications","summary":"  We present a large-scale dataset for radar adaptive signal processing (RASP)\napplications to support the development of data-driven models within the\nadaptive radar community. The dataset, RASPNet, exceeds 16 TB in size and\ncomprises 100 realistic scenarios compiled over a variety of topographies and\nland types from across the contiguous United States. For each scenario, RASPNet\nconsists of 10,000 clutter realizations from an airborne radar setting, which\ncan be used to benchmark radar and complex-valued learning algorithms. RASPNet\nintends to fill a prominent gap in the availability of a large-scale, realistic\ndataset that standardizes the evaluation of adaptive radar processing\ntechniques and complex-valued neural networks. We outline its construction,\norganization, and several applications, including a transfer learning example\nto demonstrate how RASPNet can be used for realistic adaptive radar processing\nscenarios.\n","authors":["Shyam Venkatasubramanian","Bosung Kang","Ali Pezeshki","Muralidhar Rangaswamy","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2406.09638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10331v1","updated":"2025-02-14T17:43:19Z","published":"2025-02-14T17:43:19Z","title":"InfoPos: A ML-Assisted Solution Design Support Framework for Industrial\n  Cyber-Physical Systems","summary":"  The variety of building blocks and algorithms incorporated in data-centric\nand ML-assisted solutions is high, contributing to two challenges: selection of\nmost effective set and order of building blocks, as well as achieving such a\nselection with minimum cost. Considering that ML-assisted solution design is\ninfluenced by the extent of available data, as well as available knowledge of\nthe target system, it is advantageous to be able to select matching building\nblocks. We introduce the first iteration of our InfoPos framework, allowing the\nplacement of use-cases considering the available positions (levels), i.e., from\npoor to rich, of knowledge and data dimensions. With that input, designers and\ndevelopers can reveal the most effective corresponding choice(s), streamlining\nthe solution design process. The results from our demonstrator, an anomaly\nidentification use-case for industrial Cyber-Physical Systems, reflects\nachieved effects upon the use of different building blocks throughout knowledge\nand data positions. The achieved ML model performance is considered as the\nindicator. Our data processing code and the composed data sets are publicly\navailable.\n","authors":["Uraz Odyurt","Richard Loendersloot","Tiedo Tinga"],"pdf_url":"https://arxiv.org/pdf/2502.10331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10330v1","updated":"2025-02-14T17:43:08Z","published":"2025-02-14T17:43:08Z","title":"DiOpt: Self-supervised Diffusion for Constrained Optimization","summary":"  Recent advances in diffusion models show promising potential for\nlearning-based optimization by leveraging their multimodal sampling capability\nto escape local optima. However, existing diffusion-based optimization\napproaches, often reliant on supervised training, lacks a mechanism to ensure\nstrict constraint satisfaction which is often required in real-world\napplications. One resulting observation is the distributional misalignment,\ni.e. the generated solution distribution often exhibits small overlap with the\nfeasible domain. In this paper, we propose DiOpt, a novel diffusion paradigm\nthat systematically learns near-optimal feasible solution distributions through\niterative self-training. Our framework introduces several key innovations: a\ntarget distribution specifically designed to maximize overlap with the\nconstrained solution manifold; a bootstrapped self-training mechanism that\nadaptively weights candidate solutions based on the severity of constraint\nviolations and optimality gaps; and a dynamic memory buffer that accelerates\nconvergence by retaining high-quality solutions over training iterations. To\nour knowledge, DiOpt represents the first successful integration of\nself-supervised diffusion with hard constraint satisfaction. Evaluations on\ndiverse tasks, including power grid control, motion retargeting, wireless\nallocation demonstrate its superiority in terms of both optimality and\nconstraint satisfaction.\n","authors":["Shutong Ding","Yimiao Zhou","Ke Hu","Xi Yao","Junchi Yan","Xiaoying Tang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2502.10330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10328v1","updated":"2025-02-14T17:41:44Z","published":"2025-02-14T17:41:44Z","title":"Generalised Parallel Tempering: Flexible Replica Exchange via Flows and\n  Diffusions","summary":"  Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging\nparallel computation to sample efficiently from high-dimensional, multimodal or\notherwise complex distributions via annealing. One limitation of the standard\nformulation of PT is the growth of computational resources required to generate\nhigh-quality samples, as measured by effective sample size or round trip rate,\nfor increasingly challenging distributions. To address this issue, we propose\nthe framework: Generalised Parallel Tempering (GePT) which allows for the\nincorporation of recent advances in modern generative modelling, such as\nnormalising flows and diffusion models, within Parallel Tempering, while\nmaintaining the same theoretical guarantees as MCMC-based methods. For\ninstance, we show that this allows us to utilise diffusion models in a\nparallelised manner, bypassing the usual computational cost of a large number\nof steps to generate quality samples. Further, we empirically demonstrate that\nGePT can improve sample quality and reduce the growth of computational\nresources required to handle complex distributions over the classical\nalgorithm.\n","authors":["Leo Zhang","Peter Potaptchik","Arnaud Doucet","Hai-Dang Dau","Saifuddin Syed"],"pdf_url":"https://arxiv.org/pdf/2502.10328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03391v2","updated":"2025-02-14T17:39:17Z","published":"2025-02-05T17:29:12Z","title":"Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise\n  Sufficient Reasons","summary":"  *Minimal sufficient reasons* represent a prevalent form of explanation - the\nsmallest subset of input features which, when held constant at their\ncorresponding values, ensure that the prediction remains unchanged. Previous\n*post-hoc* methods attempt to obtain such explanations but face two main\nlimitations: (1) Obtaining these subsets poses a computational challenge,\nleading most scalable methods to converge towards suboptimal, less meaningful\nsubsets; (2) These methods heavily rely on sampling out-of-distribution input\nassignments, potentially resulting in counterintuitive behaviors. To tackle\nthese limitations, we propose in this work a self-supervised training approach,\nwhich we term *sufficient subset training* (SST). Using SST, we train models to\ngenerate concise sufficient reasons for their predictions as an integral part\nof their output. Our results indicate that our framework produces succinct and\nfaithful subsets substantially more efficiently than competing post-hoc\nmethods, while maintaining comparable predictive performance.\n","authors":["Shahaf Bassan","Ron Eliav","Shlomit Gur"],"pdf_url":"https://arxiv.org/pdf/2502.03391v2.pdf","comment":"To appear in ICLR 2025"},{"id":"http://arxiv.org/abs/2212.02895v4","updated":"2025-02-14T17:35:40Z","published":"2022-12-06T11:38:22Z","title":"Training Neural Networks on Data Sources with Unknown Reliability","summary":"  When data is generated by multiple sources, conventional training methods\nupdate models assuming equal reliability for each source and do not consider\ntheir individual data quality. However, in many applications, sources have\nvaried levels of reliability that can have negative effects on the performance\nof a neural network. A key issue is that often the quality of the data for\nindividual sources is not known during training. Previous methods for training\nmodels in the presence of noisy data do not make use of the additional\ninformation that the source label can provide. Focusing on supervised learning,\nwe aim to train neural networks on each data source for a number of steps\nproportional to the source's estimated reliability by using a dynamic\nre-weighting strategy motivated by likelihood tempering. This way, we allow\ntraining on all sources during the warm-up and reduce learning on less reliable\nsources during the final training stages, when it has been shown that models\noverfit to noise. We show through diverse experiments that this can\nsignificantly improve model performance when trained on mixtures of reliable\nand unreliable data sources, and maintain performance when models are trained\non reliable sources only.\n","authors":["Alexander Capstick","Francesca Palermo","Tianyu Cui","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2212.02895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10325v1","updated":"2025-02-14T17:34:28Z","published":"2025-02-14T17:34:28Z","title":"Process Reward Models for LLM Agents: Practical Framework and Directions","summary":"  We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.\n","authors":["Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.10325v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.13740v2","updated":"2025-02-14T17:30:15Z","published":"2024-03-20T16:47:28Z","title":"Uncertainty-Aware Explanations Through Probabilistic Self-Explainable\n  Neural Networks","summary":"  The lack of transparency of Deep Neural Networks continues to be a limitation\nthat severely undermines their reliability and usage in high-stakes\napplications. Promising approaches to overcome such limitations are\nPrototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions\nrely on the similarity between the input at hand and a set of prototypical\nrepresentations of the output classes, offering therefore a deep, yet\ntransparent-by-design, architecture. In this paper, we introduce a\nprobabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point\nestimates for the prototypes with probability distributions over their values.\nThis provides not only a more flexible framework for an end-to-end learning of\nprototypes, but can also capture the explanatory uncertainty of the model,\nwhich is a missing feature in previous approaches. In addition, since the\nprototypes determine both the explanation and the prediction, Prob-PSENNs allow\nus to detect when the model is making uninformed or uncertain predictions, and\nto obtain valid explanations for them. Our experiments demonstrate that\nProb-PSENNs provide more meaningful and robust explanations than their\nnon-probabilistic counterparts, while remaining competitive in terms of\npredictive performance, thus enhancing the explainability and reliability of\nthe models.\n","authors":["Jon Vadillo","Roberto Santana","Jose A. Lozano","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2403.13740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07516v2","updated":"2025-02-14T17:24:56Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study presents the first systematic attempt to\nidentify prompts and text tokens in MIMIC-CXR that contribute the most to\ntraining data memorization. Our analysis reveals two unexpected findings: (1)\nprompts containing traces of de-identification procedures (markers introduced\nto hide Protected Health Information) are the most memorized, and (2) among all\ntokens, de-identification markers contribute the most towards memorization.\nThis highlights a broader issue with the standard anonymization practices and\nT2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time\nmemorization mitigation strategies are ineffective and fail to sufficiently\nreduce the model's reliance on memorized text tokens. On this front, we propose\nactionable strategies for different stakeholders to enhance privacy and improve\nthe reliability of generative models in medical imaging. Finally, our results\nprovide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset. The anonymized code is available at\nhttps://anonymous.4open.science/r/diffusion_memorization-8011/\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10311v1","updated":"2025-02-14T17:14:02Z","published":"2025-02-14T17:14:02Z","title":"ExplainReduce: Summarising local explanations via proxies","summary":"  Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.\n","authors":["Lauri Sepp√§l√§inen","Mudong Guo","Kai Puolam√§ki"],"pdf_url":"https://arxiv.org/pdf/2502.10311v1.pdf","comment":"22 pages with a 7 page appendix, 7 + 5 figures, 2 tables. The\n  datasets and source code used in the paper are available at\n  https://github.com/edahelsinki/explainreduce"},{"id":"http://arxiv.org/abs/2502.10308v1","updated":"2025-02-14T17:12:20Z","published":"2025-02-14T17:12:20Z","title":"LLM-Powered Preference Elicitation in Combinatorial Assignment","summary":"  We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting.\n","authors":["Ermis Soumalias","Yanchen Jiang","Kehang Zhu","Michael Curry","Sven Seuken","David C. Parkes"],"pdf_url":"https://arxiv.org/pdf/2502.10308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10307v1","updated":"2025-02-14T17:10:17Z","published":"2025-02-14T17:10:17Z","title":"SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer\n  learning using Foundation Models","summary":"  Traditional solar forecasting models are based on several years of\nsite-specific historical irradiance data, often spanning five or more years,\nwhich are unavailable for newer photovoltaic farms. As renewable energy is\nhighly intermittent, building accurate solar irradiance forecasting systems is\nessential for efficient grid management and enabling the ongoing proliferation\nof solar energy, which is crucial to achieve the United Nations' net zero\ngoals. In this work, we propose SPIRIT, a novel approach leveraging foundation\nmodels for solar irradiance forecasting, making it applicable to newer solar\ninstallations. Our approach outperforms state-of-the-art models in zero-shot\ntransfer learning by about 70%, enabling effective performance at new locations\nwithout relying on any historical data. Further improvements in performance are\nachieved through fine-tuning, as more location-specific data becomes available.\nThese findings are supported by statistical significance, further validating\nour approach. SPIRIT represents a pivotal step towards rapid, scalable, and\nadaptable solar forecasting solutions, advancing the integration of renewable\nenergy into global power systems.\n","authors":["Aditya Mishra","Ravindra T","Srinivasan Iyengar","Shivkumar Kalyanaraman","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2502.10307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01706v2","updated":"2025-02-14T17:05:36Z","published":"2023-02-03T13:04:12Z","title":"VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated\n  Learning","summary":"  This paper presents the application of Vertical Federated Learning (VFL) to\ngenerate synthetic tabular data using Generative Adversarial Networks (GANs).\nVFL is a collaborative approach to train machine learning models among distinct\ntabular data holders, such as financial institutions, who possess disjoint\nfeatures for the same group of customers. In this paper we introduce the VT-GAN\nframework, Vertical federated Tabular GAN, and demonstrate that VFL can be\nsuccessfully used to implement GANs for distributed tabular data in\nprivacy-preserving manner, with performance close to centralized GANs that\nassume shared data. We make design choices with respect to the distribution of\nGAN generator and discriminator models and introduce a training-with-shuffling\ntechnique so that no party can reconstruct training data from the GAN\nconditional vector. The paper presents (1) an implementation of VT-GAN, (2) a\ndetailed quality evaluation of the VT-GAN-generated synthetic data, (3) an\noverall scalability examination of VT-GAN framework, (4) a security analysis on\nVT-GAN's robustness against Membership Inference Attack with different settings\nof Differential Privacy, for a range of datasets with diverse distribution\ncharacteristics. Our results demonstrate that VT-GAN can consistently generate\nhigh-fidelity synthetic tabular data of comparable quality to that generated by\na centralized GAN algorithm. The difference in machine learning utility can be\nas low as 2.7%, even under extremely imbalanced data distributions across\nclients or with different numbers of clients.\n","authors":["Zilong Zhao","Han Wu","Aad Van Moorsel","Lydia Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2302.01706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10297v1","updated":"2025-02-14T16:59:05Z","published":"2025-02-14T16:59:05Z","title":"DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders","summary":"  Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers.\n","authors":["Julien Siems","Timur Carstensen","Arber Zela","Frank Hutter","Massimiliano Pontil","Riccardo Grazzi"],"pdf_url":"https://arxiv.org/pdf/2502.10297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10295v1","updated":"2025-02-14T16:57:43Z","published":"2025-02-14T16:57:43Z","title":"Fenchel-Young Variational Learning","summary":"  From a variational perspective, many statistical learning criteria involve\nseeking a distribution that balances empirical risk and regularization. In this\npaper, we broaden this perspective by introducing a new general class of\nvariational methods based on Fenchel-Young (FY) losses, treated as divergences\nthat generalize (and encompass) the familiar Kullback-Leibler divergence at the\ncore of classical variational learning. Our proposed formulation -- FY\nvariational learning -- includes as key ingredients new notions of FY free\nenergy, FY evidence, FY evidence lower bound, and FY posterior. We derive\nalternating minimization and gradient backpropagation algorithms to compute (or\nlower bound) the FY evidence, which enables learning a wider class of models\nthan previous variational formulations. This leads to generalized FY variants\nof classical algorithms, such as an FY expectation-maximization (FYEM)\nalgorithm, and latent-variable models, such as an FY variational autoencoder\n(FYVAE). Our new methods are shown to be empirically competitive, often\noutperforming their classical counterparts, and most importantly, to have\nqualitatively novel features. For example, FYEM has an adaptively sparse\nE-step, while the FYVAE can support models with sparse observations and sparse\nposteriors.\n","authors":["Sophia Sklaviadis","Sweta Agrawal","Antonio Farinhas","Andre Martins","Mario Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2502.10295v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.04799v2","updated":"2025-02-14T16:53:45Z","published":"2025-02-07T10:10:10Z","title":"A Regularized Newton Method for Nonconvex Optimization with Global and\n  Local Complexity Guarantees","summary":"  We consider the problem of finding an $\\epsilon$-stationary point of a\nnonconvex function with a Lipschitz continuous Hessian and propose a quadratic\nregularized Newton method incorporating a new class of regularizers constructed\nfrom the current and previous gradients. The method leverages a recently\ndeveloped linear conjugate gradient approach with a negative curvature monitor\nto solve the regularized Newton equation. Notably, our algorithm is adaptive,\nrequiring no prior knowledge of the Lipschitz constant of the Hessian, and\nachieves a global complexity of $O(\\epsilon^{-\\frac{3}{2}}) + \\tilde O(1)$ in\nterms of the second-order oracle calls, and $\\tilde O(\\epsilon^{-\\frac{7}{4}})$\nfor Hessian-vector products, respectively. Moreover, when the iterates converge\nto a point where the Hessian is positive definite, the method exhibits\nquadratic local convergence. Preliminary numerical results illustrate the\ncompetitiveness of our algorithm.\n","authors":["Yuhao Zhou","Jintao Xu","Chenglong Bao","Chao Ding","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.04799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10292v1","updated":"2025-02-14T16:52:50Z","published":"2025-02-14T16:52:50Z","title":"Small Loss Bounds for Online Learning Separated Function Classes: A\n  Gaussian Process Perspective","summary":"  In order to develop practical and efficient algorithms while circumventing\noverly pessimistic computational lower bounds, recent work has been interested\nin developing oracle-efficient algorithms in a variety of learning settings.\nTwo such settings of particular interest are online and differentially private\nlearning. While seemingly different, these two fields are fundamentally\nconnected by the requirement that successful algorithms in each case satisfy\nstability guarantees; in particular, recent work has demonstrated that\nalgorithms for online learning whose performance adapts to beneficial problem\ninstances, attaining the so-called small-loss bounds, require a form of\nstability similar to that of differential privacy. In this work, we identify\nthe crucial role that separation plays in allowing oracle-efficient algorithms\nto achieve this strong stability. Our notion, which we term $\\rho$-separation,\ngeneralizes and unifies several previous approaches to enforcing this strong\nstability, including the existence of small-separator sets and the recent\nnotion of $\\gamma$-approximability. We present an oracle-efficient algorithm\nthat is capable of achieving small-loss bounds with improved rates in greater\ngenerality than previous work, as well as a variant for differentially private\nlearning that attains optimal rates, again under our separation condition. In\nso doing, we prove a new stability result for minimizers of a Gaussian process\nthat strengthens and generalizes previous work.\n","authors":["Adam Block","Abhishek Shetty"],"pdf_url":"https://arxiv.org/pdf/2502.10292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10288v1","updated":"2025-02-14T16:50:33Z","published":"2025-02-14T16:50:33Z","title":"Adversarial Mixup Unlearning","summary":"  Machine unlearning is a critical area of research aimed at safeguarding data\nprivacy by enabling the removal of sensitive information from machine learning\nmodels. One unique challenge in this field is catastrophic unlearning, where\nerasing specific data from a well-trained model unintentionally removes\nessential knowledge, causing the model to deviate significantly from a\nretrained one. To address this, we introduce a novel approach that regularizes\nthe unlearning process by utilizing synthesized mixup samples, which simulate\nthe data susceptible to catastrophic effects. At the core of our approach is a\ngenerator-unlearner framework, MixUnlearn, where a generator adversarially\nproduces challenging mixup examples, and the unlearner effectively forgets\ntarget information based on these synthesized data. Specifically, we first\nintroduce a novel contrastive objective to train the generator in an\nadversarial direction: generating examples that prompt the unlearner to reveal\ninformation that should be forgotten, while losing essential knowledge. Then\nthe unlearner, guided by two other contrastive loss terms, processes the\nsynthesized and real data jointly to ensure accurate unlearning without losing\ncritical knowledge, overcoming catastrophic effects. Extensive evaluations\nacross benchmark datasets demonstrate that our method significantly outperforms\nstate-of-the-art approaches, offering a robust solution to machine unlearning.\nThis work not only deepens understanding of unlearning mechanisms but also lays\nthe foundation for effective machine unlearning with mixup augmentation.\n","authors":["Zhuoyi Peng","Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10288v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2409.17115v2","updated":"2025-02-14T16:44:08Z","published":"2024-09-25T17:28:13Z","title":"Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale","summary":"  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX\n","authors":["Fan Zhou","Zengzhi Wang","Qian Liu","Junlong Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17115v2.pdf","comment":"47 pages, 13 figures, 34 tables"},{"id":"http://arxiv.org/abs/2502.10280v1","updated":"2025-02-14T16:37:21Z","published":"2025-02-14T16:37:21Z","title":"Probabilistic Super-Resolution for High-Fidelity Physical System\n  Simulations with Uncertainty Quantification","summary":"  Super-resolution (SR) is a promising tool for generating high-fidelity\nsimulations of physical systems from low-resolution data, enabling fast and\naccurate predictions in engineering applications. However, existing\ndeep-learning based SR methods, require large labeled datasets and lack\nreliable uncertainty quantification (UQ), limiting their applicability in\nreal-world scenarios. To overcome these challenges, we propose a probabilistic\nSR framework that leverages the Statistical Finite Element Method and\nenergy-based generative modeling. Our method enables efficient high-resolution\npredictions with inherent UQ, while eliminating the need for extensive labeled\ndatasets. The method is validated on a 2D Poisson example and compared with\nbicubic interpolation upscaling. Results demonstrate a computational speed-up\nover high-resolution numerical solvers while providing reliable uncertainty\nestimates.\n","authors":["Pengyu Zhang","Connor Duffin","Alex Glyn-Davies","Arnaud Vadeboncoeur","Mark Girolami"],"pdf_url":"https://arxiv.org/pdf/2502.10280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01980v3","updated":"2025-02-14T16:35:51Z","published":"2024-09-03T15:22:41Z","title":"Large Language Models for Anomaly and Out-of-Distribution Detection: A\n  Survey","summary":"  Detecting anomalies or out-of-distribution (OOD) samples is critical for\nmaintaining the reliability and trustworthiness of machine learning systems.\nRecently, Large Language Models (LLMs) have demonstrated their effectiveness\nnot only in natural language processing but also in broader applications due to\ntheir advanced comprehension and generative capabilities. The integration of\nLLMs into anomaly and OOD detection marks a significant shift from the\ntraditional paradigm in the field. This survey focuses on the problem of\nanomaly and OOD detection under the context of LLMs. We propose a new taxonomy\nto categorize existing approaches into two classes based on the role played by\nLLMs. Following our proposed taxonomy, we further discuss the related work\nunder each of the categories and finally discuss potential challenges and\ndirections for future research in this field. We also provide an up-to-date\nreading list of relevant papers.\n","authors":["Ruiyao Xu","Kaize Ding"],"pdf_url":"https://arxiv.org/pdf/2409.01980v3.pdf","comment":"Accepted to NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2411.06426v2","updated":"2025-02-14T16:32:54Z","published":"2024-11-10T11:08:28Z","title":"SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains","summary":"  As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.\n","authors":["Bijoy Ahmed Saiem","MD Sadik Hossain Shanto","Rakib Ahsan","Md Rafi ur Rashid"],"pdf_url":"https://arxiv.org/pdf/2411.06426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10263v1","updated":"2025-02-14T16:16:02Z","published":"2025-02-14T16:16:02Z","title":"Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers","summary":"  Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.\n","authors":["Aivin V. Solatorio","Rafael Macalaba","James Liounis"],"pdf_url":"https://arxiv.org/pdf/2502.10263v1.pdf","comment":"Project GitHub repository at https://github.com/worldbank/ai4data-use"},{"id":"http://arxiv.org/abs/2410.20856v2","updated":"2025-02-14T16:09:49Z","published":"2024-10-28T09:19:29Z","title":"Strada-LLM: Graph LLM for traffic prediction","summary":"  Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.\n","authors":["Seyed Mohamad Moghadas","Yangxintong Lyu","Bruno Cornelis","Alexandre Alahi","Adrian Munteanu"],"pdf_url":"https://arxiv.org/pdf/2410.20856v2.pdf","comment":"The reviewers decided to reject it. After getting the reviews, we\n  wanted to study more."},{"id":"http://arxiv.org/abs/2502.10239v1","updated":"2025-02-14T15:49:02Z","published":"2025-02-14T15:49:02Z","title":"Efficient Zero-Order Federated Finetuning of Language Models for\n  Resource-Constrained Devices","summary":"  Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning.\n","authors":["Mohamed Aboelenien Ahmed","Kilian Pfeiffer","Ramin Khalili","Heba Khdr","J√∂rg Henkel"],"pdf_url":"https://arxiv.org/pdf/2502.10239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07780v2","updated":"2025-02-14T15:46:51Z","published":"2024-06-12T00:19:40Z","title":"A Critical Look At Tokenwise Reward-Guided Text Generation","summary":"  Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.\n","authors":["Ahmad Rashid","Ruotian Wu","Julia Grosse","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2406.07780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10236v1","updated":"2025-02-14T15:46:37Z","published":"2025-02-14T15:46:37Z","title":"Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise\n  Control","summary":"  Diffusion Probabilistic Models (DPMs) are powerful generative models that\nhave achieved unparalleled success in a number of generative tasks. In this\nwork, we aim to build inductive biases into the training and sampling of\ndiffusion models to better accommodate the target distribution of the data to\nmodel. For topologically structured data, we devise a frequency-based noising\noperator to purposefully manipulate, and set, these inductive biases. We first\nshow that appropriate manipulations of the noising forward process can lead\nDPMs to focus on particular aspects of the distribution to learn. We show that\ndifferent datasets necessitate different inductive biases, and that appropriate\nfrequency-based noise control induces increased generative performance compared\nto standard diffusion. Finally, we demonstrate the possibility of ignoring\ninformation at particular frequencies while learning. We show this in an image\ncorruption and recovery task, where we train a DPM to recover the original\ntarget distribution after severe noise corruption.\n","authors":["Thomas Jiralerspong","Berton Earnshaw","Jason Hartford","Yoshua Bengio","Luca Scimeca"],"pdf_url":"https://arxiv.org/pdf/2502.10236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10235v1","updated":"2025-02-14T15:46:19Z","published":"2025-02-14T15:46:19Z","title":"AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting","summary":"  Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https://github.com/abenechehab/AdaPTS.\n","authors":["Abdelhakim Benechehab","Vasilii Feofanov","Giuseppe Paolo","Albert Thomas","Maurizio Filippone","Bal√°zs K√©gl"],"pdf_url":"https://arxiv.org/pdf/2502.10235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03119v2","updated":"2025-02-14T15:42:44Z","published":"2024-10-04T03:26:42Z","title":"Spatial-aware decision-making with ring attractors in reinforcement\n  learning systems","summary":"  This paper explores the integration of ring attractors, a mathematical model\ninspired by neural circuit dynamics, into the Reinforcement Learning (RL)\naction selection process. Serving as specialized brain-inspired structures that\nencode spatial information and uncertainty, ring attractors offer a\nbiologically plausible mechanism to improve learning speed and accuracy in RL.\nThey do so by explicitly encoding the action space, facilitating the\norganization of neural activity, and enabling the distribution of spatial\nrepresentations across the neural network in the context of Deep Reinforcement\nLearning (DRL). For example, preserving the continuity between rotation angles\nin robotic control or adjacency between tactical moves in game-like\nenvironments. The application of ring attractors in the action selection\nprocess involves mapping actions to specific locations on the ring and decoding\nthe selected action based on neural activity. We investigate the application of\nring attractors by both building an exogenous model and integrating them as\npart of DRL agents. Our approach significantly improves state-of-the-art\nperformance on the Atari 100k benchmark, achieving a 53\\% increase in\nperformance across selected state-of-the-art baselines. Codebase available at\nhttps://anonymous.4open.science/r/RA_RL-8026.\n","authors":["Marcos Negre Saura","Richard Allmendinger","Wei Pan","Theodore Papamarkou"],"pdf_url":"https://arxiv.org/pdf/2410.03119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10233v1","updated":"2025-02-14T15:42:30Z","published":"2025-02-14T15:42:30Z","title":"Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via\n  Hierarchical and Parallel Decoding","summary":"  The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge\nin warehouse logistics, where pickers must navigate a mixed-shelves environment\nto retrieve SKUs efficiently. Traditional heuristics and optimization-based\napproaches struggle with scalability, while recent machine learning methods\noften rely on sequential decision-making, leading to high solution latency and\nsuboptimal agent coordination. In this work, we propose a novel hierarchical\nand parallel decoding approach for solving the min-max variant of the MSPRP via\nmulti-agent reinforcement learning. While our approach generates a joint\ndistribution over agent actions, allowing for fast decoding and effective\npicker coordination, our method introduces a sequential action selection to\navoid conflicts in the multi-dimensional action space. Experiments show\nstate-of-the-art performance in both solution quality and inference speed,\nparticularly for large-scale and out-of-distribution instances. Our code is\npublicly available at http://github.com/LTluttmann/marl4msprp.\n","authors":["Laurin Luttmann","Lin Xie"],"pdf_url":"https://arxiv.org/pdf/2502.10233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00677v5","updated":"2025-02-14T15:34:58Z","published":"2023-07-02T22:30:08Z","title":"SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary\n  Directed Differential with Normalized Density and Self-Adaption","summary":"  Density-based clustering is the most popular clustering algorithm since it\ncan identify clusters of arbitrary shape as long as they are separated by\nlow-density regions. However, a high-density region that is not separated by\nlow-density ones might also have different structures belonging to multiple\nclusters. As far as we know, all previous density-based clustering algorithms\nfail to detect such structures. In this paper, we provide a novel density-based\nclustering scheme to address this problem. It is the rst clustering algorithm\nthat can detect meticulous structures in a high-density region that is not\nseparated by low-density ones and thus extends the range of applications of\nclustering. The algorithm employs secondary directed differential, hierarchy,\nnormalized density, as well as the self-adaption coefficient, called Structure\nDetecting Cluster by Hierarchical Secondary Directed Differential with\nNormalized Density and Self-Adaption, dubbed SDC-HSDD-NDSA. Experiments on\nsynthetic and real datasets are implemented to verify the effectiveness,\nrobustness, and granularity independence of the algorithm, and the scheme is\ncompared to unsupervised schemes in the Python package Scikit-learn. Results\ndemonstrate that our algorithm outperforms previous ones in many situations,\nespecially significantly when clusters have regular internal structures. For\nexample, averaging over the eight noiseless synthetic datasets with structures\nemploying ARI and NMI criteria, previous algorithms obtain scores below 0.6 and\n0.7, while the presented algorithm obtains scores higher than 0.9 and 0.95,\nrespectively.\n","authors":["Hao Shu"],"pdf_url":"https://arxiv.org/pdf/2307.00677v5.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2502.10230v1","updated":"2025-02-14T15:34:36Z","published":"2025-02-14T15:34:36Z","title":"ProReco: A Process Discovery Recommender System","summary":"  Process discovery aims to automatically derive process models from historical\nexecution data (event logs). While various process discovery algorithms have\nbeen proposed in the last 25 years, there is no consensus on a dominating\ndiscovery algorithm. Selecting the most suitable discovery algorithm remains a\nchallenge due to competing quality measures and diverse user requirements.\nManually selecting the most suitable process discovery algorithm from a range\nof options for a given event log is a time-consuming and error-prone task. This\npaper introduces ProReco, a Process discovery Recommender system designed to\nrecommend the most appropriate algorithm based on user preferences and event\nlog characteristics. ProReco incorporates state-of-the-art discovery\nalgorithms, extends the feature pools from previous work, and utilizes\neXplainable AI (XAI) techniques to provide explanations for its\nrecommendations.\n","authors":["Tsung-Hao Huang","Tarek Junied","Marco Pegoraro","Wil M. P. van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2502.10230v1.pdf","comment":"8 pages, 5 figures, 9 references"},{"id":"http://arxiv.org/abs/2501.02844v3","updated":"2025-02-14T15:32:00Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08346v2","updated":"2025-02-14T15:25:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04409v2","updated":"2025-02-14T15:22:09Z","published":"2024-12-05T18:31:14Z","title":"Stabilizing and Solving Inverse Problems using Data and Machine Learning","summary":"  We consider an inverse problem involving the reconstruction of the solution\nto a nonlinear partial differential equation (PDE) with unknown boundary\nconditions. Instead of direct boundary data, we are provided with a large\ndataset of boundary observations for typical solutions (collective data) and a\nbulk measurement of a specific realization. To leverage this collective data,\nwe first compress the boundary data using proper orthogonal decomposition (POD)\nin a linear expansion. Next, we identify a possible nonlinear low-dimensional\nstructure in the expansion coefficients using an autoencoder, which provides a\nparametrization of the dataset in a lower-dimensional latent space. We then\ntrain an operator network to map the expansion coefficients representing the\nboundary data to the finite element solution of the PDE. Finally, we connect\nthe autoencoder's decoder to the operator network which enables us to solve the\ninverse problem by optimizing a data-fitting term over the latent space. We\nanalyze the underlying stabilized finite element method in the linear setting\nand establish an optimal error estimate in the $H^1$-norm. The nonlinear\nproblem is then studied numerically, demonstrating the effectiveness of our\napproach.\n","authors":["Erik Burman","Mats G. Larson","Karl Larsson","Carl Lundholm"],"pdf_url":"https://arxiv.org/pdf/2412.04409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04184v2","updated":"2025-02-14T15:19:01Z","published":"2024-06-06T15:40:29Z","title":"Shield Synthesis for LTL Modulo Theories","summary":"  In recent years, Machine Learning (ML) models have achieved remarkable\nsuccess in various domains. However, these models also tend to demonstrate\nunsafe behaviors, precluding their deployment in safety-critical systems. To\ncope with this issue, ample research focuses on developing methods that\nguarantee the safe behaviour of a given ML model. A prominent example is\nshielding which incorporates an external component (a ``shield'') that blocks\nunwanted behavior. Despite significant progress, shielding suffers from a main\nsetback: it is currently geared towards properties encoded solely in\npropositional logics (e.g., LTL) and is unsuitable for richer logics. This, in\nturn, limits the widespread applicability of shielding in many real-world\nsystems. In this work, we address this gap, and extend shielding to LTL modulo\ntheories, by building upon recent advances in reactive synthesis modulo\ntheories. This allowed us to develop a novel approach for generating shields\nconforming to complex safety specifications in these more expressive, logics.\nWe evaluated our shields and demonstrate their ability to handle rich data with\ntemporal dynamics. To the best of our knowledge, this is the first approach for\nsynthesizing shields for such expressivity.\n","authors":["Andoni Rodriguez","Guy Amir","Davide Corsi","Cesar Sanchez","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2406.04184v2.pdf","comment":"To appear in AAAI 2025"},{"id":"http://arxiv.org/abs/2405.14596v2","updated":"2025-02-14T15:13:55Z","published":"2024-05-23T14:11:26Z","title":"Linear Mode Connectivity in Differentiable Tree Ensembles","summary":"  Linear Mode Connectivity (LMC) refers to the phenomenon that performance\nremains consistent for linearly interpolated models in the parameter space. For\nindependently optimized model pairs from different random initializations,\nachieving LMC is considered crucial for understanding the stable success of the\nnon-convex optimization in modern machine learning models and for facilitating\npractical parameter-based operations such as model merging. While LMC has been\nachieved for neural networks by considering the permutation invariance of\nneurons in each hidden layer, its attainment for other models remains an open\nquestion. In this paper, we first achieve LMC for soft tree ensembles, which\nare tree-based differentiable models extensively used in practice. We show the\nnecessity of incorporating two invariances: subtree flip invariance and\nsplitting order invariance, which do not exist in neural networks but are\ninherent to tree architectures, in addition to permutation invariance of trees.\nMoreover, we demonstrate that it is even possible to exclude such additional\ninvariances while keeping LMC by designing decision list-based tree\narchitectures, where such invariances do not exist by definition. Our findings\nindicate the significance of accounting for architecture-specific invariances\nin achieving LMC.\n","authors":["Ryuichi Kanoh","Mahito Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2405.14596v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10216v1","updated":"2025-02-14T15:10:43Z","published":"2025-02-14T15:10:43Z","title":"Forget the Data and Fine-Tuning! Just Fold the Network to Compress","summary":"  We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments.\n","authors":["Dong Wang","Haris ≈†ikiƒá","Lothar Thiele","Olga Saukh"],"pdf_url":"https://arxiv.org/pdf/2502.10216v1.pdf","comment":"This paper has been accepted by The Thirteenth International\n  Conference on Learning Representations(ICLR), 2025"},{"id":"http://arxiv.org/abs/2502.10215v1","updated":"2025-02-14T15:09:15Z","published":"2025-02-14T15:09:15Z","title":"Do Large Language Models Reason Causally Like Us? Even Better?","summary":"  Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making.\n","authors":["Hanna M. Dettki","Brenden M. Lake","Charley M. Wu","Bob Rehder"],"pdf_url":"https://arxiv.org/pdf/2502.10215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10214v1","updated":"2025-02-14T15:08:37Z","published":"2025-02-14T15:08:37Z","title":"Mapping bathymetry of inland water bodies on the North Slope of Alaska\n  with Landsat using Random Forest","summary":"  The North Slope of Alaska is dominated by small waterbodies that provide\ncritical ecosystem services for local population and wildlife. Detailed\ninformation on the depth of the waterbodies is scarce due to the challenges\nwith collecting such information. In this work we have trained a machine\nlearning (Random Forest Regressor) model to predict depth from multispectral\nLandsat data in waterbodies across the North Slope of Alaska. The greatest\nchallenge is the scarcity of in situ data, which is expensive and difficult to\nobtain, to train the model. We overcame this challenge by using modeled depth\npredictions from a prior study as synthetic training data to provide a more\ndiverse training data pool for the Random Forest. The final Random Forest model\nwas more robust than models trained directly on the in situ data and when\napplied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall\n$r^{2}$ value of 0.76 on validation. The final map has been made available\nthrough the Oak Ridge National Laboratory Distribute Active Archive Center\n(ORNL-DAAC). This map represents a first of its kind regional assessment of\nwaterbody depth with per pixel estimates of depth for the entire North Slope of\nAlaska.\n","authors":["Mark L. Carroll","Margaret R. Wooten","Claire E. Simpson","Caleb S. Spradlin","Melanie J. Frost","Mariana Blanco-Rojas","Zachary W. Williams","Jordan A. Caraballo-Vega","Christopher S. R. Neigh"],"pdf_url":"https://arxiv.org/pdf/2502.10214v1.pdf","comment":"24 Pages, 6 Figures, 1 Table. This article is a US Government work.\n  Landsat data from the US Geological Survey Earth Explorer system:\n  https://earthexplorer.usgs.gov. Sonar training measurements:\n  https://doi.org/10.18739/A2JD4PP1H. Output maps from the Oak Ridge National\n  Laboratory Distribute Active Archive Center (ORNL-DAAC):\n  https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=2243"},{"id":"http://arxiv.org/abs/2502.10211v1","updated":"2025-02-14T15:06:59Z","published":"2025-02-14T15:06:59Z","title":"Control-flow anomaly detection by process mining-based feature\n  extraction and dimensionality reduction","summary":"  The business processes of organizations may deviate from normal control flow\ndue to disruptive anomalies, including unknown, skipped, and wrongly-ordered\nactivities. To identify these control-flow anomalies, process mining can check\ncontrol-flow correctness against a reference process model through conformance\nchecking, an explainable set of algorithms that allows linking any deviations\nwith model elements. However, the effectiveness of conformance checking-based\ntechniques is negatively affected by noisy event data and low-quality process\nmodels. To address these shortcomings and support the development of\ncompetitive and explainable conformance checking-based techniques for\ncontrol-flow anomaly detection, we propose a novel process mining-based feature\nextraction approach with alignment-based conformance checking. This variant\naligns the deviating control flow with a reference process model; the resulting\nalignment can be inspected to extract additional statistics such as the number\nof times a given activity caused mismatches. We integrate this approach into a\nflexible and explainable framework for developing techniques for control-flow\nanomaly detection. The framework combines process mining-based feature\nextraction and dimensionality reduction to handle high-dimensional feature\nsets, achieve detection effectiveness, and support explainability. The results\nshow that the framework techniques implementing our approach outperform the\nbaseline conformance checking-based techniques while maintaining the\nexplainable nature of conformance checking. We also provide an explanation of\nwhy existing conformance checking-based techniques may be ineffective.\n","authors":["Francesco Vitale","Marco Pegoraro","Wil M. P. van der Aalst","Nicola Mazzocca"],"pdf_url":"https://arxiv.org/pdf/2502.10211v1.pdf","comment":"16 pages, 9 figures, 7 tables, 56 references"},{"id":"http://arxiv.org/abs/2502.10208v1","updated":"2025-02-14T15:03:43Z","published":"2025-02-14T15:03:43Z","title":"SGS-GNN: A Supervised Graph Sparsification method for Graph Neural\n  Networks","summary":"  We propose SGS-GNN, a novel supervised graph sparsifier that learns the\nsampling probability distribution of edges and samples sparse subgraphs of a\nuser-specified size to reduce the computational costs required by GNNs for\ninference tasks on large graphs. SGS-GNN employs regularizers in the loss\nfunction to enhance homophily in sparse subgraphs, boosting the accuracy of\nGNNs on heterophilic graphs, where a significant number of the neighbors of a\nnode have dissimilar labels. SGS-GNN also supports conditional updates of the\nprobability distribution learning module based on a prior, which helps narrow\nthe search space for sparse graphs. SGS-GNN requires fewer epochs to obtain\nhigh accuracies since it learns the search space of subgraphs more effectively\nthan methods using fixed distributions such as random sampling. Extensive\nexperiments using 33 homophilic and heterophilic graphs demonstrate the\nfollowing: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN\nimproves the F1-scores by a geometric mean of 4% relative to the original\ngraph; on heterophilic graphs, the prediction accuracy is better up to 30%.\n(ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores\nof 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and\n(iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires\nabout half the number of epochs to converge.\n","authors":["Siddhartha Shankar Das","Naheed Anjum Arafat","Muftiqur Rahman","S M Ferdous","Alex Pothen","Mahantesh M Halappanavar"],"pdf_url":"https://arxiv.org/pdf/2502.10208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10205v1","updated":"2025-02-14T14:59:37Z","published":"2025-02-14T14:59:37Z","title":"Looking around you: external information enhances representations for\n  event sequences","summary":"  Representation learning produces models in different domains, such as store\npurchases, client transactions, and general people's behaviour. However, such\nmodels for sequential data usually process a single sequence, ignoring context\nfrom other relevant ones, even in domains with rapidly changing external\nenvironments like finance or misguiding the prediction for a user with no\nrecent events.\n  We are the first to propose a method that aggregates information from\nmultiple user representations augmenting a specific user one for a scenario of\nmultiple co-occurring event sequences. Our study considers diverse aggregation\napproaches, ranging from simple pooling techniques to trainable attention-based\napproaches, especially Kernel attention aggregation, that can highlight more\ncomplex information flow from other users. The proposed method operates atop an\nexisting encoder and supports its efficient fine-tuning. Across considered\ndatasets of financial transactions and downstream tasks, Kernel attention\nimproves ROC AUC scores, both with and without fine-tuning, while mean pooling\nyields a smaller but still significant gain.\n","authors":["Maria Kovaleva","Petr Sokerin","Sofia Krehova","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2502.10205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10203v1","updated":"2025-02-14T14:56:58Z","published":"2025-02-14T14:56:58Z","title":"AI-in-the-Loop Sensing and Communication Joint Design for Edge\n  Intelligence","summary":"  Recent breakthroughs in artificial intelligence (AI), wireless\ncommunications, and sensing technologies have accelerated the evolution of edge\nintelligence. However, conventional systems still grapple with issues such as\nlow communication efficiency, redundant data acquisition, and poor model\ngeneralization. To overcome these challenges, we propose an innovative\nframework that enhances edge intelligence through AI-in-the-loop joint sensing\nand communication (JSAC). This framework features an AI-driven closed-loop\ncontrol architecture that jointly optimizes system resources, thereby\ndelivering superior system-level performance. A key contribution of our work is\nestablishing an explicit relationship between validation loss and the system's\ntunable parameters. This insight enables dynamic reduction of the\ngeneralization error through AI-driven closed-loop control. Specifically, for\nsensing control, we introduce an adaptive data collection strategy based on\ngradient importance sampling, allowing edge devices to autonomously decide when\nto terminate data acquisition and how to allocate sample weights based on\nreal-time model feedback. For communication control, drawing inspiration from\nstochastic gradient Langevin dynamics (SGLD), our joint optimization of\ntransmission power and batch size converts channel and data noise into gradient\nperturbations that help mitigate overfitting. Experimental evaluations\ndemonstrate that our framework reduces communication energy consumption by up\nto 77 percent and sensing costs measured by the number of collected samples by\nup to 52 percent while significantly improving model generalization -- with up\nto 58 percent reductions of the final validation loss. It validates that the\nproposed scheme can harvest the mutual benefit of AI and JSAC systems by\nincorporating the model itself into the control loop of the system.\n","authors":["Zhijie Cai","Xiaowen Cao","Xu Chen","Yuanhao Cui","Guangxu Zhu","Kaibin Huang","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2502.10203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08416v2","updated":"2025-02-14T14:55:02Z","published":"2025-02-12T13:59:22Z","title":"Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators","summary":"  Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.\n","authors":["Anastasia N. Krouglova","Hayden R. Johnson","Basile Confavreux","Michael Deistler","Pedro J. Gon√ßalves"],"pdf_url":"https://arxiv.org/pdf/2502.08416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17853v2","updated":"2025-02-14T14:50:40Z","published":"2024-12-19T20:23:46Z","title":"Zero Shot Time Series Forecasting Using Kolmogorov Arnold Networks","summary":"  Accurate energy price forecasting is crucial for participants in day-ahead\nenergy markets, as it significantly influences their decision-making processes.\nWhile machine learning-based approaches have shown promise in enhancing these\nforecasts, they often remain confined to the specific markets on which they are\ntrained, thereby limiting their adaptability to new or unseen markets. In this\npaper, we introduce a cross-domain adaptation model designed to forecast energy\nprices by learning market-invariant representations across different markets\nduring the training phase. We propose a doubly residual N-BEATS network with\nKolmogorov Arnold networks at its core for time series forecasting. These\nnetworks, grounded in the Kolmogorov-Arnold representation theorem, offer a\npowerful way to approximate multivariate continuous functions. The cross domain\nadaptation model was generated with an adversarial framework. The model's\neffectiveness was tested in predicting day-ahead electricity prices in a zero\nshot fashion. In comparison with baseline models, our proposed framework shows\npromising results. By leveraging the Kolmogorov-Arnold networks, our model can\npotentially enhance its ability to capture complex patterns in energy price\ndata, thus improving forecast accuracy across diverse market conditions. This\naddition not only enriches the model's representational capacity but also\ncontributes to a more robust and flexible forecasting tool adaptable to various\nenergy markets.\n","authors":["Abhiroop Bhattacharya","Nandinee Haq"],"pdf_url":"https://arxiv.org/pdf/2412.17853v2.pdf","comment":"Published In: 2024 NeurIPS Workshop on Time Series in the Age of\n  Large Models"},{"id":"http://arxiv.org/abs/2410.15108v2","updated":"2025-02-14T14:50:35Z","published":"2024-10-19T13:43:51Z","title":"The shape of the brain's connections is predictive of cognitive\n  performance: an explainable machine learning study","summary":"  The shape of the brain's white matter connections is relatively unexplored in\ndiffusion MRI tractography analysis. While it is known that tract shape varies\nin populations and across the human lifespan, it is unknown if the variability\nin dMRI tractography-derived shape may relate to the brain's functional\nvariability across individuals. This work explores the potential of leveraging\ntractography fiber cluster shape measures to predict subject-specific cognitive\nperformance. We implement machine learning models to predict individual\ncognitive performance scores. We study a large-scale database from the HCP-YA\nstudy. We apply an atlas-based fiber cluster parcellation to the dMRI\ntractography of each individual. We compute 15 shape, microstructure, and\nconnectivity features for each fiber cluster. Using these features as input, we\ntrain a total of 210 models to predict 7 different NIH Toolbox cognitive\nperformance assessments. We apply an explainable AI technique, SHAP, to assess\nthe importance of each fiber cluster for prediction. Our results demonstrate\nthat shape measures are predictive of individual cognitive performance. The\nstudied shape measures, such as irregularity, diameter, total surface area,\nvolume, and branch volume, are as effective for prediction as microstructure\nand connectivity measures. The overall best-performing feature is a shape\nfeature, irregularity, which describes how different a cluster's shape is from\nan idealized cylinder. Further interpretation using SHAP values suggest that\nfiber clusters with features highly predictive of cognitive ability are\nwidespread throughout the brain, including fiber clusters from the superficial\nassociation, deep association, cerebellar, striatal, and projection pathways.\nThis study demonstrates the strong potential of shape descriptors to enhance\nthe study of the brain's white matter and its relationship to cognitive\nfunction.\n","authors":["Yui Lo","Yuqian Chen","Dongnan Liu","Wan Liu","Leo Zekelman","Jarrett Rushmore","Fan Zhang","Yogesh Rathi","Nikos Makris","Alexandra J. Golby","Weidong Cai","Lauren J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2410.15108v2.pdf","comment":"This work has been accepted by Human Brain Mapping for publication"},{"id":"http://arxiv.org/abs/2502.10200v1","updated":"2025-02-14T14:50:05Z","published":"2025-02-14T14:50:05Z","title":"Dynamic Reinforcement Learning for Actors","summary":"  Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly\ncontrols system dynamics, instead of the actor (action-generating neural\nnetwork) outputs at each moment, bringing about a major qualitative shift in\nreinforcement learning (RL) from static to dynamic. The actor is initially\ndesigned to generate chaotic dynamics through the loop with its environment,\nenabling the agent to perform flexible and deterministic exploration. Dynamic\nRL controls global system dynamics using a local index called \"sensitivity,\"\nwhich indicates how much the input neighborhood contracts or expands into the\ncorresponding output neighborhood through each neuron's processing. While\nsensitivity adjustment learning (SAL) prevents excessive convergence of the\ndynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them --\nto converge more to improve reproducibility around better state transitions\nwith positive TD error and to diverge more to enhance exploration around worse\ntransitions with negative TD error. Dynamic RL was applied only to the actor in\nan Actor-Critic RL architecture while applying it to the critic remains a\nchallenge. It was tested on two dynamic tasks and functioned effectively\nwithout external exploration noise or backward computation through time.\nMoreover, it exhibited excellent adaptability to new environments, although\nsome problems remain. Drawing parallels between 'exploration' and 'thinking,'\nthe author hypothesizes that \"exploration grows into thinking through learning\"\nand believes this RL could be a key technique for the emergence of thinking,\nincluding inspiration that cannot be reconstructed from massive existing text\ndata. Finally, despite being presumptuous, the author presents the argument\nthat this research should not proceed due to its potentially fatal risks,\naiming to encourage discussion.\n","authors":["Katsunari Shibata"],"pdf_url":"https://arxiv.org/pdf/2502.10200v1.pdf","comment":"31 pages, 20 figures"},{"id":"http://arxiv.org/abs/2502.07640v2","updated":"2025-02-14T14:40:12Z","published":"2025-02-11T15:27:35Z","title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving","summary":"  We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.\n","authors":["Yong Lin","Shange Tang","Bohan Lyu","Jiayun Wu","Hongzhou Lin","Kaiyu Yang","Jia Li","Mengzhou Xia","Danqi Chen","Sanjeev Arora","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2502.07640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10195v1","updated":"2025-02-14T14:39:24Z","published":"2025-02-14T14:39:24Z","title":"Exploring the Camera Bias of Person Re-identification","summary":"  We empirically investigate the camera bias of person re-identification (ReID)\nmodels. Previously, camera-aware methods have been proposed to address this\nissue, but they are largely confined to training domains of the models. We\nmeasure the camera bias of ReID models on unseen domains and reveal that camera\nbias becomes more pronounced under data distribution shifts. As a debiasing\nmethod for unseen domain data, we revisit feature normalization on embedding\nvectors. While the normalization has been used as a straightforward solution,\nits underlying causes and broader applicability remain unexplored. We analyze\nwhy this simple method is effective at reducing bias and show that it can be\napplied to detailed bias factors such as low-level image properties and body\nangle. Furthermore, we validate its generalizability across various models and\nbenchmarks, highlighting its potential as a simple yet effective test-time\npostprocessing method for ReID. In addition, we explore the inherent risk of\ncamera bias in unsupervised learning of ReID models. The unsupervised models\nremain highly biased towards camera labels even for seen domain data,\nindicating substantial room for improvement. Based on observations of the\nnegative impact of camera-biased pseudo labels on training, we suggest simple\ntraining strategies to mitigate the bias. By applying these strategies to\nexisting unsupervised learning algorithms, we show that significant performance\nimprovements can be achieved with minor modifications.\n","authors":["Myungseo Song","Jin-Woo Park","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2502.10195v1.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2502.09500v2","updated":"2025-02-14T14:39:22Z","published":"2025-02-13T17:10:43Z","title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting","summary":"  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\nhttps://github.com/amazon-science/eideticnet-training.\n","authors":["Nicholas Dronen","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.09500v2.pdf","comment":"16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training"},{"id":"http://arxiv.org/abs/2402.03970v2","updated":"2025-02-14T14:37:07Z","published":"2024-02-06T12:59:02Z","title":"Is Deep Learning finally better than Decision Trees on Tabular Data?","summary":"  Tabular data is a ubiquitous data modality due to its versatility and ease of\nuse in many real-world applications. The predominant heuristics for handling\nclassification tasks on tabular data rely on classical machine learning\ntechniques, as the superiority of deep learning models has not yet been\ndemonstrated. This raises the question of whether new deep learning paradigms\ncan surpass classical approaches. Recent studies on tabular data offer a unique\nperspective on the limitations of neural networks in this domain and highlight\nthe superiority of gradient boosted decision trees (GBDTs) in terms of\nscalability and robustness across various datasets. However, novel foundation\nmodels have not been thoroughly assessed regarding quality or fairly compared\nto existing methods for tabular classification. Our study categorizes ten\nstate-of-the-art neural models based on their underlying learning paradigm,\ndemonstrating specifically that meta-learned foundation models outperform GBDTs\nin small data regimes. Although dataset-specific neural networks generally\noutperform LLM-based tabular classifiers, they are surpassed by an AutoML\nlibrary which exhibits the best performance but at the cost of higher\ncomputational demands.\n","authors":["Guri Zab√´rgja","Arlind Kadra","Christian M. M. Frey","Josif Grabocka"],"pdf_url":"https://arxiv.org/pdf/2402.03970v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10185v1","updated":"2025-02-14T14:22:51Z","published":"2025-02-14T14:22:51Z","title":"A Powerful Random Forest Featuring Linear Extensions (RaFFLE)","summary":"  Random forests are widely used in regression. However, the decision trees\nused as base learners are poor approximators of linear relationships. To\naddress this limitation we propose RaFFLE (Random Forest Featuring Linear\nExtensions), a novel framework that integrates the recently developed PILOT\ntrees (Piecewise Linear Organic Trees) as base learners within a random forest\nensemble. PILOT trees combine the computational efficiency of traditional\ndecision trees with the flexibility of linear model trees. To ensure sufficient\ndiversity of the individual trees, we introduce an adjustable regularization\nparameter and use node-level feature sampling. These modifications improve the\naccuracy of the forest. We establish theoretical guarantees for the consistency\nof RaFFLE under weak conditions, and its faster convergence when the data are\ngenerated by a linear model. Empirical evaluations on 136 regression datasets\ndemonstrate that RaFFLE outperforms the classical CART and random forest\nmethods, the regularized linear methods Lasso and Ridge, and the\nstate-of-the-art XGBoost algorithm, across both linear and nonlinear datasets.\nBy balancing predictive accuracy and computational efficiency, RaFFLE proves to\nbe a versatile tool for tackling a wide variety of regression problems.\n","authors":["Jakob Raymaekers","Peter J. Rousseeuw","Thomas Servotte","Tim Verdonck","Ruicong Yao"],"pdf_url":"https://arxiv.org/pdf/2502.10185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10184v1","updated":"2025-02-14T14:22:16Z","published":"2025-02-14T14:22:16Z","title":"Realistic Evaluation of Deep Partial-Label Learning Algorithms","summary":"  Partial-label learning (PLL) is a weakly supervised learning problem in which\neach example is associated with multiple candidate labels and only one is the\ntrue label. In recent years, many deep PLL algorithms have been developed to\nimprove model performance. However, we find that some early developed\nalgorithms are often underestimated and can outperform many later algorithms\nwith complicated designs. In this paper, we delve into the empirical\nperspective of PLL and identify several critical but previously overlooked\nissues. First, model selection for PLL is non-trivial, but has never been\nsystematically studied. Second, the experimental settings are highly\ninconsistent, making it difficult to evaluate the effectiveness of the\nalgorithms. Third, there is a lack of real-world image datasets that can be\ncompatible with modern network architectures. Based on these findings, we\npropose PLENCH, the first Partial-Label learning bENCHmark to systematically\ncompare state-of-the-art deep PLL algorithms. We investigate the model\nselection problem for PLL for the first time, and propose novel model selection\ncriteria with theoretical guarantees. We also create Partial-Label CIFAR-10\n(PLCIFAR10), an image dataset of human-annotated partial labels collected from\nAmazon Mechanical Turk, to provide a testbed for evaluating the performance of\nPLL algorithms in more realistic scenarios. Researchers can quickly and\nconveniently perform a comprehensive and fair evaluation and verify the\neffectiveness of newly developed algorithms based on PLENCH. We hope that\nPLENCH will facilitate standardized, fair, and practical evaluation of PLL\nalgorithms in the future.\n","authors":["Wei Wang","Dong-Dong Wu","Jindong Wang","Gang Niu","Min-Ling Zhang","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2502.10184v1.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2502.10178v1","updated":"2025-02-14T14:13:55Z","published":"2025-02-14T14:13:55Z","title":"From Markov to Laplace: How Mamba In-Context Learns Markov Chains","summary":"  While transformer-based language models have driven the AI revolution thus\nfar, their computational complexity has spurred growing interest in viable\nalternatives, such as structured state space sequence models (SSMs) and\nSelective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown\nremarkable inference speed ups over transformers while achieving comparable or\nsuperior performance on complex language modeling tasks. However, despite these\narchitectural innovations and empirical successes, the fundamental learning\ncapabilities of Mamba remain poorly understood. In this paper, we address this\ngap by studying in-context learning (ICL) on Markov chains and uncovering a\nsurprising phenomenon: unlike transformers, even a single-layer Mamba\nefficiently learns the in-context Laplacian smoothing estimator, which is both\nBayes and minimax optimal, for all Markovian orders. To explain this, we\ntheoretically characterize the representation capacity of Mamba and reveal the\nfundamental role of convolution in enabling it to represent the optimal\nLaplacian smoothing. These theoretical insights align strongly with empirical\nresults and, to the best of our knowledge, represent the first formal\nconnection between Mamba and optimal statistical estimators. Finally, we\noutline promising research directions inspired by these findings.\n","authors":["Marco Bondaschi","Nived Rajaraman","Xiuying Wei","Kannan Ramchandran","Razvan Pascanu","Caglar Gulcehre","Michael Gastpar","Ashok Vardhan Makkuva"],"pdf_url":"https://arxiv.org/pdf/2502.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10173v1","updated":"2025-02-14T14:07:54Z","published":"2025-02-14T14:07:54Z","title":"Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a\n  Language Diffusion Model","summary":"  Proteins are dynamic molecular machines whose biological functions, spanning\nenzymatic catalysis, signal transduction, and structural adaptation, are\nintrinsically linked to their motions. Designing proteins with targeted dynamic\nproperties, however, remains a challenge due to the complex, degenerate\nrelationships between sequence, structure, and molecular motion. Here, we\nintroduce VibeGen, a generative AI framework that enables end-to-end de novo\nprotein design conditioned on normal mode vibrations. VibeGen employs an\nagentic dual-model architecture, comprising a protein designer that generates\nsequence candidates based on specified vibrational modes and a protein\npredictor that evaluates their dynamic accuracy. This approach synergizes\ndiversity, accuracy, and novelty during the design process. Via full-atom\nmolecular simulations as direct validation, we demonstrate that the designed\nproteins accurately reproduce the prescribed normal mode amplitudes across the\nbackbone while adopting various stable, functionally relevant structures.\nNotably, generated sequences are de novo, exhibiting no significant similarity\nto natural proteins, thereby expanding the accessible protein space beyond\nevolutionary constraints. Our work integrates protein dynamics into generative\nprotein design, and establishes a direct, bidirectional link between sequence\nand vibrational behavior, unlocking new pathways for engineering biomolecules\nwith tailored dynamical and functional properties. This framework holds broad\nimplications for the rational design of flexible enzymes, dynamic scaffolds,\nand biomaterials, paving the way toward dynamics-informed AI-driven protein\nengineering.\n","authors":["Bo Ni","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2502.10173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09470v2","updated":"2025-02-14T14:05:51Z","published":"2024-10-12T10:20:26Z","title":"Exploring Channel Distinguishability in Local Neighborhoods of the Model\n  Space in Quantum Neural Networks","summary":"  With the increasing interest in Quantum Machine Learning, Quantum Neural\nNetworks (QNNs) have emerged and gained significant attention. These models\nhave, however, been shown to be notoriously difficult to train, which we\nhypothesize is partially due to the architectures, called ansatzes, that are\nhardly studied at this point. Therefore, in this paper, we take a step back and\nanalyze ansatzes. We initially consider their expressivity, i.e., the space of\noperations they are able to express, and show that the closeness to being a\n2-design, the primarily used measure, fails at capturing this property. Hence,\nwe look for alternative ways to characterize ansatzes by considering the local\nneighborhood of the model space, in particular, analyzing model\ndistinguishability upon small perturbation of parameters. We derive an upper\nbound on their distinguishability, showcasing that QNNs with few parameters are\nhardly discriminable upon update. Our numerical experiments support our bounds\nand further indicate that there is a significant degree of variability, which\nstresses the need for warm-starting or clever initialization. Altogether, our\nwork provides an ansatz-centric perspective on training dynamics and\ndifficulties in QNNs, ultimately suggesting that iterative training of small\nquantum models may not be effective, which contrasts their initial motivation.\n","authors":["Sabrina Herbst","Sandeep Suresh Cranganore","Vincenzo De Maio","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2410.09470v2.pdf","comment":"Published at ICLR 2025 (https://openreview.net/forum?id=gDcL7cgZBt)"},{"id":"http://arxiv.org/abs/2502.10163v1","updated":"2025-02-14T13:50:46Z","published":"2025-02-14T13:50:46Z","title":"Enhancing anomaly detection with topology-aware autoencoders","summary":"  Anomaly detection in high-energy physics is essential for identifying new\nphysics beyond the Standard Model. Autoencoders provide a signal-agnostic\napproach but are limited by the topology of their latent space. This work\nexplores topology-aware autoencoders, embedding phase-space distributions onto\ncompact manifolds that reflect energy-momentum conservation. We construct\nautoencoders with spherical ($S^n$), product ($S^2 \\otimes S^2$), and\nprojective ($\\mathbb{RP}^2$) latent spaces and compare their anomaly detection\nperformance against conventional Euclidean embeddings. Our results show that\nautoencoders with topological priors significantly improve anomaly separation\nby preserving the global structure of the data manifold and reducing spurious\nreconstruction errors. Applying our approach to simulated hadronic top-quark\ndecays, we show that latent spaces with appropriate topological constraints\nenhance sensitivity and robustness in detecting anomalous events. This study\nestablishes topology-aware autoencoders as a powerful tool for unsupervised\nsearches for new physics in particle-collision data.\n","authors":["Vishal S. Ngairangbam","B≈Ça≈ºej Rozwoda","Kazuki Sakurai","Michael Spannowsky"],"pdf_url":"https://arxiv.org/pdf/2502.10163v1.pdf","comment":"12 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.09509v2","updated":"2025-02-14T13:48:01Z","published":"2025-02-13T17:21:51Z","title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling","summary":"  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n","authors":["Theodoros Kouzelis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2502.09509v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.10162v1","updated":"2025-02-14T13:46:14Z","published":"2025-02-14T13:46:14Z","title":"Revisiting Generalization Power of a DNN in Terms of Symbolic\n  Interactions","summary":"  This paper aims to analyze the generalization power of deep neural networks\n(DNNs) from the perspective of interactions. Unlike previous analysis of a\nDNN's generalization power in a highdimensional feature space, we find that the\ngeneralization power of a DNN can be explained as the generalization power of\nthe interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a\nspindle-shaped distribution. Furthermore, our theory can effectively\ndisentangle these two types of interactions from a DNN. We have verified that\nour theory can well match real interactions in a DNN in experiments.\n","authors":["Lei Cheng","Junpeng Zhang","Qihan Ren","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10162v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.19198"},{"id":"http://arxiv.org/abs/2502.10158v1","updated":"2025-02-14T13:37:02Z","published":"2025-02-14T13:37:02Z","title":"Combinatorial Reinforcement Learning with Preference Feedback","summary":"  In this paper, we consider combinatorial reinforcement learning with\npreference feedback, where a learning agent sequentially offers an action--an\nassortment of multiple items to--a user, whose preference feedback follows a\nmultinomial logistic (MNL) model. This framework allows us to model real-world\nscenarios, particularly those involving long-term user engagement, such as in\nrecommender systems and online advertising. However, this framework faces two\nmain challenges: (1) the unknown value of each item, unlike traditional MNL\nbandits that only address single-step preference feedback, and (2) the\ndifficulty of ensuring optimism while maintaining tractable assortment\nselection in the combinatorial action space with unknown values. In this paper,\nwe assume a contextual MNL preference model, where the mean utilities are\nlinear, and the value of each item is approximated by a general function. We\npropose an algorithm, MNL-VQL, that addresses these challenges, making it both\ncomputationally and statistically efficient. As a special case, for linear MDPs\n(with the MNL preference feedback), we establish the first regret lower bound\nin this framework and show that MNL-VQL achieves nearly minimax-optimal regret.\nTo the best of our knowledge, this is the first work to provide statistical\nguarantees in combinatorial RL with preference feedback.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2502.10158v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.10154v1","updated":"2025-02-14T13:32:59Z","published":"2025-02-14T13:32:59Z","title":"Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries","summary":"  We introduce EMSYNC, a video-based symbolic music generation model that\naligns music with a video's emotional content and temporal boundaries. It\nfollows a two-stage framework, where a pretrained video emotion classifier\nextracts emotional features, and a conditional music generator produces MIDI\nsequences guided by both emotional and temporal cues. We introduce boundary\noffsets, a novel temporal conditioning mechanism that enables the model to\nanticipate and align musical chords with scene cuts. Unlike existing models,\nour approach retains event-based encoding, ensuring fine-grained timing control\nand expressive musical nuances. We also propose a mapping scheme to bridge the\nvideo emotion classifier, which produces discrete emotion categories, with the\nemotion-conditioned MIDI generator, which operates on continuous-valued\nvalence-arousal inputs. In subjective listening tests, EMSYNC outperforms\nstate-of-the-art models across all subjective metrics, for music theory-aware\nparticipants as well as the general listeners.\n","authors":["Serkan Sulun","Paula Viana","Matthew E. P. Davies"],"pdf_url":"https://arxiv.org/pdf/2502.10154v1.pdf","comment":"Submitted to International Joint Conference on Artificial\n  Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2502.02844v2","updated":"2025-02-14T13:27:24Z","published":"2025-02-05T02:59:23Z","title":"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement\n  Learning","summary":"  Traditional robust methods in multi-agent reinforcement learning (MARL) often\nstruggle against coordinated adversarial attacks in cooperative scenarios. To\naddress this limitation, we propose the Wolfpack Adversarial Attack framework,\ninspired by wolf hunting strategies, which targets an initial agent and its\nassisting agents to disrupt cooperation. Additionally, we introduce the\nWolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust\nMARL policies to defend against the proposed Wolfpack attack by fostering\nsystem-wide collaboration. Experimental results underscore the devastating\nimpact of the Wolfpack attack and the significant robustness improvements\nachieved by WALL.\n","authors":["Sunwoo Lee","Jaebak Hwang","Yonghyeon Jo","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02844v2.pdf","comment":"8 pages main, 21 pages appendix with reference. Submitted to ICML\n  2025"},{"id":"http://arxiv.org/abs/2409.06123v2","updated":"2025-02-14T13:25:19Z","published":"2024-09-10T00:24:59Z","title":"Contrastive Federated Learning with Tabular Data Silos","summary":"  Learning from vertical partitioned data silos is challenging due to the\nsegmented nature of data, sample misalignment, and strict privacy concerns.\nFederated learning has been proposed as a solution. However, sample\nmisalignment across silos often hinders optimal model performance and suggests\ndata sharing within the model, which breaks privacy. Our proposed solution is\nContrastive Federated Learning with Tabular Data Silos (CFL), which offers a\nsolution for data silos with sample misalignment without the need for sharing\noriginal or representative data to maintain privacy. CFL begins with local\nacquisition of contrastive representations of the data within each silo and\naggregates knowledge from other silos through the federated learning algorithm.\nOur experiments demonstrate that CFL solves the limitations of existing\nalgorithms for data silos and outperforms existing tabular contrastive\nlearning. CFL provides performance improvements without loosening privacy.\n","authors":["Achmad Ginanjar","Xue Li","Wen Hua","Jiaming Pei"],"pdf_url":"https://arxiv.org/pdf/2409.06123v2.pdf","comment":"44 Pages. 1stversion was submitted on Artificial Intelligence\n  Journal, Jan 29, 2024, ARTINT-D-24-00098"},{"id":"http://arxiv.org/abs/2409.12915v3","updated":"2025-02-14T13:24:08Z","published":"2024-09-19T17:11:27Z","title":"Exploring Representations and Interventions in Time Series Foundation\n  Models","summary":"  Time series foundation models (TSFMs) promise to be powerful tools for a wide\nrange of applications. However, their internal representations and learned\nconcepts are still not well understood. In this study, we investigate the\nstructure and redundancy of representations across various TSFMs, examining the\nself-similarity of model layers within and across different model sizes. This\nanalysis reveals block-like redundancy in the representations, which can be\nutilized for informed pruning to improve inference speed and efficiency.\nAdditionally, we explore the concepts learned by these models - such as\nperiodicity and trends - and how these can be manipulated through latent space\nsteering to influence model behavior. Our experiments show that steering\ninterventions can introduce new features, e.g., adding periodicity or trends to\nsignals that initially lacked them. These findings underscore the value of\nrepresentational analysis for optimizing models and demonstrate how conceptual\nsteering offers new possibilities for more controlled and efficient time series\nanalysis with TSFMs.\n","authors":["Micha≈Ç Wili≈Ñski","Mononito Goswami","Nina ≈ªukowska","Willa Potosnak","Artur Dubrawski"],"pdf_url":"https://arxiv.org/pdf/2409.12915v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23326v3","updated":"2025-02-14T13:24:02Z","published":"2024-10-30T15:08:05Z","title":"MassSpecGym: A benchmark for the discovery and identification of\n  molecules","summary":"  The discovery and identification of molecules in biological and environmental\nsamples is crucial for advancing biomedical and chemical sciences. Tandem mass\nspectrometry (MS/MS) is the leading technique for high-throughput elucidation\nof molecular structures. However, decoding a molecular structure from its mass\nspectrum is exceptionally challenging, even when performed by human experts. As\na result, the vast majority of acquired MS/MS spectra remain uninterpreted,\nthereby limiting our understanding of the underlying (bio)chemical processes.\nDespite decades of progress in machine learning applications for predicting\nmolecular structures from MS/MS spectra, the development of new methods is\nseverely hindered by the lack of standard datasets and evaluation protocols. To\naddress this problem, we propose MassSpecGym -- the first comprehensive\nbenchmark for the discovery and identification of molecules from MS/MS data.\nOur benchmark comprises the largest publicly available collection of\nhigh-quality labeled MS/MS spectra and defines three MS/MS annotation\nchallenges: de novo molecular structure generation, molecule retrieval, and\nspectrum simulation. It includes new evaluation metrics and a\ngeneralization-demanding data split, therefore standardizing the MS/MS\nannotation tasks and rendering the problem accessible to the broad machine\nlearning community. MassSpecGym is publicly available at\nhttps://github.com/pluskal-lab/MassSpecGym.\n","authors":["Roman Bushuiev","Anton Bushuiev","Niek F. de Jonge","Adamo Young","Fleming Kretschmer","Raman Samusevich","Janne Heirman","Fei Wang","Luke Zhang","Kai D√ºhrkop","Marcus Ludwig","Nils A. Haupt","Apurva Kalia","Corinna Brungs","Robin Schmid","Russell Greiner","Bo Wang","David S. Wishart","Li-Ping Liu","Juho Rousu","Wout Bittremieux","Hannes Rost","Tytus D. Mak","Soha Hassoun","Florian Huber","Justin J. J. van der Hooft","Michael A. Stravs","Sebastian B√∂cker","Josef Sivic","Tom√°≈° Pluskal"],"pdf_url":"https://arxiv.org/pdf/2410.23326v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05101v4","updated":"2025-02-14T13:13:03Z","published":"2024-10-07T14:56:07Z","title":"CR-CTC: Consistency regularization on CTC for improved speech\n  recognition","summary":"  Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance. In this work, we propose the Consistency-Regularized CTC (CR-CTC),\nwhich enforces consistency between two CTC distributions obtained from\ndifferent augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC. It significantly improves\nthe CTC performance, achieving state-of-the-art results comparable to those\nattained by transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). We release our code at\nhttps://github.com/k2-fsa/icefall.\n","authors":["Zengwei Yao","Wei Kang","Xiaoyu Yang","Fangjun Kuang","Liyong Guo","Han Zhu","Zengrui Jin","Zhaoqing Li","Long Lin","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2410.05101v4.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10138v1","updated":"2025-02-14T13:07:25Z","published":"2025-02-14T13:07:25Z","title":"Provably Efficient RL under Episode-Wise Safety in Linear CMDPs","summary":"  We study the reinforcement learning (RL) problem in a constrained Markov\ndecision process (CMDP), where an agent explores the environment to maximize\nthe expected cumulative reward while satisfying a single constraint on the\nexpected total utility value in every episode. While this problem is well\nunderstood in the tabular setting, theoretical results for function\napproximation remain scarce. This paper closes the gap by proposing an RL\nalgorithm for linear CMDPs that achieves $\\widetilde{\\mathcal{O}}(\\sqrt{K})$\nregret with an episode-wise zero-violation guarantee. Furthermore, our method\nis computationally efficient, scaling polynomially with problem-dependent\nparameters while remaining independent of the state space size. Our results\nsignificantly improve upon recent linear CMDP algorithms, which either violate\nthe constraint or incur exponential computational costs.\n","authors":["Toshinori Kitamura","Arnob Ghosh","Tadashi Kozuno","Wataru Kumagai","Kazumi Kasaura","Kenta Hoshino","Yohei Hosoe","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2502.10138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12118v2","updated":"2025-02-14T12:56:45Z","published":"2024-11-18T23:12:13Z","title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers","summary":"  In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.\n","authors":["Tiberiu Musat"],"pdf_url":"https://arxiv.org/pdf/2411.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10125v1","updated":"2025-02-14T12:51:07Z","published":"2025-02-14T12:51:07Z","title":"Learning Relational Tabular Data without Shared Features","summary":"  Learning relational tabular data has gained significant attention recently,\nbut most studies focus on single tables, overlooking the potential of\ncross-table learning. Cross-table learning, especially in scenarios where\ntables lack shared features and pre-aligned data, offers vast opportunities but\nalso introduces substantial challenges. The alignment space is immense, and\ndetermining accurate alignments between tables is highly complex. We propose\nLatent Entity Alignment Learning (Leal), a novel framework enabling effective\ncross-table training without requiring shared features or pre-aligned data.\nLeal operates on the principle that properly aligned data yield lower loss than\nmisaligned data, a concept embodied in its soft alignment mechanism. This\nmechanism is coupled with a differentiable cluster sampler module, ensuring\nefficient scaling to large relational tables. Furthermore, we provide a\ntheoretical proof of the cluster sampler's approximation capacity. Extensive\nexperiments on five real-world and five synthetic datasets show that Leal\nachieves up to a 26.8% improvement in predictive performance compared to\nstate-of-the-art methods, demonstrating its effectiveness and scalability.\n","authors":["Zhaomin Wu","Shida Wang","Ziyang Wang","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2502.10125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10122v1","updated":"2025-02-14T12:41:05Z","published":"2025-02-14T12:41:05Z","title":"Modern Hopfield Networks with Continuous-Time Memories","summary":"  Recent research has established a connection between modern Hopfield networks\n(HNs) and transformer attention heads, with guarantees of exponential storage\ncapacity. However, these models still face challenges scaling storage\nefficiently. Inspired by psychological theories of continuous neural resource\nallocation in working memory, we propose an approach that compresses large\ndiscrete Hopfield memories into smaller, continuous-time memories. Leveraging\ncontinuous attention, our new energy function modifies the update rule of HNs,\nreplacing the traditional softmax-based probability mass function with a\nprobability density, over the continuous memory. This formulation aligns with\nmodern perspectives on human executive function, offering a principled link\nbetween attractor dynamics in working memory and resource-efficient memory\nallocation. Our framework maintains competitive performance with HNs while\nleveraging a compressed memory, reducing computational costs across synthetic\nand video datasets.\n","authors":["Saul Santos","Ant√≥nio Farinhas","Daniel C. McNamee","Andr√© F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2502.10122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04890v2","updated":"2025-02-14T12:36:02Z","published":"2025-02-07T12:56:39Z","title":"Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated\n  Learning","summary":"  Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack\n","authors":["Yuchen Liu","Chen Chen","Lingjuan Lyu","Yaochu Jin","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2502.04890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10119v1","updated":"2025-02-14T12:35:21Z","published":"2025-02-14T12:35:21Z","title":"SeWA: Selective Weight Average via Probabilistic Masking","summary":"  Weight averaging has become a standard technique for enhancing model\nperformance. However, methods such as Stochastic Weight Averaging (SWA) and\nLatest Weight Averaging (LAWA) often require manually designed procedures to\nsample from the training trajectory, and the results depend heavily on\nhyperparameter tuning. To minimize human effort, this paper proposes a simple\nyet efficient algorithm called Selective Weight Averaging (SeWA), which\nadaptively selects checkpoints during the final stages of training for\naveraging. Based on SeWA, we show that only a few points are needed to achieve\nbetter generalization and faster convergence. Theoretically, solving the\ndiscrete subset selection problem is inherently challenging. To address this,\nwe transform it into a continuous probabilistic optimization framework and\nemploy the Gumbel-Softmax estimator to learn the non-differentiable mask for\neach checkpoint. Further, we theoretically derive the SeWA's stability-based\ngeneralization bounds, which are sharper than that of SGD under both convex and\nnon-convex assumptions. Finally, solid extended experiments in various domains,\nincluding behavior cloning, image classification, and text classification,\nfurther validate the effectiveness of our approach.\n","authors":["Peng Wang","Shengchao Hu","Zerui Tao","Guoxia Wang","Dianhai Yu","Li Shen","Quan Zheng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.10119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10112v1","updated":"2025-02-14T12:17:59Z","published":"2025-02-14T12:17:59Z","title":"Accelerometry-based Energy Expenditure Estimation During Activities of\n  Daily Living: A Comparison Among Different Accelerometer Compositions","summary":"  Physical activity energy expenditure (PAEE) can be measured from\nbreath-by-breath respiratory data, which can serve as a reference.\nAlternatively, PAEE can be predicted from the body movements, which can be\nmeasured and estimated with accelerometers. The body center of mass (COM)\nacceleration reflects the movements of the whole body and thus serves as a good\npredictor for PAEE. However, the wrist has also become a popular location due\nto recent advancements in wrist-worn devices. Therefore, in this work, using\nthe respiratory data measured by COSMED K5 as the reference, we evaluated and\ncompared the performances of COM-based settings and wrist-based settings. The\nCOM-based settings include two different accelerometer compositions, using only\nthe pelvis accelerometer (pelvis-acc) and the pelvis accelerometer with two\naccelerometers from two thighs (3-acc). The wrist-based settings include using\nonly the left wrist accelerometer (l-wrist-acc) and only the right wrist\naccelerometer (r-wrist-acc). We implemented two existing PAEE estimation\nmethods on our collected dataset, where 9 participants performed activities of\ndaily living while wearing 5 accelerometers (i.e., pelvis, two thighs, and two\nwrists). These two methods include a linear regression (LR) model and a\nCNN-LSTM model. Both models yielded the best results with the COM-based 3-acc\nsetting (LR: $R^2$ = 0.41, CNN-LSTM: $R^2$ = 0.53). No significant difference\nwas found between the 3-acc and pelvis-acc settings (p-value = 0.278). For both\nmodels, neither the l-wrist-acc nor the r-wrist-acc settings demonstrated\npredictive power on PAEE with $R^2$ values close to 0, significantly\noutperformed by the two COM-based settings (p-values $<$ 0.05). No significant\ndifference was found between the two wrists (p-value = 0.329).\n","authors":["Shuhao Que","Remco Poelarends","Peter Veltink","Miriam Vollenbroek-Hutten","Ying Wang"],"pdf_url":"https://arxiv.org/pdf/2502.10112v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.10111v1","updated":"2025-02-14T12:17:24Z","published":"2025-02-14T12:17:24Z","title":"COMBINEX: A Unified Counterfactual Explainer for Graph Neural Networks\n  via Node Feature and Structural Perturbations","summary":"  Counterfactual explanations have emerged as a powerful tool to unveil the\nopaque decision-making processes of graph neural networks (GNNs). However,\nexisting techniques primarily focus on edge modifications, often overlooking\nthe crucial role of node feature perturbations in shaping model predictions. To\naddress this limitation, we propose COMBINEX, a novel GNN explainer that\ngenerates counterfactual explanations for both node and graph classification\ntasks. Unlike prior methods, which treat structural and feature-based changes\nindependently, COMBINEX optimally balances modifications to edges and node\nfeatures by jointly optimizing these perturbations. This unified approach\nensures minimal yet effective changes required to flip a model's prediction,\nresulting in realistic and interpretable counterfactuals. Additionally,\nCOMBINEX seamlessly handles both continuous and discrete node features,\nenhancing its versatility across diverse datasets and GNN architectures.\nExtensive experiments on real-world datasets and various GNN architectures\ndemonstrate the effectiveness and robustness of our approach over existing\nbaselines.\n","authors":["Flavio Giorgi","Fabrizio Silvestri","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.10111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07223v4","updated":"2025-02-14T12:17:07Z","published":"2024-12-10T06:19:08Z","title":"A Consolidated Volatility Prediction with Back Propagation Neural\n  Network and Genetic Algorithm","summary":"  This paper provides a unique approach with AI algorithms to predict emerging\nstock markets volatility. Traditionally, stock volatility is derived from\nhistorical volatility,Monte Carlo simulation and implied volatility as well. In\nthis paper, the writer designs a consolidated model with back-propagation\nneural network and genetic algorithm to predict future volatility of emerging\nstock markets and found that the results are quite accurate with low errors.\n","authors":["Zong Ke","Jingyu Xu","Zizhou Zhang","Yu Cheng","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2412.07223v4.pdf","comment":"6 pages, 7 figures, 1 table, The paper will be published by IEEE on\n  conference: 2024 3rd International Conference on Image Processing, Computer\n  Vision and Machine Learning (ICICML 2024) (V2)"},{"id":"http://arxiv.org/abs/2412.16264v3","updated":"2025-02-14T12:15:37Z","published":"2024-12-20T09:22:07Z","title":"Continual Learning with Strategic Selection and Forgetting for Network\n  Intrusion Detection","summary":"  Intrusion Detection Systems (IDS) are crucial for safeguarding digital\ninfrastructure. In dynamic network environments, both threat landscapes and\nnormal operational behaviors are constantly changing, resulting in concept\ndrift. While continuous learning mitigates the adverse effects of concept\ndrift, insufficient attention to drift patterns and excessive preservation of\noutdated knowledge can still hinder the IDS's adaptability. In this paper, we\npropose SSF (Strategic Selection and Forgetting), a novel continual learning\nmethod for IDS, providing continuous model updates with a constantly refreshed\nmemory buffer. Our approach features a strategic sample selection algorithm to\nselect representative new samples and a strategic forgetting mechanism to drop\noutdated samples. The proposed strategic sample selection algorithm prioritizes\nnew samples that cause the `drifted' pattern, enabling the model to better\nunderstand the evolving landscape. Additionally, we introduce strategic\nforgetting upon detecting significant drift by discarding outdated samples to\nfree up memory, allowing the incorporation of more recent data. SSF captures\nevolving patterns effectively and ensures the model is aligned with the change\nof data patterns, significantly enhancing the IDS's adaptability to concept\ndrift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15\ndatasets demonstrates its superior adaptability to concept drift for network\nintrusion detection. The code is released at\nhttps://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.\n","authors":["Xinchen Zhang","Running Zhao","Zhihan Jiang","Handi Chen","Yulong Ding","Edith C. H. Ngai","Shuang-Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2412.16264v3.pdf","comment":"Accepted by IEEE International Conference on Computer Communications\n  (INFOCOM) 2025"},{"id":"http://arxiv.org/abs/2502.10108v1","updated":"2025-02-14T12:09:49Z","published":"2025-02-14T12:09:49Z","title":"NeuroXVocal: Detection and Explanation of Alzheimer's Disease through\n  Non-invasive Analysis of Picture-prompted Speech","summary":"  The early diagnosis of Alzheimer's Disease (AD) through non invasive methods\nremains a significant healthcare challenge. We present NeuroXVocal, a novel\ndual-component system that not only classifies but also explains potential AD\ncases through speech analysis. The classification component (Neuro) processes\nthree distinct data streams: acoustic features capturing speech patterns and\nvoice characteristics, textual features extracted from speech transcriptions,\nand precomputed embeddings representing linguistic patterns. These streams are\nfused through a custom transformer-based architecture that enables robust\ncross-modal interactions. The explainability component (XVocal) implements a\nRetrieval-Augmented Generation (RAG) approach, leveraging Large Language Models\ncombined with a domain-specific knowledge base of AD research literature. This\narchitecture enables XVocal to retrieve relevant clinical studies and research\nfindings to generate evidence-based context-sensitive explanations of the\nacoustic and linguistic markers identified in patient speech. Using the IS2021\nADReSSo Challenge benchmark dataset, our system achieved state-of-the-art\nperformance with 95.77% accuracy in AD classification, significantly\noutperforming previous approaches. The explainability component was\nqualitatively evaluated using a structured questionnaire completed by medical\nprofessionals, validating its clinical relevance. NeuroXVocal's unique\ncombination of high-accuracy classification and interpretable,\nliterature-grounded explanations demonstrates its potential as a practical tool\nfor supporting clinical AD diagnosis.\n","authors":["Nikolaos Ntampakis","Konstantinos Diamantaras","Ioanna Chouvarda","Magda Tsolaki","Vasileios Argyriou","Panagiotis Sarigianndis"],"pdf_url":"https://arxiv.org/pdf/2502.10108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14695v2","updated":"2025-02-14T12:08:20Z","published":"2024-11-22T03:05:06Z","title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","summary":"  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n","authors":["Hao Chen","Francois Bremond","Nicu Sebe","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14695v2.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2502.10106v1","updated":"2025-02-14T12:08:00Z","published":"2025-02-14T12:08:00Z","title":"Data-Adaptive Low-Rank Sparse Subspace Clustering","summary":"  Low-rank sparse subspace clustering (LRSSC) algorithms built on\nself-expressive model effectively capture both the global and local structure\nof the data. However, existing solutions, primarily based on proximal operators\nassociated with Sp/Lp , p e {0, 1/2, 2/3, 1}, norms are not data-adaptive. In\nthis work, we propose an LRSSC algorithm incorporating a data-adaptive\nsurrogate for the S0/L0 quasi-norm. We provide a numerical solution for the\ncorresponding proximal operator in cases where an analytical expression is\nunavailable. The proposed LRSSC algorithm is formulated within the proximal\nmapping framework, and we present theoretical proof of its global convergence\ntoward a stationary point. We evaluate the performance of the proposed method\non three well known datasets, comparing it against LRSSC algorithms constrained\nby Sp/Lp, p e {0, 1/2, 2/3, 1}, norms.\n","authors":["Ivica Kopriva"],"pdf_url":"https://arxiv.org/pdf/2502.10106v1.pdf","comment":"5 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2502.02867v2","updated":"2025-02-14T11:57:25Z","published":"2025-02-05T03:52:36Z","title":"Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation\n  Learning with Visual Observations","summary":"  Imitation learning (IL) enables agents to mimic expert behavior without\nreward signals but faces challenges in cross-domain scenarios with\nhigh-dimensional, noisy, and incomplete visual observations. To address this,\nwe propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning\n(DIFF-IL), a novel IL method that extracts domain-invariant features from\nindividual frames and adapts them into sequences to isolate and replicate\nexpert behaviors. We also introduce a frame-wise time labeling technique to\nsegment expert behaviors by timesteps and assign rewards aligned with temporal\ncontexts, enhancing task performance. Experiments across diverse visual\nenvironments demonstrate the effectiveness of DIFF-IL in addressing complex\nvisual tasks.\n","authors":["Minung Kim","Kawon Lee","Jungmo Kim","Sungho Choi","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02867v2.pdf","comment":"8 pages main, 19 pages appendix with reference. Submitted to ICML\n  2025"},{"id":"http://arxiv.org/abs/2406.06644v4","updated":"2025-02-14T11:57:02Z","published":"2024-06-09T23:39:31Z","title":"Latent Diffusion Model-Enabled Low-Latency Semantic Communication in the\n  Presence of Semantic Ambiguities and Wireless Channel Noises","summary":"  Deep learning (DL)-based Semantic Communications (SemCom) is becoming\ncritical to maximize overall efficiency of communication networks.\nNevertheless, SemCom is sensitive to wireless channel uncertainties, source\noutliers, and suffer from poor generalization bottlenecks. To address the\nmentioned challenges, this paper develops a latent diffusion model-enabled\nSemCom system with three key contributions, i.e., i) to handle potential\noutliers in the source data, semantic errors obtained by projected gradient\ndescent based on the vulnerabilities of DL models, are utilized to update the\nparameters and obtain an outlier-robust encoder, ii) a lightweight single-layer\nlatent space transformation adapter completes one-shot learning at the\ntransmitter and is placed before the decoder at the receiver, enabling\nadaptation for out-of-distribution data and enhancing human-perceptual quality,\nand iii) an end-to-end consistency distillation (EECD) strategy is used to\ndistill the diffusion models trained in latent space, enabling deterministic\nsingle or few-step low-latency denoising in various noisy channels while\nmaintaining high semantic quality. Extensive numerical experiments across\ndifferent datasets demonstrate the superiority of the proposed SemCom system,\nconsistently proving its robustness to outliers, the capability to transmit\ndata with unknown distributions, and the ability to perform real-time channel\ndenoising tasks while preserving high human perceptual quality, outperforming\nthe existing denoising approaches in semantic metrics such as multi-scale\nstructural similarity index measure (MS-SSIM) and learned perceptual image path\nsimilarity (LPIPS).\n","authors":["Jianhua Pei","Cheng Feng","Ping Wang","Hina Tabassum","Dongyuan Shi"],"pdf_url":"https://arxiv.org/pdf/2406.06644v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10097v1","updated":"2025-02-14T11:44:17Z","published":"2025-02-14T11:44:17Z","title":"Causal Information Prioritization for Efficient Reinforcement Learning","summary":"  Current Reinforcement Learning (RL) methods often suffer from\nsample-inefficiency, resulting from blind exploration strategies that neglect\ncausal relationships among states, actions, and rewards. Although recent causal\napproaches aim to address this problem, they lack grounded modeling of\nreward-guided causal understanding of states and actions for goal-orientation,\nthus impairing learning efficiency. To tackle this issue, we propose a novel\nmethod named Causal Information Prioritization (CIP) that improves sample\nefficiency by leveraging factored MDPs to infer causal relationships between\ndifferent dimensions of states and actions with respect to rewards, enabling\nthe prioritization of causal information. Specifically, CIP identifies and\nleverages causal relationships between states and rewards to execute\ncounterfactual data augmentation to prioritize high-impact state features under\nthe causal understanding of the environments. Moreover, CIP integrates a\ncausality-aware empowerment learning objective, which significantly enhances\nthe agent's execution of reward-guided actions for more efficient exploration\nin complex environments. To fully assess the effectiveness of CIP, we conduct\nextensive experiments across 39 tasks in 5 diverse continuous control\nenvironments, encompassing both locomotion and manipulation skills learning\nwith pixel-based and sparse reward settings. Experimental results demonstrate\nthat CIP consistently outperforms existing RL methods across a wide range of\nscenarios.\n","authors":["Hongye Cao","Fan Feng","Tianpei Yang","Jing Huo","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2502.10097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10095v1","updated":"2025-02-14T11:36:04Z","published":"2025-02-14T11:36:04Z","title":"Representation Learning on Out of Distribution in Tabular Data","summary":"  The open-world assumption in model development suggests that a model might\nlack sufficient information to adequately handle data that is entirely distinct\nor out of distribution (OOD). While deep learning methods have shown promising\nresults in handling OOD data through generalization techniques, they often\nrequire specialized hardware that may not be accessible to all users. We\npresent TCL, a lightweight yet effective solution that operates efficiently on\nstandard CPU hardware. Our approach adapts contrastive learning principles\nspecifically for tabular data structures, incorporating full matrix\naugmentation and simplified loss calculation. Through comprehensive experiments\nacross 10 diverse datasets, we demonstrate that TCL outperforms existing\nmodels, including FT-Transformer and ResNet, particularly in classification\ntasks, while maintaining competitive performance in regression problems. TCL\nachieves these results with significantly reduced computational requirements,\nmaking it accessible to users with limited hardware capabilities. This study\nalso provides practical guidance for detecting and evaluating OOD data through\nstraightforward experiments and visualizations. Our findings show that TCL\noffers a promising balance between performance and efficiency in handling OOD\nprediction tasks, which is particularly beneficial for general machine learning\npractitioners working with computational constraints.\n","authors":["Achmad Ginanjar","Xue Li","Priyanka Singh","Wen Hua"],"pdf_url":"https://arxiv.org/pdf/2502.10095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03146v2","updated":"2025-02-14T11:29:18Z","published":"2025-02-05T13:14:50Z","title":"Symmetry-Aware Bayesian Flow Networks for Crystal Generation","summary":"  The discovery of new crystalline materials is essential to scientific and\ntechnological progress. However, traditional trial-and-error approaches are\ninefficient due to the vast search space. Recent advancements in machine\nlearning have enabled generative models to predict new stable materials by\nincorporating structural symmetries and to condition the generation on desired\nproperties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian\nFlow Network (BFN) for crystalline material generation that accurately\nreproduces the distribution of space groups found in experimentally observed\ncrystals. SymmBFN substantially improves efficiency, generating stable\nstructures at least 50 times faster than the next-best method. Furthermore, we\ndemonstrate its capability for property-conditioned generation, enabling the\ndesign of materials with tailored properties. Our findings establish BFNs as an\neffective tool for accelerating the discovery of crystalline materials.\n","authors":["Laura Ruple","Luca Torresi","Henrik Schopmans","Pascal Friederich"],"pdf_url":"https://arxiv.org/pdf/2502.03146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10092v1","updated":"2025-02-14T11:27:02Z","published":"2025-02-14T11:27:02Z","title":"A novel approach to data generation in generative model","summary":"  Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.\n","authors":["JaeHong Kim","Jaewon Shim"],"pdf_url":"https://arxiv.org/pdf/2502.10092v1.pdf","comment":"47 pages, 2 tables, 9 figures"},{"id":"http://arxiv.org/abs/2502.10089v1","updated":"2025-02-14T11:21:36Z","published":"2025-02-14T11:21:36Z","title":"A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS\n  ACAM for Energy-Efficient Inference","summary":"  In recent years, the development of smart edge computing systems to process\ninformation locally is on the rise. Many near-sensor machine learning (ML)\napproaches have been implemented to introduce accurate and energy efficient\ntemplate matching operations in resource-constrained edge sensing systems, such\nas wearables. To introduce novel solutions that can be viable for extreme edge\ncases, hybrid solutions combining conventional and emerging technologies have\nstarted to be proposed. Deep Neural Networks (DNN) optimised for edge\napplication alongside new approaches of computing (both device and architecture\n-wise) could be a strong candidate in implementing edge ML solutions that aim\nat competitive accuracy classification while using a fraction of the power of\nconventional ML solutions. In this work, we are proposing a hybrid\nsoftware-hardware edge classifier aimed at the extreme edge near-sensor\nsystems. The classifier consists of two parts: (i) an optimised digital tinyML\nnetwork, working as a front-end feature extractor, and (ii) a back-end\nRRAM-CMOS analogue content addressable memory (ACAM), working as a final stage\ntemplate matching system. The combined hybrid system exhibits a competitive\ntrade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$\nand $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with\n78.06$\\mu$J for the original teacher model, representing a 792-fold reduction,\nmaking it a viable solution for extreme edge applications.\n","authors":["Kieran Woodward","Eiman Kanjo","Georgios Papandroulidakis","Shady Agwa","Themis Prodromakis"],"pdf_url":"https://arxiv.org/pdf/2502.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02834v2","updated":"2025-02-14T11:19:06Z","published":"2025-02-05T02:31:50Z","title":"Task-Aware Virtual Training: Enhancing Generalization in\n  Meta-Reinforcement Learning for Out-of-Distribution Tasks","summary":"  Meta reinforcement learning aims to develop policies that generalize to\nunseen tasks sampled from a task distribution. While context-based meta-RL\nmethods improve task representation using task latents, they often struggle\nwith out-of-distribution (OOD) tasks. To address this, we propose Task-Aware\nVirtual Training (TAVT), a novel algorithm that accurately captures task\ncharacteristics for both training and OOD scenarios using metric-based\nrepresentation learning. Our method successfully preserves task characteristics\nin virtual tasks and employs a state regularization technique to mitigate\noverestimation errors in state-varying environments. Numerical results\ndemonstrate that TAVT significantly enhances generalization to OOD tasks across\nvarious MuJoCo and MetaWorld environments.\n","authors":["Jeongmo Kim","Yisak Park","Minung Kim","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02834v2.pdf","comment":"8 pages main paper, 19 pages appendices with reference, Submitted to\n  ICML 2025"},{"id":"http://arxiv.org/abs/2411.00928v2","updated":"2025-02-14T11:15:25Z","published":"2024-11-01T15:58:02Z","title":"A Bregman firmly nonexpansive proximal operator for baryconvex\n  optimization","summary":"  We present a generalization of the proximal operator defined through a convex\ncombination of convex objectives, where the coefficients are updated in a\nminimax fashion. We prove that this new operator is Bregman firmly nonexpansive\nwith respect to a Bregman divergence that combines Euclidean and information\ngeometries. Finally, we derive the associated continuous flows.\n","authors":["Mastane Achab"],"pdf_url":"https://arxiv.org/pdf/2411.00928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10919v4","updated":"2025-02-14T11:12:02Z","published":"2024-08-20T15:04:14Z","title":"CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network","summary":"  In recent years, Wi-Fi sensing has garnered significant attention due to its\nnumerous benefits, such as privacy protection, low cost, and penetration\nability. Extensive research has been conducted in this field, focusing on areas\nsuch as gesture recognition, people identification, and fall detection.\nHowever, many data-driven methods encounter challenges related to domain shift,\nwhere the model fails to perform well in environments different from the\ntraining data. One major factor contributing to this issue is the limited\navailability of Wi-Fi sensing datasets, which makes models learn excessive\nirrelevant information and over-fit to the training set. Unfortunately,\ncollecting large-scale Wi-Fi sensing datasets across diverse scenarios is a\nchallenging task. To address this problem, we propose CrossFi, a siamese\nnetwork-based approach that excels in both in-domain scenario and cross-domain\nscenario, including few-shot, zero-shot scenarios, and even works in few-shot\nnew-class scenario where testing set contains new categories. The core\ncomponent of CrossFi is a sample-similarity calculation network called CSi-Net,\nwhich improves the structure of the siamese network by using an attention\nmechanism to capture similarity information, instead of simply calculating the\ndistance or cosine similarity. Based on it, we develop an extra Weight-Net that\ncan generate a template for each class, so that our CrossFi can work in\ndifferent scenarios. Experimental results demonstrate that our CrossFi achieves\nstate-of-the-art performance across various scenarios. In gesture recognition\ntask, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%\nin one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,\nand 84.75% in one-shot new-class scenario. The code for our model is publicly\navailable at https://github.com/RS2002/CrossFi.\n","authors":["Zijian Zhao","Tingwei Chen","Zhijie Cai","Xiaoyang Li","Hang Li","Qimei Chen","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.10919v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03752v2","updated":"2025-02-14T11:02:01Z","published":"2025-02-06T03:28:45Z","title":"PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning\n  with Noisy Demonstrations","summary":"  Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen\ntasks but faces challenges in long-horizon environments. Skill-based approaches\ntackle this by decomposing state-action sequences into reusable skills and\nemploying hierarchical decision-making. However, these methods are highly\nsusceptible to noisy offline demonstrations, resulting in unstable skill\nlearning and degraded performance. To overcome this, we propose Prioritized\nRefinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates\nexploration near noisy data to generate online trajectories and combines them\nwith offline data. Through prioritization, PRISM extracts high-quality data to\nlearn task-relevant skills effectively. By addressing the impact of noise, our\nmethod ensures stable skill learning and achieves superior performance in\nlong-horizon tasks, even with noisy and sub-optimal data.\n","authors":["Sanghyeon Lee","Sangjun Bae","Yisak Park","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.03752v2.pdf","comment":"8 pages main, 19 pages appendix with reference. Submitted to ICML\n  2025"},{"id":"http://arxiv.org/abs/2502.10077v1","updated":"2025-02-14T10:59:09Z","published":"2025-02-14T10:59:09Z","title":"Towards Empowerment Gain through Causal Structure Learning in\n  Model-Based RL","summary":"  In Model-Based Reinforcement Learning (MBRL), incorporating causal structures\ninto dynamics models provides agents with a structured understanding of the\nenvironments, enabling efficient decision. Empowerment as an intrinsic\nmotivation enhances the ability of agents to actively control their\nenvironments by maximizing the mutual information between future states and\nactions. We posit that empowerment coupled with causal understanding can\nimprove controllability, while enhanced empowerment gain can further facilitate\ncausal reasoning in MBRL. To improve learning efficiency and controllability,\nwe propose a novel framework, Empowerment through Causal Learning (ECL), where\nan agent with the awareness of causal dynamics models achieves\nempowerment-driven exploration and optimizes its causal structure for task\nlearning. Specifically, ECL operates by first training a causal dynamics model\nof the environment based on collected data. We then maximize empowerment under\nthe causal structure for exploration, simultaneously using data gathered\nthrough exploration to update causal dynamics model to be more controllable\nthan dense dynamics model without causal structure. In downstream task\nlearning, an intrinsic curiosity reward is included to balance the causality,\nmitigating overfitting. Importantly, ECL is method-agnostic and is capable of\nintegrating various causal discovery methods. We evaluate ECL combined with 3\ncausal discovery methods across 6 environments including pixel-based tasks,\ndemonstrating its superior performance compared to other causal MBRL methods,\nin terms of causal discovery, sample efficiency, and asymptotic performance.\n","authors":["Hongye Cao","Fan Feng","Meng Fang","Shaokang Dong","Tianpei Yang","Jing Huo","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2502.10077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10076v1","updated":"2025-02-14T10:55:15Z","published":"2025-02-14T10:55:15Z","title":"Classification of Temporal Graphs using Persistent Homology","summary":"  Temporal graphs effectively model dynamic systems by representing\ninteractions as timestamped edges. However, analytical tools for temporal\ngraphs are limited compared to static graphs. We propose a novel method for\nanalyzing temporal graphs using Persistent Homology. Our approach leverages\n$\\delta$-temporal motifs (recurrent subgraphs) to capture temporal dynamics\n%without aggregation\n  . By evolving these motifs, we define the \\textit{average filtration} and\ncompute PH on the associated clique complex. This method captures both local\nand global temporal structures and is stable with respect to reference models.\nWe demonstrate the applicability of our approach to the temporal graph\nclassification task. Experiments verify the effectiveness of our approach,\nachieving over 92\\% accuracy, with some cases reaching 100\\%. Unlike existing\nmethods that require node classes, our approach is node class free, offering\nflexibility for a wide range of temporal graph analysis.\n","authors":["Siddharth Pritam","Rohit Roy","Madhav Cherupilil Sajeev"],"pdf_url":"https://arxiv.org/pdf/2502.10076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10070v1","updated":"2025-02-14T10:45:36Z","published":"2025-02-14T10:45:36Z","title":"Topological Neural Networks over the Air","summary":"  Topological neural networks (TNNs) are information processing architectures\nthat model representations from data lying over topological spaces (e.g.,\nsimplicial or cell complexes) and allow for decentralized implementation\nthrough localized communications over different neighborhoods. Existing TNN\narchitectures have not yet been considered in realistic communication\nscenarios, where channel effects typically introduce disturbances such as\nfading and noise. This paper aims to propose a novel TNN design, operating on\nregular cell complexes, that performs over-the-air computation, incorporating\nthe wireless communication model into its architecture. Specifically, during\ntraining and inference, the proposed method considers channel impairments such\nas fading and noise in the topological convolutional filtering operation, which\ntakes place over different signal orders and neighborhoods. Numerical results\nillustrate the architecture's robustness to channel impairments during testing\nand the superior performance with respect to existing architectures, which are\neither communication-agnostic or graph-based.\n","authors":["Simone Fiorellino","Claudio Battiloro","Paolo Di Lorenzo"],"pdf_url":"https://arxiv.org/pdf/2502.10070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06309v2","updated":"2025-02-14T10:32:42Z","published":"2025-02-10T09:56:15Z","title":"Analog In-memory Training on General Non-ideal Resistive Elements: The\n  Impact of Response Functions","summary":"  As the economic and environmental costs of training and deploying large\nvision or language models increase dramatically, analog in-memory computing\n(AIMC) emerges as a promising energy-efficient solution. However, the training\nperspective, especially its training dynamic, is underexplored. In AIMC\nhardware, the trainable weights are represented by the conductance of resistive\nelements and updated using consecutive electrical pulses. Among all the\nphysical properties of resistive elements, the response to the pulses directly\naffects the training dynamics. This paper first provides a theoretical\nfoundation for gradient-based training on AIMC hardware and studies the impact\nof response functions. We demonstrate that noisy update and asymmetric response\nfunctions negatively impact Analog SGD by imposing an implicit penalty term on\nthe objective. To overcome the issue, Tiki-Taka, a residual learning algorithm,\nconverges exactly to a critical point by optimizing a main array and a residual\narray bilevelly. The conclusion is supported by simulations validating our\ntheoretical insights.\n","authors":["Zhaoxian Wu","Quan Xiao","Tayfun Gokmen","Omobayode Fagbohungbe","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01360v2","updated":"2025-02-14T10:28:36Z","published":"2025-02-03T13:52:17Z","title":"A Relative Homology Theory of Representation in Neural Networks","summary":"  Previous research has proven that the set of maps implemented by neural\nnetworks with a ReLU activation function is identical to the set of piecewise\nlinear continuous maps. Furthermore, such networks induce a hyperplane\narrangement splitting the input domain into convex polyhedra $G_J$ over which\nthe network $\\Phi$ operates in an affine manner.\n  In this work, we leverage these properties to define the equivalence class of\ninputs $\\sim_\\Phi$, which can be split into two sets related to the local rank\nof $\\Phi_J$ and the intersections $\\cap \\text{Im}\\Phi_{J_i}$. We refer to the\nlatter as the overlap decomposition $O_\\Phi$ and prove that if the\nintersections between each polyhedron and the input manifold are convex, the\nhomology groups of neural representations are isomorphic to relative homology\ngroups $H_k(\\Phi(M)) \\simeq H_k(M,O_\\Phi)$. This lets us compute Betti numbers\nwithout the choice of an external metric. We develop methods to numerically\ncompute the overlap decomposition through linear programming and a union-find\nalgorithm.\n  Using this framework, we perform several experiments on toy datasets showing\nthat, compared to standard persistent homology, our relative homology-based\ncomputation of Betti numbers tracks purely topological rather than geometric\nfeatures. Finally, we study the evolution of the overlap decomposition during\ntraining on various classification problems while varying network width and\ndepth and discuss some shortcomings of our method.\n","authors":["Kosio Beshkov"],"pdf_url":"https://arxiv.org/pdf/2502.01360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10060v1","updated":"2025-02-14T10:26:14Z","published":"2025-02-14T10:26:14Z","title":"DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery","summary":"  Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation.\n","authors":["Utkarsh Mall","Cheng Perng Phoo","Mia Chiquier","Bharath Hariharan","Kavita Bala","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2502.10060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07827v2","updated":"2025-02-14T10:17:31Z","published":"2025-02-10T19:59:31Z","title":"Implicit Language Models are RNNs: Balancing Parallelization and\n  Expressivity","summary":"  State-space models (SSMs) and transformers dominate the language modeling\nlandscape. However, they are constrained to a lower computational complexity\nthan classical recurrent neural networks (RNNs), limiting their expressivity.\nIn contrast, RNNs lack parallelization during training, raising fundamental\nquestions about the trade off between parallelization and expressivity. We\npropose implicit SSMs, which iterate a transformation until convergence to a\nfixed point. Theoretically, we show that implicit SSMs implement the non-linear\nstate-transitions of RNNs. Empirically, we find that only approximate\nfixed-point convergence suffices, enabling the design of a scalable training\ncurriculum that largely retains parallelization, with full convergence required\nonly for a small subset of tokens. Our approach demonstrates superior\nstate-tracking capabilities on regular languages, surpassing transformers and\nSSMs. We further scale implicit SSMs to natural language reasoning tasks and\npretraining of large-scale language models up to 1.3B parameters on 207B tokens\n- representing, to our knowledge, the largest implicit model trained to date.\nNotably, our implicit models outperform their explicit counterparts on standard\nbenchmarks.\n","authors":["Mark Sch√∂ne","Babak Rahmani","Heiner Kremer","Fabian Falck","Hitesh Ballani","Jannes Gladrow"],"pdf_url":"https://arxiv.org/pdf/2502.07827v2.pdf","comment":"25 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.12343v2","updated":"2025-02-14T09:58:53Z","published":"2024-10-16T08:04:57Z","title":"Federated Temporal Graph Clustering","summary":"  Temporal graph clustering is a complex task that involves discovering\nmeaningful structures in dynamic graphs where relationships and entities change\nover time. Existing methods typically require centralized data collection,\nwhich poses significant privacy and communication challenges. In this work, we\nintroduce a novel Federated Temporal Graph Clustering (FTGC) framework that\nenables decentralized training of graph neural networks (GNNs) across multiple\nclients, ensuring data privacy throughout the process. Our approach\nincorporates a temporal aggregation mechanism to effectively capture the\nevolution of graph structures over time and a federated optimization strategy\nto collaboratively learn high-quality clustering representations. By preserving\ndata privacy and reducing communication overhead, our framework achieves\ncompetitive performance on temporal graph datasets, making it a promising\nsolution for privacy-sensitive, real-world applications involving dynamic data.\n","authors":["Yang Liu","Zihao Zhou","Xianghong Xu","Qian Li"],"pdf_url":"https://arxiv.org/pdf/2410.12343v2.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2202.12040v6","updated":"2025-02-14T09:55:19Z","published":"2022-02-24T11:40:44Z","title":"Self-Training: A Survey","summary":"  Semi-supervised algorithms aim to learn prediction functions from a small set\nof labeled observations and a large set of unlabeled observations. Because this\nframework is relevant in many applications, they have received a lot of\ninterest in both academia and industry. Among the existing techniques,\nself-training methods have undoubtedly attracted greater attention in recent\nyears. These models are designed to find the decision boundary on low density\nregions without making additional assumptions about the data distribution, and\nuse the unsigned output score of a learned classifier, or its margin, as an\nindicator of confidence. The working principle of self-training algorithms is\nto learn a classifier iteratively by assigning pseudo-labels to the set of\nunlabeled training samples with a margin greater than a certain threshold. The\npseudo-labeled examples are then used to enrich the labeled training data and\nto train a new classifier in conjunction with the labeled training set. In this\npaper, we present self-training methods for binary and multi-class\nclassification; as well as their variants and two related approaches, namely\nconsistency-based approaches and transductive learning. We examine the impact\nof significant self-training features on various methods, using different\ngeneral and image classification benchmarks, and we discuss our ideas for\nfuture research in self-training. To the best of our knowledge, this is the\nfirst thorough and complete survey on this subject.\n","authors":["Massih-Reza Amini","Vasilii Feofanov","Loic Pauletto","Lies Hadjadj","Emilie Devijver","Yury Maximov"],"pdf_url":"https://arxiv.org/pdf/2202.12040v6.pdf","comment":"43 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.04344v3","updated":"2025-02-14T09:51:46Z","published":"2024-06-06T17:59:56Z","title":"Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models","summary":"  Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.\n","authors":["Tim Z. Xiao","Robert Bamler","Bernhard Sch√∂lkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.04344v3.pdf","comment":"Published in Transactions on Machine Learning Research (116 pages, 32\n  figures, v3: refined the paper structure and added more empirical results)"},{"id":"http://arxiv.org/abs/2502.03930v2","updated":"2025-02-14T09:49:57Z","published":"2025-02-06T10:09:49Z","title":"DiTAR: Diffusion Transformer Autoregressive Modeling for Speech\n  Generation","summary":"  Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness.\n","authors":["Dongya Jia","Zhuo Chen","Jiawei Chen","Chenpeng Du","Jian Wu","Jian Cong","Xiaobin Zhuang","Chumin Li","Zhen Wei","Yuping Wang","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03930v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.10481v2","updated":"2025-02-14T09:47:42Z","published":"2024-10-14T13:18:20Z","title":"Model-Based Privacy-Preserving Knowledge Transfer for Large Language\n  Models","summary":"  As large language models (LLMs) become more prevalent, effectively utilizing\ndomain-specific knowledge while ensuring privacy has become critical. Existing\nmethods often struggle to balance utility and privacy. For instance,\nretrieval-augmented generation (RAG) enables LLMs to access domain-specific\nknowledge but compromises the privacy of sensitive data. On the other hand,\ndifferentially private data synthesis techniques offer strong privacy\nguarantees but often result in poor utility. To address this challenge, we\npropose Llamdex, a novel framework that enhances LLMs using only models trained\non domain-specific data, integrated into LLMs through carefully designed\nconnection modules. Our approach significantly enhances the accuracy of\ndomain-specific tasks, achieving up to a 26% accuracy improvement compared to\nstate-of-the-art data synthesis methods under the same differential privacy\nconstraints. Experimental results show that Llamdex not only improves the\naccuracy of LLM responses but also maintains comparable inference efficiency to\nthe original LLM, highlighting its potential for real applications.\n","authors":["Zhaomin Wu","Jizhou Guo","Junyi Hou","Bingsheng He","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01858v4","updated":"2025-02-14T09:45:24Z","published":"2024-11-30T19:53:25Z","title":"MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully\n  Homomorphic Encryption","summary":"  The integration of fully homomorphic encryption (FHE) in federated learning\n(FL) has led to significant advances in data privacy. However, during the\naggregation phase, it often results in performance degradation of the\naggregated model, hindering the development of robust representational\ngeneralization. In this work, we propose a novel multimodal quantum federated\nlearning framework that utilizes quantum computing to counteract the\nperformance drop resulting from FHE. For the first time in FL, our framework\ncombines a multimodal quantum mixture of experts (MQMoE) model with FHE,\nincorporating multimodal datasets for enriched representation and task-specific\nlearning. Our MQMoE framework enhances performance on multimodal datasets and\ncombined genomics and brain MRI scans, especially for underrepresented\ncategories. Our results also demonstrate that the quantum-enhanced approach\nmitigates the performance degradation associated with FHE and improves\nclassification accuracy across diverse datasets, validating the potential of\nquantum interventions in enhancing privacy in FL.\n","authors":["Siddhant Dutta","Nouhaila Innan","Sadok Ben Yahia","Muhammad Shafique","David Esteban Bernal Neira"],"pdf_url":"https://arxiv.org/pdf/2412.01858v4.pdf","comment":"10 pages, 5 figures, 6 Tables. Under Review"},{"id":"http://arxiv.org/abs/2502.08644v3","updated":"2025-02-14T09:18:34Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v3.pdf","comment":"13 pages, 3 figures. v.2 comments: Updated email, updated typo on\n  p.11: h -> h^2 for RMSE. v.3 comments: Updated reference style, added\n  reference to Github repository"},{"id":"http://arxiv.org/abs/2502.09473v2","updated":"2025-02-14T09:14:14Z","published":"2025-02-13T16:36:25Z","title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse\n  Measurements","summary":"  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all\ntreatment with limited success in persistent AF. This may be due to our\ninability to map the dynamics of AF with the limited resolution and coverage\nprovided by sequential contact mapping catheters, preventing effective patient\nphenotyping for personalised, targeted ablation. Here we introduce FibMap, a\ngraph recurrent neural network model that reconstructs global AF dynamics from\nsparse measurements. Trained and validated on 51 non-contact whole atria\nrecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,\nachieving a 210% lower mean absolute error and an order of magnitude higher\nperformance in tracking phase singularities compared to baseline methods.\nClinical utility of FibMap is demonstrated on real-world contact mapping\nrecordings, achieving reconstruction fidelity comparable to non-contact\nmapping. FibMap's state-spaces and patient-specific parameters offer insights\nfor electrophenotyping AF. Integrating FibMap into clinical practice could\nenable personalised AF care and improve outcomes.\n","authors":["Alexander Jenkins","Andrea Cini","Joseph Barker","Alexander Sharp","Arunashis Sau","Varun Valentine","Srushti Valasang","Xinyang Li","Tom Wong","Timothy Betts","Danilo Mandic","Cesare Alippi","Fu Siong Ng"],"pdf_url":"https://arxiv.org/pdf/2502.09473v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.10027v1","updated":"2025-02-14T09:13:33Z","published":"2025-02-14T09:13:33Z","title":"Heterogeneous Resource Allocation with Multi-task Learning for Wireless\n  Networks","summary":"  The optimal solution to an optimization problem depends on the problem's\nobjective function, constraints, and size. While deep neural networks (DNNs)\nhave proven effective in solving optimization problems, changes in the\nproblem's size, objectives, or constraints often require adjustments to the DNN\narchitecture to maintain effectiveness, or even retraining a new DNN from\nscratch. Given the dynamic nature of wireless networks, which involve multiple\nand diverse objectives that can have conflicting requirements and constraints,\nwe propose a multi-task learning (MTL) framework to enable a single DNN to\njointly solve a range of diverse optimization problems. In this framework,\noptimization problems with varying dimensionality values, objectives, and\nconstraints are treated as distinct tasks. To jointly address these tasks, we\npropose a conditional computation-based MTL approach with routing. The\nmulti-task DNN consists of two components, the base DNN (bDNN), which is the\nsingle DNN used to extract the solutions for all considered optimization\nproblems, and the routing DNN (rDNN), which manages which nodes and layers of\nthe bDNN to be used during the forward propagation of each task. The output of\nthe rDNN is a binary vector which is multiplied with all bDNN's weights during\nthe forward propagation, creating a unique computational path through the bDNN\nfor each task. This setup allows the tasks to either share parameters or use\nindependent ones, with the decision controlled by the rDNN. The proposed\nframework supports both supervised and unsupervised learning scenarios.\nNumerical results demonstrate the efficiency of the proposed MTL approach in\nsolving diverse optimization problems. In contrast, benchmark DNNs lacking the\nrDNN mechanism were unable to achieve similar levels of performance,\nhighlighting the effectiveness of the proposed architecture.\n","authors":["Nikos A. Mitsiou","Pavlos S. Bouzinis","Panagiotis G. Sarigiannidis","George K. Karagiannidis"],"pdf_url":"https://arxiv.org/pdf/2502.10027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10020v1","updated":"2025-02-14T09:01:12Z","published":"2025-02-14T09:01:12Z","title":"Improved Online Confidence Bounds for Multinomial Logistic Bandits","summary":"  In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $ for sufficiently large\n$T$, where $\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$\nis the dimension of the contexts, and $T$ is the total number of rounds.\nFurthermore, we introduce an Maximum Likelihood Estimation (MLE)-based\nalgorithm that achieves an anytime, OFU-MN$^2$L, poly($(B)$)-free regret of $O\n\\Big( d \\log (BT) \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2502.10020v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2405.14135v2","updated":"2025-02-14T08:46:31Z","published":"2024-05-23T03:19:02Z","title":"Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs","summary":"  Regional socioeconomic indicators are critical across various domains, yet\ntheir acquisition can be costly. Inferring global socioeconomic indicators from\na limited number of regional samples is essential for enhancing management and\nsustainability in urban areas and human settlements. Current inference methods\ntypically rely on spatial interpolation based on the assumption of spatial\ncontinuity, which does not adequately address the complex variations present\nwithin regional spaces. In this paper, we present GeoHG, the first space-aware\nsocioeconomic indicator inference method that utilizes a heterogeneous\ngraph-based structure to represent geospace for non-continuous inference.\nExtensive experiments demonstrate the effectiveness of GeoHG in comparison to\nexisting methods, achieving an $R^2$ score exceeding 0.8 under extreme data\nscarcity with a masked ratio of 95\\%.\n","authors":["Xingchen Zou","Jiani Huang","Xixuan Hao","Yuhao Yang","Haomin Wen","Yibo Yan","Chao Huang","Chen Chao","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2405.14135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10011v1","updated":"2025-02-14T08:45:35Z","published":"2025-02-14T08:45:35Z","title":"InterGridNet: An Electric Network Frequency Approach for Audio Source\n  Location Classification Using Convolutional Neural Networks","summary":"  A novel framework, called InterGridNet, is introduced, leveraging a shallow\nRawNet model for geolocation classification of Electric Network Frequency (ENF)\nsignatures in the SP Cup 2016 dataset. During data preparation, recordings are\nsorted into audio and power groups based on inherent characteristics, further\ndivided into 50 Hz and 60 Hz groups via spectrogram analysis. Residual blocks\nwithin the classification model extract frame-level embeddings, aiding\ndecision-making through softmax activation. The topology and the\nhyperparameters of the shallow RawNet are optimized using a Neural Architecture\nSearch. The overall accuracy of InterGridNet in the test recordings is 92%,\nindicating its effectiveness against the state-of-the-art methods tested in the\nSP Cup 2016. These findings underscore InterGridNet's effectiveness in\naccurately classifying audio recordings from diverse power grids, advancing\nstate-of-the-art geolocation estimation methods.\n","authors":["Christos Korgialas","Ioannis Tsingalis","Georgios Tzolopoulos","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2502.10011v1.pdf","comment":"The 10th International Conference on Advances in Signal, Image and\n  Video Processing (SIGNAL 2025)"},{"id":"http://arxiv.org/abs/2302.01581v2","updated":"2025-02-14T08:43:23Z","published":"2023-02-03T07:24:58Z","title":"Learning to Decouple Complex Systems","summary":"  A complex system with cluttered observations may be a coupled mixture of\nmultiple simple sub-systems corresponding to latent entities. Such sub-systems\nmay hold distinct dynamics in the continuous-time domain; therein, complicated\ninteractions between sub-systems also evolve over time. This setting is fairly\ncommon in the real world but has been less considered. In this paper, we\npropose a sequential learning approach under this setting by decoupling a\ncomplex system for handling irregularly sampled and cluttered sequential\nobservations. Such decoupling brings about not only subsystems describing the\ndynamics of each latent entity but also a meta-system capturing the interaction\nbetween entities over time. Specifically, we argue that the meta-system\nevolving within a simplex is governed by projected differential equations\n(ProjDEs). We further analyze and provide neural-friendly projection operators\nin the context of Bregman divergence. Experimental results on synthetic and\nreal-world datasets show the advantages of our approach when facing complex and\ncluttered sequential data compared to the state-of-the-art.\n","authors":["Zihan Zhou","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2302.01581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01163v2","updated":"2025-02-14T08:38:03Z","published":"2024-07-01T10:33:44Z","title":"Benchmarking Predictive Coding Networks -- Made Simple","summary":"  In this work, we tackle the problems of efficiency and scalability for\npredictive coding networks (PCNs) in machine learning. To do so, we propose a\nlibrary, called PCX, that focuses on performance and simplicity, and use it to\nimplement a large set of standard benchmarks for the community to use for their\nexperiments. As most works in the field propose their own tasks and\narchitectures, do not compare one against each other, and focus on small-scale\ntasks, a simple and fast open-source library and a comprehensive set of\nbenchmarks would address all these concerns. Then, we perform extensive tests\non such benchmarks using both existing algorithms for PCNs, as well as\nadaptations of other methods popular in the bio-plausible deep learning\ncommunity. All this has allowed us to (i) test architectures much larger than\ncommonly used in the literature, on more complex datasets; (ii)~reach new\nstate-of-the-art results in all of the tasks and datasets provided;\n(iii)~clearly highlight what the current limitations of PCNs are, allowing us\nto state important future research directions. With the hope of galvanizing\ncommunity efforts towards one of the main open problems in the field,\nscalability, we release code, tests, and benchmarks. Link to the library:\nhttps://github.com/liukidar/pcx\n","authors":["Luca Pinchetti","Chang Qi","Oleh Lokshyn","Gaspard Olivers","Cornelius Emde","Mufeng Tang","Amine M'Charrak","Simon Frieder","Bayar Menzat","Rafal Bogacz","Thomas Lukasiewicz","Tommaso Salvatori"],"pdf_url":"https://arxiv.org/pdf/2407.01163v2.pdf","comment":"34 pages, 26 figures"},{"id":"http://arxiv.org/abs/2502.05679v2","updated":"2025-02-14T08:34:44Z","published":"2025-02-08T20:00:23Z","title":"Federated Learning with Reservoir State Analysis for Time Series Anomaly\n  Detection","summary":"  With a growing data privacy concern, federated learning has emerged as a\npromising framework to train machine learning models without sharing locally\ndistributed data. In federated learning, local model training by multiple\nclients and model integration by a server are repeated only through model\nparameter sharing. Most existing federated learning methods assume training\ndeep learning models, which are often computationally demanding. To deal with\nthis issue, we propose federated learning methods with reservoir state analysis\nto seek computational efficiency and data privacy protection simultaneously.\nSpecifically, our method relies on Mahalanobis Distance of Reservoir States\n(MD-RS) method targeting time series anomaly detection, which learns a\ndistribution of reservoir states for normal inputs and detects anomalies based\non a deviation from the learned distribution. Iterative updating of statistical\nparameters in the MD-RS enables incremental federated learning (IncFed MD-RS).\nWe evaluate the performance of IncFed MD-RS using benchmark datasets for time\nseries anomaly detection. The results show that IncFed MD-RS outperforms other\nfederated learning methods with deep learning and reservoir computing models\nparticularly when clients' data are relatively short and heterogeneous. We\ndemonstrate that IncFed MD-RS is robust against reduced sample data compared to\nother methods. We also show that the computational cost of IncFed MD-RS can be\nreduced by subsampling from the reservoir states without performance\ndegradation. The proposed method is beneficial especially in anomaly detection\napplications where computational efficiency, algorithm simplicity, and low\ncommunication cost are required.\n","authors":["Keigo Nogami","Hiroto Tamura","Gouhei Tanaka"],"pdf_url":"https://arxiv.org/pdf/2502.05679v2.pdf","comment":"8 pages, 16 figures, submitted to IJCNN 2025"},{"id":"http://arxiv.org/abs/2502.10001v1","updated":"2025-02-14T08:33:31Z","published":"2025-02-14T08:33:31Z","title":"EmbBERT-Q: Breaking Memory Barriers in Embedded NLP","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM.\n","authors":["Riccardo Bravin","Massimo Pavan","Hazem Hesham Yousef Shalby","Fabrizio Pittorino","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2502.10001v1.pdf","comment":"24 pages, 4 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.09998v1","updated":"2025-02-14T08:30:04Z","published":"2025-02-14T08:30:04Z","title":"Estimation of the Learning Coefficient Using Empirical Loss","summary":"  The learning coefficient plays a crucial role in analyzing the performance of\ninformation criteria, such as the Widely Applicable Information Criterion\n(WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC), which\nSumio Watanabe developed to assess model generalization ability. In regular\nstatistical models, the learning coefficient is given by d/2, where d is the\ndimension of the parameter space. More generally, it is defined as the absolute\nvalue of the pole order of a zeta function derived from the Kullback-Leibler\ndivergence and the prior distribution. However, except for specific cases such\nas reduced-rank regression, the learning coefficient cannot be derived in a\nclosed form. Watanabe proposed a numerical method to estimate the learning\ncoefficient, which Imai further refined to enhance its convergence properties.\nThese methods utilize the asymptotic behavior of WBIC and have been shown to be\nstatistically consistent as the sample size grows. In this paper, we propose a\nnovel numerical estimation method that fundamentally differs from previous\napproaches and leverages a new quantity, \"Empirical Loss,\" which was introduced\nby Watanabe. Through numerical experiments, we demonstrate that our proposed\nmethod exhibits both lower bias and lower variance compared to those of\nWatanabe and Imai. Additionally, we provide a theoretical analysis that\nelucidates why our method outperforms existing techniques and present empirical\nevidence that supports our findings.\n","authors":["Tatsuyoshi Takio","Joe Suzuki"],"pdf_url":"https://arxiv.org/pdf/2502.09998v1.pdf","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.12690v2","updated":"2025-02-14T08:28:29Z","published":"2025-01-22T08:02:01Z","title":"Growth strategies for arbitrary DAG neural architectures","summary":"  Deep learning has shown impressive results obtained at the cost of training\nhuge neural networks. However, the larger the architecture, the higher the\ncomputational, financial, and environmental costs during training and\ninference. We aim at reducing both training and inference durations. We focus\non Neural Architecture Growth, which can increase the size of a small model\nwhen needed, directly during training using information from the\nbackpropagation. We expand existing work and freely grow neural networks in the\nform of any Directed Acyclic Graph by reducing expressivity bottlenecks in the\narchitecture. We explore strategies to reduce excessive computations and steer\nnetwork growth toward more parameter-efficient architectures.\n","authors":["Stella Douka","Manon Verbockhaven","Th√©o Rudkiewicz","St√©phane Rivaud","Fran√ßois P. Landes","Sylvain Chevallier","Guillaume Charpiat"],"pdf_url":"https://arxiv.org/pdf/2501.12690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05894v2","updated":"2025-02-14T08:27:05Z","published":"2024-10-08T10:48:50Z","title":"DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning","summary":"  In the realm of computational physics, an enduring topic is the numerical\nsolutions to partial differential equations (PDEs). Recently, the attention of\nresearchers has shifted towards Neural Operator methods, renowned for their\ncapability to approximate ``operators'' -- mappings from functions to\nfunctions. Despite the universal approximation theorem within neural operators,\nensuring error bounds often requires employing numerous Fourier layers.\nHowever, what about lightweight models? In response to this question, we\nintroduce DimOL (Dimension-aware Operator Learning), drawing insights from\ndimensional analysis. To implement DimOL, we propose the ProdLayer, which can\nbe seamlessly integrated into FNO-based and Transformer-based PDE solvers,\nenhancing their ability to handle sum-of-products structures inherent in many\nphysical systems. Empirically, DimOL models achieve up to 48% performance gain\nwithin the PDE datasets. Furthermore, by analyzing Fourier components' weights,\nwe can symbolically discern the physical significance of each term. This sheds\nlight on the opaque nature of neural networks, unveiling underlying physical\nprinciples.\n","authors":["Yichen Song","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.05894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03759v2","updated":"2025-02-14T08:25:32Z","published":"2024-11-06T08:42:53Z","title":"Variational Inference on the Boolean Hypercube with the Quantum Entropy","summary":"  In this paper, we derive variational inference upper-bounds on the\nlog-partition function of pairwise Markov random fields on the Boolean\nhypercube, based on quantum relaxations of the Kullback-Leibler divergence. We\nthen propose an efficient algorithm to compute these bounds based on\nprimal-dual optimization. An improvement of these bounds through the use of\n''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and\nwe present a greedy algorithm to select among these relaxations. We carry\nextensive numerical experiments and compare with state-of-the-art methods for\nthis inference problem.\n","authors":["Eliot Beyler","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2411.03759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09992v1","updated":"2025-02-14T08:23:51Z","published":"2025-02-14T08:23:51Z","title":"Large Language Diffusion Models","summary":"  Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs.\n","authors":["Shen Nie","Fengqi Zhu","Zebin You","Xiaolu Zhang","Jingyang Ou","Jun Hu","Jun Zhou","Yankai Lin","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09990v1","updated":"2025-02-14T08:22:51Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09985v1","updated":"2025-02-14T08:14:22Z","published":"2025-02-14T08:14:22Z","title":"On Volume Minimization in Conformal Regression","summary":"  We study the question of volume optimality in split conformal regression, a\ntopic still poorly understood in comparison to coverage control. Using the fact\nthat the calibration step can be seen as an empirical volume minimization\nproblem, we first derive a finite-sample upper-bound on the excess volume loss\nof the interval returned by the classical split method. This important quantity\nmeasures the difference in length between the interval obtained with the split\nmethod and the shortest oracle prediction interval. Then, we introduce EffOrt,\na methodology that modifies the learning step so that the base prediction\nfunction is selected in order to minimize the length of the returned intervals.\nIn particular, our theoretical analysis of the excess volume loss of the\nprediction sets produced by EffOrt reveals the links between the learning and\ncalibration steps, and notably the impact of the choice of the function class\nof the base predictor. We also introduce Ad-EffOrt, an extension of the\nprevious method, which produces intervals whose size adapts to the value of the\ncovariate. Finally, we evaluate the empirical performance and the robustness of\nour methodologies.\n","authors":["Batiste Le Bars","Pierre Humbert"],"pdf_url":"https://arxiv.org/pdf/2502.09985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09981v1","updated":"2025-02-14T08:07:03Z","published":"2025-02-14T08:07:03Z","title":"Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal\n  Dependencies in Complex Data","summary":"  Causality in time series can be difficult to determine, especially in the\npresence of non-linear dependencies. The concept of Granger causality helps\nanalyze potential relationships between variables, thereby offering a method to\ndetermine whether one time series can predict-Granger cause-future values of\nanother. Although successful, Granger causal methods still struggle with\ncapturing long-range relations between variables. To this end, we leverage the\nrecently successful Extended Long Short-Term Memory (xLSTM) architecture and\npropose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between\nthe time series components by using a novel dynamic lass penalty on the initial\nprojection. Specifically, we adaptively improve the model and identify sparsity\ncandidates. Our joint optimization procedure then ensures that the Granger\ncausal relations are recovered in a robust fashion. Our experimental\nevaluations on three datasets demonstrate the overall efficacy of our proposed\nGC-xLSTM model.\n","authors":["Harsh Poonia","Felix Divo","Kristian Kersting","Devendra Singh Dhami"],"pdf_url":"https://arxiv.org/pdf/2502.09981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12432v2","updated":"2025-02-14T08:05:28Z","published":"2024-06-18T09:28:23Z","title":"MEMS and ECM Sensor Technologies for Cardiorespiratory Sound Monitoring\n  - A Comprehensive Review","summary":"  This paper presents a comprehensive review of cardiorespiratory auscultation\nsensing devices (i.e., stethoscopes), which is useful for understanding the\ntheoretical aspects and practical design notes. In this paper, we first\nintroduce the acoustic properties of the heart and lungs, as well as a brief\nhistory of stethoscope evolution. Then, we discuss the basic concept of\nelectret condenser microphones (ECMs) and a stethoscope based on them. Then, we\ndiscuss the microelectromechanical systems (MEMSs) technology, particularly\nfocusing on piezoelectric transducer sensors. This paper comprehensively\nreviews sensing technologies for cardiorespiratory auscultation, emphasizing\nMEMS-based wearable designs in the past decade. To our knowledge, this is the\nfirst paper to summarize ECM and MEMS applications for heart and lung sound\nanalysis.\n","authors":["Yasaman Torabi","Shahram Shirani","James P. Reilly","Gail M Gauvreau"],"pdf_url":"https://arxiv.org/pdf/2406.12432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09970v1","updated":"2025-02-14T07:55:53Z","published":"2025-02-14T07:55:53Z","title":"Universal Machine Learning Interatomic Potentials are Ready for Solid\n  Ion Conductors","summary":"  With the rapid development of energy storage technology, high-performance\nsolid-state electrolytes (SSEs) have become critical for next-generation\nlithium-ion batteries. These materials require high ionic conductivity,\nexcellent electrochemical stability, and good mechanical properties to meet the\ndemands of electric vehicles and portable electronics. However, traditional\nmethods like density functional theory (DFT) and empirical force fields face\nchallenges such as high computational costs, poor scalability, and limited\naccuracy across material systems. Universal machine learning interatomic\npotentials (uMLIPs) offer a promising solution with their efficiency and\nnear-DFT-level accuracy.This study systematically evaluates six advanced uMLIP\nmodels (MatterSim, MACE, SevenNet, CHGNet, M3GNet, and ORBFF) in terms of\nenergy, forces, thermodynamic properties, elastic moduli, and lithium-ion\ndiffusion behavior. The results show that MatterSim outperforms others in\nnearly all metrics, particularly in complex material systems, demonstrating\nsuperior accuracy and physical consistency. Other models exhibit significant\ndeviations due to issues like energy inconsistency or insufficient training\ndata coverage.Further analysis reveals that MatterSim achieves excellent\nagreement with reference values in lithium-ion diffusivity calculations,\nespecially at room temperature. Studies on Li3YCl6 and Li6PS5Cl uncover how\ncrystal structure, anion disorder levels, and Na/Li arrangements influence\nionic conductivity. Appropriate S/Cl disorder levels and optimized Na/Li\narrangements enhance diffusion pathway connectivity, improving overall ionic\ntransport performance.\n","authors":["Hongwei Du","Jian Hui","Lanting Zhang","Hong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09969v1","updated":"2025-02-14T07:55:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-Tur"],"pdf_url":"https://arxiv.org/pdf/2502.09969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09956v1","updated":"2025-02-14T07:28:08Z","published":"2025-02-14T07:28:08Z","title":"KGGen: Extracting Knowledge Graphs from Plain Text with Language Models","summary":"  Recent interest in building foundation models for KGs has highlighted a\nfundamental challenge: knowledge-graph data is relatively scarce. The\nbest-known KGs are primarily human-labeled, created by pattern-matching, or\nextracted using early NLP techniques. While human-generated KGs are in short\nsupply, automatically extracted KGs are of questionable quality. We present a\nsolution to this data scarcity problem in the form of a text-to-KG generator\n(KGGen), a package that uses language models to create high-quality graphs from\nplaintext. Unlike other KG extractors, KGGen clusters related entities to\nreduce sparsity in extracted KGs. KGGen is available as a Python library\n(\\texttt{pip install kg-gen}), making it accessible to everyone. Along with\nKGGen, we release the first benchmark, Measure of of Information in Nodes and\nEdges (MINE), that tests an extractor's ability to produce a useful KG from\nplain text. We benchmark our new tool against existing extractors and\ndemonstrate far superior performance.\n","authors":["Belinda Mo","Kyssen Yu","Joshua Kazdan","Proud Mpala","Lisa Yu","Chris Cundy","Charilaos Kanatsoulis","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2502.09956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09954v1","updated":"2025-02-14T07:22:24Z","published":"2025-02-14T07:22:24Z","title":"On Space Folds of ReLU Neural Networks","summary":"  Recent findings suggest that the consecutive layers of ReLU neural networks\ncan be understood geometrically as space folding transformations of the input\nspace, revealing patterns of self-similarity. In this paper, we present the\nfirst quantitative analysis of this space folding phenomenon in ReLU neural\nnetworks. Our approach focuses on examining how straight paths in the Euclidean\ninput space are mapped to their counterparts in the Hamming activation space.\nIn this process, the convexity of straight lines is generally lost, giving rise\nto non-convex folding behavior. To quantify this effect, we introduce a novel\nmeasure based on range metrics, similar to those used in the study of random\nwalks, and provide the proof for the equivalence of convexity notions between\nthe input and activation spaces. Furthermore, we provide empirical analysis on\na geometrical analysis benchmark (CantorNet) as well as an image classification\nbenchmark (MNIST). Our work advances the understanding of the activation space\nin ReLU neural networks by leveraging the phenomena of geometric folding,\nproviding valuable insights on how these models process input information.\n","authors":["Michal Lewandowski","Hamid Eghbalzadeh","Bernhard Heinzl","Raphael Pisoni","Bernhard A. Moser"],"pdf_url":"https://arxiv.org/pdf/2502.09954v1.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR), 2025"},{"id":"http://arxiv.org/abs/2501.17878v2","updated":"2025-02-14T07:09:24Z","published":"2025-01-20T03:37:51Z","title":"Collaborative Channel Access and Transmission for NR Sidelink and Wi-Fi\n  Coexistence over Unlicensed Spectrum","summary":"  With the rapid development of various internet of things (IoT) applications,\nincluding industrial IoT (IIoT) and visual IoT (VIoT), the demand for direct\ndevice-to-device communication to support high data rates continues to grow. To\naddress this demand, 5G-Advanced has introduced sidelink communication over the\nunlicensed spectrum (SL-U) to increase data rates. However, the primary\nchallenge of SL-U in the unlicensed spectrum is ensuring fair coexistence with\nother incumbent systems, such as Wi-Fi. In this paper, we address the challenge\nby designing channel access mechanisms and power control strategies to mitigate\ninterference and ensure fair coexistence. First, we propose a novel\ncollaborative channel access (CCHA) mechanism that integrates channel access\nwith resource allocation through collaborative interactions between base\nstations (BS) and SL-U users. This mechanism ensures fair coexistence with\nincumbent systems while improving resource utilization. Second, to further\nenhance the performance of the coexistence system, we develop a cooperative\nsubgoal-based hierarchical deep reinforcement learning (C-GHDRL) algorithm\nframework. The framework enables SL-U users to make globally optimal decisions\nby leveraging cooperative operations between the BS and SL-U users, effectively\novercoming the limitations of traditional optimization methods in solving joint\noptimization problems with nonlinear constraints. Finally, we mathematically\nmodel the joint channel access and power control problem and balance the\ntrade-off between fairness and transmission rate in the coexistence system by\ndefining a suitable reward function in the C-GHDRL algorithm. Simulation\nresults demonstrate that the proposed scheme significantly enhances the\nperformance of the coexistence system while ensuring fair coexistence between\nSL-U and Wi-Fi users.\n","authors":["Zhuangzhuang Yan","Xinyu Gu","Zhenyu Liu","Liyang Lu"],"pdf_url":"https://arxiv.org/pdf/2501.17878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12261v2","updated":"2025-02-14T06:59:46Z","published":"2024-10-16T05:58:55Z","title":"CATCH: Channel-Aware multivariate Time Series Anomaly Detection via\n  Frequency Patching","summary":"  Anomaly detection in multivariate time series is challenging as heterogeneous\nsubsequence anomalies may occur. Reconstruction-based methods, which focus on\nlearning normal patterns in the frequency domain to detect diverse abnormal\nsubsequences, achieve promising results, while still falling short on capturing\nfine-grained frequency characteristics and channel correlations. To contend\nwith the limitations, we introduce CATCH, a framework based on frequency\npatching. We propose to patchify the frequency domain into frequency bands,\nwhich enhances its ability to capture fine-grained frequency characteristics.\nTo perceive appropriate channel correlations, we propose a Channel Fusion\nModule (CFM), which features a patch-wise mask generator and a masked-attention\nmechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM\nis encouraged to iteratively discover appropriate patch-wise channel\ncorrelations, and to cluster relevant channels while isolating adverse effects\nfrom irrelevant channels. Extensive experiments on 10 real-world datasets and\n12 synthetic datasets demonstrate that CATCH achieves state-of-the-art\nperformance. We make our code and datasets available at\nhttps://github.com/decisionintelligence/CATCH.\n","authors":["Xingjian Wu","Xiangfei Qiu","Zhengyu Li","Yihang Wang","Jilin Hu","Chenjuan Guo","Hui Xiong","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12261v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09947v1","updated":"2025-02-14T06:53:52Z","published":"2025-02-14T06:53:52Z","title":"Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage\n  Encoding Model","summary":"  In the analysis of remote healthcare monitoring data, time series\nrepresentation learning offers substantial value in uncovering deeper patterns\nof patient behavior, especially given the fine temporal granularity of the\ndata. In this study, we focus on a dataset of home activity records from people\nliving with Dementia. We propose a two-stage self-supervised learning approach.\nThe first stage involves converting time-series activities into text strings,\nwhich are then encoded by a fine-tuned language model. In the second stage,\nthese time-series vectors are bi-dimensionalized for applying PageRank method,\nto analyze latent state transitions to quantitatively assess participants\nbehavioral patterns and identify activity biases. These insights, combined with\ndiagnostic data, aim to support personalized care interventions.\n","authors":["Jin Cui","Alexander Capstick","Payam Barnaghi","Gregory Scott"],"pdf_url":"https://arxiv.org/pdf/2502.09947v1.pdf","comment":"NeurIPS 2024 workshop Time Series in the Age of Large Models. arXiv\n  admin note: substantial text overlap with arXiv:2502.09173"},{"id":"http://arxiv.org/abs/2501.15085v2","updated":"2025-02-14T06:50:36Z","published":"2025-01-25T05:28:44Z","title":"Data Center Cooling System Optimization Using Offline Reinforcement\n  Learning","summary":"  The recent advances in information technology and artificial intelligence\nhave fueled a rapid expansion of the data center (DC) industry worldwide,\naccompanied by an immense appetite for electricity to power the DCs. In a\ntypical DC, around 30~40% of the energy is spent on the cooling system rather\nthan on computer servers, posing a pressing need for developing new\nenergy-saving optimization technologies for DC cooling systems. However,\noptimizing such real-world industrial systems faces numerous challenges,\nincluding but not limited to a lack of reliable simulation environments,\nlimited historical data, and stringent safety and control robustness\nrequirements. In this work, we present a novel physics-informed offline\nreinforcement learning (RL) framework for energy efficiency optimization of DC\ncooling systems. The proposed framework models the complex dynamical patterns\nand physical dependencies inside a server room using a purposely designed graph\nneural network architecture that is compliant with the fundamental\ntime-reversal symmetry. Because of its well-behaved and generalizable\nstate-action representations, the model enables sample-efficient and robust\nlatent space offline policy learning using limited real-world operational data.\nOur framework has been successfully deployed and verified in a large-scale\nproduction DC for closed-loop control of its air-cooling units (ACUs). We\nconducted a total of 2000 hours of short and long-term experiments in the\nproduction DC environment. The results show that our method achieves 14~21%\nenergy savings in the DC cooling system, without any violation of the safety or\noperational constraints. Our results have demonstrated the significant\npotential of offline RL in solving a broad range of data-limited,\nsafety-critical real-world industrial control problems.\n","authors":["Xianyuan Zhan","Xiangyu Zhu","Peng Cheng","Xiao Hu","Ziteng He","Hanfei Geng","Jichao Leng","Huiwen Zheng","Chenhui Liu","Tianshun Hong","Yan Liang","Yunxin Liu","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.15085v2.pdf","comment":"Accepted in ICLR 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.10329v1","updated":"2025-02-14T17:43:01Z","published":"2025-02-14T17:43:01Z","title":"VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking\n  Effect","summary":"  The rapid advancements in AI voice cloning, fueled by machine learning, have\nsignificantly impacted text-to-speech (TTS) and voice conversion (VC) fields.\nWhile these developments have led to notable progress, they have also raised\nconcerns about the misuse of AI VC technology, causing economic losses and\nnegative public perceptions. To address this challenge, this study focuses on\ncreating active defense mechanisms against AI VC systems.\n  We propose a novel active defense method, VocalCrypt, which embeds\npseudo-timbre (jamming information) based on SFS into audio segments that are\nimperceptible to the human ear, thereby forming systematic fragments to prevent\nvoice cloning. This approach protects the voice without compromising its\nquality. In comparison to existing methods, such as adversarial noise\nincorporation, VocalCrypt significantly enhances robustness and real-time\nperformance, achieving a 500\\% increase in generation speed while maintaining\ninterference effectiveness.\n  Unlike audio watermarking techniques, which focus on post-detection, our\nmethod offers preemptive defense, reducing implementation costs and enhancing\nfeasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets\nshow that our AI-cloned speech defense system performs excellently in automatic\nspeaker verification (ASV) tests while preserving the integrity of the\nprotected audio.\n","authors":["Qingyuan Fei","Wenjie Hou","Xuan Hai","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.10329v1.pdf","comment":"9 pages, four figures"},{"id":"http://arxiv.org/abs/2502.10154v1","updated":"2025-02-14T13:32:59Z","published":"2025-02-14T13:32:59Z","title":"Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries","summary":"  We introduce EMSYNC, a video-based symbolic music generation model that\naligns music with a video's emotional content and temporal boundaries. It\nfollows a two-stage framework, where a pretrained video emotion classifier\nextracts emotional features, and a conditional music generator produces MIDI\nsequences guided by both emotional and temporal cues. We introduce boundary\noffsets, a novel temporal conditioning mechanism that enables the model to\nanticipate and align musical chords with scene cuts. Unlike existing models,\nour approach retains event-based encoding, ensuring fine-grained timing control\nand expressive musical nuances. We also propose a mapping scheme to bridge the\nvideo emotion classifier, which produces discrete emotion categories, with the\nemotion-conditioned MIDI generator, which operates on continuous-valued\nvalence-arousal inputs. In subjective listening tests, EMSYNC outperforms\nstate-of-the-art models across all subjective metrics, for music theory-aware\nparticipants as well as the general listeners.\n","authors":["Serkan Sulun","Paula Viana","Matthew E. P. Davies"],"pdf_url":"https://arxiv.org/pdf/2502.10154v1.pdf","comment":"Submitted to International Joint Conference on Artificial\n  Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2502.10145v1","updated":"2025-02-14T13:15:21Z","published":"2025-02-14T13:15:21Z","title":"Interpretable Concept-based Deep Learning Framework for Multimodal Human\n  Behavior Modeling","summary":"  In the contemporary era of intelligent connectivity, Affective Computing\n(AC), which enables systems to recognize, interpret, and respond to human\nbehavior states, has become an integrated part of many AI systems. As one of\nthe most critical components of responsible AI and trustworthiness in all\nhuman-centered systems, explainability has been a major concern in AC.\nParticularly, the recently released EU General Data Protection Regulation\nrequires any high-risk AI systems to be sufficiently interpretable, including\nbiometric-based systems and emotion recognition systems widely used in the\naffective computing field. Existing explainable methods often compromise\nbetween interpretability and performance. Most of them focus only on\nhighlighting key network parameters without offering meaningful,\ndomain-specific explanations to the stakeholders. Additionally, they also face\nchallenges in effectively co-learning and explaining insights from multimodal\ndata sources. To address these limitations, we propose a novel and\ngeneralizable framework, namely the Attention-Guided Concept Model (AGCM),\nwhich provides learnable conceptual explanations by identifying what concepts\nthat lead to the predictions and where they are observed. AGCM is extendable to\nany spatial and temporal signals through multimodal concept alignment and\nco-learning, empowering stakeholders with deeper insights into the model's\ndecision-making process. We validate the efficiency of AGCM on well-established\nFacial Expression Recognition benchmark datasets while also demonstrating its\ngeneralizability on more complex real-world human behavior understanding\napplications.\n","authors":["Xinyu Li","Marwa Mahmoud"],"pdf_url":"https://arxiv.org/pdf/2502.10145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08093v3","updated":"2025-02-14T05:33:21Z","published":"2024-08-15T11:36:18Z","title":"When Video Coding Meets Multimodal Large Language Models: A Unified\n  Paradigm for Video Coding","summary":"  Existing codecs are designed to eliminate intrinsic redundancies to create a\ncompact representation for compression. However, strong external priors from\nMultimodal Large Language Models (MLLMs) have not been explicitly explored in\nvideo compression. Herein, we introduce a unified paradigm for Cross-Modality\nVideo Coding (CMVC), which is a pioneering approach to explore multimodality\nrepresentation and video generative models in video coding. Specifically, on\nthe encoder side, we disentangle a video into spatial content and motion\ncomponents, which are subsequently transformed into distinct modalities to\nachieve very compact representation by leveraging MLLMs. During decoding,\npreviously encoded components and video generation models are leveraged to\ncreate multiple encoding-decoding modes that optimize video reconstruction\nquality for specific decoding requirements, including Text-Text-to-Video (TT2V)\nmode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)\nmode to achieve superb perceptual consistency. In addition, we propose an\nefficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)\ntuning to guarantee perceptual quality, which allows the generated motion cues\nto behave smoothly. Experiments on benchmarks indicate that TT2V achieves\neffective semantic reconstruction, while IT2V exhibits competitive perceptual\nconsistency. These results highlight potential directions for future research\nin video coding.\n","authors":["Pingping Zhang","Jinlong Li","Kecheng Chen","Meng Wang","Long Xu","Haoliang Li","Nicu Sebe","Sam Kwong","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09843v1","updated":"2025-02-14T01:05:51Z","published":"2025-02-14T01:05:51Z","title":"MuDoC: An Interactive Multimodal Document-grounded Conversational AI\n  System","summary":"  Multimodal AI is an important step towards building effective tools to\nleverage multiple modalities in human-AI communication. Building a multimodal\ndocument-grounded AI system to interact with long documents remains a\nchallenge. Our work aims to fill the research gap of directly leveraging\ngrounded visuals from documents alongside textual content in documents for\nresponse generation. We present an interactive conversational AI agent 'MuDoC'\nbased on GPT-4o to generate document-grounded responses with interleaved text\nand figures. MuDoC's intelligent textbook interface promotes trustworthiness\nand enables verification of system responses by allowing instant navigation to\nsource text and figures in the documents. We also discuss qualitative\nobservations based on MuDoC responses highlighting its strengths and\nlimitations.\n","authors":["Karan Taneja","Ashok K. Goel"],"pdf_url":"https://arxiv.org/pdf/2502.09843v1.pdf","comment":"5 pages, 3 figures, AAAI-MAKE 2025"}]},"2025-02-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.18997v2","updated":"2025-02-13T23:45:10Z","published":"2024-09-19T06:28:18Z","title":"PropaInsight: Toward Deeper Understanding of Propaganda in Terms of\n  Techniques, Appeals, and Intent","summary":"  Propaganda plays a critical role in shaping public opinion and fueling\ndisinformation. While existing research primarily focuses on identifying\npropaganda techniques, it lacks the ability to capture the broader motives and\nthe impacts of such content. To address these challenges, we introduce\npropainsight, a conceptual framework grounded in foundational social science\nresearch, which systematically dissects propaganda into techniques, arousal\nappeals, and underlying intent. propainsight offers a more granular\nunderstanding of how propaganda operates across different contexts.\nAdditionally, we present propagaze, a novel dataset that combines\nhuman-annotated data with high-quality synthetic data generated through a\nmeticulously designed pipeline. Our experiments show that off-the-shelf LLMs\nstruggle with propaganda analysis, but training with propagaze significantly\nimproves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span\nIoU in technique identification and 66.2% higher BertScore in appeal analysis\ncompared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited\nhuman-annotated data in data-sparse and cross-domain scenarios, showing its\npotential for comprehensive and generalizable propaganda analysis.\n","authors":["Jiateng Liu","Lin Ai","Zizhou Liu","Payam Karisani","Zheng Hui","May Fung","Preslav Nakov","Julia Hirschberg","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2409.18997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18649v2","updated":"2025-02-13T23:32:38Z","published":"2024-05-28T23:20:24Z","title":"LeDex: Training LLMs to Better Self-Debug and Explain Code","summary":"  In the domain of code generation, self-debugging is crucial. It allows LLMs\nto refine their generated code based on execution feedback. This is\nparticularly important because generating correct solutions in one attempt\nproves challenging for complex tasks. Prior works on self-debugging mostly\nfocus on prompting methods by providing LLMs with few-shot examples, which work\npoorly on small open-sourced LLMs. In this work, we propose LeDex, a training\nframework that significantly improves the self-debugging capability of LLMs.\nIntuitively, we observe that a chain of explanations on the wrong code followed\nby code refinement helps LLMs better analyze the wrong code and do refinement.\nWe thus propose an automated pipeline to collect a high-quality dataset for\ncode explanation and refinement by generating a number of explanations and\nrefinement trajectories from the LLM itself or a larger teacher model and\nfiltering via execution verification. We perform supervised fine-tuning (SFT)\nand further reinforcement learning (RL) on both success and failure\ntrajectories with a novel reward design considering code explanation and\nrefinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by\n9.30% over four benchmarks. RL training brings additional up to 3.54%\nimprovement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show\niterative refinement ability and can keep refining code continuously. Lastly,\nour human evaluation shows that the LLMs trained with our framework generate\nmore useful code explanations and help developers better understand bugs in\nsource code.\n","authors":["Nan Jiang","Xiaopeng Li","Shiqi Wang","Qiang Zhou","Soneya Binta Hossain","Baishakhi Ray","Varun Kumar","Xiaofei Ma","Anoop Deoras"],"pdf_url":"https://arxiv.org/pdf/2405.18649v2.pdf","comment":"This paper is accepted by The Thirty-eighth Annual Conference on\n  Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2502.09815v1","updated":"2025-02-13T23:24:25Z","published":"2025-02-13T23:24:25Z","title":"Statistical Coherence Alignment for Large Language Model Representation\n  Learning Through Tensor Field Convergence","summary":"  Representation learning plays a central role in structuring internal\nembeddings to capture the statistical properties of language, influencing the\ncoherence and contextual consistency of generated text. Statistical Coherence\nAlignment is introduced as a method to enforce structured token representations\nthrough tensor field convergence, guiding embeddings to reflect statistical\ndependencies inherent in linguistic data. A mathematical framework is\nestablished to quantify coherence alignment, integrating a loss function that\noptimizes representational consistency across training iterations. Empirical\nevaluations demonstrate that applying coherence constraints improves\nperplexity, enhances classification accuracy, and refines rare word embeddings,\ncontributing to a more stable representation space. Comparative analyses with\nbaseline models reveal that the proposed method fosters a more interpretable\ninternal structure, ensuring that embeddings retain contextual dependencies\nwhile mitigating representation collapse. The impact on coherence score\ndistributions suggests that the alignment mechanism strengthens semantic\nintegrity across diverse linguistic constructs, leading to a more balanced\norganization of learned embeddings. Computational assessments indicate that\nwhile the method introduces additional memory and training costs, the\nstructured optimization process justifies the trade-offs in applications\nrequiring heightened contextual fidelity. Experimental results validate the\neffectiveness of coherence alignment in optimizing token representations,\nproviding insights into how statistical dependencies can be leveraged to\nimprove language model training.\n","authors":["Jonathan Gale","Godfrey Aldington","Harriet Thistlewood","Thomas Tattershall","Basil Wentworth","Vincent Enoasmo"],"pdf_url":"https://arxiv.org/pdf/2502.09815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09814v1","updated":"2025-02-13T23:17:10Z","published":"2025-02-13T23:17:10Z","title":"INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for\n  16 African Languages","summary":"  Slot-filling and intent detection are well-established tasks in\nConversational AI. However, current large-scale benchmarks for these tasks\noften exclude evaluations of low-resource languages and rely on translations\nfrom English benchmarks, thereby predominantly reflecting Western-centric\nconcepts. In this paper, we introduce Injongo -- a multicultural, open-source\nbenchmark dataset for 16 African languages with utterances generated by native\nspeakers across diverse domains, including banking, travel, home, and dining.\nThrough extensive experiments, we benchmark the fine-tuning multilingual\ntransformer models and the prompting large language models (LLMs), and show the\nadvantage of leveraging African-cultural utterances over Western-centric\nutterances for improving cross-lingual transfer from the English language.\nExperimental results reveal that current LLMs struggle with the slot-filling\ntask, with GPT-4o achieving an average performance of 26 F1-score. In contrast,\nintent detection performance is notably better, with an average accuracy of\n70.6%, though it still falls behind the fine-tuning baselines. Compared to the\nEnglish language, GPT-4o and fine-tuning baselines perform similarly on intent\ndetection, achieving an accuracy of approximately 81%. Our findings suggest\nthat the performance of LLMs is still behind for many low-resource African\nlanguages, and more work is needed to further improve their downstream\nperformance.\n","authors":["Hao Yu","Jesujoba O. Alabi","Andiswa Bukula","Jian Yun Zhuang","En-Shiun Annie Lee","Tadesse Kebede Guge","Israel Abebe Azime","Happy Buzaaba","Blessing Kudzaishe Sibanda","Godson K. Kalipe","Jonathan Mukiibi","Salomon Kabongo Kabenamualu","Mmasibidi Setaka","Lolwethu Ndolela","Nkiruka Odu","Rooweither Mabuya","Shamsuddeen Hassan Muhammad","Salomey Osei","Sokhar Samb","Juliet W. Murage","Dietrich Klakow","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2502.09814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04237v2","updated":"2025-02-13T22:13:21Z","published":"2024-04-05T17:36:26Z","title":"GroundCocoa: A Benchmark for Evaluating Compositional & Conditional\n  Reasoning in Language Models","summary":"  The rapid progress of large language models (LLMs) has seen them excel and\nfrequently surpass human performance on standard benchmarks. This has enabled\nmany downstream applications, such as LLM agents, to rely on their reasoning to\naddress complex task requirements. However, LLMs are known to unexpectedly\nfalter in simple tasks and under seemingly straightforward circumstances -\nunderscoring the need for better and more diverse evaluation setups to measure\ntheir true capabilities. To this end, we choose to study compositional and\nconditional reasoning, two aspects that are central to human cognition, and\nintroduce GroundCocoa - a lexically diverse benchmark connecting these\nreasoning skills to the real-world problem of flight booking. Our task involves\naligning detailed user preferences with available flight options presented in a\nmultiple-choice format. Results indicate a significant disparity in performance\namong current state-of-the-art LLMs with even the best performing model, GPT-4\nTurbo, not exceeding 67% accuracy despite advanced prompting techniques.\n","authors":["Harsh Kohli","Sachin Kumar","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2404.04237v2.pdf","comment":"16 pages, 17 figures, 3 tables. Accepted to NAACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2410.21236v2","updated":"2025-02-13T21:50:10Z","published":"2024-10-28T17:30:01Z","title":"Flaming-hot Initiation with Regular Execution Sampling for Large\n  Language Models","summary":"  Since the release of ChatGPT, large language models (LLMs) have demonstrated\nremarkable capabilities across various domains. A key challenge in developing\nthese general capabilities is efficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-related tasks with sandbox\ncheckers, such as math or code, where the goal is to generate correct solutions\nto specific problems with higher probability. In this work, we introduce\nFlaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet\nhighly effective method to efficiently find good responses. Our empirical\nfindings show that FIRE sampling enhances inference-time generation quality and\nalso benefits training in the alignment stage. Furthermore, we explore how FIRE\nsampling improves performance by promoting diversity and analyze the impact of\nemploying FIRE at different positions within a response.\n","authors":["Weizhe Chen","Zhicheng Zhang","Guanlin Liu","Renjie Zheng","Wenlei Shi","Chen Dun","Zheng Wu","Xing Jin","Lin Yan"],"pdf_url":"https://arxiv.org/pdf/2410.21236v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11132v2","updated":"2025-02-13T21:38:42Z","published":"2024-06-17T01:23:11Z","title":"RePrompt: Planning by Automatic Prompt Engineering for Large Language\n  Models Agents","summary":"  In the past year, large language models (LLMs) have had remarkable success in\ndomains outside the traditional natural language processing, and their capacity\nis further expanded into the so-called LLM agents when connected with external\ntools. In all domains, the prompt to the LLMs has been shown to make a big\ndifference in what the LLM would generate and thus affect the performance of\nthe LLM agents. Therefore, automatic prompt engineering (APE) has become an\nimportant question for many researchers and users of LLMs. However, previous\nworks in APE rely on a final checker to evaluate the performance of the given\nprompt -- a requirement that is hard to meet in the case of LLM agents, where\nintermediate feedback is easier to obtain, and the final evaluation could be\nexpensive, inaccurate, or even missing. In this paper, we propose a novel\nmethod, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to\noptimize the step-by-step instructions in the prompts given to LLM agents,\nbased on the chat history obtained from interactions and reflections with LLM\nagents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the\nprompt without the need for a final solution checker. We evaluate our approach\non PDDL generation, TravelPlanner, and Meeting Planning to show that our method\ncould generally improve performance for different reasoning tasks.\n","authors":["Weizhe Chen","Sven Koenig","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2406.11132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v1","updated":"2025-02-13T21:33:57Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09778v1","updated":"2025-02-13T21:23:16Z","published":"2025-02-13T21:23:16Z","title":"Prompt and circumstance: A word-by-word LLM prompting approach to\n  interlinear glossing for low-resource languages","summary":"  Partly automated creation of interlinear glossed text (IGT) has the potential\nto assist in linguistic documentation. We argue that LLMs can make this process\nmore accessible to linguists because of their capacity to follow\nnatural-language instructions. We investigate the effectiveness of a\nretrieval-based LLM prompting approach to glossing, applied to the seven\nlanguages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based\nshared task baseline for every language in the morpheme-level score category,\nand we show that a simple 3-best oracle has higher word-level scores than the\nchallenge winner (a tuned sequence model) in five languages. In a case study on\nTsez, we ask the LLM to automatically create and follow linguistic\ninstructions, reducing errors on a confusing grammatical feature. Our results\nthus demonstrate the potential contributions which LLMs can make in interactive\nsystems for glossing, both in making suggestions to human annotators and\nfollowing directions.\n","authors":["Micha Elsner","David Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15124v4","updated":"2025-02-13T21:18:55Z","published":"2024-11-22T18:44:04Z","title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","summary":"  Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce Tulu 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. Tulu 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the Tulu 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the Tulu 3\napproach to more domains.\n","authors":["Nathan Lambert","Jacob Morrison","Valentina Pyatkin","Shengyi Huang","Hamish Ivison","Faeze Brahman","Lester James V. Miranda","Alisa Liu","Nouha Dziri","Shane Lyu","Yuling Gu","Saumya Malik","Victoria Graf","Jena D. Hwang","Jiangjiang Yang","Ronan Le Bras","Oyvind Tafjord","Chris Wilhelm","Luca Soldaini","Noah A. Smith","Yizhong Wang","Pradeep Dasigi","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.15124v4.pdf","comment":"Added Tulu 3 405B results and additional analyses"},{"id":"http://arxiv.org/abs/2502.09767v1","updated":"2025-02-13T20:51:25Z","published":"2025-02-13T20:51:25Z","title":"Non-Markovian Discrete Diffusion with Causal Language Models","summary":"  Discrete diffusion models have emerged as a flexible and controllable\nparadigm for structured sequence modeling, yet they still lag behind causal\nlanguage models in expressiveness. To bridge the gap between two paradigms, we\nintroduce CaDDi, a causal discrete diffusion model that unifies sequential and\ntemporal modeling within a non-Markovian diffusion framework. Unlike\nconventional diffusion models that operate step by step with no access to prior\nstates, CaDDi integrates the temporal trajectory, enabling more expressive and\ncontrollable generation. Our approach also treats causal language models as a\nspecial case, allowing seamless adoption of pretrained large language models\n(LLMs) for discrete diffusion without the need for architectural modifications.\nEmpirically, we demonstrate that CaDDi outperforms state-of-the-art discrete\ndiffusion models on both natural language and biological sequence tasks,\nnarrowing the gap between diffusion-based methods and large-scale\nautoregressive transformers.\n","authors":["Yangtian Zhang","Sizhuang He","Daniel Levine","Lawrence Zhao","David Zhang","Syed A Rizvi","Emanuele Zappala","Rex Ying","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.09767v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.12543v2","updated":"2025-02-13T20:47:20Z","published":"2024-07-17T13:27:26Z","title":"Abstraction Alignment: Comparing Model-Learned and Human-Encoded\n  Conceptual Relationships","summary":"  While interpretability methods identify a model's learned concepts, they\noverlook the relationships between concepts that make up its abstractions and\ninform its ability to generalize to new data. To assess whether models' have\nlearned human-aligned abstractions, we introduce abstraction alignment, a\nmethodology to compare model behavior against formal human knowledge.\nAbstraction alignment externalizes domain-specific human knowledge as an\nabstraction graph, a set of pertinent concepts spanning levels of abstraction.\nUsing the abstraction graph as a ground truth, abstraction alignment measures\nthe alignment of a model's behavior by determining how much of its uncertainty\nis accounted for by the human abstractions. By aggregating abstraction\nalignment across entire datasets, users can test alignment hypotheses, such as\nwhich human concepts the model has learned and where misalignments recur. In\nevaluations with experts, abstraction alignment differentiates seemingly\nsimilar errors, improves the verbosity of existing model-quality metrics, and\nuncovers improvements to current human abstractions.\n","authors":["Angie Boggust","Hyemin Bang","Hendrik Strobelt","Arvind Satyanarayan"],"pdf_url":"https://arxiv.org/pdf/2407.12543v2.pdf","comment":"20 pages, 7 figures, published in CHI 2025"},{"id":"http://arxiv.org/abs/2502.06648v2","updated":"2025-02-13T20:46:57Z","published":"2025-02-10T16:38:03Z","title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","summary":"  In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.\n","authors":["Erik Novak","Erik Calcina","Dunja Mladeniƒá","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2502.06648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16235v2","updated":"2025-02-13T20:14:27Z","published":"2025-01-27T17:33:38Z","title":"Echoes of Discord: Forecasting Hater Reactions to Counterspeech","summary":"  Hate speech (HS) erodes the inclusiveness of online users and propagates\nnegativity and division. Counterspeech has been recognized as a way to mitigate\nthe harmful consequences. While some research has investigated the impact of\nuser-generated counterspeech on social media platforms, few have examined and\nmodeled haters' reactions toward counterspeech, despite the immediate\nalteration of haters' attitudes being an important aspect of counterspeech.\nThis study fills the gap by analyzing the impact of counterspeech from the\nhater's perspective, focusing on whether the counterspeech leads the hater to\nreenter the conversation and if the reentry is hateful. We compile the Reddit\nEchoes of Hate dataset (ReEco), which consists of triple-turn conversations\nfeaturing haters' reactions, to assess the impact of counterspeech. To predict\nhaters' behaviors, we employ two strategies: a two-stage reaction predictor and\na three-way classifier. The linguistic analysis sheds insights on the language\nof counterspeech to hate eliciting different haters' reactions. Experimental\nresults demonstrate that the 3-way classification model outperforms the\ntwo-stage reaction predictor, which first predicts reentry and then determines\nthe reentry type. We conclude the study with an assessment showing the most\ncommon errors identified by the best-performing model.\n","authors":["Xiaoying Song","Sharon Lisseth Perez","Xinchen Yu","Eduardo Blanco","Lingzi Hong"],"pdf_url":"https://arxiv.org/pdf/2501.16235v2.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2407.04841v2","updated":"2025-02-13T20:10:59Z","published":"2024-07-05T19:57:49Z","title":"Associative Recurrent Memory Transformer","summary":"  This paper addresses the challenge of creating a neural architecture for very\nlong sequences that requires constant time for processing new information at\neach time step. Our approach, Associative Recurrent Memory Transformer (ARMT),\nis based on transformer self-attention for local context and segment-level\nrecurrence for storage of task specific information distributed over a long\ncontext. We demonstrate that ARMT outperfors existing alternatives in\nassociative retrieval tasks and sets a new performance record in the recent\nBABILong multi-task long-context benchmark by answering single-fact questions\nover 50 million tokens with an accuracy of 79.9%. The source code for training\nand evaluation is available on github.\n","authors":["Ivan Rodkin","Yuri Kuratov","Aydar Bulatov","Mikhail Burtsev"],"pdf_url":"https://arxiv.org/pdf/2407.04841v2.pdf","comment":"ICML 2024 Next Generation of Sequence Modeling Architectures Workshop"},{"id":"http://arxiv.org/abs/2502.09747v1","updated":"2025-02-13T20:07:03Z","published":"2025-02-13T20:07:03Z","title":"The Widespread Adoption of Large Language Model-Assisted Writing Across\n  Society","summary":"  The recent advances in large language models (LLMs) attracted significant\npublic and policymaker interest in its adoption patterns. In this paper, we\nsystematically analyze LLM-assisted writing across four domains-consumer\ncomplaints, corporate communications, job postings, and international\norganization press releases-from January 2022 to September 2024. Our dataset\nincludes 687,241 consumer complaints, 537,413 corporate press releases, 304.3\nmillion job postings, and 15,919 United Nations (UN) press releases. Using a\nrobust population-level statistical framework, we find that LLM usage surged\nfollowing the release of ChatGPT in November 2022. By late 2024, roughly 18% of\nfinancial consumer complaint text appears to be LLM-assisted, with adoption\npatterns spread broadly across regions and slightly higher in urban areas. For\ncorporate press releases, up to 24% of the text is attributable to LLMs. In job\npostings, LLM-assisted writing accounts for just below 10% in small firms, and\nis even more common among younger firms. UN press releases also reflect this\ntrend, with nearly 14% of content being generated or modified by LLMs. Although\nadoption climbed rapidly post-ChatGPT, growth appears to have stabilized by\n2024, reflecting either saturation in LLM adoption or increasing subtlety of\nmore advanced models. Our study shows the emergence of a new reality in which\nfirms, consumers and even international organizations substantially rely on\ngenerative AI for communications.\n","authors":["Weixin Liang","Yaohui Zhang","Mihai Codreanu","Jiayu Wang","Hancheng Cao","James Zou"],"pdf_url":"https://arxiv.org/pdf/2502.09747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09615v6","updated":"2025-02-13T20:05:54Z","published":"2024-02-14T23:09:15Z","title":"API Pack: A Massive Multi-Programming Language Dataset for API Call\n  Generation","summary":"  We introduce API Pack, a massive multi-programming language dataset\ncontaining over one million instruction-API calls for improving the API call\ngeneration capabilities of large language models. Our evaluation highlights\nthree key findings: First, fine-tuning on API Pack enables open-source models\nto outperform GPT-3.5 and GPT-4 in generating code for entirely new API calls.\nWe show this by fine-tuning CodeLlama-13B on 20,000 Python instances from API\nPack. Second, fine-tuning on a large dataset in one language, combined with\nsmaller datasets from others, improves API generation accuracy across multiple\nlanguages. Third, we confirm the benefits of larger datasets for API\ngeneralization, as increasing fine-tuning data to one million instances\nenhances generalization to new APIs. To support further research, we\nopen-source the API Pack dataset, trained model, and code at\nhttps://github.com/zguo0525/API-Pack.\n","authors":["Zhen Guo","Adriana Meza Soria","Wei Sun","Yikang Shen","Rameswar Panda"],"pdf_url":"https://arxiv.org/pdf/2402.09615v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15661v2","updated":"2025-02-13T19:59:25Z","published":"2024-11-23T22:09:58Z","title":"Improving Next Tokens via Second-to-Last Predictions with Generate and\n  Refine","summary":"  Autoregressive language models like GPT aim to predict next tokens, while\nautoencoding models such as BERT are trained on tasks such as predicting masked\ntokens. We train a decoder-only architecture for predicting the second to last\ntoken for a sequence of tokens. Our approach yields higher computational\ntraining efficiency than BERT-style models by employing a structured\ndeterministic approach to masking tokens. We use our model to improve the next\ntoken predictions of a standard GPT by combining both predictions in a\n``generate-then-refine'' approach. We demonstrate on different variants of\nGPT-2 and different datasets that (not unexpectedly) second to last token\npredictions are much more accurate, i.e., more than 15\\% higher accuracy than\nstandard next token predictions. The ``generate-then-refine'' approach also\ndemonstrates notable improvements in next-token predictions, yielding smaller\nyet consistent and significant gains.\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2411.15661v2.pdf","comment":"Accepted at Intelligent Data Analysis (IDA), 2025, held in Konstanz,\n  Germany"},{"id":"http://arxiv.org/abs/2502.09743v1","updated":"2025-02-13T19:58:00Z","published":"2025-02-13T19:58:00Z","title":"Partial Colexifications Improve Concept Embeddings","summary":"  While the embedding of words has revolutionized the field of Natural Language\nProcessing, the embedding of concepts has received much less attention so far.\nA dense and meaningful representation of concepts, however, could prove useful\nfor several tasks in computational linguistics, especially those involving\ncross-linguistic data or sparse data from low resource languages. First methods\nthat have been proposed so far embed concepts from automatically constructed\ncolexification networks. While these approaches depart from automatically\ninferred polysemies, attested across a larger number of languages, they are\nrestricted to the word level, ignoring lexical relations that would only hold\nfor parts of the words in a given language. Building on recently introduced\nmethods for the inference of partial colexifications, we show how they can be\nused to improve concept embeddings in meaningful ways. The learned embeddings\nare evaluated against lexical similarity ratings, recorded instances of\nsemantic shift, and word association data. We show that in all evaluation\ntasks, the inclusion of partial colexifications lead to improved concept\nrepresentations and better results. Our results further show that the learned\nembeddings are able to capture and represent different semantic relationships\nbetween concepts.\n","authors":["Arne Rubehn","Johann-Mattis List"],"pdf_url":"https://arxiv.org/pdf/2502.09743v1.pdf","comment":"Submitted to the 63rd Annual Meeting of the Association for\n  Computational Linguistics, Vienna, Austria"},{"id":"http://arxiv.org/abs/2502.09741v1","updated":"2025-02-13T19:54:59Z","published":"2025-02-13T19:54:59Z","title":"FoNE: Precise Single-Token Number Embeddings via Fourier Features","summary":"  Large Language Models (LLMs) typically represent numbers using multiple\ntokens, which requires the model to aggregate these tokens to interpret\nnumerical values. This fragmentation makes both training and inference less\nefficient and adversely affects the model's performance on number-related\ntasks. Inspired by the observation that pre-trained LLMs internally learn\nFourier-like features for number tokens, we propose Fourier Number Embedding\n(FoNE), a novel method that directly maps numbers into the embedding space with\ntheir Fourier features. FoNE encodes each number as a single token with only\ntwo embedding dimensions per digit, effectively capturing numerical values\nwithout fragmentation. This compact representation accelerates both training\nand inference. Compared to traditional subword and digit-wise embeddings, FoNE\nnot only reduces computational overhead but also achieves higher accuracy\nacross various numerical tasks including addition, subtraction and\nmultiplication. On 6-digit decimal addition, FoNE requires 64$\\times$ less data\nto achieve 99% accuracy than subword and digit-wise embeddings while using\n3$\\times$ and 6$\\times$ fewer tokens per number, respectively. Furthermore,\nFoNE is the only method that yields 100% accuracy on over 100,000 test examples\nfor addition, subtraction, and multiplication. The codes and visualization are\navailable at https://fouriernumber.github.io/.\n","authors":["Tianyi Zhou","Deqing Fu","Mahdi Soltanolkotabi","Robin Jia","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2502.09741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09723v1","updated":"2025-02-13T19:13:03Z","published":"2025-02-13T19:13:03Z","title":"Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models","summary":"  Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to systematically examine the\ngeneralizability of safety alignment. By treating LLMs as knowledge databases,\nwe translate malicious queries in natural language into code-style structured\nquery to bypass the safety alignment mechanisms of LLMs. We conduct extensive\nexperiments on mainstream LLMs, ant the results show that QueryAttack achieves\nhigh attack success rates (ASRs) across LLMs with different developers and\ncapabilities. We also evaluate QueryAttack's performance against common\ndefenses, confirming that it is difficult to mitigate with general defensive\ntechniques. To defend against QueryAttack, we tailor a defense method which can\nreduce ASR by up to 64\\% on GPT-4-1106. The code of QueryAttack can be found on\nhttps://anonymous.4open.science/r/QueryAttack-334B.\n","authors":["Qingsong Zou","Jingyu Xiao","Qing Li","Zhi Yan","Yuhang Wang","Li Xu","Wenxuan Wang","Kuofeng Gao","Ruoyu Li","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.09723v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.09715v1","updated":"2025-02-13T19:04:47Z","published":"2025-02-13T19:04:47Z","title":"Evaluating GPT's Capability in Identifying Stages of Cognitive\n  Impairment from Electronic Health Data","summary":"  Identifying cognitive impairment within electronic health records (EHRs) is\ncrucial not only for timely diagnoses but also for facilitating research.\nInformation about cognitive impairment often exists within unstructured\nclinician notes in EHRs, but manual chart reviews are both time-consuming and\nerror-prone. To address this issue, our study evaluates an automated approach\nusing zero-shot GPT-4o to determine stage of cognitive impairment in two\ndifferent tasks. First, we evaluated the ability of GPT-4o to determine the\nglobal Clinical Dementia Rating (CDR) on specialist notes from 769 patients who\nvisited the memory clinic at Massachusetts General Hospital (MGH), and achieved\na weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to\ndifferentiate between normal cognition, mild cognitive impairment (MCI), and\ndementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o\nattained a weighted kappa score of 0.91 in comparison to specialist chart\nreviews and 0.96 on cases that the clinical adjudicators rated with high\nconfidence. Our findings demonstrate GPT-4o's potential as a scalable chart\nreview tool for creating research datasets and assisting diagnosis in clinical\nsettings in the future.\n","authors":["Yu Leng","Yingnan He","Colin Magdamo","Ana-Maria Vranceanu","Christine S. Ritchie","Shibani S. Mukerji","Lidia M. V. R. Moura","John R. Dickson","Deborah Blacker","Sudeshna Das"],"pdf_url":"https://arxiv.org/pdf/2502.09715v1.pdf","comment":"Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 7 pages"},{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09601v1","updated":"2025-02-13T18:52:36Z","published":"2025-02-13T18:52:36Z","title":"CoT-Valve: Length-Compressible Chain-of-Thought Tuning","summary":"  Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.\n","authors":["Xinyin Ma","Guangnian Wan","Runpeng Yu","Gongfan Fang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09601v1.pdf","comment":"Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09589v1","updated":"2025-02-13T18:46:44Z","published":"2025-02-13T18:46:44Z","title":"Logical forms complement probability in understanding language model\n  (and human) performance","summary":"  With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results.\n","authors":["Yixuan Wang","Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09589v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09567v1","updated":"2025-02-13T18:22:31Z","published":"2025-02-13T18:22:31Z","title":"MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing","summary":"  We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.\n","authors":["Vlad Andrei Negru","Robert Vacareanu","Camelia Lemnaru","Mihai Surdeanu","Rodica Potolea"],"pdf_url":"https://arxiv.org/pdf/2502.09567v1.pdf","comment":"16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2406.05925v2","updated":"2025-02-13T18:02:34Z","published":"2024-06-09T21:58:32Z","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","summary":"  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n","authors":["Hao Li","Chenghao Yang","An Zhang","Yang Deng","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05925v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.06773v2","updated":"2025-02-13T17:50:39Z","published":"2024-06-10T20:19:55Z","title":"Evaluating Zero-Shot Long-Context LLM Compression","summary":"  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n","authors":["Chenyu Wang","Yihan Wang","Kai Li"],"pdf_url":"https://arxiv.org/pdf/2406.06773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09532v1","updated":"2025-02-13T17:49:30Z","published":"2025-02-13T17:49:30Z","title":"Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages","summary":"  Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.\n","authors":["Shreyan Biswas","Alexander Erlei","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2502.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08489v2","updated":"2025-02-13T17:33:24Z","published":"2025-02-12T15:26:08Z","title":"Salamandra Technical Report","summary":"  This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.\n","authors":["Aitor Gonzalez-Agirre","Marc P√†mies","Joan Llop","Irene Baucells","Severino Da Dalt","Daniel Tamayo","Jos√© Javier Saiz","Ferran Espu√±a","Jaume Prats","Javier Aula-Blasco","Mario Mina","I√±igo Pikabea","Adri√°n Rubio","Alexander Shvets","Anna Sall√©s","I√±aki Lacunza","Jorge Palomar","J√∫lia Falc√£o","Luc√≠a Tormo","Luis Vasquez-Reina","Montserrat Marimon","Oriol Pareras","Valle Ruiz-Fern√°ndez","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2502.08489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05331v2","updated":"2025-02-13T17:27:15Z","published":"2025-02-07T21:13:27Z","title":"Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books","summary":"  Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.\n","authors":["Sangmitra Madhusudan","Robert Morabito","Skye Reid","Nikta Gohari Sadr","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2502.05331v2.pdf","comment":"9 pages (excluding references), accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.14792v2","updated":"2025-02-13T17:22:36Z","published":"2024-08-27T05:56:04Z","title":"Measuring Human Contribution in AI-Assisted Content Generation","summary":"  With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.\n","authors":["Yueqi Xie","Tao Qi","Jingwei Yi","Xiyuan Yang","Ryan Whalen","Junming Huang","Qian Ding","Yu Xie","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06759v2","updated":"2025-02-13T17:12:34Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v2","updated":"2025-02-13T17:11:41Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  As a powerful all-weather Earth observation tool, synthetic aperture radar\n(SAR) remote sensing enables critical military reconnaissance, maritime\nsurveillance, and infrastructure monitoring. Although Vision language models\n(VLMs) have made remarkable progress in natural language processing and image\nunderstanding, their applications remain limited in professional domains due to\ninsufficient domain expertise. This paper innovatively proposes the first\nlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which\ncontains approximately 2 million high-quality image-text pairs, encompasses\ndiverse scenarios with detailed target annotations. This dataset not only\nsupports several key tasks such as visual understanding and object detection\ntasks, but also has unique innovative aspects: this study develop a\nvisual-language dataset and benchmark for the SAR domain, enabling and\nevaluating VLMs' capabilities in SAR image interpretation, which provides a\nparadigmatic framework for constructing multimodal datasets across various\nremote sensing vertical domains. Through experiments on 16 mainstream VLMs, the\neffectiveness of the dataset has been fully verified. The project will be\nreleased at https://github.com/JimmyMa99/SARChat.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09497v1","updated":"2025-02-13T17:09:52Z","published":"2025-02-13T17:09:52Z","title":"Improve LLM-based Automatic Essay Scoring with Linguistic Features","summary":"  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.\n","authors":["Zhaoyi Joey Hou","Alejandro Ciuba","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2502.09497v1.pdf","comment":"To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09690v1","updated":"2025-02-13T17:05:18Z","published":"2025-02-13T17:05:18Z","title":"Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of\n  Large Language Models to Generate Expert-Like Systems Engineering Artifacts\n  and a Characterization of Failure Modes","summary":"  Multi-purpose Large Language Models (LLMs), a subset of generative Artificial\nIntelligence (AI), have recently made significant progress. While expectations\nfor LLMs to assist systems engineering (SE) tasks are paramount; the\ninterdisciplinary and complex nature of systems, along with the need to\nsynthesize deep-domain knowledge and operational context, raise questions\nregarding the efficacy of LLMs to generate SE artifacts, particularly given\nthat they are trained using data that is broadly available on the internet. To\nthat end, we present results from an empirical exploration, where a human\nexpert-generated SE artifact was taken as a benchmark, parsed, and fed into\nvarious LLMs through prompt engineering to generate segments of typical SE\nartifacts. This procedure was applied without any fine-tuning or calibration to\ndocument baseline LLM performance. We then adopted a two-fold mixed-methods\napproach to compare AI generated artifacts against the benchmark. First, we\nquantitatively compare the artifacts using natural language processing\nalgorithms and find that when prompted carefully, the state-of-the-art\nalgorithms cannot differentiate AI-generated artifacts from the human-expert\nbenchmark. Second, we conduct a qualitative deep dive to investigate how they\ndiffer in terms of quality. We document that while the two-material appear very\nsimilar, AI generated artifacts exhibit serious failure modes that could be\ndifficult to detect. We characterize these as: premature requirements\ndefinition, unsubstantiated numerical estimates, and propensity to overspecify.\nWe contend that this study tells a cautionary tale about why the SE community\nmust be more cautious adopting AI suggested feedback, at least when generated\nby multi-purpose LLMs.\n","authors":["Taylan G. Topcu","Mohammed Husain","Max Ofsa","Paul Wach"],"pdf_url":"https://arxiv.org/pdf/2502.09690v1.pdf","comment":"41 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2502.09689v1","updated":"2025-02-13T16:48:27Z","published":"2025-02-13T16:48:27Z","title":"Large Language Models and Provenance Metadata for Determining the\n  Relevance of Images and Videos in News Stories","summary":"  The most effective misinformation campaigns are multimodal, often combining\ntext with images and videos taken out of context -- or fabricating them\nentirely -- to support a given narrative. Contemporary methods for detecting\nmisinformation, whether in deepfakes or text articles, often miss the interplay\nbetween multiple modalities. Built around a large language model, the system\nproposed in this paper addresses these challenges. It analyzes both the\narticle's text and the provenance metadata of included images and videos to\ndetermine whether they are relevant. We open-source the system prototype and\ninteractive web interface.\n","authors":["Tomas Peterka","Matyas Bohacek"],"pdf_url":"https://arxiv.org/pdf/2502.09689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09457v1","updated":"2025-02-13T16:25:16Z","published":"2025-02-13T16:25:16Z","title":"The Multilingual Mind : A Survey of Multilingual Reasoning in Language\n  Models","summary":"  While reasoning and multilingual capabilities in Language Models (LMs) have\nachieved remarkable progress in recent years, their integration into a unified\nparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning\nrequires language models to handle logical reasoning across languages while\naddressing misalignment, biases, and challenges in low-resource settings. This\nsurvey provides the first in-depth review of multilingual reasoning in LMs. In\nthis survey, we provide a systematic overview of existing methods that leverage\nLMs for multilingual reasoning, specifically outlining the challenges,\nmotivations, and foundational aspects of applying language models to reason\nacross diverse languages. We provide an overview of the standard data resources\nused for training multilingual reasoning in LMs and the evaluation benchmarks\nemployed to assess their multilingual capabilities. Next, we analyze various\nstate-of-the-art methods and their performance on these benchmarks. Finally, we\nexplore future research opportunities to improve multilingual reasoning in LMs,\nfocusing on enhancing their ability to handle diverse languages and complex\nreasoning tasks.\n","authors":["Akash Ghosh","Debayan Datta","Sriparna Saha","Chirag Agarwal"],"pdf_url":"https://arxiv.org/pdf/2502.09457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09416v1","updated":"2025-02-13T15:39:07Z","published":"2025-02-13T15:39:07Z","title":"Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use\n  a Different Evaluation Process than Human?","summary":"  One of the goals of automatic evaluation metrics in grammatical error\ncorrection (GEC) is to rank GEC systems such that it matches human preferences.\nHowever, current automatic evaluations are based on procedures that diverge\nfrom human evaluation. Specifically, human evaluation derives rankings by\naggregating sentence-level relative evaluation results, e.g., pairwise\ncomparisons, using a rating algorithm, whereas automatic evaluation averages\nsentence-level absolute scores to obtain corpus-level scores, which are then\nsorted to determine rankings. In this study, we propose an aggregation method\nfor existing automatic evaluation metrics which aligns with human evaluation\nmethods to bridge this gap. We conducted experiments using various metrics,\nincluding edit-based metrics, $n$-gram based metrics, and sentence-level\nmetrics, and show that resolving the gap improves results for the most of\nmetrics on the SEEDA benchmark. We also found that even BERT-based metrics\nsometimes outperform the metrics of GPT-4. We publish our unified\nimplementation of the metrics and meta-evaluations.\n","authors":["Takumi Goto","Yusuke Sakai","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.09416v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2310.19347v4","updated":"2025-02-13T15:25:02Z","published":"2023-10-30T08:40:16Z","title":"Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization","summary":"  Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.\n","authors":["Huawen Feng","Yan Fan","Xiong Liu","Ting-En Lin","Zekun Yao","Yuchuan Wu","Fei Huang","Yongbin Li","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2310.19347v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.09687v1","updated":"2025-02-13T15:15:53Z","published":"2025-02-13T15:15:53Z","title":"Mind What You Ask For: Emotional and Rational Faces of Persuasion by\n  Large Language Models","summary":"  Be careful what you ask for, you just might get it. This saying fits with the\nway large language models (LLMs) are trained, which, instead of being rewarded\nfor correctness, are increasingly rewarded for pleasing the recipient. So, they\nare increasingly effective at persuading us that their answers are valuable.\nBut what tricks do they use in this persuasion? In this study, we examine what\nare the psycholinguistic features of the responses used by twelve different\nlanguage models. By grouping response content according to rational or\nemotional prompts and exploring social influence principles employed by LLMs,\nwe ask whether and how we can mitigate the risks of LLM-driven mass\nmisinformation. We position this study within the broader discourse on\nhuman-centred AI, emphasizing the need for interdisciplinary approaches to\nmitigate cognitive and societal risks posed by persuasive AI responses.\n","authors":["Wiktoria Mieleszczenko-Kowszewicz","Beata Bajcar","Jolanta Babiak","Berenika Dyczek","Jakub ≈öwistak","Przemys≈Çaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2502.09687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17395v2","updated":"2025-02-13T15:11:24Z","published":"2024-12-23T08:47:42Z","title":"WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models","summary":"  Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.\n","authors":["Huawen Feng","Pu Zhao","Qingfeng Sun","Can Xu","Fangkai Yang","Lu Wang","Qianli Ma","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09387v1","updated":"2025-02-13T15:04:53Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v1.pdf","comment":"13 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.15927v2","updated":"2025-02-13T14:55:26Z","published":"2024-11-24T17:32:20Z","title":"Generative Prompt Internalization","summary":"  Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.\n","authors":["Haebin Shin","Lei Ji","Yeyun Gong","Sungdong Kim","Eunbi Choi","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2411.15927v2.pdf","comment":"NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna P√Æslar","Michiel A. Bakker","Michael Henry Tessler","Raphael K√∂ster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08514v2","updated":"2025-02-13T14:34:29Z","published":"2025-02-12T15:46:50Z","title":"Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation","summary":"  Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.\n","authors":["Mahnaz Koupaee","Jake W. Vincent","Saab Mansour","Igor Shalyminov","Han He","Hwanjun Song","Raphael Shu","Jianfeng He","Yi Nian","Amy Wing-mei Wong","Kyu J. Han","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10853v3","updated":"2025-02-13T14:13:41Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v3.pdf","comment":"LangFair repository: https://github.com/cvs-health/langfair"},{"id":"http://arxiv.org/abs/2411.05031v2","updated":"2025-02-13T14:02:53Z","published":"2024-11-06T09:52:29Z","title":"On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard","summary":"  Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement.\n","authors":["Hossam Amer","Joe Osborne","Michael Zaki","Mohamed Afify"],"pdf_url":"https://arxiv.org/pdf/2411.05031v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.09331v1","updated":"2025-02-13T13:49:30Z","published":"2025-02-13T13:49:30Z","title":"Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs","summary":"  Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.\n","authors":["Itai Mondshine","Tzuf Paz-Argaman","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2502.09331v1.pdf","comment":"Accepted for NAACL findings 2025"},{"id":"http://arxiv.org/abs/2502.09316v1","updated":"2025-02-13T13:30:54Z","published":"2025-02-13T13:30:54Z","title":"A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis","summary":"  Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities.\n","authors":["Kentaro Imajo","Masanori Hirano","Shuji Suzuki","Hiroaki Mikami"],"pdf_url":"https://arxiv.org/pdf/2502.09316v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.05497v2","updated":"2025-02-13T13:22:40Z","published":"2025-02-08T09:04:16Z","title":"DeepThink: Aligning Language Models with Domain-Specific User Intents","summary":"  Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.\n","authors":["Yang Li","Mingxuan Luo","Yeyun Gong","Chen Lin","Jian Jiao","Yi Liu","Kaili Huang"],"pdf_url":"https://arxiv.org/pdf/2502.05497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09307v1","updated":"2025-02-13T13:19:33Z","published":"2025-02-13T13:19:33Z","title":"When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models","summary":"  Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.\n","authors":["Samuel Joseph Amouyal","Aya Meltzer-Asscher","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2502.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15330v2","updated":"2025-02-13T13:06:00Z","published":"2024-06-21T17:42:52Z","title":"Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection","summary":"  Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.\n","authors":["Haoling Li","Xin Zhang","Xiao Liu","Yeyun Gong","Yifan Wang","Qi Chen","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.15330v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09284v1","updated":"2025-02-13T12:57:15Z","published":"2025-02-13T12:57:15Z","title":"SparQLe: Speech Queries to Text Translation Through LLMs","summary":"  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.\n","authors":["Amirbek Djanibekov","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2502.09284v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.09824v1","updated":"2025-02-13T23:41:29Z","published":"2025-02-13T23:41:29Z","title":"PUGS: Perceptual Uncertainty for Grasp Selection in Underwater\n  Environments","summary":"  When navigating and interacting in challenging environments where sensory\ninformation is imperfect and incomplete, robots must make decisions that\naccount for these shortcomings. We propose a novel method for quantifying and\nrepresenting such perceptual uncertainty in 3D reconstruction through occupancy\nuncertainty estimation. We develop a framework to incorporate it into grasp\nselection for autonomous manipulation in underwater environments. Instead of\ntreating each measurement equally when deciding which location to grasp from,\nwe present a framework that propagates uncertainty inherent in the multi-view\nreconstruction process into the grasp selection. We evaluate our method with\nboth simulated and the real world data, showing that by accounting for\nuncertainty, the grasp selection becomes robust against partial and noisy\nmeasurements. Code will be made available at\nhttps://onurbagoren.github.io/PUGS/\n","authors":["Onur Bagoren","Marc Micatka","Katherine A. Skinner","Aaron Marburg"],"pdf_url":"https://arxiv.org/pdf/2502.09824v1.pdf","comment":"8 pages, 4 figures Accepted to International Conference on Robotics\n  and Automation (ICRA) 2024"},{"id":"http://arxiv.org/abs/2502.06860v2","updated":"2025-02-13T23:32:41Z","published":"2025-02-07T23:57:22Z","title":"AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion","summary":"  The ability to automatically complete a partial sketch that depicts a complex\nscene, e.g., \"a woman chatting with a man in the park\", is very useful.\nHowever, existing sketch generation methods create sketches from scratch; they\ndo not complete a partial sketch in the style of the original. To address this\nchallenge, we introduce AutoSketch, a styleaware vector sketch completion\nmethod that accommodates diverse sketch styles. Our key observation is that the\nstyle descriptions of a sketch in natural language preserve the style during\nautomatic sketch completion. Thus, we use a pretrained vision-language model\n(VLM) to describe the styles of the partial sketches in natural language and\nreplicate these styles using newly generated strokes. We initially optimize the\nstrokes to match an input prompt augmented by style descriptions extracted from\nthe VLM. Such descriptions allow the method to establish a diffusion prior in\nclose alignment with that of the partial sketch. Next, we utilize the VLM to\ngenerate an executable style adjustment code that adjusts the strokes to\nconform to the desired style. We compare our method with existing methods\nacross various sketch styles and prompts, performed extensive ablation studies\nand qualitative and quantitative evaluations, and demonstrate that AutoSketch\ncan support various sketch scenarios.\n","authors":["Hsiao-Yuan Chin","I-Chao Shen","Yi-Ting Chiu","Bing-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06860v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.09819v1","updated":"2025-02-13T23:31:30Z","published":"2025-02-13T23:31:30Z","title":"A Solver-Aided Hierarchical Language for LLM-Driven CAD Design","summary":"  Large language models (LLMs) have been enormously successful in solving a\nwide variety of structured and unstructured generative tasks, but they struggle\nto generate procedural geometry in Computer Aided Design (CAD). These\ndifficulties arise from an inability to do spatial reasoning and the necessity\nto guide a model through complex, long range planning to generate complex\ngeometry. We enable generative CAD Design with LLMs through the introduction of\na solver-aided, hierarchical domain specific language (DSL) called AIDL, which\noffloads the spatial reasoning requirements to a geometric constraint solver.\nAdditionally, we show that in the few-shot regime, AIDL outperforms even a\nlanguage with in-training data (OpenSCAD), both in terms of generating visual\nresults closer to the prompt and creating objects that are easier to\npost-process and reason about.\n","authors":["Benjamin T. Jones","Felix H√§hnlein","Zihan Zhang","Maaz Ahmad","Vladimir Kim","Adriana Schulz"],"pdf_url":"https://arxiv.org/pdf/2502.09819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09818v1","updated":"2025-02-13T23:29:01Z","published":"2025-02-13T23:29:01Z","title":"On the robustness of multimodal language model towards distractions","summary":"  Although vision-language models (VLMs) have achieved significant success in\nvarious applications such as visual question answering, their resilience to\nprompt variations remains an under-explored area. Understanding how\ndistractions affect VLMs is crucial for improving their real-world\napplicability, as inputs could have noisy and irrelevant information in many\npractical scenarios. This paper aims to assess the robustness of VLMs against\nboth visual and textual distractions in the context of science question\nanswering. Built on the ScienceQA dataset, we developed a new benchmark that\nintroduces distractions in both the visual and textual contexts to evaluate the\nreasoning capacity of VLMs amid these distractions. Our findings reveal that\nmost-of-the-art VLMs, including GPT-4, are vulnerable to various types of\ndistractions, experiencing noticeable degradation in reasoning capabilities\nwhen confronted with distractions. Notably, models such as InternVL2\ndemonstrate a higher degree of robustness to these distractions. We also found\nthat models exhibit greater sensitivity to textual distractions than visual\nones. Additionally, we explored various mitigation strategies, such as prompt\nengineering, to counteract the impact of distractions. While these strategies\nimproved solution accuracy, our analysis shows that there remain significant\nopportunities for improvement.\n","authors":["Ming Liu","Hao Chen","Jindong Wang","Wensheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14201v2","updated":"2025-02-13T23:21:55Z","published":"2024-09-21T17:18:49Z","title":"LATTE: Improving Latex Recognition for Tables and Formulae with\n  Iterative Refinement","summary":"  Portable Document Format (PDF) files are dominantly used for storing and\ndisseminating scientific research, legal documents, and tax information. LaTeX\nis a popular application for creating PDF documents. Despite its advantages,\nLaTeX is not WYSWYG -- what you see is what you get, i.e., the LaTeX source and\nrendered PDF images look drastically different, especially for formulae and\ntables. This gap makes it hard to modify or export LaTeX sources for formulae\nand tables from PDF images, and existing work is still limited. First, prior\nwork generates LaTeX sources in a single iteration and struggles with complex\nLaTeX formulae. Second, existing work mainly recognizes and extracts LaTeX\nsources for formulae; and is incapable or ineffective for tables. This paper\nproposes LATTE, the first iterative refinement framework for LaTeX recognition.\nSpecifically, we propose delta-view as feedback, which compares and pinpoints\nthe differences between a pair of rendered images of the extracted LaTeX source\nand the expected correct image. Such delta-view feedback enables our fault\nlocalization model to localize the faulty parts of the incorrect recognition\nmore accurately and enables our LaTeX refinement model to repair the incorrect\nextraction more accurately. LATTE improves the LaTeX source extraction accuracy\nof both LaTeX formulae and tables, outperforming existing techniques as well as\nGPT-4V by at least 7.03% of exact match, with a success refinement rate of\n46.08% (formula) and 25.51% (table).\n","authors":["Nan Jiang","Shanchao Liang","Chengxiao Wang","Jiannan Wang","Lin Tan"],"pdf_url":"https://arxiv.org/pdf/2409.14201v2.pdf","comment":"This paper is accepted by The 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2502.09812v1","updated":"2025-02-13T23:08:05Z","published":"2025-02-13T23:08:05Z","title":"Face Deepfakes - A Comprehensive Review","summary":"  In recent years, remarkable advancements in deep- fake generation technology\nhave led to unprecedented leaps in its realism and capabilities. Despite these\nadvances, we observe a notable lack of structured and deep analysis deepfake\ntechnology. The principal aim of this survey is to contribute a thorough\ntheoretical analysis of state-of-the-art face deepfake generation and detection\nmethods. Furthermore, we provide a coherent and systematic evaluation of the\nimplications of deepfakes on face biometric recognition approaches. In\naddition, we outline key applications of face deepfake technology, elucidating\nboth positive and negative applications of the technology, provide a detailed\ndiscussion regarding the gaps in existing research, and propose key research\ndirections for further investigation.\n","authors":["Tharindu Fernando","Darshana Priyasad","Sridha Sridharan","Arun Ross","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2502.09812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01001v3","updated":"2025-02-13T22:57:06Z","published":"2023-12-02T02:09:31Z","title":"Learning county from pixels: Corn yield prediction with\n  attention-weighted multiple instance learning","summary":"  Remote sensing technology has become a promising tool in yield prediction.\nMost prior work employs satellite imagery for county-level corn yield\nprediction by spatially aggregating all pixels within a county into a single\nvalue, potentially overlooking the detailed information and valuable insights\noffered by more granular data. To this end, this research examines each county\nat the pixel level and applies multiple instance learning to leverage detailed\ninformation within a county. In addition, our method addresses the \"mixed\npixel\" issue caused by the inconsistent resolution between feature datasets and\ncrop mask, which may introduce noise into the model and therefore hinder\naccurate yield prediction. Specifically, the attention mechanism is employed to\nautomatically assign weights to different pixels, which can mitigate the\ninfluence of mixed pixels. The experimental results show that the developed\nmodel outperforms four other machine learning models over the past five years\nin the U.S. corn belt and demonstrates its best performance in 2022, achieving\na coefficient of determination (R2) value of 0.84 and a root mean square error\n(RMSE) of 0.83. This paper demonstrates the advantages of our approach from\nboth spatial and temporal perspectives. Furthermore, through an in-depth study\nof the relationship between mixed pixels and attention, it is verified that our\napproach can capture critical feature information while filtering out noise\nfrom mixed pixels.\n","authors":["Xiaoyu Wang","Yuchi Ma","Qunying Huang","Zhengwei Yang","Zhou Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.01001v3.pdf","comment":"I am writing to request the resubmission of my paper submitted to\n  arXiv"},{"id":"http://arxiv.org/abs/2502.09805v1","updated":"2025-02-13T22:43:43Z","published":"2025-02-13T22:43:43Z","title":"Towards Patient-Specific Surgical Planning for Bicuspid Aortic Valve\n  Repair: Fully Automated Segmentation of the Aortic Valve in 4D CT","summary":"  The bicuspid aortic valve (BAV) is the most prevalent congenital heart defect\nand may require surgery for complications such as stenosis, regurgitation, and\naortopathy. BAV repair surgery is effective but challenging due to the\nheterogeneity of BAV morphology. Multiple imaging modalities can be employed to\nassist the quantitative assessment of BAVs for surgical planning.\nContrast-enhanced 4D computed tomography (CT) produces volumetric temporal\nsequences with excellent contrast and spatial resolution. Segmentation of the\naortic cusps and root in these images is an essential step in creating patient\nspecific models for visualization and quantification. While deep learning-based\nmethods are capable of fully automated segmentation, no BAV-specific model\nexists. Among valve segmentation studies, there has been limited quantitative\nassessment of the clinical usability of the segmentation results. In this work,\nwe developed a fully auto- mated multi-label BAV segmentation pipeline based on\nnnU-Net. The predicted segmentations were used to carry out surgically relevant\nmorphological measurements including geometric cusp height, commissural angle\nand annulus diameter, and the results were compared against manual\nsegmentation. Automated segmentation achieved average Dice scores of over 0.7\nand symmetric mean distance below 0.7 mm for all three aortic cusps and the\nroot wall. Clinically relevant benchmarks showed good consistency between\nmanual and predicted segmentations. Overall, fully automated BAV segmentation\nof 3D frames in 4D CT can produce clinically usable measurements for surgical\nrisk stratification, but the temporal consistency of segmentations needs to be\nimproved.\n","authors":["Zaiyang Guo","Ningjun J Dong","Harold Litt","Natalie Yushkevich","Melanie Freas","Jessica Nunez","Victor Ferrari","Jilei Hao","Shir Goldfinger","Matthew A. Jolley","Joseph Bavaria","Nimesh Desai","Alison M. Pouch"],"pdf_url":"https://arxiv.org/pdf/2502.09805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09804v1","updated":"2025-02-13T22:43:28Z","published":"2025-02-13T22:43:28Z","title":"Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8,\n  ResNet50, and Inception-ResNet-v2 Deep Learning Models","summary":"  Thousands of individuals succumb annually to leukemia alone. As artificial\nintelligence-driven technologies continue to evolve and advance, the question\nof their applicability and reliability remains unresolved. This study aims to\nutilize image processing and deep learning methodologies to achieve\nstate-of-the-art results for the detection of Acute Lymphoblastic Leukemia\n(ALL) using data that best represents real-world scenarios. ALL is one of\nseveral types of blood cancer, and it is an aggressive form of leukemia. In\nthis investigation, we examine the most recent advancements in ALL detection,\nas well as the latest iteration of the YOLO series and its performance. We\naddress the question of whether white blood cells are malignant or benign.\nAdditionally, the proposed models can identify different ALL stages, including\nearly stages. Furthermore, these models can detect hematogones despite their\nfrequent misclassification as ALL. By utilizing advanced deep learning models,\nnamely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves\naccuracy rates as high as 99.7%, demonstrating the effectiveness of these\nalgorithms across multiple datasets and various real-world situations.\n","authors":["Alaa Awad","Salah A. Aly"],"pdf_url":"https://arxiv.org/pdf/2502.09804v1.pdf","comment":"12 pages, 28 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.09795v1","updated":"2025-02-13T22:10:21Z","published":"2025-02-13T22:10:21Z","title":"Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging\n  Illumination Conditions","summary":"  Planetary exploration using aerial assets has the potential for unprecedented\nscientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved\nflight in Martian atmosphere is possible, future Mars rotocrafts will require\nadvanced navigation capabilities for long-range flights. One such critical\ncapability is Map-based Localization (MbL) which registers an onboard image to\na reference map during flight in order to mitigate cumulative drift from visual\nodometry. However, significant illumination differences between rotocraft\nobservations and a reference map prove challenging for traditional MbL systems,\nrestricting the operational window of the vehicle. In this work, we investigate\na new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model\nfor image registration that is more robust under large illumination differences\nthan prior models. The system is supported by a custom simulation framework\nthat uses real orbital maps to produce large amounts of realistic images of the\nMartian terrain. Comprehensive evaluations show that our proposed system\noutperforms prior MbL efforts in terms of localization accuracy under\nsignificant lighting and scale variations. Furthermore, we demonstrate the\nvalidity of our approach across a simulated Martian day.\n","authors":["Dario Pisanti","Robert Hewitt","Roland Brockers","Georgios Georgakis"],"pdf_url":"https://arxiv.org/pdf/2502.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09793v1","updated":"2025-02-13T22:03:34Z","published":"2025-02-13T22:03:34Z","title":"Noise Controlled CT Super-Resolution with Conditional Diffusion Model","summary":"  Improving the spatial resolution of CT images is a meaningful yet challenging\ntask, often accompanied by the issue of noise amplification. This article\nintroduces an innovative framework for noise-controlled CT super-resolution\nutilizing the conditional diffusion model. The model is trained on hybrid\ndatasets, combining noise-matched simulation data with segmented details from\nreal data. Experimental results with real CT images validate the effectiveness\nof our proposed framework, showing its potential for practical applications in\nCT imaging.\n","authors":["Yuang Wang","Siyeop Yoon","Rui Hu","Baihui Yu","Duhgoon Lee","Rajiv Gupta","Li Zhang","Zhiqiang Chen","Dufan Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09793v1.pdf","comment":"The 8th International Conference on Image Formation in X-Ray Computed\n  Tomography, Bamberg, Germany, August 5 - 9, 2024"},{"id":"http://arxiv.org/abs/2412.17957v2","updated":"2025-02-13T21:57:44Z","published":"2024-12-23T20:13:27Z","title":"ArchComplete: Autoregressive 3D Architectural Design Generation with\n  Hierarchical Diffusion-Based Upsampling","summary":"  Recent advances in 3D generative models have shown promising results but\noften fall short in capturing the complexity of architectural geometries and\ntopologies and fine geometric details at high resolutions. To tackle this, we\npresent ArchComplete, a two-stage voxel-based 3D generative pipeline consisting\nof a vector-quantised model, whose composition is modelled with an\nautoregressive transformer for generating coarse shapes, followed by a\nhierarchical upsampling strategy for further enrichment with fine structures\nand details. Key to our pipeline is (i) learning a contextually rich codebook\nof local patch embeddings, optimised alongside a 2.5D perceptual loss that\ncaptures global spatial correspondence of projections onto three axis-aligned\northogonal planes, and (ii) redefining upsampling as a set of conditional\ndiffusion models learning from a hierarchy of randomly cropped coarse-to-fine\nlocal volumetric patches. Trained on our introduced dataset of 3D house models\nwith fully modelled exterior and interior, ArchComplete autoregressively\ngenerates models at the resolution of $64^{3}$ and progressively refines them\nup to $512^{3}$, with voxel sizes as small as $ \\approx 9\\text{cm}$.\nArchComplete solves a variety of tasks, including genetic interpolation and\nvariation, unconditional synthesis, shape and plan-drawing completion, as well\nas geometric detailisation, while achieving state-of-the-art performance in\nquality, diversity, and computational efficiency.\n","authors":["S. Rasoulzadeh","M. Bank","I. Kovacic","K. Schinegger","S. Rutzinger","M. Wimmer"],"pdf_url":"https://arxiv.org/pdf/2412.17957v2.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.09791v1","updated":"2025-02-13T21:55:21Z","published":"2025-02-13T21:55:21Z","title":"Atom identification in bilayer moire materials with Gomb-Net","summary":"  Moire patterns in van der Waals bilayer materials complicate the analysis of\natomic-resolution images, hindering the atomic-scale insight typically\nattainable with scanning transmission electron microscopy. Here, we report a\nmethod to detect the positions and identity of atoms in each of the individual\nlayers that compose bilayer heterostructures. We developed a deep learning\nmodel, Gomb-Net, which can distinguish atomic species in each individual layer,\neffectively deconvoluting the moire pattern to enable layer-specific mapping of\nstrain and dopant distributions, unlike other methods which struggle with\nmoire-induced complexity. Using this approach, we explored Se atom\nsubstitutional sites in a twisted fractional Janus WS2-WS2(1-x)Se2x\nheterostructure and found that layer specific implantation sites are unaffected\nby the moire pattern's local energetic or electronic modulation. This\nadvancement enables atom-identification within material regimes where it was\nnot possible before, opening new insights into previously inaccessible material\nphysics.\n","authors":["Austin C. Houston","Sumner B. Harris","Hao Wang","Yu-Chuan Lin","David B. Geohegan","Kai Xiao","Gerd Duscher"],"pdf_url":"https://arxiv.org/pdf/2502.09791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09779v1","updated":"2025-02-13T21:27:10Z","published":"2025-02-13T21:27:10Z","title":"Automated Muscle and Fat Segmentation in Computed Tomography for\n  Comprehensive Body Composition Analysis","summary":"  Body composition assessment using CT images can potentially be used for a\nnumber of clinical applications, including the prognostication of\ncardiovascular outcomes, evaluation of metabolic health, monitoring of disease\nprogression, assessment of nutritional status, prediction of treatment response\nin oncology, and risk stratification for surgical and critical care outcomes.\nWhile multiple groups have developed in-house segmentation tools for this\nanalysis, there are very limited publicly available tools that could be\nconsistently used across different applications. To mitigate this gap, we\npresent a publicly accessible, end-to-end segmentation and feature calculation\nmodel specifically for CT body composition analysis. Our model performs\nsegmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and\nvisceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in\naxial CT images. It also provides various body composition metrics, including\nmuscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle\narea/volume, and skeletal muscle index (SMI), supporting both 2D and 3D\nassessments. The model is shared for public use. To evaluate the model, the\nsegmentation was applied to both internal and external datasets, with body\ncomposition metrics analyzed across different age, sex, and race groups. The\nmodel achieved high dice coefficients on both internal and external datasets,\nexceeding 89% for skeletal muscle, SAT, and VAT segmentation. The model\noutperforms the benchmark by 2.40% on skeletal muscle and 10.26% on SAT\ncompared to the manual annotations given by the publicly available dataset.\nBody composition metrics show mean relative absolute errors (MRAEs) under 10%\nfor all measures. Furthermore, the model provided muscular fat segmentation\nwith a Dice coefficient of 56.27%, which can be utilized for additional\nanalyses as needed.\n","authors":["Yaqian Chen","Hanxue Gu","Yuwen Chen","Jicheng Yang","Haoyu Dong","Joseph Y. Cao","Adrian Camarena","Christopher Mantyh","Roy Colglazier","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2502.09779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07649v2","updated":"2025-02-13T21:11:58Z","published":"2024-11-12T08:57:21Z","title":"Maritime Search and Rescue Missions with Aerial Images: A Survey","summary":"  The speed of response by search and rescue teams at sea is of vital\nimportance, as survival may depend on it. Recent technological advancements\nhave led to the development of more efficient systems for locating individuals\ninvolved in a maritime incident, such as the use of Unmanned Aerial Vehicles\n(UAVs) equipped with cameras and other integrated sensors. Over the past\ndecade, several researchers have contributed to the development of automatic\nsystems capable of detecting people using aerial images, particularly by\nleveraging the advantages of deep learning. In this article, we provide a\ncomprehensive review of the existing literature on this topic. We analyze the\nmethods proposed to date, including both traditional techniques and more\nadvanced approaches based on machine learning and neural networks.\nAdditionally, we take into account the use of synthetic data to cover a wider\nrange of scenarios without the need to deploy a team to collect data, which is\none of the major obstacles for these systems. Overall, this paper situates the\nreader in the field of detecting people at sea using aerial images by quickly\nidentifying the most suitable methodology for each scenario, as well as\nproviding an in-depth discussion and direction for future trends.\n","authors":["Juan P. Martinez-Esteso","Francisco J. Castellanos","Jorge Calvo-Zaragoza","Antonio Javier Gallego"],"pdf_url":"https://arxiv.org/pdf/2411.07649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09775v1","updated":"2025-02-13T21:10:00Z","published":"2025-02-13T21:10:00Z","title":"CellFlow: Simulating Cellular Morphology Changes via Flow Matching","summary":"  Building a virtual cell capable of accurately simulating cellular behaviors\nin silico has long been a dream in computational biology. We introduce\nCellFlow, an image-generative model that simulates cellular morphology changes\ninduced by chemical and genetic perturbations using flow matching. Unlike prior\nmethods, CellFlow models distribution-wise transformations from unperturbed to\nperturbed cell states, effectively distinguishing actual perturbation effects\nfrom experimental artifacts such as batch effects -- a major challenge in\nbiological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined\nperturbation (JUMP) datasets, CellFlow generates biologically meaningful cell\nimages that faithfully capture perturbation-specific morphological changes,\nachieving a 35% improvement in FID scores and a 12% increase in mode-of-action\nprediction accuracy over existing methods. Additionally, CellFlow enables\ncontinuous interpolation between cellular states, providing a potential tool\nfor studying perturbation dynamics. These capabilities mark a significant step\ntoward realizing virtual cell modeling for biomedical research.\n","authors":["Yuhui Zhang","Yuchang Su","Chenyu Wang","Tianhong Li","Zoe Wefers","Jeffrey Nirschl","James Burgess","Daisy Ding","Alejandro Lozano","Emma Lundberg","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2502.09775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05336v2","updated":"2025-02-13T20:24:58Z","published":"2024-05-08T18:10:59Z","title":"Joint semi-supervised and contrastive learning enables domain\n  generalization and multi-domain segmentation","summary":"  Despite their effectiveness, current deep learning models face challenges\nwith images coming from different domains with varying appearance and content.\nWe introduce SegCLR, a versatile framework designed to segment images across\ndifferent domains, employing supervised and contrastive learning simultaneously\nto effectively learn from both labeled and unlabeled data. We demonstrate the\nsuperior performance of SegCLR through a comprehensive evaluation involving\nthree diverse clinical datasets of 3D retinal Optical Coherence Tomography\n(OCT) images, for the slice-wise segmentation of fluids with various network\nconfigurations and verification across 10 different network initializations. In\nan unsupervised domain adaptation context, SegCLR achieves results on par with\na supervised upper-bound model trained on the intended target domain. Notably,\nwe discover that the segmentation performance of SegCLR framework is marginally\nimpacted by the abundance of unlabeled data from the target domain, thereby we\nalso propose an effective domain generalization extension of SegCLR, known also\nas zero-shot domain adaptation, which eliminates the need for any target domain\ninformation. This shows that our proposed addition of contrastive loss in\nstandard supervised training for segmentation leads to superior models,\ninherently more generalizable to both in- and out-of-domain test data. We\nadditionally propose a pragmatic solution for SegCLR deployment in realistic\nscenarios with multiple domains containing labeled data. Accordingly, our\nframework pushes the boundaries of deep-learning based segmentation in\nmulti-domain applications, regardless of data availability - labeled,\nunlabeled, or nonexistent.\n","authors":["Alvaro Gomariz","Yusuke Kikuchi","Yun Yvonna Li","Thomas Albrecht","Andreas Maunz","Daniela Ferrara","Huanxiang Lu","Orcun Goksel"],"pdf_url":"https://arxiv.org/pdf/2405.05336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00160v2","updated":"2025-02-13T20:12:22Z","published":"2025-01-31T20:50:55Z","title":"Improving Quality Control Of MRI Images Using Synthetic Motion Data","summary":"  MRI quality control (QC) is challenging due to unbalanced and limited\ndatasets, as well as subjective scoring, which hinder the development of\nreliable automated QC systems. To address these issues, we introduce an\napproach that pretrains a model on synthetically generated motion artifacts\nbefore applying transfer learning for QC classification. This method not only\nimproves the accuracy in identifying poor-quality scans but also reduces\ntraining time and resource requirements compared to training from scratch. By\nleveraging synthetic data, we provide a more robust and resource-efficient\nsolution for QC automation in MRI, paving the way for broader adoption in\ndiverse research settings.\n","authors":["Charles Bricout","Kang Ik K. Cho","Michael Harms","Ofer Pasternak","Carrie E. Bearden","Patrick D. McGorry","Rene S. Kahn","John Kane","Barnaby Nelson","Scott W. Woods","Martha E. Shenton","Sylvain Bouix","Samira Ebrahimi Kahou"],"pdf_url":"https://arxiv.org/pdf/2502.00160v2.pdf","comment":"Accepted at ISBI 2025"},{"id":"http://arxiv.org/abs/2409.12467v2","updated":"2025-02-13T19:57:52Z","published":"2024-09-19T05:08:33Z","title":"SurgPLAN++: Universal Surgical Phase Localization Network for Online and\n  Offline Inference","summary":"  Surgical phase recognition is critical for assisting surgeons in\nunderstanding surgical videos. Existing studies focused more on online surgical\nphase recognition, by leveraging preceding frames to predict the current frame.\nDespite great progress, they formulated the task as a series of frame-wise\nclassification, which resulted in a lack of global context of the entire\nprocedure and incoherent predictions. Moreover, besides online analysis,\naccurate offline surgical phase recognition is also in significant clinical\nneed for retrospective analysis, and existing online algorithms do not fully\nanalyze the entire video, thereby limiting accuracy in offline analysis. To\novercome these challenges and enhance both online and offline inference\ncapabilities, we propose a universal Surgical Phase Localization Network, named\nSurgPLAN++, with the principle of temporal detection. To ensure a global\nunderstanding of the surgical procedure, we devise a phase localization\nstrategy for SurgPLAN++ to predict phase segments across the entire video\nthrough phase proposals. For online analysis, to generate high-quality phase\nproposals, SurgPLAN++ incorporates a data augmentation strategy to extend the\nstreaming video into a pseudo-complete video through mirroring,\ncenter-duplication, and down-sampling. For offline analysis, SurgPLAN++\ncapitalizes on its global phase prediction framework to continuously refine\npreceding predictions during each online inference step, thereby significantly\nimproving the accuracy of phase recognition. We perform extensive experiments\nto validate the effectiveness, and our SurgPLAN++ achieves remarkable\nperformance in both online and offline modes, which outperforms\nstate-of-the-art methods. The source code is available at\nhttps://github.com/franciszchen/SurgPLAN-Plus.\n","authors":["Zhen Chen","Xingjian Luo","Jinlin Wu","Long Bai","Zhen Lei","Hongliang Ren","Sebastien Ourselin","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2409.12467v2.pdf","comment":"This work is accepted by IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2502.09731v1","updated":"2025-02-13T19:33:26Z","published":"2025-02-13T19:33:26Z","title":"A CNN Approach to Automated Detection and Classification of Brain Tumors","summary":"  Brain tumors require an assessment to ensure timely diagnosis and effective\npatient treatment. Morphological factors such as size, location, texture, and\nvariable appearance com- plicate tumor inspection. Medical imaging presents\nchallenges, including noise and incomplete images. This research article\npresents a methodology for processing Magnetic Resonance Imag- ing (MRI) data,\nencompassing techniques for image classification and denoising. The effective\nuse of MRI images allows medical professionals to detect brain disorders,\nincluding tumors. This research aims to categorize healthy brain tissue and\nbrain tumors by analyzing the provided MRI data. Unlike alternative methods\nlike Computed Tomography (CT), MRI technology offers a more detailed\nrepresentation of internal anatomical components, mak- ing it a suitable option\nfor studying data related to brain tumors. The MRI picture is first subjected\nto a denoising technique utilizing an Anisotropic diffusion filter. The dataset\nutilized for the models creation is a publicly accessible and validated Brain\nTumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE\nwas employed for data augmentation and dataset balancing. Convolutional Neural\nNetworks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for\nthe classification procedure. EfficientNet attained an accuracy of 98%, the\nhighest recorded.\n","authors":["Md. Zahid Hasan","Abdullah Tamim","D. M. Asadujjaman","Md. Mahfujur Rahman","Md. Abu Ahnaf Mollick","Nosin Anjum Dristi"," Abdullah-Al-Noman"],"pdf_url":"https://arxiv.org/pdf/2502.09731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07062v4","updated":"2025-02-13T19:07:06Z","published":"2024-10-09T17:03:49Z","title":"TinyEmo: Scaling down Emotional Reasoning via Metric Projection","summary":"  This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems. We release code, models, and dataset at\nhttps://github.com/ggcr/TinyEmo\n","authors":["Cristian Gutierrez"],"pdf_url":"https://arxiv.org/pdf/2410.07062v4.pdf","comment":"I am withdrawing this work in favour of the confidentiality of\n  research ideas that are still under development."},{"id":"http://arxiv.org/abs/2502.09623v1","updated":"2025-02-13T18:59:50Z","published":"2025-02-13T18:59:50Z","title":"Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures","summary":"  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.\n","authors":["Francesco Ballerini","Pierluigi Zama Ramirez","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2502.09623v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09617v1","updated":"2025-02-13T18:59:19Z","published":"2025-02-13T18:59:19Z","title":"LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback\n  Over Multi-Resolution Gaussians-on-Mesh","summary":"  Generalizable rendering of an animatable human avatar from sparse inputs\nrelies on data priors and inductive biases extracted from training on large\ndata to avoid scene-specific optimization and to enable fast reconstruction.\nThis raises two main challenges: First, unlike iterative gradient-based\nadjustment in scene-specific optimization, generalizable methods must\nreconstruct the human shape representation in a single pass at inference time.\nSecond, rendering is preferably computationally efficient yet of high\nresolution. To address both challenges we augment the recently proposed dual\nshape representation, which combines the benefits of a mesh and Gaussian\npoints, in two ways. To improve reconstruction, we propose an iterative\nfeedback update framework, which successively improves the canonical human\nshape representation during reconstruction. To achieve computationally\nefficient yet high-resolution rendering, we study a coupled-multi-resolution\nGaussians-on-Mesh representation. We evaluate the proposed approach on the\nchallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an\nanimatable representation from sparse inputs in less than 1s, renders views\nwith 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of\n24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in\nrendering quality.\n","authors":["Jing Wen","Alexander G. Schwing","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09617v1.pdf","comment":"ICLR 2025; Project page: https://wenj.github.io/LIFe-GoM/"},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2502.09615v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets","summary":"  We present RigAnything, a novel autoregressive transformer-based model, which\nmakes 3D assets rig-ready by probabilistically generating joints, skeleton\ntopologies, and assigning skinning weights in a template-free manner. Unlike\nmost existing auto-rigging methods, which rely on predefined skeleton template\nand are limited to specific categories like humanoid, RigAnything approaches\nthe rigging problem in an autoregressive manner, iteratively predicting the\nnext joint based on the global input shape and the previous prediction. While\nautoregressive models are typically used to generate sequential data,\nRigAnything extends their application to effectively learn and represent\nskeletons, which are inherently tree structures. To achieve this, we organize\nthe joints in a breadth-first search (BFS) order, enabling the skeleton to be\ndefined as a sequence of 3D locations and the parent index. Furthermore, our\nmodel improves the accuracy of position prediction by leveraging diffusion\nmodeling, ensuring precise and consistent placement of joints within the\nhierarchy. This formulation allows the autoregressive model to efficiently\ncapture both spatial and hierarchical relationships within the skeleton.\nTrained end-to-end on both RigNet and Objaverse datasets, RigAnything\ndemonstrates state-of-the-art performance across diverse object types,\nincluding humanoids, quadrupeds, marine creatures, insects, and many more,\nsurpassing prior methods in quality, robustness, generalizability, and\nefficiency. Please check our website for more details:\nhttps://www.liuisabella.com/RigAnything.\n","authors":["Isabella Liu","Zhan Xu","Wang Yifan","Hao Tan","Zexiang Xu","Xiaolong Wang","Hao Su","Zifan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09615v1.pdf","comment":"Project page: https://www.liuisabella.com/RigAnything"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2502.09696v1","updated":"2025-02-13T18:59:11Z","published":"2025-02-13T18:59:11Z","title":"ZeroBench: An Impossible Visual Benchmark for Contemporary Large\n  Multimodal Models","summary":"  Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting\nimages and, by some measures, have poorer spatial cognition than small children\nor animals. Despite this, they attain high scores on many popular visual\nbenchmarks, with headroom rapidly eroded by an ongoing surge of model progress.\nTo address this, there is a pressing need for difficult benchmarks that remain\nrelevant for longer. We take this idea to its limit by introducing ZeroBench-a\nlightweight visual reasoning benchmark that is entirely impossible for\ncontemporary frontier LMMs. Our benchmark consists of 100 manually curated\nquestions and 334 less difficult subquestions. We evaluate 20 LMMs on\nZeroBench, all of which score 0.0%, and rigorously analyse the errors. To\nencourage progress in visual understanding, we publicly release ZeroBench.\n","authors":["Jonathan Roberts","Mohammad Reza Taesiri","Ansh Sharma","Akash Gupta","Samuel Roberts","Ioana Croitoru","Simion-Vlad Bogolin","Jialu Tang","Florian Langer","Vyas Raina","Vatsal Raina","Hanyi Xiong","Vishaal Udandarao","Jingyi Lu","Shiyang Chen","Sam Purkis","Tianshuo Yan","Wenye Lin","Gyungin Shin","Qiaochu Yang","Anh Totti Nguyen","Kai Han","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2502.09696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09613v1","updated":"2025-02-13T18:59:09Z","published":"2025-02-13T18:59:09Z","title":"Latent Radiance Fields with 3D-aware 2D Representations","summary":"  Latent 3D reconstruction has shown great promise in empowering 3D semantic\nunderstanding and 3D generation by distilling 2D features into the 3D space.\nHowever, existing approaches struggle with the domain gap between 2D feature\nspace and 3D representations, resulting in degraded rendering performance. To\naddress this challenge, we propose a novel framework that integrates 3D\nawareness into the 2D latent space. The framework consists of three stages: (1)\na correspondence-aware autoencoding method that enhances the 3D consistency of\n2D latent representations, (2) a latent radiance field (LRF) that lifts these\n3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field\n(VAE-RF) alignment strategy that improves image decoding from the rendered 2D\nrepresentations. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art latent 3D reconstruction approaches in terms of synthesis\nperformance and cross-dataset generalizability across diverse indoor and\noutdoor scenes. To our knowledge, this is the first work showing the radiance\nfield representations constructed from 2D latent representations can yield\nphotorealistic 3D reconstruction performance.\n","authors":["Chaoyi Zhou","Xi Liu","Feng Luo","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09613v1.pdf","comment":"Accepted to ICLR 2025; Project page:\n  https://latent-radiance-field.github.io/LRF"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09608v1","updated":"2025-02-13T18:56:05Z","published":"2025-02-13T18:56:05Z","title":"Instance Segmentation of Scene Sketches Using Natural Image Priors","summary":"  Sketch segmentation involves grouping pixels within a sketch that belong to\nthe same object or instance. It serves as a valuable tool for sketch editing\ntasks, such as moving, scaling, or removing specific components. While image\nsegmentation models have demonstrated remarkable capabilities in recent years,\nsketches present unique challenges for these models due to their sparse nature\nand wide variation in styles. We introduce SketchSeg, a method for instance\nsegmentation of raster scene sketches. Our approach adapts state-of-the-art\nimage segmentation and object detection models to the sketch domain by\nemploying class-agnostic fine-tuning and refining segmentation masks using\ndepth cues. Furthermore, our method organizes sketches into sorted layers,\nwhere occluded instances are inpainted, enabling advanced sketch editing\napplications. As existing datasets in this domain lack variation in sketch\nstyles, we construct a synthetic scene sketch segmentation dataset featuring\nsketches with diverse brush strokes and varying levels of detail. We use this\ndataset to demonstrate the robustness of our approach and will release it to\npromote further research in the field.\n  Project webpage: https://sketchseg.github.io/sketch-seg/\n","authors":["Mia Tang","Yael Vinker","Chuan Yan","Lvmin Zhang","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2502.09608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09598v1","updated":"2025-02-13T18:52:14Z","published":"2025-02-13T18:52:14Z","title":"GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis","summary":"  The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.\n","authors":["Angelos Zavras","Dimitrios Michail","Xiao Xiang Zhu","Beg√ºm Demir","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2502.09598v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.09101v2","updated":"2025-02-13T18:20:14Z","published":"2024-11-14T00:18:04Z","title":"Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery","summary":"  Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.\n","authors":["Ashim Dahal","Saydul Akbar Murad","Nick Rahimi"],"pdf_url":"https://arxiv.org/pdf/2411.09101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.09563v1","updated":"2025-02-13T18:15:10Z","published":"2025-02-13T18:15:10Z","title":"Self-Calibrating Gaussian Splatting for Large Field of View\n  Reconstruction","summary":"  In this paper, we present a self-calibrating framework that jointly optimizes\ncamera parameters, lens distortion and 3D Gaussian representations, enabling\naccurate and efficient scene reconstruction. In particular, our technique\nenables high-quality scene reconstruction from Large field-of-view (FOV)\nimagery taken with wide-angle lenses, allowing the scene to be modeled from a\nsmaller number of images. Our approach introduces a novel method for modeling\ncomplex lens distortions using a hybrid network that combines invertible\nresidual networks with explicit grids. This design effectively regularizes the\noptimization process, achieving greater accuracy than conventional camera\nmodels. Additionally, we propose a cubemap-based resampling strategy to support\nlarge FOV images without sacrificing resolution or introducing distortion\nartifacts. Our method is compatible with the fast rasterization of Gaussian\nSplatting, adaptable to a wide variety of camera lens distortion, and\ndemonstrates state-of-the-art performance on both synthetic and real-world\ndatasets.\n","authors":["Youming Deng","Wenqi Xian","Guandao Yang","Leonidas Guibas","Gordon Wetzstein","Steve Marschner","Paul Debevec"],"pdf_url":"https://arxiv.org/pdf/2502.09563v1.pdf","comment":"Project Page: https://denghilbert.github.io/self-cali/"},{"id":"http://arxiv.org/abs/2501.04001v2","updated":"2025-02-13T18:14:33Z","published":"2025-01-07T18:58:54Z","title":"Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos","summary":"  This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.\n","authors":["Haobo Yuan","Xiangtai Li","Tao Zhang","Zilong Huang","Shilin Xu","Shunping Ji","Yunhai Tong","Lu Qi","Jiashi Feng","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04001v2.pdf","comment":"Project page: https://lxtgh.github.io/project/sa2va"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2408.09110v2","updated":"2025-02-13T18:01:16Z","published":"2024-08-17T06:24:43Z","title":"Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for\n  Remote Sensing Community","summary":"  Object detection, particularly open-vocabulary object detection, plays a\ncrucial role in Earth sciences, such as environmental monitoring, natural\ndisaster assessment, and land-use planning. However, existing open-vocabulary\ndetectors, primarily trained on natural-world images, struggle to generalize to\nremote sensing images due to a significant data domain gap. Thus, this paper\naims to advance the development of open-vocabulary object detection in remote\nsensing community. To achieve this, we first reformulate the task as Locate\nAnything on Earth (LAE) with the goal of detecting any novel concepts on Earth.\nWe then developed the LAE-Label Engine which collects, auto-annotates, and\nunifies up to 10 remote sensing datasets creating the LAE-1M - the first\nlarge-scale remote sensing object detection dataset with broad category\ncoverage. Using the LAE-1M, we further propose and train the novel LAE-DINO\nModel, the first open-vocabulary foundation object detector for the LAE task,\nfeaturing Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt\nLearning (VisGT) modules. DVC dynamically constructs vocabulary for each\ntraining batch, while VisGT maps visual features to semantic space, enhancing\ntext features. We comprehensively conduct experiments on established remote\nsensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class\nLAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and\nthe effectiveness of the LAE-DINO method.\n","authors":["Jiancheng Pan","Yanxing Liu","Yuqian Fu","Muyuan Ma","Jiahao Li","Danda Pani Paudel","Luc Van Gool","Xiaomeng Huang"],"pdf_url":"https://arxiv.org/pdf/2408.09110v2.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.09533v1","updated":"2025-02-13T17:50:23Z","published":"2025-02-13T17:50:23Z","title":"Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion\n  Model","summary":"  Recent advances in conditional diffusion models have shown promise for\ngenerating realistic TalkingFace videos, yet challenges persist in achieving\nconsistent head movement, synchronized facial expressions, and accurate lip\nsynchronization over extended generations. To address these, we introduce the\n\\textbf{M}otion-priors \\textbf{C}onditional \\textbf{D}iffusion \\textbf{M}odel\n(\\textbf{MCDM}), which utilizes both archived and current clip motion priors to\nenhance motion prediction and ensure temporal consistency. The model consists\nof three key elements: (1) an archived-clip motion-prior that incorporates\nhistorical frames and a reference frame to preserve identity and context; (2) a\npresent-clip motion-prior diffusion model that captures multimodal causality\nfor accurate predictions of head movements, lip sync, and expressions; and (3)\na memory-efficient temporal attention mechanism that mitigates error\naccumulation by dynamically storing and updating motion features. We also\nrelease the \\textbf{TalkingFace-Wild} dataset, a multilingual collection of\nover 200 hours of footage across 10 languages. Experimental results demonstrate\nthe effectiveness of MCDM in maintaining identity and motion continuity for\nlong-term TalkingFace generation. Code, models, and datasets will be publicly\navailable.\n","authors":["Fei Shen","Cong Wang","Junyao Gao","Qin Guo","Jisheng Dang","Jinhui Tang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09528v1","updated":"2025-02-13T17:39:28Z","published":"2025-02-13T17:39:28Z","title":"SteROI-D: System Design and Mapping for Stereo Depth Inference on\n  Regions of Interest","summary":"  Machine learning algorithms have enabled high quality stereo depth estimation\nto run on Augmented and Virtual Reality (AR/VR) devices. However, high energy\nconsumption across the full image processing stack prevents stereo depth\nalgorithms from running effectively on battery-limited devices. This paper\nintroduces SteROI-D, a full stereo depth system paired with a mapping\nmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity\nat the system level to save energy. SteROI-D's flexible and heterogeneous\ncompute fabric supports diverse ROIs. Importantly, we introduce a systematic\nmapping methodology to effectively handle dynamic ROIs, thereby maximizing\nenergy savings. Using these techniques, our 28nm prototype SteROI-D design\nachieves up to 4.35x reduction in total system energy compared to a baseline\nASIC.\n","authors":["Jack Erhardt","Ziang Li","Reid Pinkham","Andrew Berkovich","Zhengya Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09528v1.pdf","comment":"Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin"},{"id":"http://arxiv.org/abs/2502.09520v1","updated":"2025-02-13T17:35:57Z","published":"2025-02-13T17:35:57Z","title":"SQ-GAN: Semantic Image Communications Using Masked Vector Quantization","summary":"  This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach\nintegrating generative models to optimize image compression for\nsemantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic\nsemantic segmentation and a new specifically developed semantic-conditioned\nadaptive mask module (SAMM) to selectively encode semantically significant\nfeatures of the images. SQ-GAN outperforms state-of-the-art image compression\nschemes such as JPEG2000 and BPG across multiple metrics, including perceptual\nquality and semantic segmentation accuracy on the post-decoding reconstructed\nimage, at extreme low compression rates expressed in bits per pixel.\n","authors":["Francesco Pezone","Sergio Barbarossa","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2502.09520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09501v1","updated":"2025-02-13T17:13:46Z","published":"2025-02-13T17:13:46Z","title":"Prior-Constrained Association Learning for Fine-Grained Generalized\n  Category Discovery","summary":"  This paper addresses generalized category discovery (GCD), the task of\nclustering unlabeled data from potentially known or unknown categories with the\nhelp of labeled instances from each known category. Compared to traditional\nsemi-supervised learning, GCD is more challenging because unlabeled data could\nbe from novel categories not appearing in labeled data. Current\nstate-of-the-art methods typically learn a parametric classifier assisted by\nself-distillation. While being effective, these methods do not make use of\ncross-instance similarity to discover class-specific semantics which are\nessential for representation learning and category discovery. In this paper, we\nrevisit the association-based paradigm and propose a Prior-constrained\nAssociation Learning method to capture and learn the semantic relations within\ndata. In particular, the labeled data from known categories provides a unique\nprior for the association of unlabeled data. Unlike previous methods that only\nadopts the prior as a pre or post-clustering refinement, we fully incorporate\nthe prior into the association process, and let it constrain the association\ntowards a reliable grouping outcome. The estimated semantic groups are utilized\nthrough non-parametric prototypical contrast to enhance the representation\nlearning. A further combination of both parametric and non-parametric\nclassification complements each other and leads to a model that outperforms\nexisting methods by a significant margin. On multiple GCD benchmarks, we\nperform extensive experiments and validate the effectiveness of our proposed\nmethod.\n","authors":["Menglin Wang","Zhun Zhong","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2502.09501v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09689v1","updated":"2025-02-13T16:48:27Z","published":"2025-02-13T16:48:27Z","title":"Large Language Models and Provenance Metadata for Determining the\n  Relevance of Images and Videos in News Stories","summary":"  The most effective misinformation campaigns are multimodal, often combining\ntext with images and videos taken out of context -- or fabricating them\nentirely -- to support a given narrative. Contemporary methods for detecting\nmisinformation, whether in deepfakes or text articles, often miss the interplay\nbetween multiple modalities. Built around a large language model, the system\nproposed in this paper addresses these challenges. It analyzes both the\narticle's text and the provenance metadata of included images and videos to\ndetermine whether they are relevant. We open-source the system prototype and\ninteractive web interface.\n","authors":["Tomas Peterka","Matyas Bohacek"],"pdf_url":"https://arxiv.org/pdf/2502.09689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09482v1","updated":"2025-02-13T16:45:39Z","published":"2025-02-13T16:45:39Z","title":"Standardisation of Convex Ultrasound Data Through Geometric Analysis and\n  Augmentation","summary":"  The application of ultrasound in healthcare has seen increased diversity and\nimportance. Unlike other medical imaging modalities, ultrasound research and\ndevelopment has historically lagged, particularly in the case of applications\nwith data-driven algorithms. A significant issue with ultrasound is the extreme\nvariability of the images, due to the number of different machines available\nand the possible combination of parameter settings. One outcome of this is the\nlack of standardised and benchmarking ultrasound datasets. The method proposed\nin this article is an approach to alleviating this issue of disorganisation.\nFor this purpose, the issue of ultrasound data sparsity is examined and a novel\nperspective, approach, and solution is proposed; involving the extraction of\nthe underlying ultrasound plane within the image and representing it using\nannulus sector geometry. An application of this methodology is proposed, which\nis the extraction of scan lines and the linearisation of convex planes.\nValidation of the robustness of the proposed method is performed on both\nprivate and public data. The impact of deformation and the invertibility of\naugmentation using the estimated annulus sector parameters is also studied.\nKeywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.\n","authors":["Alistair Weld","Giovanni Faoro","Luke Dixon","Sophie Camp","Arianna Menciassi","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2502.09482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09471v1","updated":"2025-02-13T16:34:59Z","published":"2025-02-13T16:34:59Z","title":"Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for\n  Weakly-supervised Oriented Object Detection","summary":"  Accurately estimating the orientation of visual objects with compact rotated\nbounding boxes (RBoxes) has become a prominent demand, which challenges\nexisting object detection paradigms that only use horizontal bounding boxes\n(HBoxes). To equip the detectors with orientation awareness, supervised\nregression/classification modules have been introduced at the high cost of\nrotation annotation. Meanwhile, some existing datasets with oriented objects\nare already annotated with horizontal boxes or even single points. It becomes\nattractive yet remains open for effectively utilizing weaker single point and\nhorizontal annotations to train an oriented object detector (OOD). We develop\nWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging\nvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in a\nunified fashion. By only using HBox for training, our Wholly-WOOD achieves\nperformance very close to that of the RBox-trained counterpart on remote\nsensing and other areas, significantly reducing the tedious efforts on\nlabor-intensive annotation for oriented objects. The source codes are available\nat https://github.com/VisionXLab/whollywood (PyTorch-based) and\nhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).\n","authors":["Yi Yu","Xue Yang","Yansheng Li","Zhenjun Han","Feipeng Da","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2502.09471v1.pdf","comment":"18 pages, 9 figures, 9 tables, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2502.07431v2","updated":"2025-02-13T16:32:33Z","published":"2025-02-11T10:19:50Z","title":"ArthroPhase: A Novel Dataset and Method for Phase Recognition in\n  Arthroscopic Video","summary":"  This study aims to advance surgical phase recognition in arthroscopic\nprocedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by\nintroducing the first arthroscopy dataset and developing a novel\ntransformer-based model. We aim to establish a benchmark for arthroscopic\nsurgical phase recognition by leveraging spatio-temporal features to address\nthe specific challenges of arthroscopic videos including limited field of view,\nocclusions, and visual distortions. We developed the ACL27 dataset, comprising\n27 videos of ACL surgeries, each labeled with surgical phases. Our model\nemploys a transformer-based architecture, utilizing temporal-aware frame-wise\nfeature extraction through a ResNet-50 and transformer layers. This approach\nintegrates spatio-temporal features and introduces a Surgical Progress Index\n(SPI) to quantify surgery progression. The model's performance was evaluated\nusing accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80\ndatasets. The proposed model achieved an overall accuracy of 72.91% on the\nACL27 dataset. On the Cholec80 dataset, the model achieved a comparable\nperformance with the state-of-the-art methods with an accuracy of 92.4%. The\nSPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80\ndatasets respectively, indicating reliable surgery progression estimation. This\nstudy introduces a significant advancement in surgical phase recognition for\narthroscopy, providing a comprehensive dataset and a robust transformer-based\nmodel. The results validate the model's effectiveness and generalizability,\nhighlighting its potential to improve surgical training, real-time assistance,\nand operational efficiency in orthopedic surgery. The publicly available\ndataset and code will facilitate future research and development in this\ncritical field.\n","authors":["Ali Bahari Malayeri","Matthias Seibold","Nicola Cavalcanti","Jonas Hein","Sascha Jecklin","Lazaros Vlachopoulos","Sandro Fucentese","Sandro Hodel","Philipp Furnstahl"],"pdf_url":"https://arxiv.org/pdf/2502.07431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09460v1","updated":"2025-02-13T16:27:23Z","published":"2025-02-13T16:27:23Z","title":"Metamorphic Testing for Pose Estimation Systems","summary":"  Pose estimation systems are used in a variety of fields, from sports\nanalytics to livestock care. Given their potential impact, it is paramount to\nsystematically test their behaviour and potential for failure. This is a\ncomplex task due to the oracle problem and the high cost of manual labelling\nnecessary to build ground truth keypoints. This problem is exacerbated by the\nfact that different applications require systems to focus on different subjects\n(e.g., human versus animal) or landmarks (e.g., only extremities versus whole\nbody and face), which makes labelled test data rarely reusable. To combat these\nproblems we propose MET-POSE, a metamorphic testing framework for pose\nestimation systems that bypasses the need for manual annotation while assessing\nthe performance of these systems under different circumstances. MET-POSE thus\nallows users of pose estimation systems to assess the systems in conditions\nthat more closely relate to their application without having to label an ad-hoc\ntest dataset or rely only on available datasets, which may not be adapted to\ntheir application domain. While we define MET-POSE in general terms, we also\npresent a non-exhaustive list of metamorphic rules that represent common\nchallenges in computer vision applications, as well as a specific way to\nevaluate these rules. We then experimentally show the effectiveness of MET-POSE\nby applying it to Mediapipe Holistic, a state of the art human pose estimation\nsystem, with the FLIC and PHOENIX datasets. With these experiments, we outline\nnumerous ways in which the outputs of MET-POSE can uncover faults in pose\nestimation systems at a similar or higher rate than classic testing using hand\nlabelled data, and show that users can tailor the rule set they use to the\nfaults and level of accuracy relevant to their application.\n","authors":["Matias Duran","Thomas Laurent","Ellen Rushe","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2502.09460v1.pdf","comment":"Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)"},{"id":"http://arxiv.org/abs/2410.10790v2","updated":"2025-02-13T16:20:05Z","published":"2024-10-14T17:56:19Z","title":"Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D\n  Scenes","summary":"  Recent advancements in human motion synthesis have focused on specific types\nof motions, such as human-scene interaction, locomotion or human-human\ninteraction, however, there is a lack of a unified system capable of generating\na diverse combination of motion types. In response, we introduce\nSitcom-Crafter, a comprehensive and extendable system for human motion\ngeneration in 3D space, which can be guided by extensive plot contexts to\nenhance workflow efficiency for anime and game designers. The system is\ncomprised of eight modules, three of which are dedicated to motion generation,\nwhile the remaining five are augmentation modules that ensure consistent fusion\nof motion sequences and system functionality. Central to the generation modules\nis our novel 3D scene-aware human-human interaction module, which addresses\ncollision issues by synthesizing implicit 3D Signed Distance Function (SDF)\npoints around motion spaces, thereby minimizing human-scene collisions without\nadditional data collection costs. Complementing this, our locomotion and\nhuman-scene interaction modules leverage existing methods to enrich the\nsystem's motion generation capabilities. Augmentation modules encompass plot\ncomprehension for command generation, motion synchronization for seamless\nintegration of different motion types, hand pose retrieval to enhance motion\nrealism, motion collision revision to prevent human collisions, and 3D\nretargeting to ensure visual fidelity. Experimental evaluations validate the\nsystem's ability to generate high-quality, diverse, and physically realistic\nmotions, underscoring its potential for advancing creative workflows. Project\npage: https://windvchen.github.io/Sitcom-Crafter.\n","authors":["Jianqi Chen","Panwen Hu","Xiaojun Chang","Zhenwei Shi","Michael Kampffmeyer","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10790v2.pdf","comment":"Accepted by ICLR 2025. Project Page:\n  https://windvchen.github.io/Sitcom-Crafter"},{"id":"http://arxiv.org/abs/2410.10719v3","updated":"2025-02-13T16:18:59Z","published":"2024-10-14T17:00:53Z","title":"4-LEGS: 4D Language Embedded Gaussian Splatting","summary":"  The emergence of neural representations has revolutionized our means for\ndigitally viewing a wide range of 3D scenes, enabling the synthesis of\nphotorealistic images rendered from novel views. Recently, several techniques\nhave been proposed for connecting these low-level representations with the\nhigh-level semantics understanding embodied within the scene. These methods\nelevate the rich semantic understanding from 2D imagery to 3D representations,\ndistilling high-dimensional spatial features onto 3D space. In our work, we are\ninterested in connecting language with a dynamic modeling of the world. We show\nhow to lift spatio-temporal features to a 4D representation based on 3D\nGaussian Splatting. This enables an interactive interface where the user can\nspatiotemporally localize events in the video from text prompts. We demonstrate\nour system on public 3D video datasets of people and animals performing various\nactions.\n","authors":["Gal Fiebelman","Tamir Cohen","Ayellet Morgenstern","Peter Hedman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2410.10719v3.pdf","comment":"Eurographics 2025. Project webpage:\n  https://tau-vailab.github.io/4-LEGS/"},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.01404v2","updated":"2025-02-13T16:06:54Z","published":"2024-10-02T10:31:10Z","title":"Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection","summary":"  Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.\n","authors":["Hongru Yan","Yu Zheng","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2410.01404v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09434v1","updated":"2025-02-13T15:56:44Z","published":"2025-02-13T15:56:44Z","title":"Redistribute Ensemble Training for Mitigating Memorization in Diffusion\n  Models","summary":"  Diffusion models, known for their tremendous ability to generate high-quality\nsamples, have recently raised concerns due to their data memorization behavior,\nwhich poses privacy risks. Recent methods for memory mitigation have primarily\naddressed the issue within the context of the text modality in cross-modal\ngeneration tasks, restricting their applicability to specific conditions. In\nthis paper, we propose a novel method for diffusion models from the perspective\nof visual modality, which is more generic and fundamental for mitigating\nmemorization. Directly exposing visual data to the model increases memorization\nrisk, so we design a framework where models learn through proxy model\nparameters instead. Specially, the training dataset is divided into multiple\nshards, with each shard training a proxy model, then aggregated to form the\nfinal model. Additionally, practical analysis of training losses illustrates\nthat the losses for easily memorable images tend to be obviously lower. Thus,\nwe skip the samples with abnormally low loss values from the current mini-batch\nto avoid memorizing. However, balancing the need to skip memorization-prone\nsamples while maintaining sufficient training data for high-quality image\ngeneration presents a key challenge. Thus, we propose IET-AGC+, which\nredistributes highly memorizable samples between shards, to mitigate these\nsamples from over-skipping. Furthermore, we dynamically augment samples based\non their loss values to further reduce memorization. Extensive experiments and\nanalysis on four datasets show that our method successfully reduces memory\ncapacity while maintaining performance. Moreover, we fine-tune the pre-trained\ndiffusion models, e.g., Stable Diffusion, and decrease the memorization score\nby 46.7\\%, demonstrating the effectiveness of our method. Code is available in:\nhttps://github.com/liuxiao-guan/IET_AGC.\n","authors":["Xiaoliu Guan","Yu Wu","Huayang Huang","Xiao Liu","Jiaxu Miao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.09434v1.pdf","comment":"12 pages,9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2407.15328"},{"id":"http://arxiv.org/abs/2502.09688v1","updated":"2025-02-13T15:53:52Z","published":"2025-02-13T15:53:52Z","title":"Towards Virtual Clinical Trials of Radiology AI with Conditional\n  Generative Modeling","summary":"  Artificial intelligence (AI) is poised to transform healthcare by enabling\npersonalized and efficient care through data-driven insights. Although\nradiology is at the forefront of AI adoption, in practice, the potential of AI\nmodels is often overshadowed by severe failures to generalize: AI models can\nhave performance degradation of up to 20% when transitioning from controlled\ntest environments to clinical use by radiologists. This mismatch raises\nconcerns that radiologists will be misled by incorrect AI predictions in\npractice and/or grow to distrust AI, rendering these promising technologies\npractically ineffectual. Exhaustive clinical trials of AI models on abundant\nand diverse data is thus critical to anticipate AI model degradation when\nencountering varied data samples. Achieving these goals, however, is\nchallenging due to the high costs of collecting diverse data samples and\ncorresponding annotations. To overcome these limitations, we introduce a novel\nconditional generative AI model designed for virtual clinical trials (VCTs) of\nradiology AI, capable of realistically synthesizing full-body CT images of\npatients with specified attributes. By learning the joint distribution of\nimages and anatomical structures, our model enables precise replication of\nreal-world patient populations with unprecedented detail at this scale. We\ndemonstrate meaningful evaluation of radiology AI models through VCTs powered\nby our synthetic CT study populations, revealing model degradation and\nfacilitating algorithmic auditing for bias-inducing data attributes. Our\ngenerative AI approach to VCTs is a promising avenue towards a scalable\nsolution to assess model robustness, mitigate biases, and safeguard patient\ncare by enabling simpler testing and evaluation of AI models in any desired\nrange of diverse patient populations.\n","authors":["Benjamin D. Killeen","Bohua Wan","Aditya V. Kulkarni","Nathan Drenkow","Michael Oberst","Paul H. Yi","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2502.09688v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2502.09425v1","updated":"2025-02-13T15:47:45Z","published":"2025-02-13T15:47:45Z","title":"A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone\n  Scans with Deep Learning Based Methods Using Geometry and Morphometry\n  Criteria","summary":"  Three-dimensional (3D) facial shape analysis has gained interest due to its\npotential clinical applications. However, the high cost of advanced 3D facial\nacquisition systems limits their widespread use, driving the development of\nlow-cost acquisition and reconstruction methods. This study introduces a novel\nevaluation methodology that goes beyond traditional geometry-based benchmarks\nby integrating morphometric shape analysis techniques, providing a statistical\nframework for assessing facial morphology preservation. As a case study, we\ncompare smartphone-based 3D scans with state-of-the-art deep learning\nreconstruction methods from 2D images, using high-end stereophotogrammetry\nmodels as ground truth. This methodology enables a quantitative assessment of\nglobal and local shape differences, offering a biologically meaningful\nvalidation approach for low-cost 3D facial acquisition and reconstruction\ntechniques.\n","authors":["√Ålvaro Heredia-Lid√≥n","Alejandro Mo√±ux-Bernal","Alejandro Gonz√°lez","Luis M. Echeverry-Quiceno","Max Rubert","Aroa Casado","Mar√≠a Esther Esteban","Mireia Andreu-Montoriol","Susanna Gallardo","Cristina Ruffo","Neus Mart√≠nez-Abad√≠as","Xavier Sevillano"],"pdf_url":"https://arxiv.org/pdf/2502.09425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15959v3","updated":"2025-02-13T15:38:06Z","published":"2024-10-21T12:43:54Z","title":"Diffusion Transformer Policy: Scaling Diffusion Transformer for\n  Generalist Vision-Language-Action Learning","summary":"  Recent large vision-language action models pretrained on diverse robot\ndatasets have demonstrated the potential for generalizing to new environments\nwith a few in-domain data. However, those approaches usually predict individual\ndiscretized or continuous action by a small action head, which limits the\nability in handling diverse action spaces. In contrast, we model the continuous\naction sequence with a large multi-modal diffusion transformer, dubbed as\nDiffusion Transformer Policy, in which we directly denoise action chunks by a\nlarge transformer model rather than a small action head for action embedding.\nBy leveraging the scaling capability of transformers, the proposed approach can\neffectively model continuous end-effector actions across large diverse robot\ndatasets, and achieve better generalization performance. Extensive experiments\ndemonstrate the effectiveness and generalization of Diffusion Transformer\nPolicy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world\nFranka arm, achieving consistent better performance on Real-to-Sim benchmark\nSimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.\nSpecifically, without bells and whistles, the proposed approach achieves\nstate-of-the-art performance with only a single third-view camera stream in the\nCalvin task ABC->D, improving the average number of tasks completed in a row of\n5 to 3.6, and the pretraining stage significantly facilitates the success\nsequence length on the Calvin by over 1.2. Project Page:\nhttps://zhihou7.github.io/dit_policy_vla/\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Hengjun Pu","Chengyang Zhao","Ronglei Tong","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09411v1","updated":"2025-02-13T15:36:12Z","published":"2025-02-13T15:36:12Z","title":"ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation","summary":"  Diffusion models enable high-quality and diverse visual content synthesis.\nHowever, they struggle to generate rare or unseen concepts. To address this\nchallenge, we explore the usage of Retrieval-Augmented Generation (RAG) with\nimage generation models. We propose ImageRAG, a method that dynamically\nretrieves relevant images based on a given text prompt, and uses them as\ncontext to guide the generation process. Prior approaches that used retrieved\nimages to improve generation, trained models specifically for retrieval-based\ngeneration. In contrast, ImageRAG leverages the capabilities of existing image\nconditioning models, and does not require RAG-specific training. Our approach\nis highly adaptable and can be applied across different model types, showing\nsignificant improvement in generating rare and fine-grained concepts using\ndifferent base models.\n  Our project page is available at: https://rotem-shalev.github.io/ImageRAG\n","authors":["Rotem Shalev-Arkushin","Rinon Gal","Amit H. Bermano","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2502.09411v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.09827v1","updated":"2025-02-13T23:53:40Z","published":"2025-02-13T23:53:40Z","title":"Data and Decision Traceability for the Welder's Arc","summary":"  Space Protocol is applying the principles derived from MITRE and NIST's\nSupply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a\ncomplex multi party system to achieve introspection, auditing, and replay of\ndata and decisions that ultimately lead to a end decision. The core goal of\ndecision traceability is to ensure transparency, accountability, and integrity\nwithin the WA system. This is accomplished by providing a clear, auditable path\nfrom the system's inputs all the way to the final decision. This traceability\nenables the system to track the various algorithms and data flows that have\ninfluenced a particular outcome.\n","authors":["Yasir Latif","Latha Pratti","Samya Bagchi"],"pdf_url":"https://arxiv.org/pdf/2502.09827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09806v1","updated":"2025-02-13T22:48:09Z","published":"2025-02-13T22:48:09Z","title":"Prioritized Ranking Experimental Design Using Recommender Systems in\n  Two-Sided Platforms","summary":"  Interdependencies between units in online two-sided marketplaces complicate\nestimating causal effects in experimental settings. We propose a novel\nexperimental design to mitigate the interference bias in estimating the total\naverage treatment effect (TATE) of item-side interventions in online two-sided\nmarketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the\nrecommender system as an instrument for experimentation. TSPR strategically\nprioritizes items based on their treatment status in the listings displayed to\nusers. We designed TSPR to provide users with a coherent platform experience by\nensuring access to all items and a consistent realization of their treatment by\nall users. We evaluate our experimental design through simulations using a\nsearch impression dataset from an online travel agency. Our methodology closely\nestimates the true simulated TATE, while a baseline item-side estimator\nsignificantly overestimates TATE.\n","authors":["Mahyar Habibi","Zahra Khanalizadeh","Negar Ziaeian"],"pdf_url":"https://arxiv.org/pdf/2502.09806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09797v1","updated":"2025-02-13T22:13:59Z","published":"2025-02-13T22:13:59Z","title":"A Survey on LLM-based News Recommender Systems","summary":"  News recommender systems play a critical role in mitigating the information\noverload problem. In recent years, due to the successful applications of large\nlanguage model technologies, researchers have utilized Discriminative Large\nLanguage Models (DLLMs) or Generative Large Language Models (GLLMs) to improve\nthe performance of news recommender systems. Although several recent surveys\nreview significant challenges for deep learning-based news recommender systems,\nsuch as fairness, privacy-preserving, and responsibility, there is a lack of a\nsystematic survey on Large Language Model (LLM)-based news recommender systems.\nIn order to review different core methodologies and explore potential issues\nsystematically, we categorize DLLM-based and GLLM-based news recommender\nsystems under the umbrella of LLM-based news recommender systems. In this\nsurvey, we first overview the development of deep learning-based news\nrecommender systems. Then, we review LLM-based news recommender systems based\non three aspects: news-oriented modeling, user-oriented modeling, and\nprediction-oriented modeling. Next, we examine the challenges from various\nperspectives, including datasets, benchmarking tools, and methodologies.\nFurthermore, we conduct extensive experiments to analyze how large language\nmodel technologies affect the performance of different news recommender\nsystems. Finally, we comprehensively explore the future directions for\nLLM-based news recommendations in the era of LLMs.\n","authors":["Rongyao Wang","Veronica Liesaputra","Zhiyi Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06648v2","updated":"2025-02-13T20:46:57Z","published":"2025-02-10T16:38:03Z","title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","summary":"  In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.\n","authors":["Erik Novak","Erik Calcina","Dunja Mladeniƒá","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2502.06648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15118v2","updated":"2025-02-13T17:27:18Z","published":"2025-01-25T08:09:37Z","title":"ABXI: Invariant Interest Adaptation for Task-Guided Cross-Domain\n  Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) has recently gained attention\nfor countering data sparsity by transferring knowledge across domains. A common\napproach merges domain-specific sequences into cross-domain sequences, serving\nas bridges to connect domains. One key challenge is to correctly extract the\nshared knowledge among these sequences and appropriately transfer it. Most\nexisting works directly transfer unfiltered cross-domain knowledge rather than\nextracting domain-invariant components and adaptively integrating them into\ndomain-specific modelings. Another challenge lies in aligning the\ndomain-specific and cross-domain sequences. Existing methods align these\nsequences based on timestamps, but this approach can cause prediction\nmismatches when the current tokens and their targets belong to different\ndomains. In such cases, the domain-specific knowledge carried by the current\ntokens may degrade performance. To address these challenges, we propose the\nA-B-Cross-to-Invariant Learning Recommender (ABXI). Specifically, leveraging\nLoRA's effectiveness for efficient adaptation, ABXI incorporates two types of\nLoRAs to facilitate knowledge adaptation. First, all sequences are processed\nthrough a shared encoder that employs a domain LoRA for each sequence, thereby\npreserving unique domain characteristics. Next, we introduce an invariant\nprojector that extracts domain-invariant interests from cross-domain\nrepresentations, utilizing an invariant LoRA to adapt these interests into\nmodeling each specific domain. Besides, to avoid prediction mismatches, all\ndomain-specific sequences are aligned to match the domains of the cross-domain\nground truths. Experimental results on three datasets demonstrate that our\napproach outperforms other CDSR counterparts by a large margin. The codes are\navailable in https://github.com/DiMarzioBian/ABXI.\n","authors":["Qingtian Bian","Marcus Vin√≠cius de Carvalho","Tieying Li","Jiaxing Xu","Hui Fang","Yiping Ke"],"pdf_url":"https://arxiv.org/pdf/2501.15118v2.pdf","comment":"Accepted by WebConf '25 (WWW '25)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09375v1","updated":"2025-02-13T14:44:15Z","published":"2025-02-13T14:44:15Z","title":"FARM: Frequency-Aware Model for Cross-Domain Live-Streaming\n  Recommendation","summary":"  Live-streaming services have attracted widespread popularity due to their\nreal-time interactivity and entertainment value. Users can engage with\nlive-streaming authors by participating in live chats, posting likes, or\nsending virtual gifts to convey their preferences and support. However, the\nlive-streaming services faces serious data-sparsity problem, which can be\nattributed to the following two points: (1) User's valuable behaviors are\nusually sparse, e.g., like, comment and gift, which are easily overlooked by\nthe model, making it difficult to describe user's personalized preference. (2)\nThe main exposure content on our platform is short-video, which is 9 times\nhigher than the exposed live-streaming, leading to the inability of\nlive-streaming content to fully model user preference. To this end, we propose\na Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation, termed\nas FARM. Specifically, we first present the intra-domain frequency aware module\nto enable our model to perceive user's sparse yet valuable behaviors, i.e.,\nhigh-frequency information, supported by the Discrete Fourier Transform (DFT).\nTo transfer user preference across the short-video and live-streaming domains,\nwe propose a novel preference align before fuse strategy, which consists of two\nparts: the cross-domain preference align module to align user preference in\nboth domains with contrastive learning, and the cross-domain preference fuse\nmodule to further fuse user preference in both domains using a serious of\ntailor-designed attention mechanisms. Extensive offline experiments and online\nA/B testing on Kuaishou live-streaming services demonstrate the effectiveness\nand superiority of FARM. Our FARM has been deployed in online live-streaming\nservices and currently serves hundreds of millions of users on Kuaishou.\n","authors":["Xiaodong Li","Ruochen Yang","Shuang Wen","Shen Wang","Yueyang Liu","Guoquan Wang","Weisong Hu","Qiang Luo","Jiawei Sheng","Tingwen Liu","Jiangxia Cao","Shuang Yang","Zhaojie Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18378v3","updated":"2025-02-13T13:35:51Z","published":"2024-12-24T12:07:48Z","title":"RaSeRec: Retrieval-Augmented Sequential Recommendation","summary":"  Although prevailing supervised and self-supervised learning augmented\nsequential recommendation (SeRec) models have achieved improved performance\nwith powerful neural network architectures, we argue that they still suffer\nfrom two limitations: (1) Preference Drift, where models trained on past data\ncan hardly accommodate evolving user preference; and (2) Implicit Memory, where\nhead patterns dominate parametric learning, making it harder to recall long\ntails. In this work, we explore retrieval augmentation in SeRec, to address\nthese limitations. Specifically, we propose a Retrieval-Augmented Sequential\nRecommendation framework, named RaSeRec, the main idea of which is to maintain\na dynamic memory bank to accommodate preference drifts and retrieve relevant\nmemories to augment user modeling explicitly. It consists of two stages: (i)\ncollaborative-based pre-training, which learns to recommend and retrieve; (ii)\nretrieval-augmented fine-tuning, which learns to leverage retrieved memories.\nExtensive experiments on three datasets fully demonstrate the superiority and\neffectiveness of RaSeRec. The implementation code is available at\nhttps://github.com/HITsz-TMG/RaSeRec.\n","authors":["Xinping Zhao","Baotian Hu","Yan Zhong","Shouzheng Huang","Zihao Zheng","Meng Wang","Haofen Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.18378v3.pdf","comment":"14 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09304v1","updated":"2025-02-13T13:16:16Z","published":"2025-02-13T13:16:16Z","title":"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG","summary":"  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%.\n","authors":["Yiqian Huang","Shiqi Zhang","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05033v2","updated":"2025-02-13T12:09:50Z","published":"2024-07-06T09:58:58Z","title":"PeaPOD: Personalized Prompt Distillation for Generative Recommendation","summary":"  Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks.\n","authors":["Jerome Ramos","Bin Wu","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2407.05033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09155v1","updated":"2025-02-13T10:36:17Z","published":"2025-02-13T10:36:17Z","title":"Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI\n  Suggestion","summary":"  This demo paper presents AirSense-R, a privacy-preserving mobile application\nthat provides real-time, pollution-aware recommendations for points of interest\n(POIs) in urban environments. By combining real-time air quality monitoring\ndata with user preferences, the proposed system aims to help users make\nhealth-conscious decisions about the locations they visit. The application\nutilizes collaborative filtering for personalized suggestions, and federated\nlearning for privacy protection, and integrates air pollutant readings from\nAirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland.\nAdditionally, the AirSENCE prediction engine can be employed to detect anomaly\nreadings and interpolate for air quality readings in areas with sparse sensor\ncoverage. This system offers a promising, health-oriented POI recommendation\nsolution that adapts dynamically to current urban air quality conditions while\nsafeguarding user privacy. The code of AirTOWN and a demonstration video is\nmade available at the following repo:\nhttps://github.com/AirtownApp/Airtown-Application.git.\n","authors":["Giuseppe Fasano","Yashar Deldjoo","Tommaso di Noia","Bianca Lau","Sina Adham-Khiabani","Eric Morris","Xia Liu","Ganga Chinna Rao Devarapu","Liam O'Faolain"],"pdf_url":"https://arxiv.org/pdf/2502.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09089v1","updated":"2025-02-13T09:01:34Z","published":"2025-02-13T09:01:34Z","title":"Semantic Ads Retrieval at Walmart eCommerce with Language Models\n  Progressively Trained on Multiple Knowledge Domains","summary":"  Sponsored search in e-commerce poses several unique and complex challenges.\nThese challenges stem from factors such as the asymmetric language structure\nbetween search queries and product names, the inherent ambiguity in user search\nintent, and the vast volume of sparse and imbalanced search corpus data. The\nrole of the retrieval component within a sponsored search system is pivotal,\nserving as the initial step that directly affects the subsequent ranking and\nbidding systems. In this paper, we present an end-to-end solution tailored to\noptimize the ads retrieval system on Walmart.com. Our approach is to pretrain\nthe BERT-like classification model with product category information, enhancing\nthe model's understanding of Walmart product semantics. Second, we design a\ntwo-tower Siamese Network structure for embedding structures to augment\ntraining efficiency. Third, we introduce a Human-in-the-loop Progressive Fusion\nTraining method to ensure robust model performance. Our results demonstrate the\neffectiveness of this pipeline. It enhances the search relevance metric by up\nto 16% compared to a baseline DSSM-based model. Moreover, our large-scale\nonline A/B testing demonstrates that our approach surpasses the ad revenue of\nthe existing production model.\n","authors":["Zhaodong Wang","Weizhi Du","Md Omar Faruk Rokon","Pooshpendu Adhikary","Yanbing Xue","Jiaxuan Xu","Jianghong Zhou","Kuang-chih Lee","Musen Wen"],"pdf_url":"https://arxiv.org/pdf/2502.09089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09058v1","updated":"2025-02-13T08:19:45Z","published":"2025-02-13T08:19:45Z","title":"Unleashing the Power of Large Language Model for Denoising\n  Recommendation","summary":"  Recommender systems are crucial for personalizing user experiences but often\ndepend on implicit feedback data, which can be noisy and misleading. Existing\ndenoising studies involve incorporating auxiliary information or learning\nstrategies from interaction data. However, they struggle with the inherent\nlimitations of external knowledge and interaction data, as well as the\nnon-universality of certain predefined assumptions, hindering accurate noise\nidentification. Recently, large language models (LLMs) have gained attention\nfor their extensive world knowledge and reasoning abilities, yet their\npotential in enhancing denoising in recommendations remains underexplored. In\nthis paper, we introduce LLaRD, a framework leveraging LLMs to improve\ndenoising in recommender systems, thereby boosting overall recommendation\nperformance. Specifically, LLaRD generates denoising-related knowledge by first\nenriching semantic insights from observational data via LLMs and inferring\nuser-item preference knowledge. It then employs a novel Chain-of-Thought (CoT)\ntechnique over user-item interaction graphs to reveal relation knowledge for\ndenoising. Finally, it applies the Information Bottleneck (IB) principle to\nalign LLM-generated denoising knowledge with recommendation targets, filtering\nout noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's\neffectiveness in enhancing denoising and recommendation accuracy.\n","authors":["Shuyao Wang","Zhi Zheng","Yongduo Sui","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.09058v1.pdf","comment":"12 pages, 5 figures, 4 tables. Accecpted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.09050v1","updated":"2025-02-13T08:05:14Z","published":"2025-02-13T08:05:14Z","title":"Leveraging Member-Group Relations via Multi-View Graph Filtering for\n  Effective Group Recommendation","summary":"  Group recommendation aims at providing optimized recommendations tailored to\ndiverse groups, enabling groups to enjoy appropriate items. On the other hand,\nmost existing group recommendation methods are built upon deep neural network\n(DNN) architectures designed to capture the intricate relationships between\nmember-level and group-level interactions. While these DNN-based approaches\nhave proven their effectiveness, they require complex and expensive training\nprocedures to incorporate group-level interactions in addition to member-level\ninteractions. To overcome such limitations, we introduce Group-GF, a new\napproach for extremely fast recommendations of items to each group via\nmulti-view graph filtering (GF) that offers a holistic view of complex\nmember-group dynamics, without the need for costly model training.\nSpecifically, in Group-GF, we first construct three item similarity graphs\nmanifesting different viewpoints for GF. Then, we discover a distinct\npolynomial graph filter for each similarity graph and judiciously aggregate the\nthree graph filters. Extensive experiments demonstrate the effectiveness of\nGroup-GF in terms of significantly reducing runtime and achieving\nstate-of-the-art recommendation accuracy.\n","authors":["Chae-Hyun Kim","Yoon-Ryung Choi","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09050v1.pdf","comment":"5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.09046v1","updated":"2025-02-13T08:01:38Z","published":"2025-02-13T08:01:38Z","title":"Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate\n  Multi-Criteria Recommendation","summary":"  Multi-criteria (MC) recommender systems, which utilize MC rating information\nfor recommendation, are increasingly widespread in various e-commerce domains.\nHowever, the MC recommendation using training-based collaborative filtering,\nrequiring consideration of multiple ratings compared to single-criterion\ncounterparts, often poses practical challenges in achieving state-of-the-art\nperformance along with scalable model training. To solve this problem, we\npropose CA-GF, a training-free MC recommendation method, which is built upon\ncriteria-aware graph filtering for efficient yet accurate MC recommendations.\nSpecifically, first, we construct an item-item similarity graph using an MC\nuser-expansion graph. Next, we design CA-GF composed of the following key\ncomponents, including 1) criterion-specific graph filtering where the optimal\nfilter for each criterion is found using various types of polynomial low-pass\nfilters and 2) criteria preference-infused aggregation where the smoothed\nsignals from each criterion are aggregated. We demonstrate that CA-GF is (a)\nefficient: providing the computational efficiency, offering the extremely fast\nruntime of less than 0.2 seconds even on the largest benchmark dataset, (b)\naccurate: outperforming benchmark MC recommendation methods, achieving\nsubstantial accuracy gains up to 24% compared to the best competitor, and (c)\ninterpretable: providing interpretations for the contribution of each criterion\nto the model prediction based on visualizations.\n","authors":["Jin-Duk Park","Jaemin Yoo","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09046v1.pdf","comment":"12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.05561v2","updated":"2025-02-13T07:44:07Z","published":"2025-02-08T13:20:13Z","title":"Diffusion Model for Interest Refinement in Multi-Interest Recommendation","summary":"  Multi-interest candidate matching plays a pivotal role in personalized\nrecommender systems, as it captures diverse user interests from their\nhistorical behaviors. Most existing methods utilize attention mechanisms to\ngenerate interest representations by aggregating historical item embeddings.\nHowever, these methods only capture overall item-level relevance, leading to\ncoarse-grained interest representations that include irrelevant information. To\naddress this issue, we propose the Diffusion Multi-Interest model (DMI), a\nnovel framework for refining user interest representations at the dimension\nlevel. Specifically, DMI first introduces controllable noise into\ncoarse-grained interest representations at the dimensional level. Then, in the\niterative reconstruction process, DMI combines a cross-attention mechanism and\nan item pruning strategy to reconstruct the personalized interest vectors with\nthe guidance of tailored collaborative information. Extensive experiments\ndemonstrate the effectiveness of DMI, surpassing state-of-the-art methods on\noffline evaluations and an online A/B test. Successfully deployed in the\nreal-world recommender system, DMI effectively enhances user satisfaction and\nsystem performance at scale, serving the major traffic of hundreds of millions\nof daily active users. \\footnote{The code will be released for reproducibility\nonce the paper is accepted.}\n","authors":["Yankun Le","Haoran Li","Baoyuan Ou","Yingjie Qin","Zhixuan Yang","Ruilong Su","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09027v1","updated":"2025-02-13T07:31:34Z","published":"2025-02-13T07:31:34Z","title":"A Contextual-Aware Position Encoding for Sequential Recommendation","summary":"  Sequential recommendation (SR), which encodes user activity to predict the\nnext action, has emerged as a widely adopted strategy in developing commercial\npersonalized recommendation systems. A critical component of modern SR models\nis the attention mechanism, which synthesizes users' historical activities.\nThis mechanism is typically order-invariant and generally relies on position\nencoding (PE). Conventional SR models simply assign a learnable vector to each\nposition, resulting in only modest gains compared to traditional recommendation\nmodels. Moreover, limited research has been conducted on position encoding\ntailored for sequential recommendation, leaving a significant gap in addressing\nits unique requirements. To bridge this gap, we propose a novel\nContextual-Aware Position Encoding method for sequential recommendation,\nabbreviated as CAPE. To the best of our knowledge, CAPE is the first PE method\nspecifically designed for sequential recommendation. Comprehensive experiments\nconducted on benchmark SR datasets demonstrate that CAPE consistently enhances\nmultiple mainstream backbone models and achieves state-of-the-art performance,\nacross small and large scale model size. Furthermore, we deployed CAPE in an\nindustrial setting on a real-world commercial platform, clearly showcasing the\neffectiveness of our approach. Our source code is available at\nhttps://github.com/yjdy/CAPE.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2502.09027v1.pdf","comment":"Accepted by WWW'25 Industry Track"},{"id":"http://arxiv.org/abs/2502.03375v3","updated":"2025-02-13T02:17:49Z","published":"2025-02-05T17:14:45Z","title":"Interactive Visualization Recommendation with Hier-SUCB","summary":"  Visualization recommendation aims to enable rapid visual analysis of massive\ndatasets. In real-world scenarios, it is essential to quickly gather and\ncomprehend user preferences to cover users from diverse backgrounds, including\nvarying skill levels and analytical tasks. Previous approaches to personalized\nvisualization recommendations are non-interactive and rely on initial user data\nfor new users. As a result, these models cannot effectively explore options or\nadapt to real-time feedback. To address this limitation, we propose an\ninteractive personalized visualization recommendation (PVisRec) system that\nlearns on user feedback from previous interactions. For more interactive and\naccurate recommendations, we propose Hier-SUCB, a contextual combinatorial\nsemi-bandit in the PVisRec setting. Theoretically, we show an improved overall\nregret bound with the same rank of time but an improved rank of action space.\nWe further demonstrate the effectiveness of Hier-SUCB through extensive\nexperiments where it is comparable to offline methods and outperforms other\nbandit algorithms in the setting of visualization recommendation.\n","authors":["Songwen Hu","Ryan A. Rossi","Tong Yu","Junda Wu","Handong Zhao","Sungchul Kim","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2502.03375v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v2","updated":"2025-02-13T01:23:29Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07538v2","updated":"2025-02-13T14:21:12Z","published":"2025-02-11T13:24:38Z","title":"Visual-based spatial audio generation system for multi-speaker\n  environments","summary":"  In multimedia applications such as films and video games, spatial audio\ntechniques are widely employed to enhance user experiences by simulating 3D\nsound: transforming mono audio into binaural formats. However, this process is\noften complex and labor-intensive for sound designers, requiring precise\nsynchronization of audio with the spatial positions of visual components. To\naddress these challenges, we propose a visual-based spatial audio generation\nsystem - an automated system that integrates face detection YOLOv8 for object\ndetection, monocular depth estimation, and spatial audio techniques. Notably,\nthe system operates without requiring additional binaural dataset training. The\nproposed system is evaluated against existing Spatial Audio generation system\nusing objective metrics. Experimental results demonstrate that our method\nsignificantly improves spatial consistency between audio and video, enhances\nspeech quality, and performs robustly in multi-speaker scenarios. By\nstreamlining the audio-visual alignment process, the proposed system enables\nsound engineers to achieve high-quality results efficiently, making it a\nvaluable tool for professionals in multimedia production.\n","authors":["Xiaojing Liu","Ogulcan Gurelli","Yan Wang","Joshua Reiss"],"pdf_url":"https://arxiv.org/pdf/2502.07538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14755v2","updated":"2025-02-13T03:15:41Z","published":"2024-04-23T05:36:33Z","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework\n  with Interactive Vision-Language Models","summary":"  With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.\n","authors":["Bo Lin","Yingjing Xu","Xuanwen Bao","Zhou Zhao","Zhouyang Wang","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2404.14755v2.pdf","comment":null}]},"2025-02-12T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.08826v1","updated":"2025-02-12T22:33:41Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18870v2","updated":"2025-02-12T21:37:54Z","published":"2024-10-24T15:57:17Z","title":"End-to-end Training for Recommendation with Language-based User Profiles","summary":"  There is a growing interest in natural language-based user profiles for\nrecommender systems, which aims to enhance transparency and scrutability\ncompared with embedding-based methods. Existing studies primarily generate\nthese profiles using zero-shot inference from large language models (LLMs), but\ntheir quality remains insufficient, leading to suboptimal recommendation\nperformance. In this paper, we introduce LangPTune, the first end-to-end\ntraining framework to optimize LLM-generated user profiles. Our method\nsignificantly outperforms zero-shot approaches by explicitly training the LLM\nfor the recommendation objective. Through extensive evaluations across diverse\ntraining configurations and benchmarks, we demonstrate that LangPTune not only\nsurpasses zero-shot baselines but can also matches the performance of\nstate-of-the-art embedding-based methods. Finally, we investigate whether the\ntraining procedure preserves the interpretability of these profiles compared to\nzero-shot inference through both GPT-4 simulations and crowdworker user\nstudies. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.\n","authors":["Zhaolin Gao","Joyce Zhou","Yijia Dai","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2410.18870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20163v3","updated":"2025-02-12T16:49:56Z","published":"2024-12-28T14:27:45Z","title":"Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems","summary":"  The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.\n","authors":["Minhye Jeon","Seokho Ahn","Young-Duk Seo"],"pdf_url":"https://arxiv.org/pdf/2412.20163v3.pdf","comment":"Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025"},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08496v1","updated":"2025-02-12T15:31:16Z","published":"2025-02-12T15:31:16Z","title":"Fine-Tuning Topics through Weighting Aspect Keywords","summary":"  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2502.08496v1.pdf","comment":"17 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2502.08326v1","updated":"2025-02-12T11:48:15Z","published":"2025-02-12T11:48:15Z","title":"Model-Free Counterfactual Subset Selection at Scale","summary":"  Ensuring transparency in AI decision-making requires interpretable\nexplanations, particularly at the instance level. Counterfactual explanations\nare a powerful tool for this purpose, but existing techniques frequently depend\non synthetic examples, introducing biases from unrealistic assumptions, flawed\nmodels, or skewed data. Many methods also assume full dataset availability, an\nimpractical constraint in real-time environments where data flows continuously.\nIn contrast, streaming explanations offer adaptive, real-time insights without\nrequiring persistent storage of the entire dataset. This work introduces a\nscalable, model-free approach to selecting diverse and relevant counterfactual\nexamples directly from observed data. Our algorithm operates efficiently in\nstreaming settings, maintaining $O(\\log k)$ update complexity per item while\nensuring high-quality counterfactual selection. Empirical evaluations on both\nreal-world and synthetic datasets demonstrate superior performance over\nbaseline methods, with robust behavior even under adversarial conditions.\n","authors":["Minh Hieu Nguyen","Viet Hung Doan","Anh Tuan Nguyen","Jun Jo","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08309v1","updated":"2025-02-12T11:23:46Z","published":"2025-02-12T11:23:46Z","title":"Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model","summary":"  Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality.\n","authors":["Bencheng Yan","Shilei Liu","Zhiyuan Zeng","Zihao Wang","Yizhen Zhang","Yujin Yuan","Langming Liu","Jiaqi Liu","Di Wang","Wenbo Su","Wang Pengjie","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.08309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02231v3","updated":"2025-02-12T10:53:30Z","published":"2024-03-04T17:21:19Z","title":"CODE-ACCORD: A Corpus of building regulatory data for rule generation\n  towards automatic compliance checking","summary":"  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and\nConstruction (AEC) sector necessitates automating the interpretation of\nbuilding regulations to achieve its full potential. Converting textual rules\ninto machine-readable formats is challenging due to the complexities of natural\nlanguage and the scarcity of resources for advanced Machine Learning (ML).\nAddressing these challenges, we introduce CODE-ACCORD, a dataset of 862\nsentences from the building regulations of England and Finland. Only the\nself-contained sentences, which express complete rules without needing\nadditional context, were considered as they are essential for ACC. Each\nsentence was manually annotated with entities and relations by a team of 12\nannotators to facilitate machine-readable rule generation, followed by careful\ncuration to ensure accuracy. The final dataset comprises 4,297 entities and\n4,329 relations across various categories, serving as a robust ground truth.\nCODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,\nincluding text classification, entity recognition, and relation extraction. It\nenables applying recent trends, such as deep neural networks and large language\nmodels, to ACC.\n","authors":["Hansi Hettiarachchi","Amna Dridi","Mohamed Medhat Gaber","Pouyan Parsafard","Nicoleta Bocaneala","Katja Breitenfelder","Gon√ßal Costa","Maria Hedblom","Mihaela Juganaru-Mathieu","Thamer Mecharnia","Sumee Park","He Tan","Abdel-Rahman H. Tawil","Edlira Vakaj"],"pdf_url":"https://arxiv.org/pdf/2403.02231v3.pdf","comment":"This is a preprint of an article submitted to the Scientific Data\n  Journal"},{"id":"http://arxiv.org/abs/2502.08271v1","updated":"2025-02-12T10:24:22Z","published":"2025-02-12T10:24:22Z","title":"MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation","summary":"  Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec.\n","authors":["Min Hou","Chenxi Bai","Le Wu","Hao Liu","Kun Zhang","Kai Zhang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03307v3","updated":"2025-02-12T08:16:44Z","published":"2025-02-05T16:08:05Z","title":"Intent Alignment between Interaction and Language Spaces for\n  Recommendation","summary":"  Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.\n","authors":["Yu Wang","Lei Sang","Yi Zhang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.03307v3.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05806v2","updated":"2025-02-12T07:38:08Z","published":"2024-10-08T08:39:15Z","title":"A Parameter Update Balancing Algorithm for Multi-task Ranking Models in\n  Recommendation Systems","summary":"  Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2410.05806v2.pdf","comment":"Accepted by ICDM'24"},{"id":"http://arxiv.org/abs/2502.08161v1","updated":"2025-02-12T07:05:59Z","published":"2025-02-12T07:05:59Z","title":"MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural\n  Network for Recommendation","summary":"  Graph neural networks have been widely used in recent recommender systems,\nwhere negative sampling plays an important role. Existing negative sampling\nmethods restrict the relationship between nodes as either hard positive pairs\nor hard negative pairs. This leads to the loss of structural information, and\nlacks the mechanism to generate positive pairs for nodes with few neighbors. To\novercome limitations, we propose a novel soft link-based sampling method,\nnamely MixDec Sampling, which consists of Mixup Sampling module and Decay\nSampling module. The Mixup Sampling augments node features by synthesizing new\nnodes and soft links, which provides sufficient number of samples for nodes\nwith few neighbors. The Decay Sampling strengthens the digestion of graph\nstructure information by generating soft links for node embedding learning. To\nthe best of our knowledge, we are the first to model sampling relationships\nbetween nodes by soft links in GNN-based recommender systems. Extensive\nexperiments demonstrate that the proposed MixDec Sampling can significantly and\nconsistently improve the recommendation performance of several representative\nGNN-based models on various recommendation benchmarks.\n","authors":["Xiangjin Xie","Yuxin Chen","Ruipeng Wang","Kai Ouyang","Zihan Zhang","Hai-Tao Zheng","Buyue Qian","Hansen Zheng","Bo Hu","Chengxiang Zhuo","Zang Li"],"pdf_url":"https://arxiv.org/pdf/2502.08161v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.00784v2","updated":"2025-02-12T06:34:11Z","published":"2024-10-17T06:44:18Z","title":"FIRE: Fact-checking with Iterative Retrieval and Verification","summary":"  Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.\n","authors":["Zhuohan Xie","Rui Xing","Yuxia Wang","Jiahui Geng","Hasan Iqbal","Dhruv Sahnan","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2411.00784v2.pdf","comment":"4 figures, 8 tables, accepted to Findings of NAACL"},{"id":"http://arxiv.org/abs/2502.08132v1","updated":"2025-02-12T05:28:08Z","published":"2025-02-12T05:28:08Z","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space\n  Models","summary":"  Sequential recommendation is a key area in the field of recommendation\nsystems aiming to model user interest based on historical interaction sequences\nwith irregular intervals. While previous recurrent neural network-based and\nattention-based approaches have achieved significant results, they have\nlimitations in capturing system continuity due to the discrete characteristics.\nIn the context of continuous-time modeling, state space model (SSM) offers a\npotential solution, as it can effectively capture the dynamic evolution of user\ninterest over time. However, existing SSM-based approaches ignore the impact of\nirregular time intervals within historical user interactions, making it\ndifficult to model complexed user-item transitions in sequences. To address\nthis issue, we propose a hybrid SSM-based model called SS4Rec for\ncontinuous-time sequential recommendation. SS4Rec integrates a time-aware SSM\nto handle irregular time intervals and a relation-aware SSM to model contextual\ndependencies, enabling it to infer user interest from both temporal and\nsequential perspectives. In the training process, the time-aware SSM and the\nrelation-aware SSM are discretized by variable stepsizes according to user\ninteraction time intervals and input data, respectively. This helps capture the\ncontinuous dependency from irregular time intervals and provides time-specific\npersonalized recommendations. Experimental studies on five benchmark datasets\ndemonstrate the superiority and effectiveness of SS4Rec.\n","authors":["Wei Xiao","Huiying Wang","Qifeng Zhou","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v3","updated":"2025-02-12T04:50:07Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2501.17630v2","updated":"2025-02-12T04:28:04Z","published":"2025-01-29T13:08:17Z","title":"Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation","summary":"  Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025\n","authors":["Wonbin Kweon","Sanghwan Jang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2501.17630v2.pdf","comment":"WWW 2025"},{"id":"http://arxiv.org/abs/2408.09380v4","updated":"2025-02-12T04:00:41Z","published":"2024-08-18T06:41:46Z","title":"ELASTIC: Efficient Linear Attention for Sequential Interest Compression","summary":"  State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.\n","authors":["Jiaxin Deng","Shiyao Wang","Song Lu","Yinfeng Li","Xinchen Luo","Yuanjun Liu","Peixing Xu","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.09380v4.pdf","comment":"We hereby withdraw this paper from arXiv due to incomplete\n  experiments. Upon further review, we have determined that additional\n  experimental work is necessary to fully validate our findings and conclusions"},{"id":"http://arxiv.org/abs/2502.08071v1","updated":"2025-02-12T02:24:26Z","published":"2025-02-12T02:24:26Z","title":"Collaborative Filtering Meets Spectrum Shift: Connecting User-Item\n  Interaction with Graph-Structured Side Information","summary":"  Graph Neural Network (GNN) has demonstrated their superiority in\ncollaborative filtering, where the user-item (U-I) interaction bipartite graph\nserves as the fundamental data format. However, when graph-structured side\ninformation (e.g., multimodal similarity graphs or social networks) is\nintegrated into the U-I bipartite graph, existing graph collaborative filtering\nmethods fall short of achieving satisfactory performance. We quantitatively\nanalyze this problem from a spectral perspective. Recall that a bipartite graph\npossesses a full spectrum within the range of [-1, 1], with the highest\nfrequency exactly achievable at -1 and the lowest frequency at 1; however, we\nobserve as more side information is incorporated, the highest frequency of the\naugmented adjacency matrix progressively shifts rightward. This spectrum shift\nphenomenon has caused previous approaches built for the full spectrum [-1, 1]\nto assign mismatched importance to different frequencies. To this end, we\npropose Spectrum Shift Correction (dubbed SSC), incorporating shifting and\nscaling factors to enable spectral GNNs to adapt to the shifted spectrum.\nUnlike previous paradigms of leveraging side information, which necessitate\ntailored designs for diverse data types, SSC directly connects traditional\ngraph collaborative filtering with any graph-structured side information.\nExperiments on social and multimodal recommendation demonstrate the\neffectiveness of SSC, achieving relative improvements of up to 23% without\nincurring any additional computational overhead.\n","authors":["Yunhang He","Cong Xu","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07219v2","updated":"2025-02-12T01:58:22Z","published":"2025-02-11T03:25:42Z","title":"DOGR: Leveraging Document-Oriented Contrastive Learning in Generative\n  Retrieval","summary":"  Generative retrieval constitutes an innovative approach in information\nretrieval, leveraging generative language models (LM) to generate a ranked list\nof document identifiers (docid) for a given query. It simplifies the retrieval\npipeline by replacing the large external index with model parameters. However,\nexisting works merely learned the relationship between queries and document\nidentifiers, which is unable to directly represent the relevance between\nqueries and documents. To address the above problem, we propose a novel and\ngeneral generative retrieval framework, namely Leveraging Document-Oriented\nContrastive Learning in Generative Retrieval (DOGR), which leverages\ncontrastive learning to improve generative retrieval tasks. It adopts a\ntwo-stage learning strategy that captures the relationship between queries and\ndocuments comprehensively through direct interactions. Furthermore, negative\nsampling methods and corresponding contrastive learning objectives are\nimplemented to enhance the learning of semantic representations, thereby\npromoting a thorough comprehension of the relationship between queries and\ndocuments. Experimental results demonstrate that DOGR achieves state-of-the-art\nperformance compared to existing generative retrieval methods on two public\nbenchmark datasets. Further experiments have shown that our framework is\ngenerally effective for common identifier construction techniques.\n","authors":["Penghao Lu","Xin Dong","Yuansheng Zhou","Lei Cheng","Chuan Yuan","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2502.07219v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.21144v4","updated":"2025-02-12T19:20:49Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v4.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08513v1","updated":"2025-02-12T15:46:47Z","published":"2025-02-12T15:46:47Z","title":"\"You'll Be Alice Adventuring in Wonderland!\" Processes, Challenges, and\n  Opportunities of Creating Animated Virtual Reality Stories","summary":"  Animated virtual reality (VR) stories, combining the presence of VR and the\nartistry of computer animation, offer a compelling way to deliver messages and\nevoke emotions. Motivated by the growing demand for immersive narrative\nexperiences, more creators are creating animated VR stories. However, a\nholistic understanding of their creation processes and challenges involved in\ncrafting these stories is still limited. Based on semi-structured interviews\nwith 21 animated VR story creators, we identify ten common stages in their\nend-to-end creation processes, ranging from idea generation to evaluation,\nwhich form diverse workflows that are story-driven or visual-driven.\nAdditionally, we highlight nine unique issues that arise during the creation\nprocess, such as a lack of reference material for multi-element plots, the\nabsence of specific functionalities for story integration, and inadequate\nsupport for audience evaluation. We compare the creation of animated VR stories\nto general XR applications and distill several future research opportunities.\n","authors":["Lin-Ping Yuan","Feilin Han","Liwenhan Xie","Junjie Zhang","Jian Zhao","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2502.08513v1.pdf","comment":"Conditionally accepted to the ACM Conference on Human Factors in\n  Computing Systems (CHI'25)"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2407.14093v3","updated":"2025-02-12T08:49:34Z","published":"2024-07-19T07:57:48Z","title":"Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models","summary":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","authors":["Qiong Wu","Zhaoxi Ke","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.14093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.08674v1","updated":"2025-02-12T03:32:28Z","published":"2025-02-12T03:32:28Z","title":"COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by\n  Silhouette Masks and Fashion Styles","summary":"  How to recommend outfits has gained considerable attention in both academia\nand industry in recent years. Many studies have been carried out regarding\nfashion compatibility learning, to determine whether the fashion items in an\noutfit are compatible or not. These methods mainly focus on evaluating the\ncompatibility of existing outfits and rarely consider applying such knowledge\nto 'design' new fashion items. We propose the new task of generating\ncomplementary and compatible fashion items based on an arbitrary number of\ngiven fashion items. In particular, given some fashion items that can make up\nan outfit, the aim of this paper is to synthesize photo-realistic images of\nother, complementary, fashion items that are compatible with the given ones. To\nachieve this, we propose an outfit generation framework, referred to as\nCOutfitGAN, which includes a pyramid style extractor, an outfit generator, a\nUNet-based real/fake discriminator, and a collocation discriminator. To train\nand evaluate this framework, we collected a large-scale fashion outfit dataset\nwith over 200K outfits and 800K fashion items from the Internet. Extensive\nexperiments show that COutfitGAN outperforms other baselines in terms of\nsimilarity, authenticity, and compatibility measurements.\n","authors":["Dongliang Zhou","Haijun Zhang","Qun Li","Jianghong Ma","Xiaofei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08674v1.pdf","comment":"This paper was accepted by IEEE TMM"},{"id":"http://arxiv.org/abs/2502.10455v1","updated":"2025-02-12T04:25:14Z","published":"2025-02-12T04:25:14Z","title":"E2LVLM:Evidence-Enhanced Large Vision-Language Model for Multimodal\n  Out-of-Context Misinformation Detection","summary":"  Recent studies in Large Vision-Language Models (LVLMs) have demonstrated\nimpressive advancements in multimodal Out-of-Context (OOC) misinformation\ndetection, discerning whether an authentic image is wrongly used in a claim.\nDespite their success, the textual evidence of authentic images retrieved from\nthe inverse search is directly transmitted to LVLMs, leading to inaccurate or\nfalse information in the decision-making phase. To this end, we present E2LVLM,\na novel evidence-enhanced large vision-language model by adapting textual\nevidence in two levels. First, motivated by the fact that textual evidence\nprovided by external tools struggles to align with LVLMs inputs, we devise a\nreranking and rewriting strategy for generating coherent and contextually\nattuned content, thereby driving the aligned and effective behavior of LVLMs\npertinent to authentic images. Second, to address the scarcity of news domain\ndatasets with both judgment and explanation, we generate a novel OOC multimodal\ninstruction-following dataset by prompting LVLMs with informative content to\nacquire plausible explanations. Further, we develop a multimodal\ninstruction-tuning strategy with convincing explanations for beyond detection.\nThis scheme contributes to E2LVLM for multimodal OOC misinformation detection\nand explanation. A multitude of experiments demonstrate that E2LVLM achieves\nsuperior performance than state-of-the-art methods, and also provides\ncompelling rationales for judgments.\n","authors":["Junjie Wu","Yumeng Fu","Nan Yu","Guohong Fu"],"pdf_url":"https://arxiv.org/pdf/2502.10455v1.pdf","comment":null}]},"2025-02-11T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.07971v1","updated":"2025-02-11T21:35:13Z","published":"2025-02-11T21:35:13Z","title":"ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval","summary":"  Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.\n","authors":["Shubham Gupta","Zichao Li","Tianyi Chen","Cem Subakan","Siva Reddy","Perouz Taslakian","Valentina Zantedeschi"],"pdf_url":"https://arxiv.org/pdf/2502.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03993v2","updated":"2025-02-11T21:10:32Z","published":"2024-03-06T19:08:28Z","title":"Personalized Negative Reservoir for Incremental Learning in Recommender\n  Systems","summary":"  Recommender systems have become an integral part of online platforms. Every\nday the volume of training data is expanding and the number of user\ninteractions is constantly increasing. The exploration of larger and more\nexpressive models has become a necessary pursuit to improve user experience.\nHowever, this progression carries with it an increased computational burden. In\ncommercial settings, once a recommendation system model has been trained and\ndeployed it typically needs to be updated frequently as new client data arrive.\nCumulatively, the mounting volume of data is guaranteed to eventually make full\nbatch retraining of the model from scratch computationally infeasible. Naively\nfine-tuning solely on the new data runs into the well-documented problem of\ncatastrophic forgetting. Despite the fact that negative sampling is a crucial\npart of training with implicit feedback, no specialized technique exists that\nis tailored to the incremental learning framework. In this work, we propose a\npersonalized negative reservoir strategy, which is used to obtain negative\nsamples for the standard triplet loss of graph-based recommendation systems.\nOur technique balances alleviation of forgetting with plasticity by encouraging\nthe model to remember stable user preferences and selectively forget when user\ninterests change. We derive the mathematical formulation of a negative sampler\nto populate and update the reservoir. We integrate our design in three SOTA and\ncommonly used incremental recommendation models. We show that these concrete\nrealizations of our negative reservoir framework achieve state-of-the-art\nresults for standard benchmarks using multiple top-k evaluation metrics.\n","authors":["Antonios Valkanas","Yuening Wang","Yingxue Zhang","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2403.03993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05502v3","updated":"2025-02-11T18:17:53Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07683v1","updated":"2025-02-11T16:35:04Z","published":"2025-02-11T16:35:04Z","title":"exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem","summary":"  The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.\n","authors":["Sajad Ebrahimi","Sara Salamat","Negar Arabzadeh","Mahdi Bashari","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.07683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07658v1","updated":"2025-02-11T15:46:28Z","published":"2025-02-11T15:46:28Z","title":"IU4Rec: Interest Unit-Based Product Organization and Recommendation for\n  E-Commerce Platform","summary":"  Most recommendation systems typically follow a product-based paradigm\nutilizing user-product interactions to identify the most engaging items for\nusers. However, this product-based paradigm has notable drawbacks for\nXianyu~\\footnote{Xianyu is China's largest online C2C e-commerce platform where\na large portion of the product are post by individual sellers}. Most of the\nproduct on Xianyu posted from individual sellers often have limited stock\navailable for distribution, and once the product is sold, it's no longer\navailable for distribution. This result in most items distributed product on\nXianyu having relatively few interactions, affecting the effectiveness of\ntraditional recommendation depending on accumulating user-item interactions. To\naddress these issues, we introduce \\textbf{IU4Rec}, an \\textbf{I}nterest\n\\textbf{U}nit-based two-stage \\textbf{Rec}ommendation system framework. We\nfirst group products into clusters based on attributes such as category, image,\nand semantics. These IUs are then integrated into the Recommendation system,\ndelivering both product and technological innovations. IU4Rec begins by\ngrouping products into clusters based on attributes such as category, image,\nand semantics, forming Interest Units (IUs). Then we redesign the\nrecommendation process into two stages. In the first stage, the focus is on\nrecommend these Interest Units, capturing broad-level interests. In the second\nstage, it guides users to find the best option among similar products within\nthe selected Interest Unit. User-IU interactions are incorporated into our\nranking models, offering the advantage of more persistent IU behaviors compared\nto item-specific interactions. Experimental results on the production dataset\nand online A/B testing demonstrate the effectiveness and superiority of our\nproposed IU-centric recommendation approach.\n","authors":["Wenhao Wu","Xiaojie Li","Lin Wang","Jialiang Zhou","Di Wu","Qinye Xie","Qingheng Zhang","Yin Zhang","Shuguang Han","Fei Huang","Junfeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07658v1.pdf","comment":"Under review at KDD25 ADS. This work has already been deployed on the\n  Xianyu platform in Alibaba. arXiv admin note: substantial text overlap with\n  arXiv:2403.06747"},{"id":"http://arxiv.org/abs/2502.06097v2","updated":"2025-02-11T14:44:47Z","published":"2025-02-10T02:06:17Z","title":"NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized\n  Recommendation Systems","summary":"  Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.\n","authors":["Shuli Wang","Xue Wei","Senjie Kou","Chi Wang","Wenshuai Chen","Qi Tang","Yinhua Zhu","Xiong Xiao","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06097v2.pdf","comment":"Accepted by WWW 2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.07491v1","updated":"2025-02-11T11:51:07Z","published":"2025-02-11T11:51:07Z","title":"Exploring Patterns Behind Sports","summary":"  This paper presents a comprehensive framework for time series prediction\nusing a hybrid model that combines ARIMA and LSTM. The model incorporates\nfeature engineering techniques, including embedding and PCA, to transform raw\ndata into a lower-dimensional representation while retaining key information.\nThe embedding technique is used to convert categorical data into continuous\nvectors, facilitating the capture of complex relationships. PCA is applied to\nreduce dimensionality and extract principal components, enhancing model\nperformance and computational efficiency. To handle both linear and nonlinear\npatterns in the data, the ARIMA model captures linear trends, while the LSTM\nmodel models complex nonlinear dependencies. The hybrid model is trained on\nhistorical data and achieves high accuracy, as demonstrated by low RMSE and MAE\nscores. Additionally, the paper employs the run test to assess the randomness\nof sequences, providing insights into the underlying patterns. Ablation studies\nare conducted to validate the roles of different components in the model,\ndemonstrating the significance of each module. The paper also utilizes the SHAP\nmethod to quantify the impact of traditional advantages on the predicted\nresults, offering a detailed understanding of feature importance. The KNN\nmethod is used to determine the optimal prediction interval, further enhancing\nthe model's accuracy. The results highlight the effectiveness of combining\ntraditional statistical methods with modern deep learning techniques for robust\ntime series forecasting in Sports.\n","authors":["Chang Liu","Chengcheng Ma","XuanQi Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07474v1","updated":"2025-02-11T11:34:33Z","published":"2025-02-11T11:34:33Z","title":"ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model","summary":"  Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212\n","authors":["Xiaochen Liu","Yanan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16125v3","updated":"2025-02-11T11:19:40Z","published":"2025-01-27T15:12:27Z","title":"SampleLLM: Optimizing Tabular Data Synthesis in Recommendations","summary":"  Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.\n","authors":["Jingtong Gao","Zhaocheng Du","Xiaopeng Li","Yichao Wang","Xiangyang Li","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.16125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00321v3","updated":"2025-02-11T10:55:02Z","published":"2025-02-01T05:06:21Z","title":"MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior\n  Modeling","summary":"  Click-Through Rate (CTR) prediction is a crucial task in recommendation\nsystems, online searches, and advertising platforms, where accurately capturing\nusers' real interests in content is essential for performance. However,\nexisting methods heavily rely on ID embeddings, which fail to reflect users'\ntrue preferences for content such as images and titles. This limitation becomes\nparticularly evident in cold-start and long-tail scenarios, where traditional\napproaches struggle to deliver effective results. To address these challenges,\nwe propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which\nconsists of three key stages: Pre-training, Content-Interest-Aware Supervised\nFine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training\nstage adapts foundational models to domain-specific data, enabling the\nextraction of high-quality multi-modal embeddings. The C-SFT stage bridges the\nsemantic gap between content and user interests by leveraging user behavior\nsignals to guide the alignment of embeddings with user preferences. Finally,\nthe CiUBM stage integrates multi-modal embeddings and ID-based collaborative\nfiltering signals into a unified framework. Comprehensive offline experiments\nand online A/B tests conducted on the Taobao, one of the world's largest\ne-commerce platforms, demonstrated the effectiveness and efficiency of MIM\nmethod. The method has been successfully deployed online, achieving a\nsignificant increase of +14.14% in CTR and +4.12% in RPM, showcasing its\nindustrial applicability and substantial impact on platform performance. To\npromote further research, we have publicly released the code and dataset at\nhttps://pan.quark.cn/s/8fc8ec3e74f3.\n","authors":["Bencheng Yan","Si Chen","Shichang Jia","Jianyu Liu","Yueran Liu","Chenghan Fu","Wanxian Guan","Hui Zhao","Xiang Zhang","Kai Zhang","Wenbo Su","Pengjie Wang","Jian Xu","Bo Zheng","Baolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00321v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17670v2","updated":"2025-02-11T09:52:43Z","published":"2025-01-29T14:20:42Z","title":"Distinguished Quantized Guidance for Diffusion-based Sequence\n  Recommendation","summary":"  Diffusion models (DMs) have emerged as promising approaches for sequential\nrecommendation due to their strong ability to model data distributions and\ngenerate high-quality items. Existing work typically adds noise to the next\nitem and progressively denoises it guided by the user's interaction sequence,\ngenerating items that closely align with user interests. However, we identify\ntwo key issues in this paradigm. First, the sequences are often heterogeneous\nin length and content, exhibiting noise due to stochastic user behaviors. Using\nsuch sequences as guidance may hinder DMs from accurately understanding user\ninterests. Second, DMs are prone to data bias and tend to generate only the\npopular items that dominate the training dataset, thus failing to meet the\npersonalized needs of different users. To address these issues, we propose\nDistinguished Quantized Guidance for Diffusion-based Sequence Recommendation\n(DiQDiff), which aims to extract robust guidance to understand user interests\nand generate distinguished items for personalized user interests within DMs. To\nextract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)\nto quantize sequences into semantic vectors (e.g., collaborative signals and\ncategory interests) using a codebook, which can enrich the guidance to better\nunderstand user interests. To generate distinguished items, DiQDiff\npersonalizes the generation through Contrastive Discrepancy Maximization (CDM),\nwhich maximizes the distance between denoising trajectories using contrastive\nloss to prevent biased generation for different users. Extensive experiments\nare conducted to compare DiQDiff with multiple baseline models across four\nwidely-used datasets. The superior recommendation performance of DiQDiff\nagainst leading approaches demonstrates its effectiveness in sequential\nrecommendation tasks.\n","authors":["Wenyu Mao","Shuchang Liu","Haoyang Liu","Haozhe Liu","Xiang Li","Lantao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03984v2","updated":"2025-02-11T09:07:15Z","published":"2023-10-06T02:45:21Z","title":"AURO: Reinforcement Learning for Adaptive User Retention Optimization in\n  Recommender Systems","summary":"  The field of Reinforcement Learning (RL) has garnered increasing attention\nfor its ability of optimizing user retention in recommender systems. A primary\nobstacle in this optimization process is the environment non-stationarity\nstemming from the continual and complex evolution of user behavior patterns\nover time, such as variations in interaction rates and retention propensities.\nThese changes pose significant challenges to existing RL algorithms for\nrecommendations, leading to issues with dynamics and reward distribution\nshifts. This paper introduces a novel approach called \\textbf{A}daptive\n\\textbf{U}ser \\textbf{R}etention \\textbf{O}ptimization (AURO) to address this\nchallenge. To navigate the recommendation policy in non-stationary\nenvironments, AURO introduces an state abstraction module in the policy\nnetwork. The module is trained with a new value-based loss function, aligning\nits output with the estimated performance of the current policy. As the policy\nperformance of RL is sensitive to environment drifts, the loss function enables\nthe state abstraction to be reflective of environment changes and notify the\nrecommendation policy to adapt accordingly. Additionally, the non-stationarity\nof the environment introduces the problem of implicit cold start, where the\nrecommendation policy continuously interacts with users displaying novel\nbehavior patterns. AURO encourages exploration guarded by performance-based\nrejection sampling to maintain a stable recommendation quality in the\ncost-sensitive online environment. Extensive empirical analysis are conducted\nin a user retention simulator, the MovieLens dataset, and a live short-video\nrecommendation platform, demonstrating AURO's superior performance against all\nevaluated baseline algorithms.\n","authors":["Zhenghai Xue","Qingpeng Cai","Bin Yang","Lantao Hu","Peng Jiang","Kun Gai","Bo An"],"pdf_url":"https://arxiv.org/pdf/2310.03984v2.pdf","comment":"The Web Conference 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.07327v1","updated":"2025-02-11T07:43:47Z","published":"2025-02-11T07:43:47Z","title":"Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated\n  Videos","summary":"  With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.\n","authors":["Haowen Gao","Liang Pang","Shicheng Xu","Leigang Qu","Tat-Seng Chua","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v2","updated":"2025-02-11T07:36:07Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20163v2","updated":"2025-02-11T07:35:58Z","published":"2024-10-26T12:34:07Z","title":"UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers","summary":"  Existing information retrieval (IR) models often assume a homogeneous\nstructure for knowledge sources and user queries, limiting their applicability\nin real-world settings where retrieval is inherently heterogeneous and diverse.\nIn this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous\nknowledge retriever that (1) builds a unified retrieval space for heterogeneous\nknowledge and (2) follows diverse user instructions to retrieve knowledge of\nspecified types. UniHGKR consists of three principal stages: heterogeneous\nself-supervised pretraining, text-anchored embedding alignment, and\ninstruction-aware retriever fine-tuning, enabling it to generalize across\nvaried retrieval contexts. This framework is highly scalable, with a BERT-based\nversion and a UniHGKR-7B version trained on large language models. Also, we\nintroduce CompMix-IR, the first native heterogeneous knowledge retrieval\nbenchmark. It includes two retrieval scenarios with various instructions, over\n9,400 question-answer (QA) pairs, and a corpus of 10 million entries, covering\nfour different types of data. Extensive experiments show that UniHGKR\nconsistently outperforms state-of-the-art methods on CompMix-IR, achieving up\nto 6.36% and 54.23% relative improvements in two scenarios, respectively.\nFinally, by equipping our retriever for open-domain heterogeneous QA systems,\nwe achieve a new state-of-the-art result on the popular ConvMix task, with an\nabsolute improvement of up to 5.90 points.\n","authors":["Dehai Min","Zhiyang Xu","Guilin Qi","Lifu Huang","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2410.20163v2.pdf","comment":"NAACL 2025, Main, Long Paper"},{"id":"http://arxiv.org/abs/2502.07315v1","updated":"2025-02-11T07:25:57Z","published":"2025-02-11T07:25:57Z","title":"Prompt-Based Document Modifications In Ranking Competitions","summary":"  We study prompting-based approaches with Large Language Models (LLMs) for\nmodifying documents so as to promote their ranking in a competitive search\nsetting. Our methods are inspired by prior work on leveraging LLMs as rankers.\nWe evaluate our approach by deploying it as a bot in previous ranking\ncompetitions and in competitions we organized. Our findings demonstrate that\nour approach effectively improves document ranking while preserving high levels\nof faithfulness to the original content and maintaining overall document\nquality.\n","authors":["Niv Bardas","Tommy Mordo","Oren Kurland","Moshe Tennenholtz","Gal Zur"],"pdf_url":"https://arxiv.org/pdf/2502.07315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05523v2","updated":"2025-02-11T07:21:41Z","published":"2025-02-08T11:05:22Z","title":"Adaptive Domain Scaling for Personalized Sequential Modeling in\n  Recommenders","summary":"  Users generally exhibit complex behavioral patterns and diverse intentions in\nmultiple business scenarios of super applications like Douyin, presenting great\nchallenges to current industrial multi-domain recommenders. To mitigate the\ndiscrepancies across diverse domains, researches and industrial practices\ngenerally emphasize sophisticated network structures to accomodate diverse data\ndistributions, while neglecting the inherent understanding of user behavioral\nsequence from the multi-domain perspective. In this paper, we present Adaptive\nDomain Scaling (ADS) model, which comprehensively enhances the personalization\ncapability in target-aware sequence modeling across multiple domains.\nSpecifically, ADS comprises of two major modules, including personalized\nsequence representation generation (PSRG) and personalized candidate\nrepresentation generation (PCRG). The modules contribute to the tailored\nmulti-domain learning by dynamically learning both the user behavioral sequence\nitem representation and the candidate target item representation under\ndifferent domains, facilitating adaptive user intention understanding.\nExperiments are performed on both a public dataset and two billion-scaled\nindustrial datasets, and the extensive results verify the high effectiveness\nand compatibility of ADS. Besides, we conduct online experiments on two\ninfluential business scenarios including Douyin Advertisement Platform and\nDouyin E-commerce Service Platform, both of which show substantial business\nimprovements. Currently, ADS has been fully deployed in many recommendation\nservices at ByteDance, serving billions of users.\n","authors":["Zheng Chai","Hui Lu","Di Chen","Qin Ren","Yuchao Zheng","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07307v1","updated":"2025-02-11T07:09:49Z","published":"2025-02-11T07:09:49Z","title":"CreAgent: Towards Long-Term Evaluation of Recommender System under\n  Platform-Creator Information Asymmetry","summary":"  Ensuring the long-term sustainability of recommender systems (RS) emerges as\na crucial issue. Traditional offline evaluation methods for RS typically focus\non immediate user feedback, such as clicks, but they often neglect the\nlong-term impact of content creators. On real-world content platforms, creators\ncan strategically produce and upload new items based on user feedback and\npreference trends. While previous studies have attempted to model creator\nbehavior, they often overlook the role of information asymmetry. This asymmetry\narises because creators primarily have access to feedback on the items they\nproduce, while platforms possess data on the entire spectrum of user feedback.\nCurrent RS simulators, however, fail to account for this asymmetry, leading to\ninaccurate long-term evaluations. To address this gap, we propose CreAgent, a\nLarge Language Model (LLM)-empowered creator simulation agent. By incorporating\ngame theory's belief mechanism and the fast-and-slow thinking framework,\nCreAgent effectively simulates creator behavior under conditions of information\nasymmetry. Additionally, we enhance CreAgent's simulation ability by\nfine-tuning it using Proximal Policy Optimization (PPO). Our credibility\nvalidation experiments show that CreAgent aligns well with the behaviors\nbetween real-world platform and creator, thus improving the reliability of\nlong-term RS evaluations. Moreover, through the simulation of RS involving\nCreAgents, we can explore how fairness- and diversity-aware RS algorithms\ncontribute to better long-term performance for various stakeholders. CreAgent\nand the simulation platform are publicly available at\nhttps://github.com/shawnye2000/CreAgent.\n","authors":["Xiaopeng Ye","Chen Xu","Zhongxiang Sun","Jun Xu","Gang Wang","Zhenhua Dong","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.07307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07303v1","updated":"2025-02-11T07:01:19Z","published":"2025-02-11T07:01:19Z","title":"Flow Matching for Collaborative Filtering","summary":"  Generative models have shown great promise in collaborative filtering by\ncapturing the underlying distribution of user interests and preferences.\nHowever, existing approaches struggle with inaccurate posterior approximations\nand misalignment with the discrete nature of recommendation data, limiting\ntheir expressiveness and real-world performance. To address these limitations,\nwe propose FlowCF, a novel flow-based recommendation system leveraging flow\nmatching for collaborative filtering. We tailor flow matching to the unique\nchallenges in recommendation through two key innovations: (1) a behavior-guided\nprior that aligns with user behavior patterns to handle the sparse and\nheterogeneous user-item interactions, and (2) a discrete flow framework to\npreserve the binary nature of implicit feedback while maintaining the benefits\nof flow matching, such as stable training and efficient inference. Extensive\nexperiments demonstrate that FlowCF achieves state-of-the-art recommendation\naccuracy across various datasets with the fastest inference speed, making it a\ncompelling approach for real-world recommender systems.\n","authors":["Chengkai Liu","Yangtian Zhang","Jianling Wang","Rex Ying","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2502.07303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09401v6","updated":"2025-02-11T06:26:34Z","published":"2023-10-13T20:53:50Z","title":"CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate\n  Personalized News Recommendation","summary":"  Personalized news recommendation aims to assist users in finding news\narticles that align with their interests, which plays a pivotal role in\nmitigating users' information overload problem. Although many recent works have\nbeen studied for better personalized news recommendation, the following\nchallenges should be explored more: (C1) Comprehending manifold intents coupled\nwithin a news article, (C2) Differentiating varying post-read preferences of\nnews articles, and (C3) Addressing the cold-start user problem. To tackle the\naforementioned challenges together, in this paper, we propose a novel\npersonalized news recommendation framework (CROWN) that employs (1)\ncategory-guided intent disentanglement for (C1), (2) consistency-based news\nrepresentation for (C2), and (3) GNN-enhanced hybrid user representation for\n(C3). Furthermore, we incorporate a category prediction into the training\nprocess of CROWN as an auxiliary task, which provides supplementary supervisory\nsignals to enhance intent disentanglement. Extensive experiments on two\nreal-world datasets reveal that (1) CROWN provides consistent performance\nimprovements over ten state-of-the-art news recommendation methods and (2) the\nproposed strategies significantly improve the accuracy of CROWN.\n","authors":["Yunyong Ko","Seongeun Ryu","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2310.09401v6.pdf","comment":"11 pages, 7 figures, 9 tables, the ACM Web Conference (WWW) 2025"},{"id":"http://arxiv.org/abs/2502.06101v2","updated":"2025-02-11T05:53:00Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v2.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.00950v3","updated":"2025-02-11T18:24:41Z","published":"2025-02-02T23:04:55Z","title":"Fast Audio Codec Identification Using Overlapping LCS","summary":"  Audio data are widely exchanged over telecommunications networks. Due to the\nlimitations of network resources, these data are typically compressed before\ntransmission. Various methods are available for compressing audio data. To\naccess such audio information, it is first necessary to identify the codec used\nfor compression. One of the most effective approaches for audio codec\nidentification involves analyzing the content of received packets. In these\nmethods, statistical features extracted from the packets are utilized to\ndetermine the codec employed. This paper proposes a novel method for audio\ncodec classification based on features derived from the overlapped longest\ncommon sub-string and sub-sequence (LCS). The simulation results, which\nachieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of\nthe proposed method over conventional approaches. This method divides each 8 KB\npacket into fifteen 1 KB packets with a 50% overlap. The results indicate that\nthis division has no significant impact on the simulation outcomes, while\nsignificantly speeding up the feature extraction, being eight times faster than\nthe traditional method for extracting LCS features.\n","authors":["Farzane Jafari"],"pdf_url":"https://arxiv.org/pdf/2502.00950v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.07711v1","updated":"2025-02-11T17:10:31Z","published":"2025-02-11T17:10:31Z","title":"RenderBox: Expressive Performance Rendering with Text Control","summary":"  Expressive music performance rendering involves interpreting symbolic scores\nwith variations in timing, dynamics, articulation, and instrument-specific\ntechniques, resulting in performances that capture musical can emotional\nintent. We introduce RenderBox, a unified framework for text-and-score\ncontrolled audio performance generation across multiple instruments, applying\ncoarse-level controls through natural language descriptions and granular-level\ncontrols using music scores. Based on a diffusion transformer architecture and\ncross-attention joint conditioning, we propose a curriculum-based paradigm that\ntrains from plain synthesis to expressive performance, gradually incorporating\ncontrollable factors such as speed, mistakes, and style diversity.\n  RenderBox achieves high performance compared to baseline models across key\nmetrics such as FAD and CLAP, and also tempo and pitch accuracy under different\nprompting tasks. Subjective evaluation further demonstrates that RenderBox is\nable to generate controllable expressive performances that sound natural and\nmusically engaging, aligning well with prompts and intent.\n","authors":["Huan Zhang","Akira Maezawa","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2502.07711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07411v1","updated":"2025-02-11T09:45:06Z","published":"2025-02-11T09:45:06Z","title":"EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering","summary":"  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for\negocentric QA assistance involving scene text. EgoTextVQA contains 1.5K\nego-view videos and 7K scene-text aware questions that reflect real-user needs\nin outdoor driving and indoor house-keeping activities. The questions are\ndesigned to elicit identification and reasoning on scene text in an egocentric\nand dynamic environment. With EgoTextVQA, we comprehensively evaluate 10\nprominent multimodal large language models. Currently, all models struggle, and\nthe best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the\nsevere deficiency of these techniques in egocentric QA assistance. Our further\ninvestigations suggest that precise temporal grounding and multi-frame\nreasoning, along with high resolution and auxiliary scene-text inputs, are key\nfor better performance. With thorough analyses and heuristic suggestions, we\nhope EgoTextVQA can serve as a solid testbed for research in egocentric\nscene-text QA assistance.\n","authors":["Sheng Zhou","Junbin Xiao","Qingyun Li","Yicong Li","Xun Yang","Dan Guo","Meng Wang","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07160v1","updated":"2025-02-11T00:56:44Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2404.03161v2","updated":"2025-02-11T00:45:46Z","published":"2024-04-04T02:22:37Z","title":"BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes","summary":"  This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/\n","authors":["Tomohiro Nishimoto","Taichi Nishimura","Koki Yamamoto","Keisuke Shirai","Hirotaka Kameko","Yuto Haneji","Tomoya Yoshida","Keiya Kajimura","Taiyu Cui","Chihiro Nishiwaki","Eriko Daikoku","Natsuko Okuda","Fumihito Ono","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2404.03161v2.pdf","comment":"6 pages"}]},"2025-02-10T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.07067v1","updated":"2025-02-10T21:59:01Z","published":"2025-02-10T21:59:01Z","title":"Repository-level Code Search with Neural Retrieval Methods","summary":"  This paper presents a multi-stage reranking system for repository-level code\nsearch, which leverages the vastly available commit histories of large\nopen-source repositories to aid in bug fixing. We define the task of\nrepository-level code search as retrieving the set of files from the current\nstate of a code repository that are most relevant to addressing a user's\nquestion or bug. The proposed approach combines BM25-based retrieval over\ncommit messages with neural reranking using CodeBERT to identify the most\npertinent files. By learning patterns from diverse repositories and their\ncommit histories, the system can surface relevant files for the task at hand.\nThe system leverages both commit messages and source code for relevance\nmatching, and is evaluated in both normal and oracle settings. Experiments on a\nnew dataset created from 7 popular open-source repositories demonstrate\nsubstantial improvements of up to 80% in MAP, MRR and P@1 over the BM25\nbaseline, across a diverse set of queries, demonstrating the effectiveness this\napproach. We hope this work aids LLM agents as a tool for better code search\nand understanding. Our code and results obtained are publicly available.\n","authors":["Siddharth Gandhi","Luyu Gao","Jamie Callan"],"pdf_url":"https://arxiv.org/pdf/2502.07067v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2412.06949v2","updated":"2025-02-10T21:34:00Z","published":"2024-12-09T19:53:13Z","title":"Bridging Conversational and Collaborative Signals for Conversational\n  Recommendation","summary":"  Conversational recommendation systems (CRS) leverage contextual information\nfrom conversations to generate recommendations but often struggle due to a lack\nof collaborative filtering (CF) signals, which capture user-item interaction\npatterns essential for accurate recommendations. We introduce Reddit-ML32M, a\ndataset that links Reddit conversations with interactions on MovieLens 32M, to\nenrich item representations by leveraging collaborative knowledge and\naddressing interaction sparsity in conversational datasets. We propose an\nLLM-based framework that uses Reddit-ML32M to align LLM-generated\nrecommendations with CF embeddings, refining rankings for better performance.\nWe evaluate our framework against three sets of baselines: CF-based\nrecommenders using only interactions from CRS tasks, traditional CRS models,\nand LLM-based methods relying on conversational context without item\nrepresentations. Our approach achieves consistent improvements, including a\n12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the\nbest-performing baseline that relies on conversational context but lacks\ncollaborative item representations.\n","authors":["Ahmad Bin Rabiah","Nafis Sadeq","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2412.06949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06705v1","updated":"2025-02-10T17:33:22Z","published":"2025-02-10T17:33:22Z","title":"RSAttAE: An Information-Aware Attention-based Autoencoder Recommender\n  System","summary":"  Recommender systems play a crucial role in modern life, including information\nretrieval, the pharmaceutical industry, retail, and entertainment. The\nentertainment sector, in particular, attracts significant attention and\ngenerates substantial profits. This work proposes a new method for predicting\nunknown user-movie ratings to enhance customer satisfaction. To achieve this,\nwe utilize the MovieLens 100K dataset. Our approach introduces an\nattention-based autoencoder to create meaningful representations and the\nXGBoost method for rating predictions. The results demonstrate that our\nproposal outperforms most of the existing state-of-the-art methods.\nAvailability: github.com/ComputationIASBS/RecommSys\n","authors":["Amirhossein Dadashzadeh Taromi","Sina Heydari","Mohsen Hooshmand","Majid Ramezani"],"pdf_url":"https://arxiv.org/pdf/2502.06705v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06557v1","updated":"2025-02-10T15:24:55Z","published":"2025-02-10T15:24:55Z","title":"LiveForesighter: Generating Future Information for Live-Streaming\n  Recommendations at Kuaishou","summary":"  Live-streaming, as a new-generation media to connect users and authors, has\nattracted a lot of attention and experienced rapid growth in recent years.\nCompared with the content-static short-video recommendation, the live-streaming\nrecommendation faces more challenges in giving our users a satisfactory\nexperience: (1) Live-streaming content is dynamically ever-changing along time.\n(2) valuable behaviors (e.g., send digital-gift, buy products) always require\nusers to watch for a long-time (>10 min). Combining the two attributes, here\nraising a challenging question for live-streaming recommendation: How to\ndiscover the live-streamings that the content user is interested in at the\ncurrent moment, and further a period in the future?\n","authors":["Yucheng Lu","Jiangxia Cao","Xu Kuan","Wei Cheng","Wei Jiang","Jiaming Zhang","Yang Shuang","Liu Zhaojie","Liyin Hong"],"pdf_url":"https://arxiv.org/pdf/2502.06557v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.09425v2","updated":"2025-02-10T15:17:49Z","published":"2024-11-14T13:22:41Z","title":"MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity","summary":"  Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.\n","authors":["Xiao Lv","Jiangxia Cao","Shijie Guan","Xiaoyou Zhou","Zhiguang Qi","Yaqiang Zang","Ming Li","Ben Wang","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.09425v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.06062v4","updated":"2025-02-10T10:55:08Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06269v1","updated":"2025-02-10T09:08:37Z","published":"2025-02-10T09:08:37Z","title":"Progressive Collaborative and Semantic Knowledge Fusion for Generative\n  Recommendation","summary":"  With the recent surge in interest surrounding generative paradigms,\ngenerative recommendation has increasingly attracted the attention of\nresearchers in the recommendation community. This paradigm generally consists\nof two stages. In the first stage, pretrained semantic embeddings or\ncollaborative ID embeddings are quantized to create item codes, aiming to\ncapture and preserve rich semantic or collaborative knowledge within these\ncodes. The second stage involves utilizing these discrete codes to perform an\nautoregressive sequence generation task. Existing methods often either overlook\ncollaborative or semantic knowledge, or combine the two roughly. In this paper,\nwe observe that naively concatenating representations from semantic and\ncollaborative modality leads to a semantic domination issue, where the\nresulting representation is overly influenced by semantic information,\neffectively overshadowing the collaborative representation. Consequently,\ndownstream recommendation tasks fail to fully exploit the knowledge from both\nmodalities, resulting in suboptimal performance. To address this, we propose a\nprogressive collaborative and semantic knowledge fusion model for generative\nrecommendation, named PRORec, which integrates semantic and collaborative\nknowledge with a unified code through a two-stage framework. Specifically, in\nthe first stage, we propose a cross-modality knowledge alignment task, which\nintegrates semantic knowledge into collaborative embeddings, enhancing their\nrepresentational capability. In the second stage, we propose an in-modality\nknowledge distillation task, designed to effectively capture and integrate\nknowledge from both semantic and collaborative modalities. Extensive\nexperiments on three widely used benchmarks validate the effectiveness of our\napproach, demonstrating its superiority compared to existing methods.\n","authors":["Longtao Xiao","Haozhao Wang","Cheng Wang","Linfei Ji","Yifan Wang","Jieming Zhu","Zhenhua Dong","Rui Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.06269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01457v2","updated":"2025-02-10T08:58:30Z","published":"2024-11-03T06:47:45Z","title":"Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\n  Recommendation","summary":"  Sequential recommendation (SR) systems excel at capturing users' dynamic\npreferences by leveraging their interaction histories. Most existing SR systems\nassign a single embedding vector to each item to represent its features, and\nvarious types of models are adopted to combine these item embeddings into a\nsequence representation vector to capture the user intent. However, we argue\nthat this representation alone is insufficient to capture an item's\nmulti-faceted nature (e.g., movie genres, starring actors). Besides, users\noften exhibit complex and varied preferences within these facets (e.g., liking\nboth action and musical films in the facet of genre), which are challenging to\nfully represent. To address the issues above, we propose a novel structure\ncalled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\nRecommendation (FAME). We leverage sub-embeddings from each head in the last\nmulti-head attention layer to predict the next item separately. This approach\ncaptures the potential multi-faceted nature of items without increasing model\ncomplexity. A gating mechanism integrates recommendations from each head and\ndynamically determines their importance. Furthermore, we introduce a\nMixture-of-Experts (MoE) network in each attention head to disentangle various\nuser preferences within each facet. Each expert within the MoE focuses on a\nspecific preference. A learnable router network is adopted to compute the\nimportance weight for each expert and aggregate them. We conduct extensive\nexperiments on four public sequential recommendation datasets and the results\ndemonstrate the effectiveness of our method over existing baseline models.\n","authors":["Mingrui Liu","Sixiao Zhang","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2411.01457v2.pdf","comment":"This paper has been accepted by WSDM'25"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2502.06252v1","updated":"2025-02-10T08:33:47Z","published":"2025-02-10T08:33:47Z","title":"Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap\n  Perspective","summary":"  Entity retrieval plays a crucial role in the utilization of Electronic Health\nRecords (EHRs) and is applied across a wide range of clinical practices.\nHowever, a comprehensive evaluation of this task is lacking due to the absence\nof a public benchmark. In this paper, we propose the development and release of\na novel benchmark for evaluating entity retrieval in EHRs, with a particular\nfocus on the semantic gap issue. Using discharge summaries from the MIMIC-III\ndataset, we incorporate ICD codes and prescription labels associated with the\nnotes as queries, and annotate relevance judgments using GPT-4. In total, we\nuse 1,000 patient notes, generate 1,246 queries, and provide over 77,000\nrelevance annotations. To offer the first assessment of the semantic gap, we\nintroduce a novel classification system for relevance matches. Leveraging\nGPT-4, we categorize each relevant pair into one of five categories: string,\nsynonym, abbreviation, hyponym, and implication. Using the proposed benchmark,\nwe evaluate several retrieval methods, including BM25, query expansion, and\nstate-of-the-art dense retrievers. Our findings show that BM25 provides a\nstrong baseline but struggles with semantic matches. Query expansion\nsignificantly improves performance, though it slightly reduces string match\ncapabilities. Dense retrievers outperform traditional methods, particularly for\nsemantic matches, and general-domain dense retrievers often surpass those\ntrained specifically in the biomedical domain.\n","authors":["Zhengyun Zhao","Hongyi Yuan","Jingjing Liu","Haichao Chen","Huaiyuan Ying","Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06252v1.pdf","comment":"Under review, and the dataset will be made public upon reception of\n  our paper"},{"id":"http://arxiv.org/abs/2502.06220v1","updated":"2025-02-10T07:52:47Z","published":"2025-02-10T07:52:47Z","title":"FunduSAM: A Specialized Deep Learning Model for Enhanced Optic Disc and\n  Cup Segmentation in Fundus Images","summary":"  The Segment Anything Model (SAM) has gained popularity as a versatile image\nsegmentation method, thanks to its strong generalization capabilities across\nvarious domains. However, when applied to optic disc (OD) and optic cup (OC)\nsegmentation tasks, SAM encounters challenges due to the complex structures,\nlow contrast, and blurred boundaries typical of fundus images, leading to\nsuboptimal performance. To overcome these challenges, we introduce a novel\nmodel, FunduSAM, which incorporates several Adapters into SAM to create a deep\nnetwork specifically designed for OD and OC segmentation. The FunduSAM utilizes\nAdapter into each transformer block after encoder for parameter fine-tuning\n(PEFT). It enhances SAM's feature extraction capabilities by designing a\nConvolutional Block Attention Module (CBAM), addressing issues related to\nblurred boundaries and low contrast. Given the unique requirements of OD and OC\nsegmentation, polar transformation is used to convert the original fundus OD\nimages into a format better suited for training and evaluating FunduSAM. A\njoint loss is used to achieve structure preservation between the OD and OC,\nwhile accurate segmentation. Extensive experiments on the REFUGE dataset,\ncomprising 1,200 fundus images, demonstrate the superior performance of\nFunduSAM compared to five mainstream approaches.\n","authors":["Jinchen Yu","Yongwei Nie","Fei Qi","Wenxiong Liao","Hongmin Cai"],"pdf_url":"https://arxiv.org/pdf/2502.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15797v4","updated":"2025-02-10T05:31:43Z","published":"2025-01-27T05:46:06Z","title":"LemmaHead: RAG Assisted Proof Generation Using Large Language Models","summary":"  Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.\n","authors":["Tianbo Yang","Mingqi Yan","Hongyi Zhao","Tianshuo Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06148v1","updated":"2025-02-10T04:29:36Z","published":"2025-02-10T04:29:36Z","title":"Optimizing Knowledge Integration in Retrieval-Augmented Generation with\n  Self-Selection","summary":"  Retrieval-Augmented Generation (RAG), which integrates external knowledge\ninto Large Language Models (LLMs), has proven effective in enabling LLMs to\nproduce more accurate and reliable responses. However, it remains a significant\nchallenge how to effectively integrate external retrieved knowledge with\ninternal parametric knowledge in LLMs. In this work, we propose a novel\nSelf-Selection RAG framework, where the LLM is made to select from pairwise\nresponses generated with internal parametric knowledge solely and with external\nretrieved knowledge together to achieve enhanced accuracy. To this end, we\ndevise a Self-Selection-RGP method to enhance the capabilities of the LLM in\nboth generating and selecting the correct answer, by training the LLM with\nDirect Preference Optimization (DPO) over a curated Retrieval Generation\nPreference (RGP) dataset. Experimental results with two open-source LLMs (i.e.,\nLlama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our\napproach over other baseline methods on Natural Questions (NQ) and TrivialQA\ndatasets.\n","authors":["Yan Weng","Fengbin Zhu","Tong Ye","Haoyan Liu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.06148v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.12949v2","updated":"2025-02-10T00:45:01Z","published":"2024-11-20T00:43:32Z","title":"Epidemiology-informed Network for Robust Rumor Detection","summary":"  The rapid spread of rumors on social media has posed significant challenges\nto maintaining public trust and information integrity. Since an information\ncascade process is essentially a propagation tree, recent rumor detection\nmodels leverage graph neural networks to additionally capture information\npropagation patterns, thus outperforming text-only solutions. Given the\nvariations in topics and social impact of the root node, different source\ninformation naturally has distinct outreach capabilities, resulting in\ndifferent heights of propagation trees. This variation, however, impedes the\ndata-driven design of existing graph-based rumor detectors. Given a shallow\npropagation tree with limited interactions, it is unlikely for graph-based\napproaches to capture sufficient cascading patterns, questioning their ability\nto handle less popular news or early detection needs. In contrast, a deep\npropagation tree is prone to noisy user responses, and this can in turn\nobfuscate the predictions. In this paper, we propose a novel\nEpidemiology-informed Network (EIN) that integrates epidemiological knowledge\nto enhance performance by overcoming data-driven methods sensitivity to data\nquality. Meanwhile, to adapt epidemiology theory to rumor detection, it is\nexpected that each users stance toward the source information will be\nannotated. To bypass the costly and time-consuming human labeling process, we\ntake advantage of large language models to generate stance labels, facilitating\noptimization objectives for learning epidemiology-informed representations. Our\nexperimental results demonstrate that the proposed EIN not only outperforms\nstate-of-the-art methods on real-world datasets but also exhibits enhanced\nrobustness across varying tree depths.\n","authors":["Wei Jiang","Tong Chen","Xinyi Gao","Wentao Zhang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10440v1","updated":"2025-02-10T09:15:56Z","published":"2025-02-10T09:15:56Z","title":"Towards Copyright Protection for Knowledge Bases of Retrieval-augmented\n  Language Models via Ownership Verification with Reasoning","summary":"  Large language models (LLMs) are increasingly integrated into real-world\napplications through retrieval-augmented generation (RAG) mechanisms to\nsupplement their responses with up-to-date and domain-specific knowledge.\nHowever, the valuable and often proprietary nature of the knowledge bases used\nin RAG introduces the risk of unauthorized usage by adversaries. Existing\nmethods that can be generalized as watermarking techniques to protect these\nknowledge bases typically involve poisoning attacks. However, these methods\nrequire to alter the results of verification samples (\\eg, generating incorrect\noutputs), inevitably making them susceptible to anomaly detection and even\nintroduce new security risks. To address these challenges, we propose \\name{}\nfor `harmless' copyright protection of knowledge bases. Instead of manipulating\nLLM's final output, \\name{} implants distinct verification behaviors in the\nspace of chain-of-thought (CoT) reasoning, maintaining the correctness of the\nfinal answer. Our method has three main stages: (1) \\textbf{Generating CoTs}:\nFor each verification question, we generate two CoTs, including a target CoT\nfor building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and\nTarget CoTs}: We optimize them to minimize retrieval errors under the black-box\nsetting of suspicious LLM, ensuring that the watermarked verification queries\nactivate the target CoTs without being activated in non-watermarked ones; (3)\n\\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to\nstatistically verify whether a suspicious LLM is augmented with the protected\nknowledge base by comparing its responses to watermarked and benign\nverification queries. Our experiments on diverse benchmarks demonstrate that\n\\name{} effectively protects knowledge bases against unauthorized usage while\npreserving the integrity and performance of the RAG.\n","authors":["Junfeng Guo","Yiming Li","Ruibo Chen","Yihan Wu","Chenxi Liu","Yanshuo Chen","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.10440v1.pdf","comment":"The first two authors contributed equally to this work. 19 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07128v1","updated":"2025-02-10T23:47:35Z","published":"2025-02-10T23:47:35Z","title":"Cardiverse: Harnessing LLMs for Novel Card Game Prototyping","summary":"  The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.\n","authors":["Danrui Li","Sen Zhang","Sam S. Sohn","Kaidong Hu","Muhammad Usman","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2502.07128v1.pdf","comment":"13 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.06710v1","updated":"2025-02-10T17:41:57Z","published":"2025-02-10T17:41:57Z","title":"Learning Musical Representations for Music Performance Question\n  Answering","summary":"  Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.\n","authors":["Xingjian Diao","Chunhui Zhang","Tingxuan Wu","Ming Cheng","Zhongyu Ouyang","Weiyi Wu","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06710v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.06616v1","updated":"2025-02-10T16:12:47Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v1.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2502.06490v1","updated":"2025-02-10T14:08:25Z","published":"2025-02-10T14:08:25Z","title":"Recent Advances in Discrete Speech Tokens: A Review","summary":"  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n","authors":["Yiwei Guo","Zhihan Li","Hankun Wang","Bohan Li","Chongtian Shao","Hanglei Zhang","Chenpeng Du","Xie Chen","Shujie Liu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06490v1.pdf","comment":"26 pages, 8 figures, 3 tables. Work in progress"},{"id":"http://arxiv.org/abs/2408.14823v2","updated":"2025-02-10T11:59:52Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS with 23% of the original model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","G√©raldine Morin","Simone Gasparini","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v2.pdf","comment":"3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:\n  https://github.com/nus-vv-streams/lapis-gs"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2406.02345v2","updated":"2025-02-10T06:05:46Z","published":"2024-06-04T14:21:41Z","title":"Progressive Confident Masking Attention Network for Audio-Visual\n  Segmentation","summary":"  Audio and visual signals typically occur simultaneously, and humans possess\nan innate ability to correlate and synchronize information from these two\nmodalities. Recently, a challenging problem known as Audio-Visual Segmentation\n(AVS) has emerged, intending to produce segmentation maps for sounding objects\nwithin a scene. However, the methods proposed so far have not sufficiently\nintegrated audio and visual information, and the computational costs have been\nextremely high. Additionally, the outputs of different stages have not been\nfully utilized. To facilitate this research, we introduce a novel Progressive\nConfident Masking Attention Network (PMCANet). It leverages attention\nmechanisms to uncover the intrinsic correlations between audio signals and\nvisual frames. Furthermore, we design an efficient and effective\ncross-attention module to enhance semantic perception by selecting query\ntokens. This selection is determined through confidence-driven units based on\nthe network's multi-stage predictive outputs. Experiments demonstrate that our\nnetwork outperforms other AVS methods while requiring less computational\nresources. The code is available at: https://github.com/PrettyPlate/PCMANet.\n","authors":["Yuxuan Wang","Jinchao Zhu","Feng Dong","Shuyue Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.02345v2.pdf","comment":"23 pages, 11 figures, submitted to Elsevier Knowledge-Based System"}]},"2025-02-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.12150v1","updated":"2025-02-17T18:59:02Z","published":"2025-02-17T18:59:02Z","title":"Idiosyncrasies in Large Language Models","summary":"  In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning existing text embedding models on LLM-generated texts\nyields excellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, particularly for training on synthetic\ndata and inferring model similarity. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.\n","authors":["Mingjie Sun","Yida Yin","Zhiqiu Xu","J. Zico Kolter","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12150v1.pdf","comment":"Website at\n  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html"},{"id":"http://arxiv.org/abs/2502.12149v1","updated":"2025-02-17T18:58:36Z","published":"2025-02-17T18:58:36Z","title":"HARBOR: Exploring Persona Dynamics in Multi-Agent Competition","summary":"  We investigate factors contributing to LLM agents' success in competitive\nmulti-agent environments, using auctions as a testbed where agents bid to\nmaximize profit. The agents are equipped with bidding domain knowledge,\ndistinct personas that reflect item preferences, and a memory of auction\nhistory. Our work extends the classic auction scenario by creating a realistic\nenvironment where multiple agents bid on houses, weighing aspects such as size,\nlocation, and budget to secure the most desirable homes at the lowest prices.\nParticularly, we investigate three key questions: (a) How does a persona\ninfluence an agent's behavior in a competitive setting? (b) Can an agent\neffectively profile its competitors' behavior during auctions? (c) How can\npersona profiling be leveraged to create an advantage using strategies such as\ntheory of mind? Through a series of experiments, we analyze the behaviors of\nLLM agents and shed light on new findings. Our testbed, called HARBOR, offers a\nvaluable platform for deepening our understanding of multi-agent workflows in\ncompetitive environments.\n","authors":["Kenan Jiang","Li Xiong","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09589v2","updated":"2025-02-17T18:56:30Z","published":"2025-02-13T18:46:44Z","title":"Logical forms complement probability in understanding language model\n  (and human) performance","summary":"  With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as important factors. In addition, we show\nsimilarities and discrepancies between the logical reasoning performances of\nhumans and LLMs by collecting and comparing behavioral data from both.\n","authors":["Yixuan Wang","Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09589v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.07381v4","updated":"2025-02-17T18:54:59Z","published":"2024-11-11T21:32:06Z","title":"MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement --\n  LLaMA3.1 and GPT-4o for Complete Abstract Adaptation","summary":"  This report is the system description of the MaLei team (Manchester and\nLeiden) for the shared task Plain Language Adaptation of Biomedical Abstracts\n(PLABA) 2024 (we had an earlier name BeeManc following last year), affiliated\nwith TREC2024 (33rd Text REtrieval Conference\nhttps://ir.nist.gov/evalbase/conf/trec-2024). This report contains two sections\ncorresponding to the two sub-tasks in PLABA-2024. In task one (term\nreplacement), we applied fine-tuned ReBERTa-Base models to identify and\nclassify the difficult terms, jargon, and acronyms in the biomedical abstracts\nand reported the F1 score (Task 1A and 1B). In task two (complete abstract\nadaptation), we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot\nprompts to complete the abstract adaptation and reported the scores in BLEU,\nSARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024\non Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd\nand 2nd respectively on the two sub-tasks, and the 1st on averaged F1 scores\nacross the two tasks from 9 evaluated systems. Our LLaMA-3.1-70B-instructed\nmodel achieved the highest Completeness score for Task 2. We share our source\ncodes, fine-tuned models, and related resources at\nhttps://github.com/HECTA-UoM/PLABA2024\n","authors":["Zhidong Ling","Zihao Li","Pablo Romero","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2411.07381v4.pdf","comment":"ongoing work - system report for PLABA2024 with TREC-2024"},{"id":"http://arxiv.org/abs/2502.12137v1","updated":"2025-02-17T18:53:42Z","published":"2025-02-17T18:53:42Z","title":"REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to\n  Enhance Wikipedia Tail Biographies through Personal Narratives","summary":"  Wikipedia is an invaluable resource for factual information about a wide\nrange of entities. However, the quality of articles on less-known entities\noften lags behind that of the well-known ones. This study proposes a novel\napproach to enhancing Wikipedia's B and C category biography articles by\nleveraging personal narratives such as autobiographies and biographies. By\nutilizing a multi-staged retrieval-augmented generation technique -- REVerSum\n-- we aim to enrich the informational content of these lesser-known articles.\nOur study reveals that personal narratives can significantly improve the\nquality of Wikipedia articles, providing a rich source of reliable information\nthat has been underutilized in previous studies. Based on crowd-based\nevaluation, REVerSum generated content outperforms the best performing baseline\nby 17% in terms of integrability to the original Wikipedia article and 28.5\\%\nin terms of informativeness. Code and Data are available at:\nhttps://github.com/sayantan11995/wikipedia_enrichment\n","authors":["Sayantan Adak","Pauras Mangesh Meher","Paramita Das","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12137v1.pdf","comment":"Accepted at COLING2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.12134v1","updated":"2025-02-17T18:52:29Z","published":"2025-02-17T18:52:29Z","title":"SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs","summary":"  Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nsuffer from catastrophic forgetting, limiting their applicability to\nstate-of-the-art LLMs that already perform well in zero-shot settings with a\nproper instruction. To address this challenge, we propose a novel approach for\ncontinuous-space reasoning that does not require modifying the underlying LLM.\nSpecifically, we employ a lightweight assistant model to generate\ninstance-specific soft thought tokens speculatively as the initial chain of\nthoughts, which are then mapped into the LLM's representation space via a\nprojection module. Experimental results on five reasoning benchmarks\ndemonstrate that our method enhances LLM reasoning performance through\nsupervised, parameter-efficient fine-tuning.\n","authors":["Yige Xu","Xu Guo","Zhiwei Zeng","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2502.12134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13018v2","updated":"2025-02-17T18:51:33Z","published":"2024-12-17T15:38:42Z","title":"OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain","summary":"  As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.\n","authors":["Shuting Wang","Jiejun Tan","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.13018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09606v2","updated":"2025-02-17T18:48:26Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20582v2","updated":"2025-02-17T18:48:09Z","published":"2024-05-31T02:28:41Z","title":"The Point of View of a Sentiment: Towards Clinician Bias Detection in\n  Psychiatric Notes","summary":"  Negative patient descriptions and stigmatizing language can contribute to\ngenerating healthcare disparities in two ways: (1) read by patients, they can\nharm their trust and engagement with the medical center; (2) read by\nphysicians, they may negatively influence their perspective of a future\npatient. In psychiatry, the patient-clinician therapeutic alliance is a major\ndeterminant of clinical outcomes. Therefore, language usage in psychiatric\nclinical notes may not only create healthcare disparities, but also perpetuate\nthem. Recent advances in NLP systems have facilitated the efforts to detect\ndiscriminatory language in healthcare. However, such attempts have only focused\non the perspectives of the medical center and its physicians. Considering both\nphysicians and non-physicians' point of view is a more translatable approach to\nidentifying potentially harmful language in clinical notes. By leveraging\npre-trained and large language models (PLMs and LLMs), this work aims to\ncharacterize potentially harmful language usage in psychiatric notes by\nidentifying the sentiment expressed in sentences describing patients based on\nthe reader's point of view. Extracting 39 sentences from the Mount Sinai Health\nSystem containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa,\nGatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and\nfew-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to\nclassify the sentiment of the sentences according to the physician or\nnon-physician point of view. Results showed that GPT-3.5 aligned best to\nphysician point of view and Mistral aligned best to non-physician point of\nview. These results underline the importance of recognizing the reader's point\nof view, not only for improving the note writing process, but also for the\nquantification, identification, and reduction of bias in computational systems\nfor downstream analyses.\n","authors":["Alissa A. Valentine","Lauren A. Lepow","Lili Chan","Alexander W. Charney","Isotta Landi"],"pdf_url":"https://arxiv.org/pdf/2405.20582v2.pdf","comment":"Oral presentation at NAACL 2024 Queer in AI Workshop"},{"id":"http://arxiv.org/abs/2502.12124v1","updated":"2025-02-17T18:46:46Z","published":"2025-02-17T18:46:46Z","title":"RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for\n  Inspirational Quote Extraction from Long Documents","summary":"  Inspirational quotes from famous individuals are often used to convey\nthoughts in news articles, essays, and everyday conversations. In this paper,\nwe propose a novel context-based quote extraction system that aims to extract\nthe most relevant quote from a long text. We formulate this quote extraction as\nan open domain question answering problem first by employing a vector-store\nbased retriever and then applying a multi-task reader. We curate three\ncontext-based quote extraction datasets and introduce a novel multi-task\nframework RA-MTR that improves the state-of-the-art performance, achieving a\nmaximum improvement of 5.08% in BoW F1-score.\n","authors":["Sayantan Adak","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12124v1.pdf","comment":"Accepted at COLING2025-MAIN"},{"id":"http://arxiv.org/abs/2502.12123v1","updated":"2025-02-17T18:46:32Z","published":"2025-02-17T18:46:32Z","title":"On the Query Complexity of Verifier-Assisted Language Generation","summary":"  Recently, a plethora of works have proposed inference-time algorithms (e.g.\nbest-of-n), which incorporate verifiers to assist the generation process. Their\nquality-efficiency trade-offs have been empirically benchmarked on a variety of\nconstrained generation tasks, but the algorithmic design landscape is still\nlargely poorly understood. In this paper, we develop a mathematical framework\nfor reasoning about constrained generation using a pre-trained language model\ngenerator oracle and a process verifier--which can decide whether a prefix can\nbe extended to a string which satisfies the constraints of choice. We show that\neven in very simple settings, access to a verifier can render an intractable\nproblem (information-theoretically or computationally) to a tractable one. In\nfact, we show even simple algorithms, like tokenwise rejection sampling, can\nenjoy significant benefits from access to a verifier. Empirically, we show that\na natural modification of tokenwise rejection sampling, in which the sampler is\nallowed to \"backtrack\" (i.e., erase the final few generated tokens) has robust\nand substantive benefits over natural baselines (e.g. (blockwise) rejection\nsampling, nucleus sampling)--both in terms of computational efficiency,\naccuracy and diversity.\n","authors":["Edoardo Botta","Yuchen Li","Aashay Mehta","Jordan T. Ash","Cyril Zhang","Andrej Risteski"],"pdf_url":"https://arxiv.org/pdf/2502.12123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12120v1","updated":"2025-02-17T18:45:25Z","published":"2025-02-17T18:45:25Z","title":"LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws","summary":"  Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.\n","authors":["Prasanna Mayilvahanan","Thadd√§us Wiedemer","Sayak Mallick","Matthias Bethge","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2502.12120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12119v1","updated":"2025-02-17T18:43:41Z","published":"2025-02-17T18:43:41Z","title":"PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection","summary":"  Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.\n","authors":["Jinhe Bi","Yifan Wang","Danqi Yan","Xun Xiao","Artur Hecker","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2502.12119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12118v1","updated":"2025-02-17T18:43:24Z","published":"2025-02-17T18:43:24Z","title":"Scaling Test-Time Compute Without Verification or RL is Suboptimal","summary":"  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n","authors":["Amrith Setlur","Nived Rajaraman","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.12118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v2","updated":"2025-02-17T18:42:31Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v2.pdf","comment":"We will reflect comments from the reviewers and re-submit"},{"id":"http://arxiv.org/abs/2406.11785v3","updated":"2025-02-17T18:37:13Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12110v1","updated":"2025-02-17T18:36:14Z","published":"2025-02-17T18:36:14Z","title":"A-MEM: Agentic Memory for LLM Agents","summary":"  While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code is available at https://github.com/WujiangXu/AgenticMemory.\n","authors":["Wujiang Xu","Zujie Liang","Kai Mei","Hang Gao","Juntao Tan","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12109v1","updated":"2025-02-17T18:31:57Z","published":"2025-02-17T18:31:57Z","title":"Personality Structured Interview for Large Language Model Simulation in\n  Personality Research","summary":"  Although psychometrics researchers have recently explored the use of large\nlanguage models (LLMs) as proxies for human participants, LLMs often fail to\ngenerate heterogeneous data with human-like diversity, which diminishes their\nvalue in advancing social science research. To address these challenges, we\nexplored the potential of the theory-informed Personality Structured Interview\n(PSI) as a tool for simulating human responses in personality research. In this\napproach, the simulation is grounded in nuanced real-human interview\ntranscripts that target the personality construct of interest. We have provided\na growing set of 357 structured interview transcripts from a representative\nsample, each containing an individual's response to 32 open-ended questions\ncarefully designed to gather theory-based personality evidence. Additionally,\ngrounded in psychometric research, we have summarized an evaluation framework\nto systematically validate LLM-generated psychometric data. Results from three\nexperiments demonstrate that well-designed structured interviews could improve\nhuman-like heterogeneity in LLM-simulated personality data and predict\npersonality-related behavioral outcomes (i.e., organizational citizenship\nbehaviors and counterproductive work behavior). We further discuss the role of\ntheory-informed structured interviews in LLM-based simulation and outline a\ngeneral framework for designing structured interviews to simulate human-like\ndata for psychometric research.\n","authors":["Pengda Wang","Huiqi Zou","Hanjie Chen","Tianjun Sun","Ziang Xiao","Frederick L. Oswald"],"pdf_url":"https://arxiv.org/pdf/2502.12109v1.pdf","comment":"41 Pages, 30 Tables, 5 Figures"},{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2412.18367v5","updated":"2025-02-17T18:13:38Z","published":"2024-12-24T11:50:18Z","title":"Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset (GIST)","summary":"  The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduce GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms are translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality is benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST is integrated into translation workflows using\npost-translation refinement methods that require no retraining, where LLM\nprompting consistently improves BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.\n","authors":["Jiarui Liu","Iman Ouzzani","Wenkai Li","Lechen Zhang","Tianyue Ou","Houda Bouamor","Zhijing Jin","Mona Diab"],"pdf_url":"https://arxiv.org/pdf/2412.18367v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12094v1","updated":"2025-02-17T18:12:36Z","published":"2025-02-17T18:12:36Z","title":"A Study on Leveraging Search and Self-Feedback for Agent Reasoning","summary":"  Recent works have demonstrated that incorporating search during inference can\nsignificantly improve reasoning capabilities of language agents. Some\napproaches may make use of the ground truth or rely on model's own generated\nfeedback. The search algorithm uses this feedback to then produce values that\nwill update its criterion for exploring and exploiting various reasoning paths.\nIn this study, we investigate how search and model's self-feedback can be\nleveraged for reasoning tasks. First, we explore differences in ground-truth\nfeedback and self-feedback during search for math reasoning. Second, we observe\nlimitations in applying search techniques to more complex tasks like\ntool-calling and design domain-specific approaches to address these gaps. Our\nexperiments reveal challenges related to generalization when solely relying on\nself-feedback during search. For search to work effectively, either access to\nthe ground-truth is needed or feedback mechanisms need to be carefully designed\nfor the specific task.\n","authors":["Karthikeyan K","Michelle Yuan","Elman Mansimov","Katerina Margatina","Anurag Pratik","Daniele Bonadiman","Monica Sunkara","Yi Zhang","Yassine Benajiba"],"pdf_url":"https://arxiv.org/pdf/2502.12094v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.03035v2","updated":"2025-02-17T18:11:20Z","published":"2025-01-06T14:23:02Z","title":"Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning","summary":"  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n","authors":["Zhen Li","Yupeng Su","Runming Yang","Congkai Xie","Zheng Wang","Zhongwei Xie","Ngai Wong","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2501.03035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18057v3","updated":"2025-02-17T18:08:17Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for removing private or hazardous\ninformation from deep learning models. While MU has advanced significantly in\nunimodal (text or vision) settings, multimodal unlearning (MMU) remains\nunderexplored due to the lack of open benchmarks for evaluating cross-modal\ndata removal. To address this gap, we introduce CLEAR, the first open-source\nbenchmark designed specifically for MMU. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We conduct a comprehensive\nanalysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four\nevaluation sets, demonstrating that jointly unlearning both modalities\noutperforms single-modality approaches. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16491v2","updated":"2025-02-17T18:05:21Z","published":"2024-10-21T20:32:27Z","title":"BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded\n  Data","summary":"  In this work, we tackle the challenge of embedding realistic human\npersonality traits into LLMs. Previous approaches have primarily focused on\nprompt-based methods that describe the behavior associated with the desired\npersonality traits, suffering from realism and validity issues. To address\nthese limitations, we introduce BIG5-CHAT, a large-scale dataset containing\n100,000 dialogues designed to ground models in how humans express their\npersonality in language. Leveraging this dataset, we explore Supervised\nFine-Tuning and Direct Preference Optimization as training-based methods to\nalign LLMs more naturally with human personality patterns. Our methods\noutperform prompting on personality assessments such as BFI and IPIP-NEO, with\ntrait correlations more closely matching human data. Furthermore, our\nexperiments reveal that models trained to exhibit higher conscientiousness,\nhigher agreeableness, lower extraversion, and lower neuroticism display better\nperformance on reasoning tasks, aligning with psychological findings on how\nthese traits impact human cognitive performance. To our knowledge, this work is\nthe first comprehensive study to demonstrate how training-based methods can\nshape LLM personalities through learning from real human behaviors.\n","authors":["Wenkai Li","Jiarui Liu","Andy Liu","Xuhui Zhou","Mona Diab","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2410.16491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12085v1","updated":"2025-02-17T17:59:56Z","published":"2025-02-17T17:59:56Z","title":"APB: Accelerating Distributed Long-Context Inference by Passing\n  Compressed Context Blocks across GPUs","summary":"  While long-context inference is crucial for advancing large language model\n(LLM) applications, its prefill speed remains a significant bottleneck. Current\napproaches, including sequence parallelism strategies and compute reduction\nthrough approximate attention mechanisms, still fall short of delivering\noptimal inference efficiency. This hinders scaling the inputs to longer\nsequences and processing long-context queries in a timely manner. To address\nthis, we introduce APB, an efficient long-context inference framework that\nleverages multi-host approximate attention to enhance prefill speed by reducing\ncompute and enhancing parallelism simultaneously. APB introduces a\ncommunication mechanism for essential key-value pairs within a sequence\nparallelism framework, enabling a faster inference speed while maintaining task\nperformance. We implement APB by incorporating a tailored FlashAttn kernel\nalongside optimized distribution strategies, supporting diverse models and\nparallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x\ncompared with FlashAttn, RingAttn, and StarAttn, respectively, without any\nobservable task performance degradation. We provide the implementation and\nexperiment code of APB in https://github.com/thunlp/APB.\n","authors":["Yuxiang Huang","Mingye Li","Xu Han","Chaojun Xiao","Weilin Zhao","Sun Ao","Hao Zhou","Jie Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.12085v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12084v1","updated":"2025-02-17T17:57:50Z","published":"2025-02-17T17:57:50Z","title":"VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\n  Matching Visual Cues","summary":"  Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across eight open-source VLMs and GPT-4o, along with\nfurther analysis of various language-side and vision-side prompting methods,\nleads to a total of eight key findings. We identify critical challenges in\nmodels' ability to link visual cues, highlighting a significant performance gap\nwhere even GPT-4o lags 34.80% behind humans. Based on these insights, we\nadvocate for (i) enhancing core visual capabilities to improve adaptability and\nreduce reliance on prior knowledge, (ii) establishing clearer principles for\nintegrating language-based reasoning in vision-centric tasks to prevent\nunnecessary biases, and (iii) shifting vision-text training paradigms toward\nfostering models' ability to independently structure and infer relationships\namong visual cues.\n","authors":["Jianshu Zhang","Dongyu Yao","Renjie Pi","Paul Pu Liang","Yi R."," Fung"],"pdf_url":"https://arxiv.org/pdf/2502.12084v1.pdf","comment":"Project Page: https://vlm2-bench.github.io/"},{"id":"http://arxiv.org/abs/2502.12082v1","updated":"2025-02-17T17:56:23Z","published":"2025-02-17T17:56:23Z","title":"AdaSplash: Adaptive Sparse Flash Attention","summary":"  The computational cost of softmax-based attention in transformers limits\ntheir applicability to long-context tasks. Adaptive sparsity, of which\n$\\alpha$-entmax attention is an example, offers a flexible data-dependent\nalternative, but existing implementations are inefficient and do not leverage\nthe sparsity to obtain runtime and memory gains. In this work, we propose\nAdaSplash, which combines the efficiency of GPU-optimized algorithms with the\nsparsity benefits of $\\alpha$-entmax. We first introduce a hybrid\nHalley-bisection algorithm, resulting in a 7-fold reduction in the number of\niterations needed to compute the $\\alpha$-entmax transformation. Then, we\nimplement custom Triton kernels to efficiently handle adaptive sparsity.\nExperiments with RoBERTa and ModernBERT for text classification and\nsingle-vector retrieval, along with GPT-2 for language modeling, show that our\nmethod achieves substantial improvements in runtime and memory efficiency\ncompared to existing $\\alpha$-entmax implementations. It approaches -- and in\nsome cases surpasses -- the efficiency of highly optimized softmax\nimplementations like FlashAttention-2, enabling long-context training while\nmaintaining strong task performance.\n","authors":["Nuno Gon√ßalves","Marcos Treviso","Andr√© F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2502.12082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12081v1","updated":"2025-02-17T17:55:55Z","published":"2025-02-17T17:55:55Z","title":"Unhackable Temporal Rewarding for Scalable Video MLLMs","summary":"  In the pursuit of superior video-processing MLLMs, we have encountered a\nperplexing paradox: the \"anti-scaling law\", where more data and larger models\nlead to worse performance. This study unmasks the culprit: \"temporal hacking\",\na phenomenon where models shortcut by fixating on select frames, missing the\nfull video narrative. In this work, we systematically establish a comprehensive\ntheory of temporal hacking, defining it from a reinforcement learning\nperspective, introducing the Temporal Perplexity (TPL) score to assess this\nmisalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework\nto mitigate the temporal hacking. Both theoretically and empirically, TPL\nproves to be a reliable indicator of temporal modeling quality, correlating\nstrongly with frame activation patterns. Extensive experiments reveal that UTR\nnot only counters temporal hacking but significantly elevates video\ncomprehension capabilities. This work not only advances video-AI systems but\nalso illuminates the critical importance of aligning proxy rewards with true\nobjectives in MLLM development.\n","authors":["En Yu","Kangheng Lin","Liang Zhao","Yana Wei","Zining Zhu","Haoran Wei","Jianjian Sun","Zheng Ge","Xiangyu Zhang","Jingyu Wang","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2502.12081v1.pdf","comment":"Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/"},{"id":"http://arxiv.org/abs/2501.11613v6","updated":"2025-02-17T17:55:47Z","published":"2025-01-20T17:19:02Z","title":"Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems","summary":"  This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.\n","authors":["Giorgio Robino"],"pdf_url":"https://arxiv.org/pdf/2501.11613v6.pdf","comment":"Added Experimental Results sections"},{"id":"http://arxiv.org/abs/2502.12073v1","updated":"2025-02-17T17:43:08Z","published":"2025-02-17T17:43:08Z","title":"Can LLMs Simulate Social Media Engagement? A Study on Action-Guided\n  Response Generation","summary":"  Social media enables dynamic user engagement with trending topics, and recent\nresearch has explored the potential of large language models (LLMs) for\nresponse generation. While some studies investigate LLMs as agents for\nsimulating user behavior on social media, their focus remains on practical\nviability and scalability rather than a deeper understanding of how well LLM\naligns with human behavior. This paper analyzes LLMs' ability to simulate\nsocial media engagement through action guided response generation, where a\nmodel first predicts a user's most likely engagement action-retweet, quote, or\nrewrite-towards a trending post before generating a personalized response\nconditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and\nDeepSeek-R1 in social media engagement simulation regarding a major societal\nevent discussed on X. Our findings reveal that zero-shot LLMs underperform BERT\nin action prediction, while few-shot prompting initially degrades the\nprediction accuracy of LLMs with limited examples. However, in response\ngeneration, few-shot LLMs achieve stronger semantic alignment with ground truth\nposts.\n","authors":["Zhongyi Qiu","Hanjia Lyu","Wei Xiong","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12067v1","updated":"2025-02-17T17:37:26Z","published":"2025-02-17T17:37:26Z","title":"TokenSkip: Controllable Chain-of-Thought Compression in LLMs","summary":"  Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.\n","authors":["Heming Xia","Yongqi Li","Chak Tou Leong","Wenjie Wang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.12067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12065v1","updated":"2025-02-17T17:34:48Z","published":"2025-02-17T17:34:48Z","title":"Formalizing Complex Mathematical Statements with LLMs: A Study on\n  Mathematical Definitions","summary":"  Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.\n","authors":["Lan Zhang","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2502.12065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11427v2","updated":"2025-02-17T17:34:45Z","published":"2024-06-17T11:25:57Z","title":"DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors","summary":"  Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.\n","authors":["Keon Lee","Dong Won Kim","Jaehyeon Kim","Seungjun Chung","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2406.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12064v1","updated":"2025-02-17T17:32:55Z","published":"2025-02-17T17:32:55Z","title":"AI-generated Text Detection with a GLTR-based Approach","summary":"  The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.\n","authors":["Luc√≠a Yan Wu","Isabel Segura-Bedmar"],"pdf_url":"https://arxiv.org/pdf/2502.12064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12057v1","updated":"2025-02-17T17:25:11Z","published":"2025-02-17T17:25:11Z","title":"Culture is Not Trivia: Sociocultural Theory for Cultural NLP","summary":"  The field of cultural NLP has recently experienced rapid growth, driven by a\npressing need to ensure that language technologies are effective and safe\nacross a pluralistic user base. This work has largely progressed without a\nshared conception of culture, instead choosing to rely on a wide array of\ncultural proxies. However, this leads to a number of recurring limitations:\ncoarse national boundaries fail to capture nuanced differences that lay within\nthem, limited coverage restricts datasets to only a subset of usually\nhighly-represented cultures, and a lack of dynamicity results in static\ncultural benchmarks that do not change as culture evolves. In this position\npaper, we argue that these methodological limitations are symptomatic of a\ntheoretical gap. We draw on a well-developed theory of culture from\nsociocultural linguistics to fill this gap by 1) demonstrating in a case study\nhow it can clarify methodological constraints and affordances, 2) offering\ntheoretically-motivated paths forward to achieving cultural competence, and 3)\narguing that localization is a more useful framing for the goals of much\ncurrent work in cultural NLP.\n","authors":["Naitian Zhou","David Bamman","Isaac L. Bleaman"],"pdf_url":"https://arxiv.org/pdf/2502.12057v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2405.01474v3","updated":"2025-02-17T17:24:42Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v3.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.12055v1","updated":"2025-02-17T17:24:37Z","published":"2025-02-17T17:24:37Z","title":"Designing Role Vectors to Improve LLM Inference Behaviour","summary":"  The influence of personas on Large Language Models (LLMs) has been widely\nstudied, yet their direct impact on performance remains uncertain. This work\nexplores a novel approach to guiding LLM behaviour through role vectors, an\nalternative to persona-based prompting. We construct 29 role vectors derived\nfrom model activations and evaluate their impact on benchmark performance\nacross multiple domains. Our analysis investigates whether these vectors can\neffectively steer models toward domain-specific expertise. We measure two key\ninterventions: (i) activation addition, which reinforces role-specific\ndirections, and (ii) directional ablation, which removes them. Results on\nwell-established benchmarks indicate that role vectors do, in fact, influence\nmodel behaviour, improving task performance in relevant domains while\nmarginally affecting unrelated tasks. This, in turn, suggests that manipulating\ninternal model representations has a greater impact on outcomes than\npersona-based prompting.\n","authors":["Daniele Potert√¨","Andrea Seveso","Fabio Mercorio"],"pdf_url":"https://arxiv.org/pdf/2502.12055v1.pdf","comment":"Submitted to ARR 2025 February cycle"},{"id":"http://arxiv.org/abs/2502.12052v1","updated":"2025-02-17T17:22:49Z","published":"2025-02-17T17:22:49Z","title":"A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability","summary":"  In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives.\n","authors":["Xinyu Hu","Mingqi Gao","Li Lin","Zhenghan Yu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2502.12052v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.12051v1","updated":"2025-02-17T17:20:41Z","published":"2025-02-17T17:20:41Z","title":"How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines","summary":"  Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.\n","authors":["Ayan Sengupta","Yash Goel","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2502.12051v1.pdf","comment":"20 pages, 8 tables, 4 figures"},{"id":"http://arxiv.org/abs/2502.12050v1","updated":"2025-02-17T17:18:39Z","published":"2025-02-17T17:18:39Z","title":"SpeechT: Findings of the First Mentorship in Speech Translation","summary":"  This work presents the details and findings of the first mentorship in speech\ntranslation (SpeechT), which took place in December 2024 and January 2025. To\nfulfil the requirements of the mentorship, the participants engaged in key\nactivities, including data preparation, modelling, and advanced research.\n","authors":["Yasmin Moslem","Juan Juli√°n Cea Mor√°n","Mariano Gonzalez-Gomez","Muhammad Hazim Al Farouq","Farah Abdou","Satarupa Deb"],"pdf_url":"https://arxiv.org/pdf/2502.12050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05171v2","updated":"2025-02-17T17:14:04Z","published":"2025-02-07T18:55:02Z","title":"Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach","summary":"  We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.\n","authors":["Jonas Geiping","Sean McLeish","Neel Jain","John Kirchenbauer","Siddharth Singh","Brian R. Bartoldson","Bhavya Kailkhura","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.05171v2.pdf","comment":"The model is available at\n  https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can\n  be found at https://github.com/seal-rg/recurrent-pretraining"},{"id":"http://arxiv.org/abs/2502.12025v1","updated":"2025-02-17T16:57:56Z","published":"2025-02-17T16:57:56Z","title":"SafeChain: Safety of Language Models with Long Chain-of-Thought\n  Reasoning Capabilities","summary":"  Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage\nlong chain-of-thought (CoT) reasoning to generate structured intermediate\nsteps, enhancing their reasoning capabilities. However, long CoT does not\ninherently guarantee safe outputs, potentially leading to harmful consequences\nsuch as the introduction of security vulnerabilities in code or the spread of\nmisinformation. Current research on large language model (LLM) safety usually\nfocuses on short-answer responses, overlooking the long CoT style outputs of\nLRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,\nwe investigate safety evaluators calibrated against human annotations. Using\nour newly developed metrics, we thoroughly assess the safety of 12\nstate-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results\nshow that LRMs are not safe compared to their reasoning advance. Further, we\nperform a fine-grained analysis of the reasoning trace and final answer. We\nfind that three decoding strategies-ZeroThink, LessThink, and MoreThink-can\nimprove model safety without additional training. However, these strategies\neither use constrained reasoning traces or incur high inference costs. To\nbetter strengthen LRM safety, we introduce SafeChain, the first-of-its-kind\nsafety training dataset in CoT style. We fine-tune two LRMs with SafeChain,\nshowing that it not only enhances model safety but also preserves performance\nacross 6 reasoning benchmarks.\n","authors":["Fengqing Jiang","Zhangchen Xu","Yuetai Li","Luyao Niu","Zhen Xiang","Bo Li","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2502.12025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12022v1","updated":"2025-02-17T16:56:23Z","published":"2025-02-17T16:56:23Z","title":"Teaching LLMs According to Their Aptitude: Adaptive Reasoning for\n  Mathematical Problem Solving","summary":"  Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.\n","authors":["Xin Xu","Yan Xu","Tianhao Chen","Yuchen Yan","Chengwu Liu","Zaoyu Chen","Yufei Wang","Yichun Yin","Yasheng Wang","Lifeng Shang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12022v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.12018v1","updated":"2025-02-17T16:52:42Z","published":"2025-02-17T16:52:42Z","title":"Atom of Thoughts for Markov LLM Test-Time Scaling","summary":"  Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.\n","authors":["Fengwei Teng","Zhaoyang Yu","Quan Shi","Jiayi Zhang","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00997v3","updated":"2025-02-17T16:51:23Z","published":"2025-02-03T02:34:46Z","title":"MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs","summary":"  The recent success of specialized Large Language Models (LLMs) in domains\nsuch as mathematical reasoning and coding has led to growing interest in\nmethods for merging these expert LLMs into a unified Mixture-of-Experts (MoE)\nmodel, with the goal of enhancing performance in each domain while retaining\neffectiveness on general tasks. However, the effective merging of expert models\nremains an open challenge, especially for models with highly divergent weight\nparameters or different architectures. State-of-the-art MoE merging methods\nonly work with homogeneous model architectures and rely on simple unweighted\naveraging to merge expert layers, which does not address parameter interference\nand requires extensive fine-tuning of the merged MoE to restore performance. To\naddress these limitations, this paper introduces new MoE merging techniques,\nincluding strategies to mitigate parameter interference, routing heuristics to\nreduce the need for MoE fine-tuning, and a novel method for merging experts\nwith different architectures. Extensive experiments across multiple domains\ndemonstrate the effectiveness of our proposed methods, reducing fine-tuning\ncosts, improving performance over state-of-the-art methods, and expanding the\napplicability of MoE merging.\n","authors":["Yuhang Zhou","Giannis Karamanolakis","Victor Soto","Anna Rumshisky","Mayank Kulkarni","Furong Huang","Wei Ai","Jianhua Lu"],"pdf_url":"https://arxiv.org/pdf/2502.00997v3.pdf","comment":"Accepted by NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.12007v1","updated":"2025-02-17T16:43:47Z","published":"2025-02-17T16:43:47Z","title":"Demographic Attributes Prediction from Speech Using WavLM Embeddings","summary":"  This paper introduces a general classifier based on WavLM features, to infer\ndemographic characteristics, such as age, gender, native language, education,\nand country, from speech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital forensics,\nenabling more personalized and inclusive technologies. Leveraging pretrained\nmodels for embedding extraction, the proposed framework identifies key acoustic\nand linguistic fea-tures associated with demographic attributes, achieving a\nMean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy\nfor gender classification across various datasets. Our system improves upon\nexisting models by up to relative 30% in MAE and up to relative 10% in accuracy\nand F1 scores across tasks, leveraging a diverse range of datasets and large\npretrained models to ensure robustness and generalizability. This study offers\nnew insights into speaker diversity and provides a strong foundation for future\nresearch in speech-based demographic profiling.\n","authors":["Yuchen Yang","Thomas Thebaud","Najim Dehak"],"pdf_url":"https://arxiv.org/pdf/2502.12007v1.pdf","comment":"6 pages, accepted by The Conference on Information Sciences and\n  Systems (CISS)"},{"id":"http://arxiv.org/abs/2502.12001v1","updated":"2025-02-17T16:39:28Z","published":"2025-02-17T16:39:28Z","title":"Merging Language and Domain Specific Models: The Impact on Technical\n  Vocabulary Acquisition","summary":"  This paper investigates the integration of technical vocabulary in merged\nlanguage models. We explore the knowledge transfer mechanisms involved when\ncombining a general-purpose language-specific model with a domain-specific\nmodel, focusing on the resulting model's comprehension of technical jargon. Our\nexperiments analyze the impact of this merging process on the target model's\nproficiency in handling specialized terminology. We present a quantitative\nevaluation of the performance of the merged model, comparing it with that of\nthe individual constituent models. The findings offer insights into the\neffectiveness of different model merging methods for enhancing domain-specific\nknowledge and highlight potential challenges and future directions in\nleveraging these methods for cross-lingual knowledge transfer in Natural\nLanguage Processing.\n","authors":["Thibault Rousset","Taisei Kakibuchi","Yusuke Sasaki","Yoshihide Nomura"],"pdf_url":"https://arxiv.org/pdf/2502.12001v1.pdf","comment":"Presented at the 263rd IPSJ-NL Workshop"},{"id":"http://arxiv.org/abs/2502.11995v1","updated":"2025-02-17T16:35:15Z","published":"2025-02-17T16:35:15Z","title":"Presumed Cultural Identity: How Names Shape LLM Responses","summary":"  Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.\n","authors":["Siddhesh Pawar","Arnav Arora","Lucie-Aim√©e Kaffee","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2502.11995v1.pdf","comment":"23 Pages, 13 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2502.09969v2","updated":"2025-02-17T16:26:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-T√ºr"],"pdf_url":"https://arxiv.org/pdf/2502.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14483v2","updated":"2025-02-17T16:21:10Z","published":"2024-11-19T20:16:26Z","title":"Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat","summary":"  Deciding which large language model (LLM) to use is a complex challenge.\nPairwise ranking has emerged as a new method for evaluating human preferences\nfor LLMs. This approach entails humans evaluating pairs of model outputs based\non a predefined criterion. By collecting these comparisons, a ranking can be\nconstructed using methods such as Elo. However, applying these algorithms as\nconstructed in the context of LLM evaluation introduces several challenges. In\nthis paper, we explore the effectiveness of ranking systems for head-to-head\ncomparisons of LLMs. We formally define a set of fundamental principles for\neffective ranking and conduct a series of extensive evaluations on the\nrobustness of several ranking algorithms in the context of LLMs. Our analysis\nuncovers key insights into the factors that affect ranking accuracy and\nefficiency, offering guidelines for selecting the most appropriate methods\nbased on specific evaluation contexts and resource constraints.\n","authors":["Roland Daynauth","Christopher Clarke","Krisztian Flautner","Lingjia Tang","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2411.14483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11973v1","updated":"2025-02-17T16:20:22Z","published":"2025-02-17T16:20:22Z","title":"Generating Text from Uniform Meaning Representation","summary":"  Uniform Meaning Representation (UMR) is a recently developed graph-based\nsemantic representation, which expands on Abstract Meaning Representation (AMR)\nin a number of ways, in particular through the inclusion of document-level\ninformation and multilingual flexibility. In order to effectively adopt and\nleverage UMR for downstream tasks, efforts must be placed toward developing a\nUMR technological ecosystem. Though still limited amounts of UMR annotations\nhave been produced to date, in this work, we investigate the first approaches\nto producing text from multilingual UMR graphs: (1) a pipeline conversion of\nUMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large\nlanguage models with UMR data, and (3) fine-tuning existing AMR-to-text\ngeneration models with UMR data. Our best performing model achieves a\nmultilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared\nto the reference, which is a promising indication of the effectiveness of\nfine-tuning approaches for UMR-to-text generation with even limited amounts of\nUMR data.\n","authors":["Emma Markle","Reihaneh Iranmanesh","Shira Wein"],"pdf_url":"https://arxiv.org/pdf/2502.11973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v2","updated":"2025-02-17T16:11:44Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023","summary":"  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v2.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.11962v1","updated":"2025-02-17T16:10:30Z","published":"2025-02-17T16:10:30Z","title":"Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware\n  Instruction Fine-Tuning","summary":"  Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language\nModels (LLMs), but it may lower their truthfulness. This trade-off arises\nbecause IFT steers LLMs to generate responses with long-tail knowledge that is\nnot well covered during pre-training, leading to more informative but less\ntruthful answers when generalizing to unseen tasks. In this paper, we\nempirically demonstrate this helpfulness-truthfulness trade-off in IFT and\npropose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs\nto recognize their uncertainty and explicitly reflect it at the end of their\nresponses. Experimental results show that UNIT-tuned models maintain their\nhelpfulness while distinguishing between certain and uncertain claims, thereby\nreducing hallucinations.\n","authors":["Tianyi Wu","Jingwei Ni","Bryan Hooi","Jiaheng Zhang","Elliott Ash","See-Kiong Ng","Mrinmaya Sachan","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2502.11962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11948v1","updated":"2025-02-17T16:01:41Z","published":"2025-02-17T16:01:41Z","title":"Can Your Uncertainty Scores Detect Hallucinated Entity?","summary":"  To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research.\n","authors":["Min-Hsuan Yeh","Max Kamachee","Seongheon Park","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.11948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12691v5","updated":"2025-02-17T16:01:40Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v5.pdf","comment":"13 pages, under review"},{"id":"http://arxiv.org/abs/2502.11946v1","updated":"2025-02-17T15:58:56Z","published":"2025-02-17T15:58:56Z","title":"Step-Audio: Unified Understanding and Generation in Intelligent Speech\n  Interaction","summary":"  Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.\n","authors":["Ailin Huang","Boyong Wu","Bruce Wang","Chao Yan","Chen Hu","Chengli Feng","Fei Tian","Feiyu Shen","Jingbei Li","Mingrui Chen","Peng Liu","Ruihang Miao","Wang You","Xi Chen","Xuerui Yang","Yechang Huang","Yuxiang Zhang","Zheng Gong","Zixin Zhang","Brian Li","Changyi Wan","Hanpeng Hu","Ranchen Ming","Song Yuan","Xuelin Zhang","Yu Zhou","Bingxin Li","Buyun Ma","Kang An","Wei Ji","Wen Li","Xuan Wen","Yuankai Ma","Yuanwei Liang","Yun Mou","Bahtiyar Ahmidi","Bin Wang","Bo Li","Changxin Miao","Chen Xu","Chengting Feng","Chenrun Wang","Dapeng Shi","Deshan Sun","Dingyuan Hu","Dula Sai","Enle Liu","Guanzhe Huang","Gulin Yan","Heng Wang","Haonan Jia","Haoyang Zhang","Jiahao Gong","Jianchang Wu","Jiahong Liu","Jianjian Sun","Jiangjie Zhen","Jie Feng","Jie Wu","Jiaoren Wu","Jie Yang","Jinguo Wang","Jingyang Zhang","Junzhe Lin","Kaixiang Li","Lei Xia","Li Zhou","Longlong Gu","Mei Chen","Menglin Wu","Ming Li","Mingxiao Li","Mingyao Liang","Na Wang","Nie Hao","Qiling Wu","Qinyuan Tan","Shaoliang Pang","Shiliang Yang","Shuli Gao","Siqi Liu","Sitong Liu","Tiancheng Cao","Tianyu Wang","Wenjin Deng","Wenqing He","Wen Sun","Xin Han","Xiaomin Deng","Xiaojia Liu","Xu Zhao","Yanan Wei","Yanbo Yu","Yang Cao","Yangguang Li","Yangzhen Ma","Yanming Xu","Yaqiang Shi","Yilei Wang","Yinmin Zhong","Yu Luo","Yuanwei Lu","Yuhe Yin","Yuting Yan","Yuxiang Yang","Zhe Xie","Zheng Ge","Zheng Sun","Zhewei Huang","Zhichao Chang","Zidong Yang","Zili Zhang","Binxing Jiao","Daxin Jiang","Heung-Yeung Shum","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18547v4","updated":"2025-02-17T15:55:08Z","published":"2024-12-24T16:55:45Z","title":"Token-Budget-Aware LLM Reasoning","summary":"  Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.\n","authors":["Tingxu Han","Zhenting Wang","Chunrong Fang","Shiyu Zhao","Shiqing Ma","Zhenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18547v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11932v1","updated":"2025-02-17T15:42:01Z","published":"2025-02-17T15:42:01Z","title":"On Representational Dissociation of Language and Arithmetic in Large\n  Language Models","summary":"  The association between language and (non-linguistic) thinking ability in\nhumans has long been debated, and recently, neuroscientific evidence of brain\nactivity patterns has been considered. Such a scientific context naturally\nraises an interdisciplinary question -- what about such a language-thought\ndissociation in large language models (LLMs)? In this paper, as an initial\nforay, we explore this question by focusing on simple arithmetic skills (e.g.,\n$1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in\nLLMs' representation space. Our experiments with linear classifiers and cluster\nseparability tests demonstrate that simple arithmetic equations and general\nlanguage input are encoded in completely separated regions in LLMs' internal\nrepresentation space across all the layers, which is also supported with more\ncontrolled stimuli (e.g., spelled-out equations). These tentatively suggest\nthat arithmetic reasoning is mapped into a distinct region from general\nlanguage input, which is in line with the neuroscientific observations of human\nbrain activations, while we also point out their somewhat cognitively\nimplausible geometric properties.\n","authors":["Riku Kisako","Tatsuki Kuribayashi","Ryohei Sasano"],"pdf_url":"https://arxiv.org/pdf/2502.11932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11926v1","updated":"2025-02-17T15:39:50Z","published":"2025-02-17T15:39:50Z","title":"BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages","summary":"  People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER-- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility.\n","authors":["Shamsuddeen Hassan Muhammad","Nedjma Ousidhoum","Idris Abdulmumin","Jan Philip Wahle","Terry Ruas","Meriem Beloucif","Christine de Kock","Nirmal Surange","Daniela Teodorescu","Ibrahim Said Ahmad","David Ifeoluwa Adelani","Alham Fikri Aji","Felermino D. M. A. Ali","Ilseyar Alimova","Vladimir Araujo","Nikolay Babakov","Naomi Baes","Ana-Maria Bucur","Andiswa Bukula","Guanqun Cao","Rodrigo Tufino Cardenas","Rendi Chevi","Chiamaka Ijeoma Chukwuneke","Alexandra Ciobotaru","Daryna Dementieva","Murja Sani Gadanya","Robert Geislinger","Bela Gipp","Oumaima Hourrane","Oana Ignat","Falalu Ibrahim Lawan","Rooweither Mabuya","Rahmad Mahendra","Vukosi Marivate","Andrew Piper","Alexander Panchenko","Charles Henrique Porto Ferreira","Vitaly Protasov","Samuel Rutunda","Manish Shrivastava","Aura Cristina Udrea","Lilian Diana Awuor Wanzare","Sophie Wu","Florian Valentin Wunderlich","Hanif Muhammad Zhafran","Tianhui Zhang","Yi Zhou","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2502.11926v1.pdf","comment":"20 pages, under review"},{"id":"http://arxiv.org/abs/2502.11919v1","updated":"2025-02-17T15:32:54Z","published":"2025-02-17T15:32:54Z","title":"From Text to Trust: Empowering AI-assisted Decision Making with Adaptive\n  LLM-powered Analysis","summary":"  AI-assisted decision making becomes increasingly prevalent, yet individuals\noften fail to utilize AI-based decision aids appropriately especially when the\nAI explanations are absent, potentially as they do not %understand reflect on\nAI's decision recommendations critically. Large language models (LLMs), with\ntheir exceptional conversational and analytical capabilities, present great\nopportunities to enhance AI-assisted decision making in the absence of AI\nexplanations by providing natural-language-based analysis of AI's decision\nrecommendation, e.g., how each feature of a decision making task might\ncontribute to the AI recommendation. In this paper, via a randomized\nexperiment, we first show that presenting LLM-powered analysis of each task\nfeature, either sequentially or concurrently, does not significantly improve\npeople's AI-assisted decision performance. To enable decision makers to better\nleverage LLM-powered analysis, we then propose an algorithmic framework to\ncharacterize the effects of LLM-powered analysis on human decisions and\ndynamically decide which analysis to present. Our evaluation with human\nsubjects shows that this approach effectively improves decision makers'\nappropriate reliance on AI in AI-assisted decision making.\n","authors":["Zhuoyan Li","Hangxiao Zhu","Zhuoran Lu","Ziang Xiao","Ming Yin"],"pdf_url":"https://arxiv.org/pdf/2502.11919v1.pdf","comment":"CHI 2025"},{"id":"http://arxiv.org/abs/2502.11916v1","updated":"2025-02-17T15:31:59Z","published":"2025-02-17T15:31:59Z","title":"EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models","summary":"  Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.\n","authors":["Jiamin Su","Yibo Yan","Fangteng Fu","Han Zhang","Jingheng Ye","Xiang Liu","Jiahao Huo","Huiyu Zhou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11916v1.pdf","comment":"JS and YY are co-first authors. XH is the corresponding author"},{"id":"http://arxiv.org/abs/2502.10051v2","updated":"2025-02-17T15:30:22Z","published":"2025-02-14T10:00:20Z","title":"ORI: O Routing Intelligence","summary":"  Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models.\n","authors":["Ahmad Shadid","Rahul Kumar","Mohit Mayank"],"pdf_url":"https://arxiv.org/pdf/2502.10051v2.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.11903v1","updated":"2025-02-17T15:24:49Z","published":"2025-02-17T15:24:49Z","title":"MMRC: A Large-Scale Benchmark for Understanding Multimodal Large\n  Language Model in Real-World Conversation","summary":"  Recent multimodal large language models (MLLMs) have demonstrated significant\npotential in open-ended conversation, generating more accurate and personalized\nresponses. However, their abilities to memorize, recall, and reason in\nsustained interactions within real-world scenarios remain underexplored. This\npaper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for\nevaluating six core open-ended abilities of MLLMs: information extraction,\nmulti-turn reasoning, information update, image management, memory recall, and\nanswer refusal. With data collected from real-world scenarios, MMRC comprises\n5,120 conversations and 28,720 corresponding manually labeled questions, posing\na significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC\nindicate an accuracy drop during open-ended interactions. We identify four\ncommon failure patterns: long-term memory degradation, inadequacies in updating\nfactual knowledge, accumulated assumption of error propagation, and reluctance\nto say no. To mitigate these issues, we propose a simple yet effective\nNOTE-TAKING strategy, which can record key information from the conversation\nand remind the model during its responses, enhancing conversational\ncapabilities. Experiments across six MLLMs demonstrate significant performance\nimprovements.\n","authors":["Haochen Xue","Feilong Tang","Ming Hu","Yexin Liu","Qidong Huang","Yulong Li","Chengzhi Liu","Zhongxing Xu","Chong Zhang","Chun-Mei Feng","Yutong Xie","Imran Razzak","Zongyuan Ge","Jionglong Su","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2502.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11901v1","updated":"2025-02-17T15:24:11Z","published":"2025-02-17T15:24:11Z","title":"Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o\n  Under Data Scarsity","summary":"  Existing LMs struggle with proof-oriented programming due to data scarcity,\nwhich manifest in two key ways: (1) a lack of sufficient corpora for\nproof-oriented programming languages such as F*, and (2) the absence of\nlarge-scale, project-level proof-oriented implementations that can teach the\nmodel the intricate reasoning process when performing proof-oriented\nprogramming. We present the first on synthetic data augmentation for project\nlevel proof oriented programming for both generation and repair. Our method\naddresses data scarcity by synthesizing basic proof-oriented programming\nproblems for proficiency in that language; incorporating diverse coding data\nfor reasoning capability elicitation and creating new proofs and repair data\nwithin existing repositories. This approach enables language models to both\nsynthesize and repair proofs for function- and repository-level code. We show\nthat our fine-tuned 14B parameter model, PoPilot, can exceed the performance of\nthe models that outperforms GPT-4o in project-level proof-oriented programming\nby 64% relative margin, and can improve GPT-4o's performance by 54% by\nrepairing its outputs over GPT-4o's self-repair.\n","authors":["Dylan Zhang","Justin Wang","Tianran Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06600v2","updated":"2025-02-17T15:22:32Z","published":"2025-02-10T16:00:00Z","title":"Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?","summary":"  The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.\n","authors":["Gon√ßalo Gomes","Chrysoula Zerva","Bruno Martins"],"pdf_url":"https://arxiv.org/pdf/2502.06600v2.pdf","comment":"Accepted in Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.11890v1","updated":"2025-02-17T15:16:44Z","published":"2025-02-17T15:16:44Z","title":"Revisiting Classification Taxonomy for Grammatical Errors","summary":"  Grammatical error classification plays a crucial role in language learning\nsystems, but existing classification taxonomies often lack rigorous validation,\nleading to inconsistencies and unreliable feedback. In this paper, we revisit\nprevious classification taxonomies for grammatical errors by introducing a\nsystematic and qualitative evaluation framework. Our approach examines four\naspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability.\nThen, we construct a high-quality grammatical error classification dataset\nannotated with multiple classification taxonomies and evaluate them grounding\non our proposed evaluation framework. Our experiments reveal the drawbacks of\nexisting taxonomies. Our contributions aim to improve the precision and\neffectiveness of error analysis, providing more understandable and actionable\nfeedback for language learners.\n","authors":["Deqing Zou","Jingheng Ye","Yulu Liu","Yu Wu","Zishan Xu","Yinghui Li","Hai-Tao Zheng","Bingxu An","Zhao Wei","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11890v1.pdf","comment":"30 pages, 4 figures and 5 tables"},{"id":"http://arxiv.org/abs/2502.11886v1","updated":"2025-02-17T15:13:29Z","published":"2025-02-17T15:13:29Z","title":"LIMR: Less is More for RL Scaling","summary":"  In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.\n","authors":["Xuefeng Li","Haoyang Zou","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11886v1.pdf","comment":"6pages"},{"id":"http://arxiv.org/abs/2502.11882v1","updated":"2025-02-17T15:09:45Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2502.11881v1","updated":"2025-02-17T15:08:50Z","published":"2025-02-17T15:08:50Z","title":"Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models","summary":"  Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.\n","authors":["Hyunwoo Kim","Melanie Sclar","Tan Zhi-Xuan","Lance Ying","Sydney Levine","Yang Liu","Joshua B. Tenenbaum","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2502.11881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11880v1","updated":"2025-02-17T15:06:28Z","published":"2025-02-17T15:06:28Z","title":"Bitnet.cpp: Efficient Edge Inference for Ternary LLMs","summary":"  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shijie Cao","Yan Xia","Ting Cao","Jianyu Wei","Shuming Ma","Hongyu Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.11880v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.02039v2","updated":"2025-02-17T15:05:40Z","published":"2024-07-02T08:11:18Z","title":"Prompt Stability Scoring for Text Annotation with Large Language Models","summary":"  Researchers are increasingly using language models (LMs) for text annotation.\nThese approaches rely only on a prompt telling the model to return a given\noutput according to a set of instructions. The reproducibility of LM outputs\nmay nonetheless be vulnerable to small changes in the prompt design. This calls\ninto question the replicability of classification routines. To tackle this\nproblem, researchers have typically tested a variety of semantically similar\nprompts to determine what we call ``prompt stability.\" These approaches remain\nad-hoc and task specific. In this article, we propose a general framework for\ndiagnosing prompt stability by adapting traditional approaches to intra- and\ninter-coder reliability scoring. We call the resulting metric the Prompt\nStability Score (PSS) and provide a Python package \\texttt{promptstability} for\nits estimation. Using six different datasets and twelve outcomes, we classify\n$\\sim$3.1m rows of data and $\\sim$300m input tokens to: a) diagnose when prompt\nstability is low; and b) demonstrate the functionality of the package. We\nconclude by providing best practice recommendations for applied researchers.\n","authors":["Christopher Barrie","Elli Palaiologou","Petter T√∂rnberg"],"pdf_url":"https://arxiv.org/pdf/2407.02039v2.pdf","comment":"39 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.02079v2","updated":"2025-02-17T15:05:06Z","published":"2024-05-03T13:12:28Z","title":"Argumentative Large Language Models for Explainable and Contestable\n  Decision-Making","summary":"  The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.\n","authors":["Gabriel Freedman","Adam Dejl","Deniz Gorur","Xiang Yin","Antonio Rago","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2405.02079v2.pdf","comment":"18 pages, 18 figures, Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.11874v1","updated":"2025-02-17T15:02:09Z","published":"2025-02-17T15:02:09Z","title":"VAQUUM: Are Vague Quantifiers Grounded in Visual Data?","summary":"  Vague quantifiers such as \"a few\" and \"many\" are influenced by many\ncontextual factors, including how many objects are present in a given context.\nIn this work, we evaluate the extent to which vision-and-language models (VLMs)\nare compatible with humans when producing or judging the appropriateness of\nvague quantifiers in visual contexts. We release a novel dataset, VAQUUM,\ncontaining 20300 human ratings on quantified statements across a total of 1089\nimages. Using this dataset, we compare human judgments and VLM predictions\nusing three different evaluation methods. Our findings show that VLMs, like\nhumans, are influenced by object counts in vague quantifier use. However, we\nfind significant inconsistencies across models in different evaluation\nsettings, suggesting that judging and producing vague quantifiers rely on two\ndifferent processes.\n","authors":["Hugh Mee Wong","Rick Nouwen","Albert Gatt"],"pdf_url":"https://arxiv.org/pdf/2502.11874v1.pdf","comment":"Submitted to ARR ACL 2025, 12 pages for main paper (5 figures), 15\n  pages including appendix (2 figures)"},{"id":"http://arxiv.org/abs/2502.11866v1","updated":"2025-02-17T14:57:47Z","published":"2025-02-17T14:57:47Z","title":"Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire\n  Articles Beyond the Front Page","summary":"  I introduce a new large-scale dataset of historical wire articles from U.S.\nSouthern newspapers, spanning 1960-1975 and covering multiple wire services:\nThe Associated Press, United Press International, Newspaper Enterprise\nAssociation. Unlike prior work focusing on front-page content, this dataset\ncaptures articles across the entire newspaper, offering broader insight into\nmid-century Southern coverage. The dataset includes a version that has\nundergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its\nsuitability for quantitative text analysis. Additionally, duplicate versions of\narticles are retained to enable analysis of editorial differences in language\nand framing across newspapers. Each article is tagged by wire service,\nfacilitating comparative studies of editorial patterns across agencies. This\nresource opens new avenues for research in computational social science,\ndigital humanities, and historical linguistics, providing a detailed\nperspective on how Southern newspapers relayed national and international news\nduring a transformative period in American history. The dataset will be made\navailable upon publication or request for research purposes.\n","authors":["Michael McRae"],"pdf_url":"https://arxiv.org/pdf/2502.11866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11862v1","updated":"2025-02-17T14:53:49Z","published":"2025-02-17T14:53:49Z","title":"Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu","summary":"  In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.\n","authors":["Renhao Pei","Yihong Liu","Peiqin Lin","Fran√ßois Yvon","Hinrich Sch√ºtze"],"pdf_url":"https://arxiv.org/pdf/2502.11862v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.11861v1","updated":"2025-02-17T14:53:23Z","published":"2025-02-17T14:53:23Z","title":"Exploring Large Language Models in Healthcare: Insights into Corpora\n  Sources, Customization Strategies, and Evaluation Metrics","summary":"  This study reviewed the use of Large Language Models (LLMs) in healthcare,\nfocusing on their training corpora, customization techniques, and evaluation\nmetrics. A systematic search of studies from 2021 to 2024 identified 61\narticles. Four types of corpora were used: clinical resources, literature,\nopen-source datasets, and web-crawled data. Common construction techniques\nincluded pre-training, prompt engineering, and retrieval-augmented generation,\nwith 44 studies combining multiple methods. Evaluation metrics were categorized\ninto process, usability, and outcome metrics, with outcome metrics divided into\nmodel-based and expert-assessed outcomes. The study identified critical gaps in\ncorpus fairness, which contributed to biases from geographic, cultural, and\nsocio-economic factors. The reliance on unverified or unstructured data\nhighlighted the need for better integration of evidence-based clinical\nguidelines. Future research should focus on developing a tiered corpus\narchitecture with vetted sources and dynamic weighting, while ensuring model\ntransparency. Additionally, the lack of standardized evaluation frameworks for\ndomain-specific models called for comprehensive validation of LLMs in\nreal-world healthcare settings.\n","authors":["Shuqi Yang","Mingrui Jing","Shuai Wang","Jiaxin Kou","Manfei Shi","Weijie Xing","Yan Hu","Zheng Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11861v1.pdf","comment":"45 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2502.11859v1","updated":"2025-02-17T14:50:53Z","published":"2025-02-17T14:50:53Z","title":"Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics","summary":"  The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.\n","authors":["Wenrui Xu","Dalin Lyu","Weihang Wang","Jie Feng","Chen Gao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.11859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11856v1","updated":"2025-02-17T14:48:18Z","published":"2025-02-17T14:48:18Z","title":"LLMs as a synthesis between symbolic and continuous approaches to\n  language","summary":"  Since the middle of the 20th century, a fierce battle is being fought between\nsymbolic and continuous approaches to language and cognition. The success of\ndeep learning models, and LLMs in particular, has been alternatively taken as\nshowing that the continuous camp has won, or dismissed as an irrelevant\nengineering development. However, in this position paper I argue that deep\nlearning models for language actually represent a synthesis between the two\ntraditions. This is because 1) deep learning architectures allow for both\ncontinuous/distributed and symbolic/discrete-like representations and\ncomputations; 2) models trained on language make use this flexibility. In\nparticular, I review recent research in mechanistic interpretability that\nshowcases how a substantial part of morphosyntactic knowledge is encoded in a\nnear-discrete fashion in LLMs. This line of research suggests that different\nbehaviors arise in an emergent fashion, and models flexibly alternate between\nthe two modes (and everything in between) as needed. This is possibly one of\nthe main reasons for their wild success; and it is also what makes them\nparticularly interesting for the study of language and cognition. Is it time\nfor peace?\n","authors":["Gemma Boleda"],"pdf_url":"https://arxiv.org/pdf/2502.11856v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.11843v1","updated":"2025-02-17T14:36:39Z","published":"2025-02-17T14:36:39Z","title":"Can LLM Agents Maintain a Persona in Discourse?","summary":"  Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.\n","authors":["Pranav Bhandari","Nicolas Fay","Michael Wise","Amitava Datta","Stephanie Meek","Usman Naseem","Mehwish Nasim"],"pdf_url":"https://arxiv.org/pdf/2502.11843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14838v2","updated":"2025-02-17T14:34:58Z","published":"2024-12-19T13:28:42Z","title":"DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs","summary":"  Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.\n","authors":["Xiabin Zhou","Wenbin Wang","Minyan Zeng","Jiaxian Guo","Xuebo Liu","Li Shen","Min Zhang","Liang Ding"],"pdf_url":"https://arxiv.org/pdf/2412.14838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08356v2","updated":"2025-02-17T14:29:48Z","published":"2025-02-12T12:39:51Z","title":"Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.\n","authors":["Kushagra Bhushan","Yatin Nandwani","Dinesh Khandelwal","Sonam Gupta","Gaurav Pandey","Dinesh Raghu","Sachindra Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08356v2.pdf","comment":"22 pages, 14 tables, to be published in NAACL 2025"},{"id":"http://arxiv.org/abs/2502.11830v1","updated":"2025-02-17T14:25:54Z","published":"2025-02-17T14:25:54Z","title":"Text Classification in the LLM Era - Where do we stand?","summary":"  Large Language Models revolutionized NLP and showed dramatic performance\nimprovements across several tasks. In this paper, we investigated the role of\nsuch language models in text classification and how they compare with other\napproaches relying on smaller pre-trained language models. Considering 32\ndatasets spanning 8 languages, we compared zero-shot classification, few-shot\nfine-tuning and synthetic data based classifiers with classifiers built using\nthe complete human labeled dataset. Our results show that zero-shot approaches\ndo well for sentiment classification, but are outperformed by other approaches\nfor the rest of the tasks, and synthetic data sourced from multiple LLMs can\nbuild better classifiers than zero-shot open LLMs. We also see wide performance\ndisparities across languages in all the classification scenarios. We expect\nthat these findings would guide practitioners working on developing text\nclassification systems across languages.\n","authors":["Sowmya Vajjala","Shwetali Shimangaud"],"pdf_url":"https://arxiv.org/pdf/2502.11830v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2502.11829v1","updated":"2025-02-17T14:25:45Z","published":"2025-02-17T14:25:45Z","title":"Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code\n  Generation Capabilities","summary":"  This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.\n","authors":["Hanbin Wang","Xiaoxuan Zhou","Zhipeng Xu","Keyuan Cheng","Yuxin Zuo","Kai Tian","Jingwei Song","Junting Lu","Wenhui Hu","Xueyang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11829v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.11824v1","updated":"2025-02-17T14:16:01Z","published":"2025-02-17T14:16:01Z","title":"M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research.\n","authors":["Chengyan Wu","Bolei Ma","Yihong Liu","Zheyu Zhang","Ningyuan Deng","Yanshu Li","Baolan Chen","Yi Zhang","Barbara Plank","Yun Xue"],"pdf_url":"https://arxiv.org/pdf/2502.11824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v1","updated":"2025-02-17T13:59:41Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.11811v1","updated":"2025-02-17T13:55:42Z","published":"2025-02-17T13:55:42Z","title":"FineFilter: A Fine-grained Noise Filtering Mechanism for\n  Retrieval-Augmented Large Language Models","summary":"  Retrieved documents containing noise will hinder Retrieval-Augmented\nGeneration (RAG) from detecting answer clues, necessitating noise filtering\nmechanisms to enhance accuracy.Existing methods use re-ranking or summarization\nto identify the most relevant sentences, but directly and accurately locating\nanswer clues from these large-scale and complex documents remains challenging.\nUnlike these document-level operations, we treat noise filtering as a\nsentence-level MinMax optimization problem: first identifying the potential\nclues from multiple documents using contextual information, then ranking them\nby relevance, and finally retaining the least clues through truncation. In this\npaper, we propose FineFilter, a novel fine-grained noise filtering mechanism\nfor RAG consisting of a clue extractor, a re-ranker, and a truncator. We\noptimize each module to tackle complex reasoning challenges: (1) Clue extractor\nfirstly uses sentences containing the answer and similar ones as fine-tuned\ntargets, aiming at extracting sufficient potential clues; (2) Re-ranker is\ntrained to prioritize effective clues based on the real feedback from\ngeneration module, with clues capable of generating correct answer as positive\nsamples and others as negative; (3) Truncator takes the minimum clues needed to\nanswer the question (truncation point) as fine-tuned targets, and performs\ntruncation on the re-ranked clues to achieve fine-grained noise filtering.\nExperiments on three QA datasets demonstrate that FineFilter significantly\noutperforms baselines in terms of performance and inference cost. Further\nanalysis on each module shows the effectiveness of our optimizations for\ncomplex reasoning.\n","authors":["Qianchi Zhang","Hainan Zhang","Liang Pang","Hongwei Zheng","Yongxin Tong","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11806v1","updated":"2025-02-17T13:50:29Z","published":"2025-02-17T13:50:29Z","title":"Exploring Translation Mechanism of Large Language Models","summary":"  Large language models (LLMs) have succeeded remarkably in multilingual\ntranslation tasks. However, the inherent translation mechanisms of LLMs remain\npoorly understood, largely due to sophisticated architectures and vast\nparameter scales. In response to this issue, this study explores the\ntranslation mechanism of LLM from the perspective of computational components\n(e.g., attention heads and MLPs). Path patching is utilized to explore causal\nrelationships between components, detecting those crucial for translation tasks\nand subsequently analyzing their behavioral patterns in human-interpretable\nterms. Comprehensive analysis reveals that translation is predominantly\nfacilitated by a sparse subset of specialized attention heads (less than 5\\%),\nwhich extract source language, indicator, and positional features. MLPs\nsubsequently integrate and process these features by transiting towards\nEnglish-centric latent representations. Notably, building on the above\nfindings, targeted fine-tuning of only 64 heads achieves translation\nimprovement comparable to full-parameter tuning while preserving general\ncapabilities.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Xiucheng Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.10711v3","updated":"2025-02-17T13:49:45Z","published":"2025-01-18T09:51:57Z","title":"How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks\n  For LLMs","summary":"  Various benchmarks have been proposed to assess the performance of large\nlanguage models (LLMs) in different coding scenarios. We refer to them as\ncode-related benchmarks. However, there are no systematic guidelines by which\nsuch a benchmark should be developed to ensure its quality, reliability, and\nreproducibility. We propose How2Bench, which is comprised of a 55-criteria\nchecklist as a set of guidelines to govern the development of code-related\nbenchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks\nreleased within the past decade and found concerning issues. Nearly 70% of the\nbenchmarks did not take measures for data quality assurance; over 10% did not\neven open source or only partially open source. Many highly cited benchmarks\nhave loopholes, including duplicated samples, incorrect reference\ncodes/tests/prompts, and unremoved sensitive/confidential information. Finally,\nwe conducted a human study involving 49 participants, which revealed\nsignificant gaps in awareness of the importance of data quality,\nreproducibility, and transparency.\n","authors":["Jialun Cao","Yuk-Kit Chan","Zixuan Ling","Wenxuan Wang","Shuqing Li","Mingwei Liu","Ruixi Qiao","Yuting Han","Chaozheng Wang","Boxi Yu","Pinjia He","Shuai Wang","Zibin Zheng","Michael R. Lyu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2501.10711v3.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2403.19318v3","updated":"2025-02-17T13:45:00Z","published":"2024-03-28T11:21:12Z","title":"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office\n  Usage Scenarios","summary":"  We introduce TableLLM, a robust large language model (LLM) with 8 billion\nparameters, purpose-built for proficiently handling tabular data manipulation\ntasks, whether they are embedded within documents or spreadsheets, catering to\nreal-world office scenarios. We propose a distant supervision method for\ntraining, which comprises a reasoning process extension strategy, aiding in\ntraining LLMs to understand reasoning patterns more effectively as well as a\ncross-way validation strategy, ensuring the quality of the automatically\ngenerated data. To evaluate the performance of TableLLM, we have crafted\nbenchmarks tailored to address both document and spreadsheet formats as well as\nconstructed a well-organized evaluation pipeline capable of handling both\nscenarios. Thorough evaluations underscore the advantages of TableLLM when\ncompared to various existing general-purpose and tabular data-focused LLMs. We\nhave publicly released the model checkpoint, source code, benchmarks, and a web\napplication for user interaction. Our codes and data are publicly available at\nhttps://github.com/TableLLM/TableLLM.\n","authors":["Xiaokang Zhang","Sijia Luo","Bohan Zhang","Zeyao Ma","Jing Zhang","Yang Li","Guanlin Li","Zijun Yao","Kangli Xu","Jinchang Zhou","Daniel Zhang-Li","Jifan Yu","Shu Zhao","Juanzi Li","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.19318v3.pdf","comment":"https://tablellm.github.io/"},{"id":"http://arxiv.org/abs/2502.11799v1","updated":"2025-02-17T13:42:12Z","published":"2025-02-17T13:42:12Z","title":"Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning","summary":"  Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.\n","authors":["Peiying Yu","Guoxin Chen","Jingjing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01434v2","updated":"2025-02-17T13:30:15Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions through\nsubnetworks that can be composed to perform more complex tasks. Recent advances\nin mechanistic interpretability have made progress in identifying\n$\\textit{circuits}$, which represent the minimal computational subgraphs\nresponsible for a model's behavior on specific tasks. However, most studies\nfocus on identifying circuits for individual tasks without investigating how\nfunctionally similar circuits $\\textit{relate}$ to each other. To address this\ngap, we study the modularity of neural networks by analyzing circuits for\nhighly compositional subtasks within a transformer-based language model.\nSpecifically, given a probabilistic context-free grammar, we identify and\ncompare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through set operations to represent more\ncomplex functional model capabilities.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v2.pdf","comment":"22 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.11789v1","updated":"2025-02-17T13:28:14Z","published":"2025-02-17T13:28:14Z","title":"Personality Editing for Language Models through Relevant Knowledge\n  Editing","summary":"  Large Language Models (LLMs) play a vital role in applications like\nconversational agents and content creation, where controlling a model's\npersonality is crucial for maintaining tone, consistency, and engagement.\nHowever, traditional prompt-based techniques for controlling personality often\nfall short, as they do not effectively mitigate the model's inherent biases. In\nthis paper, we introduce a novel method PALETTE that enhances personality\ncontrol through knowledge editing. By generating adjustment queries inspired by\npsychological assessments, our approach systematically adjusts responses to\npersonality-related queries similar to modifying factual knowledge, thereby\nachieving controlled shifts in personality traits. Experimental results from\nboth automatic and human evaluations demonstrate that our method enables more\nstable and well-balanced personality control in LLMs.\n","authors":["Seojin Hwang","Yumin Kim","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11789v1.pdf","comment":"15 pages, 3 figures, 16 tables"},{"id":"http://arxiv.org/abs/2501.07824v2","updated":"2025-02-17T13:26:52Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07173v2","updated":"2025-02-17T13:25:17Z","published":"2024-10-09T17:59:33Z","title":"Better Language Models Exhibit Higher Visual Alignment","summary":"  How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01220v2","updated":"2025-02-17T13:20:37Z","published":"2025-02-03T10:24:55Z","title":"Language Models Struggle to Achieve a Consistent Temporal Representation\n  of Facts","summary":"  Language Models (LMs) have shown substantial improvements in handling factual\nknowledge, yet their capability to consistently represent temporal facts, which\nare valid only within specific timeframes, remains underexplored. To\ninvestigate this, we introduce TimeStress, a novel dataset comprising 521K\nstatements on 2003 of the most popular temporal facts in Wikidata. Each\nstatement contextualizes a fact with correct and incorrect dates across three\nprecisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to\ndiscern between correct and incorrect temporal statements based on their\nprobability of being generated. We assess 18 LMs across various architectures\nusing two metrics: the win rate, indicating how often correct dates outperform\nincorrect ones, and robustness, reflecting consistent performance across all\ndates. Our findings reveal that while some LMs achieve a win rate exceeding\n80\\%, robustness remains low, with the best model achieving only 6\\%.\nFurthermore, robust knowledge at one date precision does not reliably transfer\nto others, highlighting a significant generalization gap. These results\nunderscore the struggle of LMs to maintain a consistent temporal\nrepresentation, supporting their limitations as reliable sources of temporal\nknowledge. We provide all data and code for further research.\n","authors":["Hichem Ammar Khodja","Fr√©d√©ric B√©chet","Quentin Brabant","Alexis Nasr","Gw√©nol√© Lecorv√©"],"pdf_url":"https://arxiv.org/pdf/2502.01220v2.pdf","comment":"preprint v2"},{"id":"http://arxiv.org/abs/2412.12499v2","updated":"2025-02-17T13:20:15Z","published":"2024-12-17T03:03:17Z","title":"LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for\n  Low-Resource Language Reasoning","summary":"  Large language models (LLMs) have exhibited impressive multilingual reasoning\ncapabilities, driven by extensive multilingual pre-training corpora and\ninstruction fine-tuning data. However, a performance gap exists between high-\nand low-resource language reasoning tasks due to the language imbalance in the\npre-training corpus, which is exacerbated by evaluation bias in existing\nreasoning benchmarks lacking low-resource language coverage. To alleviate this\nissue, we propose LinguaLIFT, a two-stage instruction tuning framework for\nadvancing low-resource language reasoning. LinguaLIFT employs a language\nalignment layer to capture multilingual alignment in a code-switched tuning way\nwithout requiring multilingual instruction or parallel data, thereby\ntransferring the cross-lingual reasoning capabilities to low-resource languages\nthrough English-only instruction tuning data. To comprehensively evaluate the\nmultilingual reasoning capabilities, we introduce the Multilingual Math World\nProblem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and\n10 high-resource languages. Experimental results show that LinguaLIFT\noutperforms several competitive baselines across MMWP and four widely used\nbenchmarks.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v2","updated":"2025-02-17T13:16:00Z","published":"2025-02-13T08:10:45Z","title":"Adapting Language-Specific LLMs to a Reasoning Model in One Day via\n  Model Merging - An Open Recipe","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.11779v1","updated":"2025-02-17T13:14:11Z","published":"2025-02-17T13:14:11Z","title":"Efficient Response Generation Method Selection for Fine-Tuning Large\n  Language Models","summary":"  The training data for fine-tuning large language models (LLMs) is typically\nstructured as input-output pairs. However, for many tasks, there can be\nmultiple equally valid output variations for the same input. Recent studies\nhave observed that the choice of output variation used in training can affect\nthe model's performance. This raises an important question: how can we generate\nthe most effective output from the many possible response generation strategy\noptions? Rather than relying on the traditional but resource-intensive\ntrain-and-evaluate approach, this paper proposes a scalable, approximate method\nfor estimating the quality of a small subset of generated training data derived\nfrom the same input. We then evaluate how well this small subset of generated\noutput fits the target model we are trying to train. We present a large-scale\nbenchmark covering diverse reasoning-based datasets to support our study.\n  The central idea is that a good output should closely resemble the output\ngenerated by the target LLM. We formalize this 'closeness' as the expected\nalignment score between a candidate output and the output sampled from the\ntarget LLM. We connect this measurement to the perplexity metric used in\nprevious literature and demonstrate that leveraging an alignment-based metric\ncan provide better predictions of model performance. Using this strategy, we\ncan evaluate a small subset of the generated output from each response\ngeneration strategy option, then select the most effective strategy. We show\nthat an LLM trained on data generated by the selected strategy could lead to a\nsignificant performance gain in many cases.\n","authors":["Xuan Ren","Qi Chen","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13669v2","updated":"2025-02-17T13:10:33Z","published":"2025-01-23T13:54:53Z","title":"How to Alleviate Catastrophic Forgetting in LLMs Finetuning?\n  Hierarchical Layer-Wise and Element-Wise Regularization","summary":"  Large Language Models (LLMs) exhibit strong general language capabilities.\nHowever, fine-tuning these models on domain-specific tasks often leads to\ncatastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss based on\nelement-wise parameter importance, which constrains the updates to parameters\ncrucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10-15% of the storage, highlighting the practical efficiency. The\ncode will be released.\n","authors":["Shezheng Song","Hao Xu","Jun Ma","Shasha Li","Long Peng","Qian Wan","Xiaodong Liu","Jie Yu"],"pdf_url":"https://arxiv.org/pdf/2501.13669v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.11771v1","updated":"2025-02-17T13:00:44Z","published":"2025-02-17T13:00:44Z","title":"The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It","summary":"  The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why current LLMs struggle to detect even simple arithmetic errors.\n","authors":["Leonardo Bertolazzi","Philipp Mondorf","Barbara Plank","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2502.11771v1.pdf","comment":"34 pages, 31 figures"},{"id":"http://arxiv.org/abs/2501.16207v2","updated":"2025-02-17T13:00:34Z","published":"2025-01-27T17:00:56Z","title":"From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs","summary":"  The research in AI-based formal mathematical reasoning has shown an unstop-\npable growth trend. These studies have excelled in mathematical competitions\nlike IMO and have made significant progress. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and breaks\nit down into sub-tasks. We constructed 18k high-quality instruction-response\npairs across five formal specification languages (Coq, Lean4, Dafny, ACSL, and\nTLA+) by distilling gpt-4o and evaluated against ten open-sourced LLMs,\nincluding recent popular DeepSeek-R1. We also fine-tuned several 7~8B small\nmodels to achieve comparable performance with Deepseek-R1-671B. Interestingly,\nwe observed that fine-tuning with formal data also enhances mathematics,\nreasoning, and coding capabilities. Fine-tuned models are released at https:\n//huggingface.co/fm-universe.\n","authors":["Jialun Cao","Yaojie Lu","Meiziniu Li","Haoyang Ma","Haokun Li","Mengda He","Cheng Wen","Le Sun","Hongyu Zhang","Shengchao Qin","Shing-Chi Cheung","Cong Tian"],"pdf_url":"https://arxiv.org/pdf/2501.16207v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2502.11767v1","updated":"2025-02-17T12:58:17Z","published":"2025-02-17T12:58:17Z","title":"From Selection to Generation: A Survey of LLM-based Active Learning","summary":"  Active Learning (AL) has been a powerful paradigm for improving model\nefficiency and performance by selecting the most informative data points for\nlabeling and training. In recent active learning frameworks, Large Language\nModels (LLMs) have been employed not only for selection but also for generating\nentirely new data instances and providing more cost-effective annotations.\nMotivated by the increasing importance of high-quality data and efficient model\ntraining in the era of LLMs, we present a comprehensive survey on LLM-based\nActive Learning. We introduce an intuitive taxonomy that categorizes these\ntechniques and discuss the transformative roles LLMs can play in the active\nlearning loop. We further examine the impact of AL on LLM learning paradigms\nand its applications across various domains. Finally, we identify open\nchallenges and propose future research directions. This survey aims to serve as\nan up-to-date resource for researchers and practitioners seeking to gain an\nintuitive understanding of LLM-based AL techniques and deploy them to new\napplications.\n","authors":["Yu Xia","Subhojyoti Mukherjee","Zhouhang Xie","Junda Wu","Xintong Li","Ryan Aponte","Hanjia Lyu","Joe Barrow","Hongjie Chen","Franck Dernoncourt","Branislav Kveton","Tong Yu","Ruiyi Zhang","Jiuxiang Gu","Nesreen K. Ahmed","Yu Wang","Xiang Chen","Hanieh Deilamsalehy","Sungchul Kim","Zhengmian Hu","Yue Zhao","Nedim Lipka","Seunghyun Yoon","Ting-Hao Kenneth Huang","Zichao Wang","Puneet Mathur","Soumyabrata Pal","Koyel Mukherjee","Zhehao Zhang","Namyong Park","Thien Huu Nguyen","Jiebo Luo","Ryan A. Rossi","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11766v1","updated":"2025-02-17T12:58:12Z","published":"2025-02-17T12:58:12Z","title":"Warmup-Distill: Bridge the Distribution Mismatch between Teacher and\n  Student before Knowledge Distillation","summary":"  The widespread deployment of Large Language Models (LLMs) is hindered by the\nhigh computational demands, making knowledge distillation (KD) crucial for\ndeveloping compact smaller ones. However, the conventional KD methods endure\nthe distribution mismatch issue between the teacher and student models, leading\nto the poor performance of distillation. For instance, the widely-used KL-based\nmethods suffer the mode-averaging and mode-collapsing problems, since the\nmismatched probabitliy distribution between both models. Previous studies\nmainly optimize this issue via different distance calculations towards the\ndistribution of both models. Unfortunately, the distribution mismatch issue\nstill exists in the early stage of the distillation. Hence, to reduce the\nimpact of distribution mismatch, we propose a simple yet efficient method,\nnamed Warmup-Distill, which aligns the distillation of the student to that of\nthe teacher in advance of distillation. Specifically, we first detect the\ndistribution of the student model in practical scenarios with its internal\nknowledge, and then modify the knowledge with low probability via the teacher\nas the checker. Consequently, Warmup-Distill aligns the internal student's\nknowledge to that of the teacher, which expands the distribution of the student\nwith the teacher's, and assists the student model to learn better in the\nsubsequent distillation. Experiments on the seven benchmarks demonstrate that\nWarmup-Distill could provide a warmup student more suitable for distillation,\nwhich outperforms the vanilla student by as least +0.4 averaged score among all\nbenchmarks. Noteably, with the assistance of Warmup-Distill, the distillation\non the math task could yield a further improvement, at most +1.9% accuracy.\n","authors":["Zengkui Sun","Yijin Liu","Fandong Meng","Yufeng Chen","Jinan Xu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11766v1.pdf","comment":"11 Pages, 4 figures, Code at https://github.com/Acerkoo/WarmupDistill"},{"id":"http://arxiv.org/abs/2406.04116v2","updated":"2025-02-17T12:44:01Z","published":"2024-06-06T14:36:07Z","title":"Promoting the Responsible Development of Speech Datasets for Mental\n  Health and Neurological Disorders Research","summary":"  Current research in machine learning and artificial intelligence is largely\ncentered on modeling and performance evaluation, less so on data collection.\nHowever, recent research demonstrated that limitations and biases in data may\nnegatively impact trustworthiness and reliability. These aspects are\nparticularly impactful on sensitive domains such as mental health and\nneurological disorders, where speech data are used to develop AI applications\nfor patients and healthcare providers. In this paper, we chart the landscape of\navailable speech datasets for this domain, to highlight possible pitfalls and\nopportunities for improvement and promote fairness and diversity. We present a\ncomprehensive list of desiderata for building speech datasets for mental health\nand neurological disorders and distill it into an actionable checklist focused\non ethical concerns to foster more responsible research.\n","authors":["Eleonora Mancini","Ana Tanevska","Andrea Galassi","Alessio Galatolo","Federico Ruggeri","Paolo Torroni"],"pdf_url":"https://arxiv.org/pdf/2406.04116v2.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2411.06438v3","updated":"2025-02-17T12:33:10Z","published":"2024-11-10T11:49:36Z","title":"Conditional [MASK] Discrete Diffusion Language Model","summary":"  Although auto-regressive models excel in natural language processing, they\noften struggle to generate diverse text and provide limited controllability.\nNon-auto-regressive methods could be an alternative but often produce\ndegenerate outputs and exhibit shortcomings in conditional generation. To\naddress these challenges, we propose Diffusion-EAGS, a novel framework that\nintegrates conditional masked language models into diffusion language models\nthrough the theoretical lens of a conditional Markov Random Field. In doing so,\nwe propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling\nto counterbalance each model's shortcomings. Experimental results show that\nDiffusion-EAGS outperforms baselines and achieves the best quality-diversity\ntradeoff, demonstrating its effectiveness in non-autoregressive text\ngeneration.\n","authors":["Hyukhun Koh","Minha Jhang","Dohyung Kim","Sangmook Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2411.06438v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11736v1","updated":"2025-02-17T12:22:11Z","published":"2025-02-17T12:22:11Z","title":"ReviewEval: An Evaluation Framework for AI-Generated Reviews","summary":"  The escalating volume of academic research, coupled with a shortage of\nqualified reviewers, necessitates innovative approaches to peer review. While\nlarge language model (LLMs) offer potential for automating this process, their\ncurrent limitations include superficial critiques, hallucinations, and a lack\nof actionable insights. This research addresses these challenges by introducing\na comprehensive evaluation framework for AI-generated reviews, that measures\nalignment with human evaluations, verifies factual accuracy, assesses\nanalytical depth, and identifies actionable insights. We also propose a novel\nalignment mechanism that tailors LLM-generated reviews to the unique evaluation\npriorities of individual conferences and journals. To enhance the quality of\nthese reviews, we introduce a self-refinement loop that iteratively optimizes\nthe LLM's review prompts. Our framework establishes standardized metrics for\nevaluating AI-based review systems, thereby bolstering the reliability of\nAI-generated reviews in academic research.\n","authors":["Chavvi Kirtani","Madhav Krishan Garg","Tejash Prasad","Tanmay Singhal","Murari Mandal","Dhruv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.11736v1.pdf","comment":"Under review: 8 pages, 2 figures, 2 tables, 3 pages for appendix"},{"id":"http://arxiv.org/abs/2502.11735v1","updated":"2025-02-17T12:21:13Z","published":"2025-02-17T12:21:13Z","title":"MT-RAIG: Novel Benchmark and Evaluation Framework for\n  Retrieval-Augmented Insight Generation over Multiple Tables","summary":"  Recent advancements in table-based reasoning have expanded beyond\nfactoid-level QA to address insight-level tasks, where systems should\nsynthesize implicit knowledge in the table to provide explainable analyses.\nAlthough effective, existing studies remain confined to scenarios where a\nsingle gold table is given alongside the user query, failing to address cases\nwhere users seek comprehensive insights from multiple unknown tables. To bridge\nthese gaps, we propose MT-RAIG Bench, design to evaluate systems on\nRetrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to\ntackle the suboptimality of existing automatic evaluation methods in the table\ndomain, we further introduce a fine-grained evaluation framework MT-RAIG Eval,\nwhich achieves better alignment with human quality judgments on the generated\ninsights. We conduct extensive experiments and reveal that even frontier LLMs\nstill struggle with complex multi-table reasoning, establishing our MT-RAIG\nBench as a challenging testbed for future research.\n","authors":["Kwangwook Seo","Donguk Kwon","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11735v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.11733v1","updated":"2025-02-17T12:20:39Z","published":"2025-02-17T12:20:39Z","title":"Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking\n  Practical Reasoning and Situation Modelling in a Text-Simulated Situated\n  Environment","summary":"  Large language models (LLMs) have risen to prominence as 'chatbots' for users\nto interact via natural language. However, their abilities to capture\ncommon-sense knowledge make them seem promising as language-based planners of\nsituated or embodied action as well. We have implemented a simple text-based\nenvironment -- similar to others that have before been used for\nreinforcement-learning of agents -- that simulates, very abstractly, a\nhousehold setting. We use this environment and the detailed error-tracking\ncapabilities we implemented for targeted benchmarking of LLMs on the problem of\npractical reasoning: Going from goals and observations to actions. Our findings\nshow that environmental complexity and game restrictions hamper performance,\nand concise action planning is demanding for current LLMs.\n","authors":["Jonathan Jordan","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2502.11733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06089v3","updated":"2025-02-17T12:17:35Z","published":"2023-09-12T09:37:08Z","title":"Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms:\n  Exploring Tuning Strategies","summary":"  The cross-lingual transfer is a promising technique to solve tasks in\nless-resourced languages. In this empirical study, we compare two fine-tuning\napproaches combined with zero-shot and full-shot learning approaches for large\nlanguage models in a cross-lingual setting. As fine-tuning strategies, we\ncompare parameter-efficient adapter methods with fine-tuning of all parameters.\nAs cross-lingual transfer strategies, we compare the intermediate-training\n(\\textit{IT}) that uses each language sequentially and cross-lingual validation\n(\\textit{CLV}) that uses a target language already in the validation phase of\nfine-tuning. We assess the success of transfer and the extent of catastrophic\nforgetting in a source language due to cross-lingual transfer, i.e., how much\npreviously acquired knowledge is lost when we learn new information in a\ndifferent language. The results on two different classification problems, hate\nspeech detection and product reviews, each containing datasets in several\nlanguages, show that the \\textit{IT} cross-lingual strategy outperforms\n\\textit{CLV} for the target language. Our findings indicate that, in the\nmajority of cases, the \\textit{CLV} strategy demonstrates superior retention of\nknowledge in the base language (English) compared to the \\textit{IT} strategy,\nwhen evaluating catastrophic forgetting in multiple cross-lingual transfers.\n","authors":["Boshko Koloski","Bla≈æ ≈†krlj","Marko Robnik-≈†ikonja","Senja Pollak"],"pdf_url":"https://arxiv.org/pdf/2309.06089v3.pdf","comment":"Accepted to IEEE Access"},{"id":"http://arxiv.org/abs/2408.08590v2","updated":"2025-02-17T12:09:50Z","published":"2024-08-16T07:47:39Z","title":"A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive\n  Language Models","summary":"  Recent studies on logical reasoning in Language Models (LMs) have sparked a\ndebate on whether they can learn systematic reasoning principles during\npre-training or merely exploit superficial patterns in the training data. This\npaper presents a mechanistic interpretation of syllogistic reasoning in LMs to\nadvance the understanding of internal dynamics. Specifically, we present a\nmethodology for circuit discovery aimed at interpreting content-independent\nreasoning mechanisms. Through two distinct intervention methods, we uncover a\nsufficient and necessary circuit involving middle-term suppression that\nelucidates how LMs transfer information to derive valid conclusions from\npremises. Furthermore, we investigate how belief biases manifest in syllogistic\nreasoning, finding evidence of partial contamination from additional attention\nheads responsible for encoding commonsense and contextualized knowledge.\nFinally, we explore the generalization of the discovered mechanisms across\nvarious syllogistic schemes, model sizes and architectures, finding that the\nidentified circuit is sufficient and necessary for the schemes on which the\nmodels achieve high downstream accuracy (> 60%), and that the activation\npatterns apply to models of different families. Overall, our findings suggest\nthat LMs indeed learn transferable content-independent reasoning mechanisms,\nbut that, at the same time, such mechanisms do not involve generalizable and\nabstract logical primitives, being susceptible to contamination by the same\nworld knowledge acquired during pre-training.\n","authors":["Geonhee Kim","Marco Valentino","Andr√© Freitas"],"pdf_url":"https://arxiv.org/pdf/2408.08590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05952v2","updated":"2025-02-17T12:04:53Z","published":"2025-01-10T13:27:04Z","title":"Scalable Vision Language Model Training via High Quality Data Curation","summary":"  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning\nvia High QuaLity Data Curation), an open-source vision language model (VLM)\nseries achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.\nThe following three key improvements contribute to SAIL-VL's leading\nperformance: (1) Scalable high-quality visual understanding data construction:\nWe implement a data construction pipeline to enable hundred-million-scale\nhigh-quality recaption data annotation, and the resulted dataset SAIL-Caption\nis validated to be of the highest data quality compared with opensource\nalternatives. (2) Scalable Pretraining with High-Quality Visual Understanding\nData: We scale SAIL-VL's pretraining budget up to 655B tokens and show that\neven a 2B VLM benefits from scaled up training data sizes, exhibiting expected\ndata size scaling laws in visual understanding and instruction following\nperformance. (3) Scalable SFT via data quantity and complexity scaling: We\ncurate a high-quality SFT dataset collection which outperforms opensource\nalternatives in data quantity scaling effectiveness. We also demonstrate that\ntraining with progressively higher-complexity data surpasses baseline one-stage\ntraining by a large margin. SAIL-VL series models achieve the highest average\nscore in 18 widely used VLM benchmarks in our evaluation, with the 2B model\ntakes the top position over VLMs of comparable sizes on OpenCompass 2024\n(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust\nvisual comprehension abilities. SAIL-VL series models are released at\nHuggingFace (https://huggingface.co/BytedanceDouyinContent).\n","authors":["Hongyuan Dong","Zijian Kang","Weijie Yin","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"https://arxiv.org/pdf/2501.05952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11041v2","updated":"2025-02-17T12:03:22Z","published":"2024-12-15T03:58:38Z","title":"Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety\n  Re-Alignment for Fine-Tuned Language Models","summary":"  Although large language models (LLMs) achieve effective safety alignment at\nthe time of release, they still face various safety challenges. A key issue is\nthat fine-tuning often compromises the safety alignment of LLMs. To address\nthis issue, we propose a method named IRR (Identify, Remove, and Recalibrate\nfor Safety Realignment) that performs safety realignment for LLMs. The core of\nIRR is to identify and remove unsafe delta parameters from the fine-tuned\nmodels, while recalibrating the retained ones. We evaluate the effectiveness of\nIRR across various datasets, including both full fine-tuning and LoRA methods.\nOur results demonstrate that IRR significantly enhances the safety performance\nof fine-tuned models on safety benchmarks, such as harmful queries and\njailbreak attacks, while maintaining their performance on downstream tasks. The\nsource code is available at: https://anonymous.4open.science/r/IRR-BD4F.\n","authors":["Di Wu","Xin Lu","Yanyan Zhao","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2412.11041v2.pdf","comment":"16 pages, 14 figures,"},{"id":"http://arxiv.org/abs/2502.11718v1","updated":"2025-02-17T12:02:23Z","published":"2025-02-17T12:02:23Z","title":"\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for\n  Large Vision Language Models","summary":"  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n","authors":["Jihao Gu","Yingyao Wang","Pi Bu","Chen Wang","Ziming Wang","Tengtao Song","Donglai Wei","Jiale Yuan","Yingxiu Zhao","Yancheng He","Shilong Li","Jiaheng Liu","Meng Cao","Jun Song","Yingshui Tan","Xiang Li","Wenbo Su","Zhicheng Zheng","Xiaoyong Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11718v1.pdf","comment":"24 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.08279v2","updated":"2025-02-17T12:01:02Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12929v2","updated":"2025-02-17T11:49:57Z","published":"2024-09-19T17:30:45Z","title":"LogicPro: Improving Complex Logical Reasoning via Program-Guided\n  Learning","summary":"  In this paper, we propose a new data synthesis method called\n\\textbf{LogicPro}, which leverages LeetCode-style algorithm\n\\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to\nsynthesize Complex \\underline{Logic}al Reasoning data in text format. First, we\nsynthesize complex reasoning problems through source algorithm problems and\ntest cases. Then, standard answers and intermediate variable outputs are\nobtained for each problem based on standard python solutions and test cases.\nFinally, with the guidance of code intermediate variables, we synthesize the\ntext reasoning process for each reasoning problems. Through this method, we can\nsynthesize data that is difficult, scalable, effective, and comes with golden\nstandard answers and high-quality reasoning processes. As a result, with our\n540K synthesized dataset constructed solely from 2,360 algorithm problems, our\napproach\n  Code and data are publicly available at\nhttps://github.com/jiangjin1999/LogicPro achieves significant improvements in\nmultiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench},\n\\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide\nrange of existing reasoning datasets.\n","authors":["Jin Jiang","Yuchen Yan","Yang Liu","Yonggang Jin","Shuai Peng","Mengdi Zhang","Xunliang Cai","Yixin Cao","Liangcai Gao","Zhi Tang"],"pdf_url":"https://arxiv.org/pdf/2409.12929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11707v1","updated":"2025-02-17T11:46:46Z","published":"2025-02-17T11:46:46Z","title":"Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating\n  Large Language Models","summary":"  This study utilizes the game Codenames as a benchmarking tool to evaluate\nlarge language models (LLMs) with respect to specific linguistic and cognitive\nskills. LLMs play each side of the game, where one side generates a clue word\ncovering several target words and the other guesses those target words. We\ndesigned various experiments by controlling the choice of words (abstract vs.\nconcrete words, ambiguous vs. monosemic) or the opponent (programmed to be\nfaster or slower in revealing words). Recent commercial and open-weight models\nwere compared side-by-side to find out factors affecting their performance. The\nevaluation reveals details about their strategies, challenging cases, and\nlimitations of LLMs.\n","authors":["Sherzod Hakimov","Lara Pfennigschmidt","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2502.11707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13126v2","updated":"2025-02-17T11:44:35Z","published":"2025-01-21T13:12:13Z","title":"Preference Curriculum: LLMs Should Always Be Pretrained on Their\n  Preferred Data","summary":"  Large language models (LLMs) generally utilize a consistent data distribution\nthroughout the pretraining process. However, as the model's capability\nimproves, it is intuitive that its data preferences dynamically change,\nindicating the need for pretraining with different data at various training\nstages. To achieve it, we propose the Perplexity Difference (PD) based\nPreference Curriculum learning (PDPC) framework, which always perceives and\nuses the data preferred by LLMs to train and boost them. First, we introduce\nthe PD metric to quantify the difference in how challenging a sample is for\nweak versus strong models. Samples with high PD are more challenging for weak\nmodels to learn and are more suitable to be arranged in the later stage of\npretraining. Second, we propose the preference function to approximate and\npredict the data preference of the LLM at any training step, so as to complete\nthe arrangement of the dataset offline and ensure continuous training without\ninterruption. Experimental results on 1.3B and 3B models demonstrate that PDPC\nsignificantly surpasses baselines. Notably, the 3B model trained on 1T tokens\nachieves an increased average accuracy of over 8.1% across MMLU and CMMLU.\n","authors":["Xuemiao Zhang","Liangyu Xu","Feiyu Duan","Yongwei Zhou","Sirui Wang","Rongxiang Weng","Jingang Wang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2501.13126v2.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.11705v1","updated":"2025-02-17T11:44:11Z","published":"2025-02-17T11:44:11Z","title":"LLM Agents Making Agent Tools","summary":"  Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains which demand\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, a novel agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a short task\ndescription and a repository URL, ToolMaker autonomously installs required\ndependencies and generates code to perform the task, using a closed-loop\nself-correction mechanism to iteratively diagnose and rectify errors. To\nevaluate our approach, we introduce a benchmark comprising 15 diverse and\ncomplex computational tasks spanning both medical and non-medical domains with\nover 100 unit tests to objectively assess tool correctness and robustness.\nToolMaker correctly implements 80% of the tasks, substantially outperforming\ncurrent state-of-the-art software engineering agents. ToolMaker therefore is a\nstep towards fully autonomous agent-based scientific workflows.\n","authors":["Georg W√∂lflein","Dyke Ferber","Daniel Truhn","Ognjen Arandjeloviƒá","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2502.11705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11703v1","updated":"2025-02-17T11:40:48Z","published":"2025-02-17T11:40:48Z","title":"CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation","summary":"  Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepo https://anonymous.4open.science/r/C-MQCIC-1151.\n","authors":["Guangya Yu","Yanhao Li","Zongying Jiang","Yuxiong Jin","Li Dai","Yupian Lin","Ruihui Hou","Weiyan Zhang","Yongqi Fan","Qi Ye","Jingping Liu","Tong Ruan"],"pdf_url":"https://arxiv.org/pdf/2502.11703v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.03888v2","updated":"2025-02-17T11:34:19Z","published":"2024-11-06T13:06:43Z","title":"Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech\n  Detection with Vision-Language Models","summary":"  Warning: this paper contains content that may be offensive or upsetting\n  Hate speech moderation on global platforms poses unique challenges due to the\nmultimodal and multilingual nature of content, along with the varying cultural\nperceptions. How well do current vision-language models (VLMs) navigate these\nnuances? To investigate this, we create the first multimodal and multilingual\nparallel hate speech dataset, annotated by a multicultural set of annotators,\ncalled Multi3Hate. It contains 300 parallel meme samples across 5 languages:\nEnglish, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural\nbackground significantly affects multimodal hate speech annotation in our\ndataset. The average pairwise agreement among countries is just 74%,\nsignificantly lower than that of randomly selected annotator groups. Our\nqualitative analysis indicates that the lowest pairwise label agreement-only\n67% between the USA and India-can be attributed to cultural factors. We then\nconduct experiments with 5 large VLMs in a zero-shot setting, finding that\nthese models align more closely with annotations from the US than with those\nfrom other cultures, even when the memes and prompts are presented in the\ndominant language of the other culture. Code and dataset are available at\nhttps://github.com/MinhDucBui/Multi3Hate.\n","authors":["Minh Duc Bui","Katharina von der Wense","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2411.03888v2.pdf","comment":"Accepted to NAACL 2025 Main (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2502.11689v1","updated":"2025-02-17T11:28:43Z","published":"2025-02-17T11:28:43Z","title":"Improve LLM-as-a-Judge Ability as a General Ability","summary":"  LLM-as-a-Judge leverages the generative and reasoning capabilities of large\nlanguage models (LLMs) to evaluate LLM responses across diverse scenarios,\nproviding accurate preference signals. This approach plays a vital role in\naligning LLMs with human values, ensuring ethical and reliable AI outputs that\nalign with societal norms. Recent studies have raised many methods to train LLM\nas generative judges, but most of them are data consuming or lack accuracy, and\nonly focus on LLM's judge ability. In this work, we regard judge ability as a\ngeneral ability of LLM and implement a two-stage training approach, comprising\nsupervised fine-tuning (SFT) warm-up and direct preference optimization (DPO)\nenhancement, to achieve judge style adaptation and improve judgment accuracy.\nAdditionally, we introduce an efficient data synthesis method to generate\njudgmental content. Experimental results demonstrate that our approach,\nutilizing only about 2% to 40% of the data required by other methods, achieves\nSOTA performance on RewardBench. Furthermore, our training method enhances the\ngeneral capabilities of the model by constructing complicated judge task, and\nthe judge signals provided by our model have significantly enhanced the\ndownstream DPO training performance of our internal models in our test to\noptimize policy model with Judge Model. We also open-source our model weights\nand training data to facilitate further research.\n","authors":["Jiachen Yu","Shaoning Sun","Xiaohui Hu","Jiaxu Yan","Kaidong Yu","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.11689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11688v1","updated":"2025-02-17T11:25:32Z","published":"2025-02-17T11:25:32Z","title":"From Isolates to Families: Using Neural Networks for Automated Language\n  Affiliation","summary":"  In historical linguistics, the affiliation of languages to a common language\nfamily is traditionally carried out using a complex workflow that relies on\nmanually comparing individual languages. Large-scale standardized collections\nof multilingual wordlists and grammatical language structures might help to\nimprove this and open new avenues for developing automated language affiliation\nworkflows. Here, we present neural network models that use lexical and\ngrammatical data from a worldwide sample of more than 1,000 languages with\nknown affiliations to classify individual languages into families. In line with\nthe traditional assumption of most linguists, our results show that models\ntrained on lexical data alone outperform models solely based on grammatical\ndata, whereas combining both types of data yields even better performance. In\nadditional experiments, we show how our models can identify long-ranging\nrelations between entire subgroups, how they can be employed to investigate\npotential relatives of linguistic isolates, and how they can help us to obtain\nfirst hints on the affiliation of so far unaffiliated languages. We conclude\nthat models for automated language affiliation trained on lexical and\ngrammatical data provide comparative linguists with a valuable tool for\nevaluating hypotheses about deep and unknown language relations.\n","authors":["Frederic Blum","Steffen Herbold","Johann-Mattis List"],"pdf_url":"https://arxiv.org/pdf/2502.11688v1.pdf","comment":"Submitted to the 63rd Annual Meeting of the Association for\n  Computational Linguistics, Vienna, Austria"},{"id":"http://arxiv.org/abs/2502.11684v1","updated":"2025-02-17T11:22:24Z","published":"2025-02-17T11:22:24Z","title":"MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps\n  through Fill-in-the-Middle Task","summary":"  Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures.\n","authors":["Yuchen Yan","Yongliang Shen","Yang Liu","Jin Jiang","Xin Xu","Mengdi Zhang","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2502.11684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13144v5","updated":"2025-02-17T11:17:41Z","published":"2024-06-19T01:37:10Z","title":"DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party\n  Dialogue Understanding of Conversation Systems","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversation systems, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nsystems often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, a conversation system is assigned the role of a\ncharacter from popular TV shows, requiring it to respond to spontaneous\nquestions using past dialogue information and to distinguish between known and\nunknown information. Key features of DialSim include assessing the system's\nability to respond within a reasonable time limit, handling long-term\nmulti-party dialogues, and evaluating performance under randomized questioning\nwith LongDialQA, a novel, high-quality question-answering dataset. Our\nexperiments using DialSim reveal the strengths and weaknesses of the latest\nconversation systems, offering valuable insights for future advancements in\nconversational AI. DialSim is available at https://dialsim.github.io/.\n","authors":["Jiho Kim","Woosog Chay","Hyeonji Hwang","Daeun Kyung","Hyunseung Chung","Eunbyeol Cho","Yohan Jo","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2406.13144v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11681v1","updated":"2025-02-17T11:16:19Z","published":"2025-02-17T11:16:19Z","title":"RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars","summary":"  Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.\n","authors":["Yuncheng Hua","Lizhen Qu","Zhuang Li","Hao Xue","Flora D. Salim","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2502.11681v1.pdf","comment":"37 pages, 1 figure, 20 tables; The paper is under review"},{"id":"http://arxiv.org/abs/2502.11678v1","updated":"2025-02-17T11:12:47Z","published":"2025-02-17T11:12:47Z","title":"Exploring LLM-based Student Simulation for Metacognitive Cultivation","summary":"  Metacognitive education plays a crucial role in cultivating students'\nself-regulation and reflective thinking, providing essential support for those\nwith learning difficulties through academic advising. Simulating students with\ninsufficient learning capabilities using large language models offers a\npromising approach to refining pedagogical methods without ethical concerns.\nHowever, existing simulations often fail to authentically represent students'\nlearning struggles and face challenges in evaluation due to the lack of\nreliable metrics and ethical constraints in data collection. To address these\nissues, we propose a pipeline for automatically generating and filtering\nhigh-quality simulated student agents. Our approach leverages a two-round\nautomated scoring system validated by human experts and employs a score\npropagation module to obtain more consistent scores across the student graph.\nExperimental results demonstrate that our pipeline efficiently identifies\nhigh-quality student agents, and we discuss the traits that influence the\nsimulation's effectiveness. By simulating students with varying degrees of\nlearning difficulties, our work paves the way for broader applications in\npersonalized learning and educational assessment.\n","authors":["Haoxuan Li","Jifan Yu","Xin Cong","Yang Dang","Yisi Zhan","Huiqin Liu","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11678v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.11677v1","updated":"2025-02-17T11:11:09Z","published":"2025-02-17T11:11:09Z","title":"Towards Fully Exploiting LLM Internal States to Enhance Knowledge\n  Boundary Perception","summary":"  Large language models (LLMs) exhibit impressive performance across diverse\ntasks but often struggle to accurately gauge their knowledge boundaries,\nleading to confident yet incorrect responses. This paper explores leveraging\nLLMs' internal states to enhance their perception of knowledge boundaries from\nefficiency and risk perspectives. We investigate whether LLMs can estimate\ntheir confidence using internal states before response generation, potentially\nsaving computational resources. Our experiments on datasets like Natural\nQuestions, HotpotQA, and MMLU reveal that LLMs demonstrate significant\npre-generation perception, which is further refined post-generation, with\nperception gaps remaining stable across varying conditions. To mitigate risks\nin critical domains, we introduce Consistency-based Confidence Calibration\n($C^3$), which assesses confidence consistency through question reformulation.\n$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,\nenhancing the unknown perception rate by 5.6\\% on NQ and 4.9\\% on HotpotQA. Our\nfindings suggest that pre-generation confidence estimation can optimize\nefficiency, while $C^3$ effectively controls output risks, advancing the\nreliability of LLMs in practical applications.\n","authors":["Shiyu Ni","Keping Bi","Jiafeng Guo","Lulu Yu","Baolong Bi","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.11677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14157v2","updated":"2025-02-17T11:10:46Z","published":"2024-10-18T03:48:53Z","title":"Beyond Autoregression: Discrete Diffusion for Complex Reasoning and\n  Planning","summary":"  Autoregressive language models, despite their impressive capabilities,\nstruggle with complex reasoning and long-term planning tasks. We introduce\ndiscrete diffusion models as a novel solution to these challenges. Through the\nlens of subgoal imbalance, we demonstrate how diffusion models effectively\nlearn difficult subgoals that elude autoregressive approaches. We propose\nMulti-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on\ndifficulty during learning. On complex tasks like Countdown, Sudoku, and\nBoolean Satisfiability Problems, MDM significantly outperforms autoregressive\nmodels without using search techniques. For instance, MDM achieves 91.5\\% and\n100\\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\\% and\n20.7\\% for autoregressive models. Our work highlights the potential of\ndiffusion-based approaches in advancing AI capabilities for sophisticated\nlanguage understanding and problem-solving tasks.\n","authors":["Jiacheng Ye","Jiahui Gao","Shansan Gong","Lin Zheng","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.14157v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2402.02243v2","updated":"2025-02-17T11:09:58Z","published":"2024-02-03T19:19:34Z","title":"Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding","summary":"  Apart from what (little) OpenAI may be concealing from us, we all know\n(roughly) how ChatGPT works (its huge text database, its statistics, its vector\nrepresentations, and their huge number of parameters, its next-word training,\nand so on). But none of us can say (hand on heart) that we are not surprised by\nwhat ChatGPT has proved to be able to do with these resources. This has even\ndriven some of us to conclude that ChatGPT actually understands. It is not true\nthat it understands. But it is also not true that we understand how it can do\nwhat it can do. I will suggest some hunches about benign biases: convergent\nconstraints that emerge at LLM scale that may be helping ChatGPT do so much\nbetter than we would have expected. These biases are inherent in the nature of\nlanguage itself, at LLM scale, and they are closely linked to what it is that\nChatGPT lacks, which is direct sensorimotor grounding to connect its words to\ntheir referents and its propositions to their meanings. These convergent biases\nare related to (1) the parasitism of indirect verbal grounding on direct\nsensorimotor grounding, (2) the circularity of verbal definition, (3) the\nmirroring of language production and comprehension, (4) iconicity in\npropositions at LLM scale, (5) computational counterparts of human categorical\nperception in category learning by neural nets, and perhaps also (6) a\nconjecture by Chomsky about the laws of thought. The exposition will be in the\nform of a dialogue with ChatGPT-4.\n","authors":["Stevan Harnad"],"pdf_url":"https://arxiv.org/pdf/2402.02243v2.pdf","comment":"54 pages, 29 references"},{"id":"http://arxiv.org/abs/2502.11671v1","updated":"2025-02-17T11:00:40Z","published":"2025-02-17T11:00:40Z","title":"Diversity-Oriented Data Augmentation with Large Language Models","summary":"  Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.\n","authors":["Zaitian Wang","Jinghan Zhang","Xinhao Zhang","Kunpeng Liu","Pengfei Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11656v1","updated":"2025-02-17T10:47:17Z","published":"2025-02-17T10:47:17Z","title":"Uncovering the Impact of Chain-of-Thought Reasoning for Direct\n  Preference Optimization: Lessons from Text-to-SQL","summary":"  Direct Preference Optimization (DPO) has proven effective in complex\nreasoning tasks like math word problems and code generation. However, when\napplied to Text-to-SQL datasets, it often fails to improve performance and can\neven degrade it. Our investigation reveals the root cause: unlike math and code\ntasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO,\nText-to-SQL datasets typically include only final answers (gold SQL queries)\nwithout detailed CoT solutions. By augmenting Text-to-SQL datasets with\nsynthetic CoT solutions, we achieve, for the first time, consistent and\nsignificant performance improvements using DPO. Our analysis shows that CoT\nreasoning is crucial for unlocking DPO's potential, as it mitigates reward\nhacking, strengthens discriminative capabilities, and improves scalability.\nThese findings offer valuable insights for building more robust Text-to-SQL\nmodels. To support further research, we publicly release the code and\nCoT-enhanced datasets.\n","authors":["Hanbing Liu","Haoyang Li","Xiaokang Zhang","Ruotong Chen","Haiyong Xu","Tian Tian","Qi Qi","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07546v2","updated":"2025-02-17T10:41:01Z","published":"2024-11-12T04:50:10Z","title":"Contrastive Language Prompting to Ease False Positives in Medical\n  Anomaly Detection","summary":"  A pre-trained visual-language model, contrastive language-image pre-training\n(CLIP), successfully accomplishes various downstream tasks with text prompts,\nsuch as finding images or localizing regions within the image. Despite CLIP's\nstrong multi-modal data capabilities, it remains limited in specialized\nenvironments, such as medical applications. For this purpose, many CLIP\nvariants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives\nrelated to normal regions persist. Thus, we aim to present a simple yet\nimportant goal of reducing false positives in medical anomaly detection. We\nintroduce a Contrastive LAnguage Prompting (CLAP) method that leverages both\npositive and negative text prompts. This straightforward approach identifies\npotential lesion regions by visual attention to the positive prompts in the\ngiven image. To reduce false positives, we attenuate attention on normal\nregions using negative prompts. Extensive experiments with the BMAD dataset,\nincluding six biomedical benchmarks, demonstrate that CLAP method enhances\nanomaly detection performance. Our future plans include developing an automated\nfine prompting method for more practical usage.\n","authors":["YeongHyeon Park","Myung Jin Kim","Hyeong Seok Kim"],"pdf_url":"https://arxiv.org/pdf/2411.07546v2.pdf","comment":"4 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.06635v3","updated":"2025-02-17T10:40:56Z","published":"2024-09-10T16:46:18Z","title":"MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders","summary":"  The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.\n","authors":["Wenyu Zhang","Shuo Sun","Bin Wang","Xunlong Zou","Zhuohan Liu","Yingxu He","Geyu Lin","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2409.06635v3.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.11645v1","updated":"2025-02-17T10:39:04Z","published":"2025-02-17T10:39:04Z","title":"Deviation Ratings: A General, Clone-Invariant Rating Method","summary":"  Many real-world multi-agent or multi-task evaluation scenarios can be\nnaturally modelled as normal-form games due to inherent strategic (adversarial,\ncooperative, and mixed motive) interactions. These strategic interactions may\nbe agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or\ncomplementary (e.g. niche finding and specialization). In such a formulation,\nit is the strategies (actions, policies, agents, models, tasks, prompts, etc.)\nthat are rated. However, the rating problem is complicated by redundancy and\ncomplexity of N-player strategic interactions. Repeated or similar strategies\ncan distort ratings for those that counter or complement them. Previous work\nproposed ``clone invariant'' ratings to handle such redundancies, but this was\nlimited to two-player zero-sum (i.e. strictly competitive) interactions. This\nwork introduces the first N-player general-sum clone invariant rating, called\ndeviation ratings, based on coarse correlated equilibria. The rating is\nexplored on several domains including LLMs evaluation.\n","authors":["Luke Marris","Siqi Liu","Ian Gemp","Georgios Piliouras","Marc Lanctot"],"pdf_url":"https://arxiv.org/pdf/2502.11645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11633v1","updated":"2025-02-17T10:24:07Z","published":"2025-02-17T10:24:07Z","title":"CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and\n  Training Efficiency","summary":"  Cross-modal text-molecule retrieval task bridges molecule structures and\nnatural language descriptions. Existing methods predominantly focus on aligning\ntext modality and molecule modality, yet they overlook adaptively adjusting the\nlearning states at different training stages and enhancing training efficiency.\nTo tackle these challenges, this paper proposes a Curriculum Learning-bAsed\ncroSS-modal text-molecule training framework (CLASS), which can be integrated\nwith any backbone to yield promising performance improvement. Specifically, we\nquantify the sample difficulty considering both text modality and molecule\nmodality, and design a sample scheduler to introduce training samples via an\neasy-to-difficult paradigm as the training advances, remarkably reducing the\nscale of training samples at the early stage of training and improving training\nefficiency. Moreover, we introduce adaptive intensity learning to increase the\ntraining intensity as the training progresses, which adaptively controls the\nlearning intensity across all curriculum stages. Experimental results on the\nChEBI-20 dataset demonstrate that our proposed method gains superior\nperformance, simultaneously achieving prominent time savings.\n","authors":["Hongyan Wu","Peijian Zeng","Weixiong Zheng","Lianxi Wang","Nankai Lin","Shengyi Jiang","Aimin Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11633v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2410.02892v2","updated":"2025-02-17T10:22:52Z","published":"2024-10-03T18:30:47Z","title":"The Role of Deductive and Inductive Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning tasks, yet their reliance on static prompt structures and limited\nadaptability to complex scenarios remains a significant challenge. In this\npaper, we propose the Deductive and InDuctive(DID) method, a novel framework\nthat enhances LLM reasoning by dynamically integrating both deductive and\ninductive reasoning approaches. Drawing from cognitive science principles, DID\nimplements a dual-metric complexity evaluation system that combines Littlestone\ndimension and information entropy to precisely assess task difficulty and guide\ndecomposition strategies. DID enables the model to progressively adapt its\nreasoning pathways based on problem complexity, mirroring human cognitive\nprocesses. We evaluate DID's effectiveness across multiple benchmarks,\nincluding the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset\nfor temporal reasoning. Our results demonstrate significant improvements in\nreasoning quality and solution accuracy - achieving 70.3% accuracy on AIW\n(compared to 62.2% for Tree of Thought) while maintaining lower computational\ncosts. The success of DID in improving LLM performance while preserving\ncomputational efficiency suggests promising directions for developing more\ncognitively aligned and capable language models. Our work contributes a\ntheoretically grounded, input-centric approach to enhancing LLM reasoning\ncapabilities, offering an efficient alternative to traditional\noutput-exploration methods.\n","authors":["Chengkun Cai","Xu Zhao","Haoliang Liu","Zhongyu Jiang","Tianfang Zhang","Zongkai Wu","Jenq-Neng Hwang","Serge Belongie","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.02892v2.pdf","comment":"4 figures"},{"id":"http://arxiv.org/abs/2410.10724v2","updated":"2025-02-17T10:07:26Z","published":"2024-10-14T17:04:41Z","title":"Large Language Models Are Active Critics in NLG Evaluation","summary":"  The conventional paradigm of using large language models (LLMs) for natural\nlanguage generation (NLG) evaluation relies on pre-defined task definitions and\nevaluation criteria, positioning LLMs as \"passive critics\" that strictly follow\ndeveloper-provided guidelines. However, human evaluators often apply implicit\ncriteria, and their expectations in practice can vary widely based on specific\nend-user needs. Consequently, these rigid evaluation methods struggle to adapt\nto diverse scenarios without extensive prompt customization. To address this,\nwe introduce Active-Critic, a novel LLM-based evaluator that transforms LLMs\ninto \"active critics'' capable of adapting to diverse NLG tasks using limited\nexample data. Active-Critic consists of two stages: (1) self-inferring the\ntarget NLG task and relevant evaluation criteria, and (2) dynamically\noptimizing prompts to produce human-aligned scores along with detailed\njustifications. Our experiments show that Active-Critic can generate nuanced,\ncontext-aware evaluation criteria, enabling it to achieve superior alignment\nwith human judgments across multiple tasks.\n","authors":["Shuying Xu","Junjie Hu","Ming Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.10724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11614v1","updated":"2025-02-17T09:56:46Z","published":"2025-02-17T09:56:46Z","title":"Is Human-Like Text Liked by Humans? Multilingual Human Detection and\n  Preference Against AI","summary":"  Prior studies have shown that distinguishing text generated by large language\nmodels (LLMs) from human-written one is highly challenging, and often no better\nthan random guessing. To verify the generalizability of this finding across\nlanguages and domains, we perform an extensive case study to identify the upper\nbound of human detection accuracy. Across 16 datasets covering 9 languages and\n9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus\nchallenging previous conclusions. We find that major gaps between human and\nmachine text lie in concreteness, cultural nuances, and diversity. Prompting by\nexplicitly explaining the distinctions in the prompts can partially bridge the\ngaps in over 50% of the cases. However, we also find that humans do not always\nprefer human-written text, particularly when they cannot clearly identify its\nsource.\n","authors":["Yuxia Wang","Rui Xing","Jonibek Mansurov","Giovanni Puccetti","Zhuohan Xie","Minh Ngoc Ta","Jiahui Geng","Jinyan Su","Mervat Abassy","Saad El Dine Ahmed","Kareem Elozeiri","Nurkhan Laiyk","Maiya Goloburda","Tarek Mahmoud","Raj Vardhan Tomar","Alexander Aziz","Ryuto Koike","Masahiro Kaneko","Artem Shelmanov","Ekaterina Artemova","Vladislav Mikhailov","Akim Tsvigun","Alham Fikri Aji","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2502.11614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11611v1","updated":"2025-02-17T09:55:32Z","published":"2025-02-17T09:55:32Z","title":"Identifying Gender Stereotypes and Biases in Automated Translation from\n  English to Italian using Similarity Networks","summary":"  This paper is a collaborative effort between Linguistics, Law, and Computer\nScience to evaluate stereotypes and biases in automated translation systems. We\nadvocate gender-neutral translation as a means to promote gender inclusion and\nimprove the objectivity of machine translation. Our approach focuses on\nidentifying gender bias in English-to-Italian translations. First, we define\ngender bias following human rights law and linguistics literature. Then we\nproceed by identifying gender-specific terms such as she/lei and he/lui as key\nelements. We then evaluate the cosine similarity between these target terms and\nothers in the dataset to reveal the model's perception of semantic relations.\nUsing numerical features, we effectively evaluate the intensity and direction\nof the bias. Our findings provide tangible insights for developing and training\ngender-neutral translation algorithms.\n","authors":["Fatemeh Mohammadi","Marta Annamaria Tamborini","Paolo Ceravolo","Costanza Nardocci","Samira Maghool"],"pdf_url":"https://arxiv.org/pdf/2502.11611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16176v2","updated":"2025-02-17T09:53:43Z","published":"2024-06-23T18:01:56Z","title":"GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large\n  Language Models on Graph Datasets","summary":"  Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to manipulate, program, and reason about\nstructured data, especially graphs. We introduce GraphEval36K, the first\ncomprehensive graph dataset, comprising 40 graph coding problems and 36,900\ntest cases to evaluate the ability of LLMs on graph problem-solving. Our\ndataset is categorized into eight primary and four sub-categories to ensure a\nthorough evaluation across different types of graphs. We benchmark ten LLMs,\nfinding that private models outperform open-source ones, though the gap is\nnarrowing. We also analyze the performance of LLMs across directed vs\nundirected graphs, different kinds of graph concepts, and network models.\nFurthermore, to improve the usability of our evaluation framework, we propose\nStructured Symbolic Decomposition (SSD), an instruction-based method designed\nto enhance LLM performance on complex graph tasks. Results show that SSD\nimproves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and\nClaude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.\n","authors":["Qiming Wu","Zichen Chen","Will Corcoran","Misha Sra","Ambuj K. Singh"],"pdf_url":"https://arxiv.org/pdf/2406.16176v2.pdf","comment":"The first two authors contributed equally to this work. This paper\n  has been accepted by NAACL 2025. GraphEval36K is available at\n  https://grapheval36k.github.io/"},{"id":"http://arxiv.org/abs/2502.11603v1","updated":"2025-02-17T09:43:36Z","published":"2025-02-17T09:43:36Z","title":"DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware\n  Prompting with Demonstration and Reasoning","summary":"  Large Language Models (LLMs) exhibit strong natural language processing\ncapabilities but also inherit and amplify societal biases, including gender\nbias, raising fairness concerns. Existing debiasing methods face significant\nlimitations: parameter tuning requires access to model weights, prompt-based\napproaches often degrade model utility, and optimization-based techniques lack\ngeneralizability. To address these challenges, we propose DR.GAP (Demonstration\nand Reasoning for Gender-Aware Prompting), an automated and model-agnostic\napproach that mitigates gender bias while preserving model performance. DR.GAP\nselects bias-revealing examples and generates structured reasoning to guide\nmodels toward more impartial responses. Extensive experiments on coreference\nresolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and\nLlama2-Alpaca) demonstrate its effectiveness, generalization ability, and\nrobustness. DR.GAP can generalize to vision-language models (VLMs), achieving\nsignificant bias reduction.\n","authors":["Hongye Qiu","Yue Xu","Meikang Qiu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11598v1","updated":"2025-02-17T09:34:19Z","published":"2025-02-17T09:34:19Z","title":"Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?","summary":"  The radioactive nature of Large Language Model (LLM) watermarking enables the\ndetection of watermarks inherited by student models when trained on the outputs\nof watermarked teacher models, making it a promising tool for preventing\nunauthorized knowledge distillation. However, the robustness of watermark\nradioactivity against adversarial actors remains largely unexplored. In this\npaper, we investigate whether student models can acquire the capabilities of\nteacher models through knowledge distillation while avoiding watermark\ninheritance. We propose two categories of watermark removal approaches:\npre-distillation removal through untargeted and targeted training data\nparaphrasing (UP and TP), and post-distillation removal through inference-time\nwatermark neutralization (WN). Extensive experiments across multiple model\npairs, watermarking schemes and hyper-parameter settings demonstrate that both\nTP and WN thoroughly eliminate inherited watermarks, with WN achieving this\nwhile maintaining knowledge transfer efficiency and low computational overhead.\nGiven the ongoing deployment of watermarking techniques in production LLMs,\nthese findings emphasize the urgent need for more robust defense strategies.\nOur code is available at\nhttps://github.com/THU-BPM/Watermark-Radioactivity-Attack.\n","authors":["Leyi Pan","Aiwei Liu","Shiyu Huang","Yijian Lu","Xuming Hu","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.11598v1.pdf","comment":"22 pages, 12 figures, 13 tables"},{"id":"http://arxiv.org/abs/2405.19799v3","updated":"2025-02-17T09:22:19Z","published":"2024-05-30T08:10:50Z","title":"Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation\n  in Dialogue","summary":"  In dialogue systems, discourse plays a crucial role in managing\nconversational focus and coordinating interactions. It consists of two key\nstructures: rhetorical structure and topic structure. The former captures the\nlogical flow of conversations, while the latter detects transitions between\ntopics. Together, they improve the ability of a dialogue system to track\nconversation dynamics and generate contextually relevant high-quality\nresponses. These structures are typically identified through discourse parsing\nand topic segmentation, respectively. However, existing supervised methods rely\non costly manual annotations, while unsupervised methods often focus on a\nsingle task, overlooking the deep linguistic interplay between rhetorical and\ntopic structures. To address these issues, we first introduce a unified\nrepresentation that integrates rhetorical and topic structures, ensuring\nsemantic consistency between them. Under the unified representation, we further\npropose two linguistically grounded hypotheses based on discourse theories: (1)\nLocal Discourse Coupling, where rhetorical cues dynamically enhance topic-aware\ninformation flow, and (2) Global Topology Constraint, where topic structure\npatterns probabilistically constrain rhetorical relation distributions.\nBuilding on the unified representation and two hypotheses, we propose an\nunsupervised mutual learning framework (UMLF) that jointly models rhetorical\nand topic structures, allowing them to mutually reinforce each other without\nrequiring additional annotations. We evaluate our approach on two rhetorical\ndatasets and three topic segmentation datasets. Experimental results\ndemonstrate that our method surpasses all strong baselines built on pre-trained\nlanguage models. Furthermore, when applied to LLMs, our framework achieves\nnotable improvements, demonstrating its effectiveness in improving discourse\nstructure modeling.\n","authors":["Jiahui Xu","Feng Jiang","Anningzhe Gao","Luis Fernando D'Haro","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2405.19799v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11578v1","updated":"2025-02-17T09:09:58Z","published":"2025-02-17T09:09:58Z","title":"Language Complexity Measurement as a Noisy Zero-Shot Proxy for\n  Evaluating LLM Performance","summary":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage generation but often face challenges in tasks requiring precise\ncalculations and structural analysis. This paper investigates the performance\nof state-of-the-art LLMs on language complexity measurement tasks, through the\ncomputation of the LIX readability metric and Average Dependency Distance\n(ADD). Using Swedish high school and university-level essays, we evaluate the\nmodels' abilities to compute LIX scores and perform dependency parsing,\ncomparing their results to established ground truths. Our findings reveal that\nwhile all models demonstrate some capacity for these tasks, ChatGPT-o1-mini\nperforms most consistently, achieving the highest accuracy in both LIX\ncomputation and dependency parsing. Additionally, we observe a strong\nsignificant correlation -0.875 p 0.026 (N=6) between the models' accuracy in\ncomputing LIX and their overall performance on the Massive Multitask Language\nUnderstanding (MMLU) benchmark. These results suggest that language complexity\nmeasurement abilities can serve as a noisy zero-shot proxies for assessing the\ngeneral capabilities of LLMs, providing a practical method for model evaluation\nwithout the need for extensive benchmarking datasets.\n","authors":["Birger Moell","Johan Boye"],"pdf_url":"https://arxiv.org/pdf/2502.11578v1.pdf","comment":"Submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2502.11573v1","updated":"2025-02-17T09:07:32Z","published":"2025-02-17T09:07:32Z","title":"InfiR : Crafting Effective Small Language Models and Multimodal Small\n  Language Models in Reasoning","summary":"  Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave made significant advancements in reasoning capabilities. However, they\nstill face challenges such as high computational demands and privacy concerns.\nThis paper focuses on developing efficient Small Language Models (SLMs) and\nMultimodal Small Language Models (MSLMs) that retain competitive reasoning\nabilities. We introduce a novel training pipeline that enhances reasoning\ncapabilities and facilitates deployment on edge devices, achieving\nstate-of-the-art performance while minimizing development costs. \\InfR~ aims to\nadvance AI systems by improving reasoning, reducing adoption barriers, and\naddressing privacy concerns through smaller model sizes. Resources are\navailable at https://github. com/Reallm-Labs/InfiR.\n","authors":["Congkai Xie","Shuo Cai","Wenjun Wang","Pengxiang Li","Zhijie Sang","Kejing Yang","Yiming Zhang","Zhen Li","Guanghao Zhu","Zeyu Liu","Yang Yu","Yuhang Liu","Su Lu","Baoyi He","Qi Zhou","Xiaotian Han","Jianbo Yuan","Shengyu Zhang","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11571v1","updated":"2025-02-17T09:05:21Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v1.pdf","comment":"to appear in ACL 2025"},{"id":"http://arxiv.org/abs/2407.16205v4","updated":"2025-02-17T09:00:28Z","published":"2024-07-23T06:14:41Z","title":"LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.\n","authors":["Shi Lin","Hongming Yang","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23166v2","updated":"2025-02-17T08:59:45Z","published":"2024-10-30T16:18:22Z","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","summary":"  The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.\n","authors":["Wenxiao Wang","Lihui Gu","Liye Zhang","Yunxiang Luo","Yi Dai","Chen Shen","Liang Xie","Binbin Lin","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.23166v2.pdf","comment":"20 pages, 5 figures, 12 tables. The code has been availabel:\n  https://github.com/cheerss/SciPIP"},{"id":"http://arxiv.org/abs/2502.11569v1","updated":"2025-02-17T08:59:16Z","published":"2025-02-17T08:59:16Z","title":"Towards Reasoning Ability of Small Language Models","summary":"  Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks.\n","authors":["Gaurav Srivastava","Shuxiang Cao","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10248v2","updated":"2025-02-17T08:58:33Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v2.pdf","comment":"36 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.11562v1","updated":"2025-02-17T08:52:39Z","published":"2025-02-17T08:52:39Z","title":"Reinforced Information Retrieval","summary":"  While retrieval techniques are widely used in practice, they still face\nsignificant challenges in cross-domain scenarios. Recently,\ngeneration-augmented methods have emerged as a promising solution to this\nproblem. These methods enhance raw queries by incorporating additional\ninformation from an LLM-based generator, facilitating more direct retrieval of\nrelevant documents. However, existing methods struggle with highly specialized\nsituations that require extensive domain expertise. To address this problem, we\npresent \\textbf{Reinforced-IR}, a novel approach that jointly adapts a\npre-trained retriever and generator for precise cross-domain retrieval. A key\ninnovation of Reinforced-IR is its \\textbf{Self-Boosting} framework, which\nenables retriever and generator to learn from each other's feedback.\nSpecifically, the generator is reinforced to generate query augmentations that\nenhance the retriever's performance, while the retriever is trained to better\ndiscriminate the relevant documents identified by the generator. This iterative\nprocess allows the end-to-end retrieval performance to be progressively\noptimized using an unlabeled corpus from the target domain. In our experiment,\nReinforced-IR outperforms existing domain adaptation methods by a large margin,\nleading to substantial improvements in retrieval quality across a wide range of\napplication scenarios.\n","authors":["Chaofan Li","Zheng Liu","Jianlyv Chen","Defu Lian","Yingxia Shao"],"pdf_url":"https://arxiv.org/pdf/2502.11562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16516v2","updated":"2025-02-17T08:46:24Z","published":"2024-12-21T07:33:55Z","title":"HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile\n  Device Scenarios","summary":"  Evaluating the performance of LLMs in multi-turn human-agent interactions\npresents significant challenges, particularly due to the complexity and\nvariability of user behavior. In this paper, we introduce HammerBench, a novel\nbenchmark framework for assessing LLMs' function-calling capabilities in\nreal-world, multi-turn dialogues. HammerBench simulates diverse mobile\nassistant use cases, incorporating imperfect instructions, dynamic\nquestion-answer trajectories, intent and argument shifts, and the indirect use\nof external information through pronouns. To construct this benchmark, we\ncurate a comprehensive dataset derived from popular mobile app functionalities\nand anonymized user logs, complemented by a cost-effective data generation\npipeline leveraging open-source models. HammerBench is further augmented with\nfine-grained interaction snapshots and metrics, enabling detailed evaluation of\nfunction-calling performance across individual conversational turns. We\ndemonstrate the effectiveness of HammerBench by evaluating several leading LLMs\nand uncovering key performance trends. Our experiments reveal that different\ntypes of parameter name errors are a significant source of failure across\ndifferent interaction scenarios, highlighting critical areas for further\nimprovement in LLM robustness for mobile assistant applications.\n","authors":["Jun Wang","Jiamu Zhou","Muning Wen","Xiaoyun Mo","Haoyu Zhang","Qiqiang Lin","Cheng Jin","Xihuai Wang","Weinan Zhang","Qiuying Peng","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01737v3","updated":"2025-02-17T08:44:36Z","published":"2024-01-29T09:07:40Z","title":"Assistive Large Language Model Agents for Socially-Aware Negotiation\n  Dialogues","summary":"  We develop assistive agents based on Large Language Models (LLMs) that aid\ninterlocutors in business negotiations. Specifically, we simulate business\nnegotiations by letting two LLM-based agents engage in role play. A third LLM\nacts as a remediator agent to rewrite utterances violating norms for improving\nnegotiation outcomes. We introduce a simple tuning-free and label-free\nIn-Context Learning (ICL) method to identify high-quality ICL exemplars for the\nremediator, where we propose a novel select criteria, called value impact, to\nmeasure the quality of the negotiation outcomes. We provide rich empirical\nevidence to demonstrate its effectiveness in negotiations across three\ndifferent negotiation topics. We have released our source code and the\ngenerated dataset at: https://github.com/tk1363704/SADAS.\n","authors":["Yuncheng Hua","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2402.01737v3.pdf","comment":"28 pages, 3 figures, 14 tables; The paper has been published in the\n  Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.11559v1","updated":"2025-02-17T08:44:04Z","published":"2025-02-17T08:44:04Z","title":"Auto-Search and Refinement: An Automated Framework for Gender Bias\n  Mitigation in Large Language Models","summary":"  Pre-training large language models (LLMs) on vast text corpora enhances\nnatural language processing capabilities but risks encoding social biases,\nparticularly gender bias. While parameter-modification methods like fine-tuning\nmitigate bias, they are resource-intensive, unsuitable for closed-source\nmodels, and lack adaptability to evolving societal norms. Instruction-based\napproaches offer flexibility but often compromise task performance. To address\nthese limitations, we propose $\\textit{FaIRMaker}$, an automated and\nmodel-independent framework that employs an $\\textbf{auto-search and\nrefinement}$ paradigm to adaptively generate Fairwords, which act as\ninstructions integrated into input queries to reduce gender bias and enhance\nresponse quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$\nautomatically searches for and dynamically refines Fairwords, effectively\nmitigating gender bias while preserving task integrity and ensuring\ncompatibility with both API-based and open-source LLMs.\n","authors":["Yue Xu","Chengyan Fu","Li Xiong","Sibei Yang","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15490v2","updated":"2025-02-17T08:36:54Z","published":"2024-06-18T13:01:30Z","title":"Causal Discovery Inspired Unsupervised Domain Adaptation for\n  Emotion-Cause Pair Extraction","summary":"  This paper tackles the task of emotion-cause pair extraction in the\nunsupervised domain adaptation setting. The problem is challenging as the\ndistributions of the events causing emotions in target domains are dramatically\ndifferent than those in source domains, despite the distributions of emotional\nexpressions between domains are overlapped. Inspired by causal discovery, we\npropose a novel deep latent model in the variational autoencoder (VAE)\nframework, which not only captures the underlying latent structures of data but\nalso utilizes the easily transferable knowledge of emotions as the bridge to\nlink the distributions of events in different domains. To facilitate knowledge\ntransfer across domains, we also propose a novel variational posterior\nregularization technique to disentangle the latent representations of emotions\nfrom those of events in order to mitigate the damage caused by the spurious\ncorrelations related to the events in source domains. Through extensive\nexperiments, we demonstrate that our model outperforms the strongest baseline\nby approximately 11.05\\% on a Chinese benchmark and 2.45\\% on a English\nbenchmark in terms of weighted-average F1 score. We have released our source\ncode and the generated dataset publicly at:\nhttps://github.com/tk1363704/CAREL-VAE.\n","authors":["Yuncheng Hua","Yujin Huang","Shuo Huang","Tao Feng","Lizhen Qu","Chris Bain","Richard Bassed","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.15490v2.pdf","comment":"18 pages, 6 figures, 5 tables. The paper has been published in the\n  Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.11554v1","updated":"2025-02-17T08:36:12Z","published":"2025-02-17T08:36:12Z","title":"Toward Metaphor-Fluid Conversation Design for Voice User Interfaces","summary":"  Metaphors play a critical role in shaping user experiences with Voice User\nInterfaces (VUIs), yet existing designs often rely on static, human-centric\nmetaphors that fail to adapt to diverse contexts and user needs. This paper\nintroduces Metaphor-Fluid Design, a novel approach that dynamically adjusts\nmetaphorical representations based on conversational use-contexts. We compare\nthis approach to a Default VUI, which characterizes the present implementation\nof commercial VUIs commonly designed around the persona of an assistant,\noffering a uniform interaction style across contexts. In Study 1 (N=130),\nmetaphors were mapped to four key use-contexts-commands, information seeking,\nsociality, and error recovery-along the dimensions of formality and hierarchy,\nrevealing distinct preferences for task-specific metaphorical designs. Study 2\n(N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the\nMetaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and\nlikability by aligning better with user expectations for different contexts.\nHowever, individual differences in metaphor preferences highlight the need for\npersonalization. These findings challenge the one-size-fits-all paradigm of VUI\ndesign and demonstrate the potential of Metaphor-Fluid Design to create more\nadaptive and engaging human-AI interactions.\n","authors":["Smit Desai","Jessie Chin","Dakuo Wang","Benjamin Cowan","Michael Twidale"],"pdf_url":"https://arxiv.org/pdf/2502.11554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11546v1","updated":"2025-02-17T08:28:29Z","published":"2025-02-17T08:28:29Z","title":"DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection","summary":"  The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and clean multilingual datasets. In this\npaper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus built using newly extracted Common Crawl data\nand existing multilingual datasets. DCAD-2000 includes over 2,282 languages,\n46.72TB of data, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof current data cleaning methods, which rely on manual heuristic thresholds, we\npropose reframing data cleaning as an anomaly detection task. This dynamic\nfiltering approach significantly enhances data quality by identifying and\nremoving noisy or anomalous content. We evaluate the quality of DCAD-2000 on\nthe FineTask benchmark, demonstrating substantial improvements in multilingual\ndataset quality and task performance.\n","authors":["Yingli Shen","Wen Lai","Shuo Wang","Xueren Zhang","Kangyang Luo","Alexander Fraser","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11544v1","updated":"2025-02-17T08:23:46Z","published":"2025-02-17T08:23:46Z","title":"Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through\n  Comprehensive Analysis","summary":"  The o1-Like LLMs are transforming AI by simulating human cognitive processes,\nbut their performance in multilingual machine translation (MMT) remains\nunderexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks\nand (2) what factors influence their translation quality. We evaluate multiple\no1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o.\nResults show that o1-Like LLMs establish new multilingual translation\nbenchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They\ndemonstrate strengths in historical and cultural translation but exhibit a\ntendency for rambling issues in Chinese-centric outputs. Further analysis\nreveals three key insights: (1) High inference costs and slower processing\nspeeds make complex translation tasks more resource-intensive. (2) Translation\nquality improves with model size, enhancing commonsense reasoning and cultural\ntranslation. (3) The temperature parameter significantly impacts output\nquality-lower temperatures yield more stable and accurate translations, while\nhigher temperatures reduce coherence and precision.\n","authors":["Andong Chen","Yuchen Song","Wenxin Zhu","Kehai Chen","Muyun Yang","Tiejun Zhao","Min zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11686v4","updated":"2025-02-17T08:19:48Z","published":"2024-07-16T13:03:58Z","title":"CCoE: A Compact and Efficient LLM Framework with Multi-Expert\n  Collaboration for Resource-Limited Settings","summary":"  Large Language Models (LLMs) have achieved exceptional performance across\ndiverse domains through training on massive datasets. However, scaling LLMs to\nsupport multiple downstream domain applications remains a significant\nchallenge, especially under resource constraints. Existing approaches often\nstruggle to balance performance across multiple domains with resource\nefficiency, limiting their broader applicability. To address this, we introduce\nthe CCoE architecture, a modular framework that seamlessly integrates\ndomain-specific experts into a unified LLM. By leveraging independently trained\nexpert subnetworks on a shared backbone partition, CCoE achieves\nstate-of-the-art performance while significantly reducing the resource\nrequirements for multi-expert deployments. Furthermore, rule-based gating and\nexpert planning in CCoE enable flexible task allocation, promoting expert\ncollaboration to handle complex reasoning tasks. CCoE not only reduces\ninference costs but also provides a flexible and scalable solution for\nintegrating domain expertise across diverse applications. Experiments on five\ndomains demonstrate that CCoE achieves comparable performance to current\ndomain-specific LLMs. Moreover, compared to existing multi-domain model\nensemble methods, CCoE reduces memory usage by 61.3%, while improving inference\nefficiency by 0.76x over parameter-efficient multi-expert integration\napproaches.\n","authors":["Shaomang Huang","Jianfeng Pan","Min Peng","Hanzhong Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.11686v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11541v1","updated":"2025-02-17T08:12:49Z","published":"2025-02-17T08:12:49Z","title":"MuSC: Improving Complex Instruction Following with Multi-granularity\n  Self-Contrastive Training","summary":"  Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods.\n","authors":["Hui Huang","Jiaheng Liu","Yancheng He","Shilong Li","Bing Xu","Conghui Zhu","Muyun Yang","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.11541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11790v2","updated":"2025-02-17T08:06:13Z","published":"2025-01-20T23:41:22Z","title":"Benchmarking Large Language Models via Random Variables","summary":"  Recent studies have raised concerns about the reliability of current\nmathematical benchmarks, highlighting issues such as simplistic design and\npotential data contamination. Therefore, creating a reliable benchmark that\neffectively evaluates the genuine capabilities of large language models (LLMs)\nin mathematical reasoning remains a significant challenge. To address this, we\npropose RV-Bench, a framework for Benchmarking LLMs via Random Variables in\nmathematical reasoning. Specifically, the background content of a random\nvariable question (RV question) mirrors the original problem in existing\nbenchmarks, but the variable combinations are randomized, making it \"unseen\" by\nthe LLMs. Models must completely understand the question pattern of the\noriginal problem to correctly answer RV questions with various variable values.\nAs a result, the LLM's genuine capability in mathematical reasoning is\nreflected by its accuracy and robustness on RV-Bench. We conducted extensive\nexperiments on over 30 representative LLMs across more than 1000 RV questions.\nOur findings suggest that LLMs exhibit an imbalance in proficiency between\nencountered and \"unseen\" data domains. Proficiency generalization across\nsimilar mathematical reasoning tasks is verified to be limited by accuracy and\nrobustness, but it can still be enhanced through test-time scaling.\n","authors":["Zijin Hong","Hao Wu","Su Dong","Junnan Dong","Yilin Xiao","Yujing Zhang","Zhu Wang","Feiran Huang","Linyi Li","Hongxia Yang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2501.11790v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.11533v1","updated":"2025-02-17T08:04:52Z","published":"2025-02-17T08:04:52Z","title":"Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of\n  Stealing Privacy","summary":"  Model merging is a widespread technology in large language models (LLMs) that\nintegrates multiple task-specific LLMs into a unified one, enabling the merged\nmodel to inherit the specialized capabilities of these LLMs. Most task-specific\nLLMs are sourced from open-source communities and have not undergone rigorous\nauditing, potentially imposing risks in model merging. This paper highlights an\noverlooked privacy risk: \\textit{an unsafe model could compromise the privacy\nof other LLMs involved in the model merging.} Specifically, we propose PhiMM, a\nprivacy attack approach that trains a phishing model capable of stealing\nprivacy using a crafted privacy phishing instruction dataset. Furthermore, we\nintroduce a novel model cloaking method that mimics a specialized capability to\nconceal attack intent, luring users into merging the phishing model. Once\nvictims merge the phishing model, the attacker can extract personally\nidentifiable information (PII) or infer membership information (MI) by querying\nthe merged model with the phishing instruction. Experimental results show that\nmerging a phishing model increases the risk of privacy breaches. Compared to\nthe results before merging, PII leakage increased by 3.9\\% and MI leakage\nincreased by 17.4\\% on average. We release the code of PhiMM through a link.\n","authors":["Zhenyuan Guo","Yi Shi","Wenlong Meng","Chen Gong","Chengkun Wei","Wenzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09566v2","updated":"2025-02-17T08:04:06Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v2.pdf","comment":"13 pages, 4 figures, 4 tables (updated version, fixed typos and\n  formatting)"},{"id":"http://arxiv.org/abs/2502.11525v1","updated":"2025-02-17T07:54:50Z","published":"2025-02-17T07:54:50Z","title":"Training Large Language Models to be Better Rule Followers","summary":"  Large language models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, they often exhibit unexpected failures in seemingly\nstraightforward tasks, suggesting a reliance on case-based reasoning rather\nthan rule-based reasoning. While the vast training corpus of LLMs contains\nnumerous textual \"rules\", current training methods fail to leverage these rules\neffectively. Crucially, the relationships between these \"rules\" and their\ncorresponding \"instances\" are not explicitly modeled. As a result, while LLMs\ncan often recall rules with ease, they fail to apply these rules strictly and\nconsistently in relevant reasoning scenarios. In this paper, we investigate the\nrule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning\n(Meta-RFFT) to enhance the cross-task transferability of rule-following\nabilities. We first construct a dataset of 88 tasks requiring following rules,\nencompassing diverse reasoning domains. We demonstrate through extensive\nexperiments that models trained on large-scale rule-following tasks are better\nrule followers, outperforming the baselines in both downstream fine-tuning and\nfew-shot prompting scenarios. This highlights the cross-task transferability of\nmodels with the aid of Meta-RFFT. Furthermore, we examine the influence of\nfactors such as dataset size, rule formulation, and in-context learning.\n","authors":["Yi Hu","Shijia Kang","Haotong Yang","Haotian Xu","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11520v1","updated":"2025-02-17T07:41:27Z","published":"2025-02-17T07:41:27Z","title":"AURORA:Automated Training Framework of Universal Process Reward Models\n  via Ensemble Prompting and Reverse Verification","summary":"  The reasoning capabilities of advanced large language models (LLMs) like o1\nhave revolutionized artificial intelligence applications. Nevertheless,\nevaluating and optimizing complex reasoning processes remain significant\nchallenges due to diverse policy distributions and the inherent limitations of\nhuman effort and accuracy. In this paper, we present AURORA, a novel automated\nframework for training universal process reward models (PRMs) using ensemble\nprompting and reverse verification. The framework employs a two-phase approach:\nFirst, it uses diverse prompting strategies and ensemble methods to perform\nautomated annotation and evaluation of processes, ensuring robust assessments\nfor reward learning. Second, it leverages practical reference answers for\nreverse verification, enhancing the model's ability to validate outputs and\nimproving training accuracy. To assess the framework's performance, we extend\nbeyond the existing ProcessBench benchmark by introducing UniversalBench, which\nevaluates reward predictions across full trajectories under diverse policy\ndistribtion with long Chain-of-Thought (CoT) outputs. Experimental results\ndemonstrate that AURORA enhances process evaluation accuracy, improves PRMs'\naccuracy for diverse policy distributions and long-CoT responses. The project\nwill be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is\navailable at https://huggingface.co/infly/Universal-PRM-7B.\n","authors":["Xiaoyu Tan","Tianchu Yao","Chao Qu","Bin Li","Minghao Yang","Dakuan Lu","Haozhe Wang","Xihe Qiu","Wei Chu","Yinghui Xu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2502.11520v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.11517v1","updated":"2025-02-17T07:39:16Z","published":"2025-02-17T07:39:16Z","title":"Learning to Keep a Promise: Scaling Language Model Decoding Parallelism\n  with Learned Asynchronous Decoding","summary":"  Decoding with autoregressive large language models (LLMs) traditionally\noccurs sequentially, generating one token after another. An emerging line of\nwork explored parallel decoding by identifying and simultaneously generating\nsemantically independent chunks of LLM responses. However, these techniques\nrely on hand-crafted heuristics tied to syntactic structures like lists and\nparagraphs, making them rigid and imprecise. We present PASTA, a learning-based\nsystem that teaches LLMs to identify semantic independence and express parallel\ndecoding opportunities in their own responses. At its core are PASTA-LANG and\nits interpreter: PASTA-LANG is an annotation language that enables LLMs to\nexpress semantic independence in their own responses; the language interpreter\nacts on these annotations to orchestrate parallel decoding on-the-fly at\ninference time. Through a two-stage finetuning process, we train LLMs to\ngenerate PASTA-LANG annotations that optimize both response quality and\ndecoding speed. Evaluation on AlpacaEval, an instruction following benchmark,\nshows that our approach Pareto-dominates existing methods in terms of decoding\nspeed and response quality; our results demonstrate geometric mean speedups\nranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to\n-7.1%, measured by length-controlled win rates against sequential decoding\nbaseline.\n","authors":["Tian Jin","Ellie Y. Cheng","Zack Ankner","Nikunj Saunshi","Blake M. Elias","Amir Yazdanbakhsh","Jonathan Ragan-Kelley","Suvinay Subramanian","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2502.11517v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.02904v3","updated":"2025-02-17T07:37:57Z","published":"2025-02-05T05:57:37Z","title":"ScholaWrite: A Dataset of End-to-End Scholarly Writing Process","summary":"  Writing is a cognitively demanding task involving continuous decision-making,\nheavy use of working memory, and frequent switching between multiple\nactivities. Scholarly writing is particularly complex as it requires authors to\ncoordinate many pieces of multiform knowledge. To fully understand writers'\ncognitive thought process, one should fully decode the end-to-end writing data\n(from individual ideas to final manuscript) and understand their complex\ncognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset, a\nfirst-of-its-kind keystroke corpus of an end-to-end scholarly writing process\nfor complete manuscripts, with thorough annotations of cognitive writing\nintentions behind each keystroke. Our dataset includes LaTeX-based keystroke\ndata from five preprints with nearly 62K total text changes and annotations\nacross 4 months of paper writing. ScholaWrite shows promising usability and\napplications (e.g., iterative self-writing), demonstrating the importance of\ncollection of end-to-end writing data, rather than the final manuscript, for\nthe development of future writing assistants to support the cognitive thinking\nprocess of scientists. Our de-identified data examples and code are available\non our project page.\n","authors":["Linghe Wang","Minhwa Lee","Ross Volkov","Luan Tuyen Chau","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2502.02904v3.pdf","comment":"Equal contribution: Linghe Wang, Minhwa Lee | project page:\n  https://minnesotanlp.github.io/scholawrite/"},{"id":"http://arxiv.org/abs/2502.11514v1","updated":"2025-02-17T07:29:01Z","published":"2025-02-17T07:29:01Z","title":"Investigating Inference-time Scaling for Chain of Multi-modal Thought: A\n  Preliminary Study","summary":"  Recently, inference-time scaling of chain-of-thought (CoT) has been\ndemonstrated as a promising approach for addressing multi-modal reasoning\ntasks. While existing studies have predominantly centered on text-based\nthinking, the integration of both visual and textual modalities within the\nreasoning process remains unexplored. In this study, we pioneer the exploration\nof inference-time scaling with multi-modal thought, aiming to bridge this gap.\nTo provide a comprehensive analysis, we systematically investigate popular\nsampling-based and tree search-based inference-time scaling methods on 10\nchallenging tasks spanning various domains. Besides, we uniformly adopt a\nconsistency-enhanced verifier to ensure effective guidance for both methods\nacross different thought paradigms. Results show that multi-modal thought\npromotes better performance against conventional text-only thought, and\nblending the two types of thought fosters more diverse thinking. Despite these\nadvantages, multi-modal thoughts necessitate higher token consumption for\nprocessing richer visual inputs, which raises concerns in practical\napplications. We hope that our findings on the merits and drawbacks of this\nresearch line will inspire future works in the field.\n","authors":["Yujie Lin","Ante Wang","Moye Chen","Jingyao Liu","Hao Liu","Jinsong Su","Xinyan Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.11514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11139v3","updated":"2025-02-17T07:25:50Z","published":"2024-06-17T01:54:27Z","title":"Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance","summary":"  The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.\n","authors":["Somnath Banerjee","Avik Halder","Rajarshi Mandal","Sayan Layek","Ian Soboroff","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2406.11139v3.pdf","comment":"Accepted at NAACL 2025 (Industry track)"},{"id":"http://arxiv.org/abs/2410.12480v2","updated":"2025-02-17T07:23:58Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema matching (SM) and entity matching (EM) tasks are crucial for data\nintegration. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. This study presents the Knowledge-Compliant Matching Framework\n(KcMF), an LLM-based approach that addresses these issues without the need for\ndomain-specific fine-tuning. KcMF employs a once-and-for-all pseudo-code-based\ntask decomposition strategy to adopt natural language statements that guide LLM\nreasoning and reduce confusion across various task types. We also propose two\nmechanisms, Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build\ndomain knowledge sets when unstructured domain knowledge is lacking. Moreover,\nwe introduce a result-ensemble strategy to leverage multiple knowledge sources\nand suppress badly formatted outputs. Extensive evaluations confirm that KcMF\nclearly enhances five LLM backbones in both SM and EM tasks while outperforming\nthe non-LLM competitors by an average F1-score of 17.93%.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v2.pdf","comment":"under reveiw; new results and analysis added, typos corrected"},{"id":"http://arxiv.org/abs/2404.02115v3","updated":"2025-02-17T07:23:38Z","published":"2024-04-02T17:18:48Z","title":"GINopic: Topic Modeling with Graph Isomorphism Network","summary":"  Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2404.02115v3.pdf","comment":"Accepted as a long paper for NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2502.11508v1","updated":"2025-02-17T07:17:27Z","published":"2025-02-17T07:17:27Z","title":"Chinese Spelling Correction: A Comprehensive Survey of Progress,\n  Challenges, and Opportunities","summary":"  Chinese Spelling Correction (CSC) is a critical task in natural language\nprocessing, aimed at detecting and correcting spelling errors in Chinese text.\nThis survey provides a comprehensive overview of CSC, tracing its evolution\nfrom pre-trained language models to large language models, and critically\nanalyzing their respective strengths and weaknesses in this domain. Moreover,\nwe further present a detailed examination of existing benchmark datasets,\nhighlighting their inherent challenges and limitations. Finally, we propose\npromising future research directions, particularly focusing on leveraging the\npotential of LLMs and their reasoning capabilities for improved CSC\nperformance. To the best of our knowledge, this is the first comprehensive\nsurvey dedicated to the field of CSC. We believe this work will serve as a\nvaluable resource for researchers, fostering a deeper understanding of the\nfield and inspiring future advancements.\n","authors":["Changchun Liu","Kai Zhang","Junzhe Jiang","Zixiao Kong","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v3","updated":"2025-02-17T07:13:46Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  As a powerful all-weather Earth observation tool, synthetic aperture radar\n(SAR) remote sensing enables critical military reconnaissance, maritime\nsurveillance, and infrastructure monitoring. Although Vision language models\n(VLMs) have made remarkable progress in natural language processing and image\nunderstanding, their applications remain limited in professional domains due to\ninsufficient domain expertise. This paper innovatively proposes the first\nlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which\ncontains approximately 2 million high-quality image-text pairs, encompasses\ndiverse scenarios with detailed target annotations. This dataset not only\nsupports several key tasks such as visual understanding and object detection\ntasks, but also has unique innovative aspects: this study develop a\nvisual-language dataset and benchmark for the SAR domain, enabling and\nevaluating VLMs' capabilities in SAR image interpretation, which provides a\nparadigmatic framework for constructing multimodal datasets across various\nremote sensing vertical domains. Through experiments on 16 mainstream VLMs, the\neffectiveness of the dataset has been fully verified. The project will be\nreleased at https://github.com/JimmyMa99/SARChat.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15628v2","updated":"2025-02-17T07:12:33Z","published":"2024-12-20T07:35:42Z","title":"Can Input Attributions Interpret the Inductive Reasoning Process\n  Elicited in In-Context Learning?","summary":"  Interpreting the internal process of neural models has long been a challenge.\nThis challenge remains relevant in the era of large language models (LLMs) and\nin-context learning (ICL); for example, ICL poses a new issue of interpreting\nwhich example in the few-shot examples contributed to identifying/solving the\ntask. To this end, in this paper, we design synthetic diagnostic tasks of\ninductive reasoning, inspired by the generalization tests in linguistics; here,\nmost in-context examples are ambiguous w.r.t. their underlying rule, and one\ncritical example disambiguates the task demonstrated. The question is whether\nconventional input attribution (IA) methods can track such a reasoning process,\ni.e., identify the influential example, in ICL. Our experiments provide several\npractical findings; for example, a certain simple IA method works the best, and\nthe larger the model, the generally harder it is to interpret the ICL with\ngradient-based IA methods.\n","authors":["Mengyu Ye","Tatsuki Kuribayashi","Goro Kobayashi","Jun Suzuki"],"pdf_url":"https://arxiv.org/pdf/2412.15628v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.17519v2","updated":"2025-02-17T07:07:19Z","published":"2024-06-25T12:59:38Z","title":"Entropy-Based Decoding for Retrieval-Augmented Large Language Models","summary":"  Augmenting Large Language Models (LLMs) with retrieved external knowledge has\nproven effective for improving the factual accuracy of generated responses.\nDespite their success, retrieval-augmented LLMs still face the distractibility\nissue, where the generated responses are negatively influenced by noise from\nboth external and internal knowledge sources. In this paper, we introduce a\nnovel, training-free decoding method guided by entropy considerations to\nmitigate this issue. Our approach utilizes entropy-based document-parallel\nensemble decoding to prioritize low-entropy distributions from retrieved\ndocuments, thereby enhancing the extraction of relevant information of context.\nAdditionally, it incorporates a contrastive decoding mechanism that contrasts\nthe obtained low-entropy ensemble distribution with the high-entropy\ndistribution derived from the model's internal knowledge across layers, which\nensures a greater emphasis on reliable external information. Extensive\nexperiments on open-domain question answering datasets demonstrate the\nsuperiority of our method.\n","authors":["Zexuan Qiu","Zijing Ou","Bin Wu","Jingjing Li","Aiwei Liu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2406.17519v2.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.11501v1","updated":"2025-02-17T07:05:36Z","published":"2025-02-17T07:05:36Z","title":"Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?","summary":"  Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.\n","authors":["Zichen Wen","Yifeng Gao","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11501v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.16756v3","updated":"2025-02-17T06:59:54Z","published":"2024-08-29T17:54:14Z","title":"How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities\n  of Large Language Models","summary":"  The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.\n","authors":["Jiyue Jiang","Pengan Chen","Liheng Chen","Sheng Wang","Qinghang Bao","Lingpeng Kong","Yu Li","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16756v3.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.11495v1","updated":"2025-02-17T06:56:33Z","published":"2025-02-17T06:56:33Z","title":"Balanced Multi-Factor In-Context Learning for Multilingual Large\n  Language Models","summary":"  Multilingual large language models (MLLMs) are able to leverage in-context\nlearning (ICL) to achieve high performance by leveraging cross-lingual\nknowledge transfer without parameter updates. However, their effectiveness is\nhighly sensitive to example selection, particularly in multilingual settings.\nBased on the findings of existing work, three key factors influence\nmultilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3)\nlanguage-specific performance. However, existing approaches address these\nfactors independently, without explicitly disentangling their combined impact,\nleaving optimal example selection underexplored. To address this gap, we\npropose balanced multi-factor ICL (\\textbf{BMF-ICL}), a method that quantifies\nand optimally balances these factors for improved example selection.\nExperiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL\noutperforms existing methods. Further analysis highlights the importance of\nincorporating all three factors and the importance of selecting examples from\nmultiple languages.\n","authors":["Masahiro Kaneko","Alham Fikri Aji","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2502.11495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11494v1","updated":"2025-02-17T06:56:28Z","published":"2025-02-17T06:56:28Z","title":"Stop Looking for Important Tokens in Multimodal Language Models:\n  Duplication Matters More","summary":"  Vision tokens in multimodal large language models often dominate huge\ncomputational overhead due to their excessive length compared to linguistic\nmodality. Abundant recent methods aim to solve this problem with token pruning,\nwhich first defines an importance criterion for tokens and then prunes the\nunimportant vision tokens during inference. However, in this paper, we show\nthat the importance is not an ideal indicator to decide whether a token should\nbe pruned. Surprisingly, it usually results in inferior performance than random\ntoken pruning and leading to incompatibility to efficient attention computation\noperators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),\nwhich prunes tokens based on its duplication with other tokens, leading to\nsignificant and training-free acceleration. Concretely, DART selects a small\nsubset of pivot tokens and then retains the tokens with low duplication to the\npivots, ensuring minimal information loss during token pruning. Experiments\ndemonstrate that DART can prune 88.9% vision tokens while maintaining\ncomparable performance, leading to a 1.99$\\times$ and 2.99$\\times$ speed-up in\ntotal time and prefilling stage, respectively, with good compatibility to\nefficient attention operators. Our codes are available at\nhttps://github.com/ZichenWen1/DART.\n","authors":["Zichen Wen","Yifeng Gao","Shaobo Wang","Junyuan Zhang","Qintong Zhang","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11494v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.11493v1","updated":"2025-02-17T06:55:13Z","published":"2025-02-17T06:55:13Z","title":"DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft\n  Tokens","summary":"  Large Language Models (LLMs) face computational inefficiencies and redundant\nprocessing when handling long context inputs, prompting a focus on compression\ntechniques. While existing semantic vector-based compression methods achieve\npromising performance, these methods fail to account for the intrinsic\ninformation density variations between context chunks, instead allocating soft\ntokens uniformly across context chunks. This uniform distribution inevitably\ndiminishes allocation to information-critical regions. To address this, we\npropose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method\nthat leverages the LLM's intrinsic understanding of contextual relevance to\nguide compression. DAST combines perplexity-based local information with\nattention-driven global information to dynamically allocate soft tokens to the\ninformative-rich chunks, enabling effective, context-aware compression.\nExperimental results across multiple benchmarks demonstrate that DAST surpasses\nstate-of-the-art methods.\n","authors":["Shaoshen Chen","Yangning Li","Zishan Xu","Yinghui Li","Xin Su","Zifei Shan","Hai-tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11492v1","updated":"2025-02-17T06:54:49Z","published":"2025-02-17T06:54:49Z","title":"Why Vision Language Models Struggle with Visual Arithmetic? Towards\n  Enhanced Chart and Geometry Understanding","summary":"  Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks.\n","authors":["Kung-Hsiang Huang","Can Qin","Haoyi Qiu","Philippe Laban","Shafiq Joty","Caiming Xiong","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11491v1","updated":"2025-02-17T06:53:15Z","published":"2025-02-17T06:53:15Z","title":"Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on\n  Knowledge Graph Question Answering","summary":"  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing. However, in knowledge graph question answering tasks\n(KGQA), there remains the issue of answering questions that require multi-hop\nreasoning. Existing methods rely on entity vector matching, but the purpose of\nthe question is abstract and difficult to match with specific entities. As a\nresult, it is difficult to establish reasoning paths to the purpose, which\nleads to information loss and redundancy. To address this issue, inspired by\nhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a\nnovel framework that constructs reasoning paths from purposes back to\nconditions. ORT operates in three key phases: (1) using LLM to extract purpose\nlabels and condition labels, (2) constructing label reasoning paths based on\nthe KG ontology, and (3) using the label reasoning paths to guide knowledge\nretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves\nstate-of-the-art performance and significantly enhances the capability of LLMs\nfor KGQA.\n","authors":["Runxuan Liu","Bei Luo","Jiaqi Li","Baoxin Wang","Ming Liu","Dayong Wu","Shijin Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2502.11491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08211v3","updated":"2025-02-17T06:40:44Z","published":"2024-03-13T03:15:05Z","title":"Large Language Models are Contrastive Reasoners","summary":"  Prompting methods play a crucial role in enhancing the capabilities of\npre-trained large language models (LLMs). We explore how contrastive prompting\n(CP) significantly improves the ability of large language models to perform\ncomplex reasoning. We demonstrate that LLMs are decent contrastive reasoners by\nsimply adding \"Let's give a correct and a wrong answer.\" before LLMs provide\nanswers. Experiments on various large language models show that zero-shot\ncontrastive prompting improves the performance of standard zero-shot prompting\non a range of arithmetic, commonsense, and symbolic reasoning tasks without any\nhand-crafted few-shot examples, such as increasing the accuracy on GSM8K from\n35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4\nmodel. Our method not only surpasses zero-shot CoT and few-shot CoT in most\narithmetic and commonsense reasoning tasks but also can seamlessly integrate\nwith existing prompting methods, resulting in improved or comparable results\nwhen compared to state-of-the-art methods. Our code is available at\nhttps://github.com/yao8839836/cp\n","authors":["Liang Yao"],"pdf_url":"https://arxiv.org/pdf/2403.08211v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19817v2","updated":"2025-02-17T06:39:16Z","published":"2024-10-18T01:38:24Z","title":"Step Guided Reasoning: Improving Mathematical Reasoning using Guidance\n  Generation and Step Reasoning","summary":"  Mathematical reasoning has been challenging for large language models (LLMs).\nHowever, the introduction of step-by-step Chain-of-Thought (CoT) inference has\nsignificantly advanced the mathematical capabilities of LLMs. Despite this\nprogress, current approaches either necessitate extensive inference datasets\nfor training or depend on few-shot methods that frequently compromise\ncomputational accuracy. To address these bottlenecks in mathematical reasoning,\nwe propose a novel method called Step Guidied Reasoning, which is more stable\nand generalizable than few-shot methods and does not involve further\nfine-tuning of the model. In this approach, LLMs reflect on small reasoning\nsteps, similar to how humans deliberate and focus attention on what to do next.\nBy incorporating this reflective process into the inference stage, LLMs can\neffectively guide their reasoning from one step to the next. Through extensive\nexperiments, we demonstrate the significant effect of Step Guidied Reasoning in\naugmenting mathematical performance in state-of-the-art language models.\nQwen2-72B-Instruct outperforms its math-specific counterpart,\nQwen2.5-72B-Math-Instruct, on MMLU- STEM with a score of 90.9%, compared to\n87.3%. The average scores of Qwen2-7B-Instruct and Qwen2-72B-Instruct increase\nfrom 27.1% to 36.3% and from 36.5% to 47.4% on the mathematics domain,\nrespectively.\n","authors":["Lang Cao","Chao Peng","Renhong Chen","Wu Ning","Yingtian Zou","Yitong Li"],"pdf_url":"https://arxiv.org/pdf/2410.19817v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11482v1","updated":"2025-02-17T06:35:42Z","published":"2025-02-17T06:35:42Z","title":"DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free\n  Continual Learning","summary":"  Continual learning (CL) is essential for Large Language Models (LLMs) to\nadapt to evolving real-world demands, yet they are susceptible to catastrophic\nforgetting (CF). While traditional CF solutions rely on expensive data\nrehearsal, recent rehearsal-free methods employ model-based and\nregularization-based strategies to address this issue. However, these\napproaches often neglect the model's plasticity, which is crucial to achieving\noptimal performance on newly learned tasks. Consequently, a key challenge in CL\nis striking a balance between preserving plasticity and mitigating CF. To\ntackle this challenge, we propose the $\\textbf{D}$ecomposed\n$\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which\nexplicitly decouples and learns both task-specific and task-shared knowledge\nusing high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA\ndynamically adjusts the weights of adapters of different ranks based on their\nrelevance and distinction from previous tasks, allowing the model to acquire\nnew task-specific skills while effectively retaining previously learned\nknowledge. Specifically, we implement a decomposed component weighting strategy\ncomprising learnable components that collectively generate attention-based\nweights, allowing the model to integrate and utilize diverse knowledge from\neach DATA. Extensive experiments on three widely used benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance. Notably, our\napproach significantly enhances model plasticity and mitigates CF by extending\nlearnable components and employing stochastic restoration during training\niterations.\n","authors":["Huanxuan Liao","Shizhu He","Yupu Hao","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11476v1","updated":"2025-02-17T06:27:57Z","published":"2025-02-17T06:27:57Z","title":"FastMCTS: A Simple Sampling Strategy for Data Synthesis","summary":"  Synthetic high-quality multi-step reasoning data can significantly enhance\nthe performance of large language models on various tasks. However, most\nexisting methods rely on rejection sampling, which generates trajectories\nindependently and suffers from inefficiency and imbalanced sampling across\nproblems of varying difficulty. In this work, we introduce FastMCTS, an\ninnovative data synthesis strategy inspired by Monte Carlo Tree Search.\nFastMCTS provides a more efficient sampling method for multi-step reasoning\ndata, offering step-level evaluation signals and promoting balanced sampling\nacross problems of different difficulty levels. Experiments on both English and\nChinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more\ncorrect reasoning paths compared to rejection sampling as the number of\ngenerated tokens scales up. Furthermore, under comparable synthetic data\nbudgets, models trained on FastMCTS-generated data outperform those trained on\nrejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight\nsampling strategy, FastMCTS offers a practical and efficient alternative for\nsynthesizing high-quality reasoning data. Our code will be released soon.\n","authors":["Peiji Li","Kai Lv","Yunfan Shao","Yichuan Ma","Linyang Li","Xiaoqing Zheng","Xipeng Qiu","Qipeng Guo"],"pdf_url":"https://arxiv.org/pdf/2502.11476v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2501.03226v3","updated":"2025-02-17T06:27:16Z","published":"2025-01-06T18:59:13Z","title":"BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning","summary":"  Large language models (LLMs) have demonstrated impressive ability in solving\ncomplex mathematical problems with multi-step reasoning and can be further\nenhanced with well-designed in-context learning (ICL) examples. However, this\npotential is often constrained by two major challenges in ICL: granularity\nmismatch and irrelevant information. We observe that while LLMs excel at\ndecomposing mathematical problems, they often struggle with reasoning errors in\nfine-grained steps. Moreover, ICL examples retrieved at the question level may\nomit critical steps or even mislead the model with irrelevant details. To\naddress this issue, we propose BoostStep, a method that enhances reasoning\naccuracy through step-aligned ICL, a novel mechanism that carefully aligns\nretrieved reference steps with the corresponding reasoning steps. Additionally,\nBoostStep incorporates an effective \"first-try\" strategy to deliver exemplars\nhighly relevant to the current state of reasoning. BoostStep is a flexible and\npowerful method that integrates seamlessly with chain-of-thought (CoT) and tree\nsearch algorithms, refining both candidate selection and decision-making.\nEmpirical results show that BoostStep improves GPT-4o's CoT performance by 4.6%\nacross mathematical benchmarks, significantly surpassing traditional few-shot\nlearning's 1.2%. Moreover, it can achieve an additional 7.5\\% gain combined\nwith tree search. Surprisingly, it enhances state-of-the-art LLMs to solve\nchallenging math problems using simpler examples. It improves\nDeepSeek-R1-671B's performance on AIME by 2.2%, leveraging simple examples only\nfrom the MATH dataset.\n","authors":["Beichen Zhang","Yuhong Liu","Xiaoyi Dong","Yuhang Zang","Pan Zhang","Haodong Duan","Yuhang Cao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.03226v3.pdf","comment":"Codes and Data are available at\n  https://github.com/beichenzbc/BoostStep"},{"id":"http://arxiv.org/abs/2410.14735v4","updated":"2025-02-17T06:26:58Z","published":"2024-10-16T20:27:15Z","title":"Agent Skill Acquisition for Large Language Models via CycleQD","summary":"  Training large language models to acquire specific skills remains a\nchallenging endeavor. Conventional training approaches often struggle with data\ndistribution imbalances and inadequacies in objective functions that do not\nalign well with task-specific performance. To address these challenges, we\nintroduce CycleQD, a novel approach that leverages the Quality Diversity\nframework through a cyclic adaptation of the algorithm, along with a model\nmerging based crossover and an SVD-based mutation. In CycleQD, each task's\nperformance metric is alternated as the quality measure while the others serve\nas the behavioral characteristics. This cyclic focus on individual tasks allows\nfor concentrated effort on one task at a time, eliminating the need for data\nratio tuning and simplifying the design of the objective function. Empirical\nresults from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT\nbased models not only enables them to surpass traditional fine-tuning methods\nin coding, operating systems, and database tasks, but also achieves performance\non par with GPT-3.5-TURBO, which potentially contains much more parameters,\nacross these domains. Crucially, this enhanced performance is achieved while\nretaining robust language capabilities, as evidenced by its performance on\nwidely adopted language benchmark tasks. We highlight the key design choices in\nCycleQD, detailing how these contribute to its effectiveness. Furthermore, our\nmethod is general and can be applied to image segmentation models, highlighting\nits applicability across different domains.\n","authors":["So Kuroki","Taishi Nakamura","Takuya Akiba","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.14735v4.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2408.15549v2","updated":"2025-02-17T06:14:31Z","published":"2024-08-28T05:53:46Z","title":"WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback","summary":"  As large language models (LLMs) continue to advance, aligning these models\nwith human preferences has emerged as a critical challenge. Traditional\nalignment methods, relying on human or LLM annotated datasets, are limited by\ntheir resource-intensive nature, inherent subjectivity, misalignment with\nreal-world user preferences, and the risk of feedback loops that amplify model\nbiases. To overcome these limitations, we introduce WildFeedback, a novel\nframework that leverages in-situ user feedback during conversations with LLMs\nto create preference datasets automatically. Given a corpus of multi-turn\nuser-LLM conversation, WildFeedback identifies and classifies user feedback to\nLLM responses between conversation turns. The user feedback is then used to\ncreate examples of preferred and dispreferred responses according to users'\npreference. Our experiments demonstrate that LLMs fine-tuned on WildFeedback\ndataset exhibit significantly improved alignment with user preferences, as\nevidenced by both traditional benchmarks and our proposed checklist-guided\nevaluation. By incorporating in-situ feedback from actual users, WildFeedback\naddresses the scalability, subjectivity, and bias challenges that plague\nexisting approaches, marking a significant step toward developing LLMs that are\nmore responsive to the diverse and evolving needs of their users.\n","authors":["Taiwei Shi","Zhuoer Wang","Longqi Yang","Ying-Chun Lin","Zexue He","Mengting Wan","Pei Zhou","Sujay Jauhar","Sihao Chen","Shan Xia","Hongfei Zhang","Jieyu Zhao","Xiaofeng Xu","Xia Song","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2408.15549v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2502.11471v1","updated":"2025-02-17T06:02:59Z","published":"2025-02-17T06:02:59Z","title":"GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.\n","authors":["Kangyang Luo","Yuzhuo Bai","Cheng Gao","Shuzheng Si","Yingli Shen","Zhu Liu","Zhitong Wang","Cunliang Kong","Wenhao Li","Yufei Huang","Ye Tian","Xuantang Xiong","Lei Han","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11469v1","updated":"2025-02-17T05:58:25Z","published":"2025-02-17T05:58:25Z","title":"If Attention Serves as a Cognitive Model of Human Memory Retrieval, What\n  is the Plausible Memory Representation?","summary":"  Recent work in computational psycholinguistics has revealed intriguing\nparallels between attention mechanisms and human memory retrieval, focusing\nprimarily on Transformer architectures that operate on token-level\nrepresentations. However, computational psycholinguistic research has also\nestablished that syntactic structures provide compelling explanations for human\nsentence processing that word-level factors alone cannot fully account for. In\nthis study, we investigate whether the attention mechanism of Transformer\nGrammar (TG), which uniquely operates on syntactic structures as\nrepresentational units, can serve as a cognitive model of human memory\nretrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis\nbetween model behavior and human processing difficulty. Our experiments\ndemonstrate that TG's attention achieves superior predictive power for\nself-paced reading times compared to vanilla Transformer's, with further\nanalyses revealing independent contributions from both models. These findings\nsuggest that human sentence processing involves dual memory representations --\none based on syntactic structures and another on token sequences -- with\nattention serving as the general retrieval algorithm, while highlighting the\nimportance of incorporating syntactic structures as representational units.\n","authors":["Ryo Yoshida","Shinnosuke Isono","Kohei Kajikawa","Taiga Someya","Yushi Sugimito","Yohei Oseki"],"pdf_url":"https://arxiv.org/pdf/2502.11469v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.11466v1","updated":"2025-02-17T05:52:44Z","published":"2025-02-17T05:52:44Z","title":"GiFT: Gibbs Fine-Tuning for Code Generation","summary":"  Training Large Language Models (LLMs) with synthetic data is a prevalent\npractice in code generation. A key approach is self-training, where LLMs are\niteratively trained on self-generated correct code snippets. In this case, the\nself-generated codes are drawn from a conditional distribution, conditioned on\na specific seed description. However, the seed description is not the only\nvalid representation that aligns with its intended meaning. With all valid\ndescriptions and codes forming a joint space, codes drawn from the conditional\ndistribution would lead to an underrepresentation of the full description-code\nspace. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training\nmethod inspired by Gibbs sampling. GiFT allows self-generated data to be drawn\nfrom the marginal distribution of the joint space, thereby mitigating the\nbiases inherent in conditional sampling. We provide a theoretical analysis\ndemonstrating the potential benefits of fine-tuning LLMs with code derived from\nthe marginal distribution. Furthermore, we propose a perplexity-based code\nselection method to mitigate the imbalanced long-tail distribution of the\nself-generated codes. Empirical evaluation of two LLMs across four datasets\ndemonstrates that GiFT achieves superior performance, particularly on more\nchallenging benchmarks.\n","authors":["Haochen Li","Wanjin Feng","Xin Zhou","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2502.11466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11460v1","updated":"2025-02-17T05:37:02Z","published":"2025-02-17T05:37:02Z","title":"UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet code generation remains a major challenge. Current\napproaches for obtaining high-quality code data primarily focus on (i)\ncollecting large-scale pre-training data and (ii) synthesizing instruction data\nthrough prompt engineering with powerful models. While pre-training data faces\nquality consistency issues, instruction-based synthesis suffers from limited\ninstruction diversity and inherent biases of LLMs. To address this gap, we\nintroduce UnitCoder, a systematic pipeline leveraging model-generated unit\ntests to both guide and validate the code generation process. Combined with\nlarge-scale package-based retrieval from pre-training corpus, we generate a\ndataset of 500K+ verifiable programs containing diverse API calls. Evaluations\non multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that\nmodels fine-tuned on our synthetic data exhibit consistent performance\nimprovements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and\n28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work\npresents a scalable approach that leverages model-generated unit tests to guide\nthe synthesis of high-quality code data from pre-training corpora,\ndemonstrating the potential for producing diverse and high-quality\npost-training data at scale. All code and data will be released\n(https://github.com).\n","authors":["Yichuan Ma","Yunfan Shao","Peiji Li","Demin Song","Qipeng Guo","Linyang Li","Xipeng Qiu","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11460v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2404.09077v2","updated":"2025-02-17T05:32:59Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document Question Answering with\n  LLM-Enhanced Knowledge Graph Reasoning","summary":"  Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.\n","authors":["Zukang Yang","Zixuan Zhu","Xuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11457v1","updated":"2025-02-17T05:32:56Z","published":"2025-02-17T05:32:56Z","title":"Aligning Sentence Simplification with ESL Learner's Proficiency for\n  Language Acquisition","summary":"  Text simplification is crucial for improving accessibility and comprehension\nfor English as a Second Language (ESL) learners. This study goes a step further\nand aims to facilitate ESL learners' language acquisition by simplification.\nSpecifically, we propose simplifying complex sentences to appropriate levels\nfor learners while also increasing vocabulary coverage of the target level in\nthe simplifications. We achieve this without a parallel corpus by conducting\nreinforcement learning on a large language model. Our method employs\ntoken-level and sentence-level rewards, and iteratively trains the model on its\nself-generated outputs to guide the model to search for simplification\nhypotheses that satisfy the target attributes. Experiment results on CEFR-SP\nand TurkCorpus datasets show that the proposed method can effectively increase\nthe frequency and diversity of vocabulary of the target level by more than\n$20\\%$ compared to baseline models, while maintaining high simplification\nquality.\n","authors":["Guanlin Li","Yuki Arase","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2502.11457v1.pdf","comment":"NAACL2025 main"},{"id":"http://arxiv.org/abs/2409.18339v2","updated":"2025-02-17T05:28:55Z","published":"2024-09-26T23:25:21Z","title":"AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language\n  Models","summary":"  Recent advancements in Large Language Models (LLMs) have demonstrated great\nsuccess in many Natural Language Processing (NLP) tasks. In addition to their\ncognitive intelligence, exploring their capabilities in emotional intelligence\nis also crucial, as it enables more natural and empathetic conversational AI.\nRecent studies have shown LLMs' capability in recognizing emotions, but they\noften focus on single emotion labels and overlook the complex and ambiguous\nnature of human emotions. This study is the first to address this gap by\nexploring the potential of LLMs in recognizing ambiguous emotions, leveraging\ntheir strong generalization capabilities and in-context learning. We design\nzero-shot and few-shot prompting and incorporate past dialogue as context\ninformation for ambiguous emotion recognition. Experiments conducted using\nthree datasets indicate significant potential for LLMs in recognizing ambiguous\nemotions, and highlight the substantial benefits of including context\ninformation. Furthermore, our findings indicate that LLMs demonstrate a high\ndegree of effectiveness in recognizing less ambiguous emotions and exhibit\npotential for identifying more ambiguous emotions, paralleling human perceptual\ncapabilities.\n","authors":["Xin Hong","Yuan Gong","Vidhyasaharan Sethu","Ting Dang"],"pdf_url":"https://arxiv.org/pdf/2409.18339v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.11454v1","updated":"2025-02-17T05:28:12Z","published":"2025-02-17T05:28:12Z","title":"UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with\n  Unified Multi-Objective Optimization","summary":"  Human preference plays a significant role in measuring large language models\nand guiding them to align with human values. Unfortunately, current\ncomparing-based evaluation (CBE) methods typically focus on a single\noptimization objective, failing to effectively utilize scarce yet valuable\npreference signals. To address this, we delve into key factors that can enhance\nthe accuracy, convergence, and scalability of CBE: suppressing sampling bias,\nbalancing descending process of uncertainty, and mitigating updating\nuncertainty. Following the derived guidelines, we propose UniCBE, a unified\nuniformity-driven CBE framework which simultaneously optimize these core\nobjectives by constructing and integrating three decoupled sampling probability\nmatrices, each designed to ensure uniformity in specific aspects. We further\nablate the optimal tuple sampling and preference aggregation strategies to\nachieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of\nevaluation budgets while achieving a Pearson correlation with ground truth\nexceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios\nwhere new models are continuously introduced, UniCBE can even save over 50% of\nevaluation costs, highlighting its improved scalability.\n","authors":["Peiwen Yuan","Shaoxiong Feng","Yiwei Li","Xinglin Wang","Yueqi Zhang","Jiayi Shi","Chuyi Tan","Boyuan Pan","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2502.11454v1.pdf","comment":"ICLR 2025 spotlight"},{"id":"http://arxiv.org/abs/2407.08952v3","updated":"2025-02-17T05:25:32Z","published":"2024-07-12T03:15:01Z","title":"Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection","summary":"  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.\n","authors":["Ye Liu","Jiajun Zhu","Xukai Liu","Haoyu Tang","Yanghai Zhang","Kai Zhang","Xiaofang Zhou","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08952v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11451v1","updated":"2025-02-17T05:24:30Z","published":"2025-02-17T05:24:30Z","title":"From Personas to Talks: Revisiting the Impact of Personas on\n  LLM-Synthesized Emotional Support Conversations","summary":"  The rapid advancement of Large Language Models (LLMs) has revolutionized the\ngeneration of emotional support conversations (ESC), offering scalable\nsolutions with reduced costs and enhanced data privacy. This paper explores the\nrole of personas in the creation of ESC by LLMs. Our research utilizes\nestablished psychological frameworks to measure and infuse persona traits into\nLLMs, which then generate dialogues in the emotional support scenario. We\nconduct extensive evaluations to understand the stability of persona traits in\ndialogues, examining shifts in traits post-generation and their impact on\ndialogue quality and strategy distribution. Experimental results reveal several\nnotable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in\nemotionality and extraversion occur, influencing the dialogue dynamics, and 3)\nthe application of persona traits modifies the distribution of emotional\nsupport strategies, enhancing the relevance and empathetic quality of the\nresponses. These findings highlight the potential of persona-driven LLMs in\ncrafting more personalized, empathetic, and effective emotional support\ndialogues, which has significant implications for the future design of\nAI-driven emotional support systems.\n","authors":["Shenghan Wu","Yang Deng","Yimo Zhu","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11451v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.16770v2","updated":"2025-02-17T18:59:56Z","published":"2024-08-29T17:59:54Z","title":"3D Whole-body Grasp Synthesis with Directional Controllability","summary":"  Synthesizing 3D whole bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Moreover, training\ndata for this task is really scarce, while capturing new data is expensive.\nRecent work goes beyond finite datasets via a divide-and-conquer approach; it\nfirst generates a \"guiding\" right-hand grasp, and then searches for bodies that\nmatch this. However, the guiding-hand synthesis lacks controllability and\nreceptacle awareness, so it likely has an implausible direction (i.e., a body\ncan't match this without penetrating the receptacle) and needs corrections\nthrough major post-processing. Moreover, the body search needs exhaustive\nsampling and is expensive. These are strong limitations. We tackle these with a\nnovel method called CWGrasp. Our key idea is that performing geometry-based\nreasoning \"early on,\" instead of \"too late,\" provides rich \"control\" signals\nfor inference. To this end, CWGrasp first samples a plausible\nreaching-direction vector (used later for both the arm and hand) from a\nprobabilistic model built via ray-casting from the object and collision\nchecking. Moreover, CWGrasp uniquely tackles both right and left-hand grasps.\nWe evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms\nbaselines, at lower runtime and budget, while all components help performance.\nCode and models are available at https://gpaschalidis.github.io/cwgrasp.\n","authors":["Georgios Paschalidis","Romana Wilschut","Dimitrije Antiƒá","Omid Taheri","Dimitrios Tzionas"],"pdf_url":"https://arxiv.org/pdf/2408.16770v2.pdf","comment":"3DV 2025"},{"id":"http://arxiv.org/abs/2502.12154v1","updated":"2025-02-17T18:59:50Z","published":"2025-02-17T18:59:50Z","title":"Diffusion Models without Classifier-free Guidance","summary":"  This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.\n","authors":["Zhicong Tang","Jianmin Bao","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2502.12154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12151v1","updated":"2025-02-17T18:59:03Z","published":"2025-02-17T18:59:03Z","title":"VoLUT: Efficient Volumetric streaming enhanced by LUT-based\n  super-resolution","summary":"  3D volumetric video provides immersive experience and is gaining traction in\ndigital media. Despite its rising popularity, the streaming of volumetric video\ncontent poses significant challenges due to the high data bandwidth\nrequirement. A natural approach to mitigate the bandwidth issue is to reduce\nthe volumetric video's data rate by downsampling the content prior to\ntransmission. The video can then be upsampled at the receiver's end using a\nsuper-resolution (SR) algorithm to reconstruct the high-resolution details.\nWhile super-resolution techniques have been extensively explored and advanced\nfor 2D video content, there is limited work on SR algorithms tailored for\nvolumetric videos.\n  To address this gap and the growing need for efficient volumetric video\nstreaming, we have developed VoLUT with a new SR algorithm specifically\ndesigned for volumetric content. Our algorithm uniquely harnesses the power of\nlookup tables (LUTs) to facilitate the efficient and accurate upscaling of\nlow-resolution volumetric data. The use of LUTs enables our algorithm to\nquickly reference precomputed high-resolution values, thereby significantly\nreducing the computational complexity and time required for upscaling. We\nfurther apply adaptive video bit rate algorithm (ABR) to dynamically determine\nthe downsampling rate according to the network condition and stream the\nselected video rate to the receiver. Compared to related work, VoLUT is the\nfirst to enable high-quality 3D SR on commodity mobile devices at line-rate.\nOur evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by\n36.7% for volumetric video streaming and achieve\n  3D SR speed-up with no quality compromise.\n","authors":["Chendong Wang","Anlan Zhang","Yifan Yang","Lili Qiu","Yuqing Yang","Xinyang Jiang","Feng Qian","Suman Banerjee"],"pdf_url":"https://arxiv.org/pdf/2502.12151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12148v1","updated":"2025-02-17T18:57:51Z","published":"2025-02-17T18:57:51Z","title":"HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation","summary":"  The remarkable success of the autoregressive paradigm has made significant\nadvancement in Multimodal Large Language Models (MLLMs), with powerful models\nlike Show-o, Transfusion and Emu3 achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we propose HermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take the homologous data as input to curate homologous\npreference data of both understanding and generation. Through Pair-DPO and\nself-play iterative optimization, HermesFlow effectively aligns multimodal\nunderstanding and generation using homologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential of HermesFlow as a general\nalignment framework for next-generation multimodal foundation models. Code:\nhttps://github.com/Gen-Verse/HermesFlow\n","authors":["Ling Yang","Xinchen Zhang","Ye Tian","Chenming Shang","Minghao Xu","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2502.12148v1.pdf","comment":"Code: https://github.com/Gen-Verse/HermesFlow"},{"id":"http://arxiv.org/abs/2502.12146v1","updated":"2025-02-17T18:57:26Z","published":"2025-02-17T18:57:26Z","title":"Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising\n  Trajectory Sharpening","summary":"  We propose Diffusion-Sharpening, a fine-tuning approach that enhances\ndownstream alignment by optimizing sampling trajectories. Existing RL-based\nfine-tuning methods focus on single training timesteps and neglect\ntrajectory-level alignment, while recent sampling trajectory optimization\nmethods incur significant inference NFE costs. Diffusion-Sharpening overcomes\nthis by using a path integral framework to select optimal trajectories during\ntraining, leveraging reward feedback, and amortizing inference costs. Our\nmethod demonstrates superior training efficiency with faster convergence, and\nbest inference efficiency without requiring additional NFEs. Extensive\nexperiments show that Diffusion-Sharpening outperforms RL-based fine-tuning\nmethods (e.g., Diffusion-DPO) and sampling trajectory optimization methods\n(e.g., Inference Scaling) across diverse metrics including text alignment,\ncompositional capabilities, and human preferences, offering a scalable and\nefficient solution for future diffusion model fine-tuning. Code:\nhttps://github.com/Gen-Verse/Diffusion-Sharpening\n","authors":["Ye Tian","Ling Yang","Xinchen Zhang","Yunhai Tong","Mengdi Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2502.12146v1.pdf","comment":"Code: https://github.com/Gen-Verse/Diffusion-Sharpening"},{"id":"http://arxiv.org/abs/2502.12138v1","updated":"2025-02-17T18:54:05Z","published":"2025-02-17T18:54:05Z","title":"FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views","summary":"  We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/\n","authors":["Shangzhan Zhang","Jianyuan Wang","Yinghao Xu","Nan Xue","Christian Rupprecht","Xiaowei Zhou","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2502.12138v1.pdf","comment":"8 pages. Website: https://zhanghe3z.github.io/FLARE/"},{"id":"http://arxiv.org/abs/2502.12135v1","updated":"2025-02-17T18:53:27Z","published":"2025-02-17T18:53:27Z","title":"MagicArticulate: Make Your 3D Models Articulation-Ready","summary":"  With the explosive growth of 3D content creation, there is an increasing\ndemand for automatically converting static 3D models into articulation-ready\nversions that support realistic animation. Traditional approaches rely heavily\non manual annotation, which is both time-consuming and labor-intensive.\nMoreover, the lack of large-scale benchmarks has hindered the development of\nlearning-based solutions. In this work, we present MagicArticulate, an\neffective framework that automatically transforms static 3D models into\narticulation-ready assets. Our key contributions are threefold. First, we\nintroduce Articulation-XL, a large-scale benchmark containing over 33k 3D\nmodels with high-quality articulation annotations, carefully curated from\nObjaverse-XL. Second, we propose a novel skeleton generation method that\nformulates the task as a sequence modeling problem, leveraging an\nauto-regressive transformer to naturally handle varying numbers of bones or\njoints within skeletons and their inherent dependencies across different 3D\nmodels. Third, we predict skinning weights using a functional diffusion process\nthat incorporates volumetric geodesic distance priors between vertices and\njoints. Extensive experiments demonstrate that MagicArticulate significantly\noutperforms existing methods across diverse object categories, achieving\nhigh-quality articulation that enables realistic animation. Project page:\nhttps://chaoyuesong.github.io/MagicArticulate.\n","authors":["Chaoyue Song","Jianfeng Zhang","Xiu Li","Fan Yang","Yiwen Chen","Zhongcong Xu","Jun Hao Liew","Xiaoyang Guo","Fayao Liu","Jiashi Feng","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2502.12135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12119v1","updated":"2025-02-17T18:43:41Z","published":"2025-02-17T18:43:41Z","title":"PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection","summary":"  Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.\n","authors":["Jinhe Bi","Yifan Wang","Danqi Yan","Xun Xiao","Artur Hecker","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2502.12119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12113v1","updated":"2025-02-17T18:38:27Z","published":"2025-02-17T18:38:27Z","title":"A Monocular Event-Camera Motion Capture System","summary":"  Motion capture systems are a widespread tool in research to record\nground-truth poses of objects. Commercial systems use reflective markers\nattached to the object and then triangulate pose of the object from multiple\ncamera views. Consequently, the object must be visible to multiple cameras\nwhich makes such multi-view motion capture systems unsuited for deployments in\nnarrow, confined spaces (e.g. ballast tanks of ships). In this technical report\nwe describe a monocular event-camera motion capture system which overcomes this\nlimitation and is ideally suited for narrow spaces. Instead of passive markers\nit relies on active, blinking LED markers such that each marker can be uniquely\nidentified from the blinking frequency. The markers are placed at known\nlocations on the tracking object. We then solve the PnP (perspective-n-points)\nproblem to obtain the position and orientation of the object. The developed\nsystem has millimeter accuracy, millisecond latency and we demonstrate that its\nstate estimate can be used to fly a small, agile quadrotor.\n","authors":["Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2502.12113v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2412.04453v2","updated":"2025-02-17T18:27:27Z","published":"2024-12-05T18:58:17Z","title":"NaVILA: Legged Robot Vision-Language-Action Model for Navigation","summary":"  This paper proposes to solve the problem of Vision-and-Language Navigation\nwith legged robots, which not only provides a flexible way for humans to\ncommand but also allows the robot to navigate through more challenging and\ncluttered scenes. However, it is non-trivial to translate human language\ninstructions all the way to low-level leg joint actions. We propose NaVILA, a\n2-level framework that unifies a Vision-Language-Action model (VLA) with\nlocomotion skills. Instead of directly predicting low-level actions from VLA,\nNaVILA first generates mid-level actions with spatial information in the form\nof language, (e.g., \"moving forward 75cm\"), which serves as an input for a\nvisual locomotion RL policy for execution. NaVILA substantially improves\nprevious approaches on existing benchmarks. The same advantages are\ndemonstrated in our newly developed benchmarks with IsaacLab, featuring more\nrealistic scenes, low-level controls, and real-world robot experiments. We show\nmore results at https://navila-bot.github.io/\n","authors":["An-Chieh Cheng","Yandong Ji","Zhaojing Yang","Zaitian Gongye","Xueyan Zou","Jan Kautz","Erdem Bƒ±yƒ±k","Hongxu Yin","Sifei Liu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.04453v2.pdf","comment":"Website: https://navila-bot.github.io/"},{"id":"http://arxiv.org/abs/2404.01438v2","updated":"2025-02-17T18:22:03Z","published":"2024-04-01T19:22:43Z","title":"Generation and Detection of Sign Language Deepfakes - A Linguistic and\n  Visual Analysis","summary":"  This research explores the positive application of deepfake technology for\nupper body generation, specifically sign language for the Deaf and Hard of\nHearing (DHoH) community. Given the complexity of sign language and the\nscarcity of experts, the generated videos are vetted by a sign language expert\nfor accuracy. We construct a reliable deepfake dataset, evaluating its\ntechnical and visual credibility using computer vision and natural language\nprocessing models. The dataset, consisting of over 1200 videos featuring both\nseen and unseen individuals, is also used to detect deepfake videos targeting\nvulnerable individuals. Expert annotations confirm that the generated videos\nare comparable to real sign language content. Linguistic analysis, using\ntextual similarity scores and interpreter evaluations, shows that the\ninterpretation of generated videos is at least 90% similar to authentic sign\nlanguage. Visual analysis demonstrates that convincingly realistic deepfakes\ncan be produced, even for new subjects. Using a pose/style transfer model, we\npay close attention to detail, ensuring hand movements are accurate and align\nwith the driving video. We also apply machine learning algorithms to establish\na baseline for deepfake detection on this dataset, contributing to the\ndetection of fraudulent sign language videos.\n","authors":["Shahzeb Naeem","Muhammad Riyyan Khan","Usman Tariq","Abhinav Dhall","Carlos Ivan Colon","Hasan Al-Nashash"],"pdf_url":"https://arxiv.org/pdf/2404.01438v2.pdf","comment":"10 pages, 11 figures, IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL\n  SYSTEM"},{"id":"http://arxiv.org/abs/2502.12096v1","updated":"2025-02-17T18:14:18Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Unified Framework for Cross-modal Context-aware\n  Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a unified\nframework to leverage cross-modal context information in generative semantic\ncommunications (GenSC). TokCom is a new paradigm, motivated by the recent\nsuccess of generative foundation models and multimodal large language models\n(GFM/MLLMs), where the communication units are tokens, enabling efficient\ntransformer-based token processing at the transmitter and receiver. In this\npaper, we introduce the potential opportunities and challenges of leveraging\ncontext in GenSC, explore how to integrate GFM/MLLMs-based token processing\ninto semantic communication systems to leverage cross-modal context\neffectively, present the key principles for efficient TokCom at various layers\nin future wireless networks. We demonstrate the corresponding TokCom benefits\nin a GenSC setup for image, leveraging cross-modal context information, which\nincreases the bandwidth efficiency by 70.8% with negligible loss of\nsemantic/perceptual quality. Finally, the potential research directions are\nidentified to facilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12095v1","updated":"2025-02-17T18:13:42Z","published":"2025-02-17T18:13:42Z","title":"Descriminative-Generative Custom Tokens for Vision-Language Models","summary":"  This paper explores the possibility of learning custom tokens for\nrepresenting new concepts in Vision-Language Models (VLMs). Our aim is to learn\ntokens that can be effective for both discriminative and generative tasks while\ncomposing well with words to form new input queries. The targeted concept is\nspecified in terms of a small set of images and a parent concept described\nusing text. We operate on CLIP text features and propose to use a combination\nof a textual inversion loss and a classification loss to ensure that text\nfeatures of the learned token are aligned with image features of the concept in\nthe CLIP embedding space. We restrict the learned token to a low-dimensional\nsubspace spanned by tokens for attributes that are appropriate for the given\nsuper-class. These modifications improve the quality of compositions of the\nlearned token with natural language for generating new scenes. Further, we show\nthat learned custom tokens can be used to form queries for text-to-image\nretrieval task, and also have the important benefit that composite queries can\nbe visualized to ensure that the desired concept is faithfully encoded. Based\non this, we introduce the method of Generation Aided Image Retrieval, where the\nquery is modified at inference time to better suit the search intent. On the\nDeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over\nrelevant baselines by 7%.\n","authors":["Pramuditha Perera","Matthew Trager","Luca Zancato","Alessandro Achille","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2502.12095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18057v3","updated":"2025-02-17T18:08:17Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for removing private or hazardous\ninformation from deep learning models. While MU has advanced significantly in\nunimodal (text or vision) settings, multimodal unlearning (MMU) remains\nunderexplored due to the lack of open benchmarks for evaluating cross-modal\ndata removal. To address this gap, we introduce CLEAR, the first open-source\nbenchmark designed specifically for MMU. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We conduct a comprehensive\nanalysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four\nevaluation sets, demonstrating that jointly unlearning both modalities\noutperforms single-modality approaches. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12081v1","updated":"2025-02-17T17:55:55Z","published":"2025-02-17T17:55:55Z","title":"Unhackable Temporal Rewarding for Scalable Video MLLMs","summary":"  In the pursuit of superior video-processing MLLMs, we have encountered a\nperplexing paradox: the \"anti-scaling law\", where more data and larger models\nlead to worse performance. This study unmasks the culprit: \"temporal hacking\",\na phenomenon where models shortcut by fixating on select frames, missing the\nfull video narrative. In this work, we systematically establish a comprehensive\ntheory of temporal hacking, defining it from a reinforcement learning\nperspective, introducing the Temporal Perplexity (TPL) score to assess this\nmisalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework\nto mitigate the temporal hacking. Both theoretically and empirically, TPL\nproves to be a reliable indicator of temporal modeling quality, correlating\nstrongly with frame activation patterns. Extensive experiments reveal that UTR\nnot only counters temporal hacking but significantly elevates video\ncomprehension capabilities. This work not only advances video-AI systems but\nalso illuminates the critical importance of aligning proxy rewards with true\nobjectives in MLLM development.\n","authors":["En Yu","Kangheng Lin","Liang Zhao","Yana Wei","Zining Zhu","Haoran Wei","Jianjian Sun","Zheng Ge","Xiangyu Zhang","Jingyu Wang","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2502.12081v1.pdf","comment":"Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/"},{"id":"http://arxiv.org/abs/2502.12080v1","updated":"2025-02-17T17:55:27Z","published":"2025-02-17T17:55:27Z","title":"HumanGif: Single-View Human Diffusion with Generative Prior","summary":"  While previous single-view-based 3D human reconstruction methods made\nsignificant progress in novel view synthesis, it remains a challenge to\nsynthesize both view-consistent and pose-consistent results for animatable\nhuman avatars from a single image input. Motivated by the success of 2D\ncharacter animation, we propose <strong>HumanGif</strong>, a single-view human\ndiffusion model with generative prior. Specifically, we formulate the\nsingle-view-based 3D human novel view and pose synthesis as a\nsingle-view-conditioned human diffusion process, utilizing generative priors\nfrom foundational diffusion models. To ensure fine-grained and consistent novel\nview and pose synthesis, we introduce a Human NeRF module in HumanGif to learn\nspatially aligned features from the input image, implicitly capturing the\nrelative camera and human pose transformation. Furthermore, we introduce an\nimage-level loss during optimization to bridge the gap between latent and image\nspaces in diffusion models. Extensive experiments on RenderPeople and\nDNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual\nperformance, with better generalizability for novel view and pose synthesis.\n","authors":["Shoukang Hu","Takuya Narihira","Kazumi Fukuda","Ryosuke Sawata","Takashi Shibuya","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.12080v1.pdf","comment":"Project page: https://skhu101.github.io/HumanGif/"},{"id":"http://arxiv.org/abs/2412.09115v2","updated":"2025-02-17T17:50:21Z","published":"2024-12-12T09:49:16Z","title":"Vision CNNs trained to estimate spatial latents learned similar\n  ventral-stream-aligned representations","summary":"  Studies of the functional role of the primate ventral visual stream have\ntraditionally focused on object categorization, often ignoring -- despite much\nprior evidence -- its role in estimating \"spatial\" latents such as object\nposition and pose. Most leading ventral stream models are derived by optimizing\nnetworks for object categorization, which seems to imply that the ventral\nstream is also derived under such an objective. Here, we explore an alternative\nhypothesis: Might the ventral stream be optimized for estimating spatial\nlatents? And a closely related question: How different -- if at all -- are\nrepresentations learned from spatial latent estimation compared to\ncategorization? To ask these questions, we leveraged synthetic image datasets\ngenerated by a 3D graphic engine and trained convolutional neural networks\n(CNNs) to estimate different combinations of spatial and category latents. We\nfound that models trained to estimate just a few spatial latents achieve neural\nalignment scores comparable to those trained on hundreds of categories, and the\nspatial latent performance of models strongly correlates with their neural\nalignment. Spatial latent and category-trained models have very similar -- but\nnot identical -- internal representations, especially in their early and middle\nlayers. We provide evidence that this convergence is partly driven by\nnon-target latent variability in the training data, which facilitates the\nimplicit learning of representations of those non-target latents. Taken\ntogether, these results suggest that many training objectives, such as spatial\nlatents, can lead to similar models aligned neurally with the ventral stream.\nThus, one should not assume that the ventral stream is optimized for object\ncategorization only. As a field, we need to continue to sharpen our measures of\ncomparing models to brains to better understand the functional roles of the\nventral stream.\n","authors":["Yudi Xie","Weichen Huang","Esther Alter","Jeremy Schwartz","Joshua B. Tenenbaum","James J. DiCarlo"],"pdf_url":"https://arxiv.org/pdf/2412.09115v2.pdf","comment":"30 pages, 21 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01474v3","updated":"2025-02-17T17:24:42Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v3.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.09838v2","updated":"2025-02-17T17:17:44Z","published":"2025-02-14T00:42:36Z","title":"HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation","summary":"  We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.\n","authors":["Tianwei Lin","Wenqiao Zhang","Sijing Li","Yuqian Yuan","Binhe Yu","Haoyuan Li","Wanggui He","Hao Jiang","Mengze Li","Xiaohui Song","Siliang Tang","Jun Xiao","Hui Lin","Yueting Zhuang","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2502.09838v2.pdf","comment":"Comments: added project page"},{"id":"http://arxiv.org/abs/2502.12027v1","updated":"2025-02-17T16:59:37Z","published":"2025-02-17T16:59:37Z","title":"Enhancing Transparent Object Pose Estimation: A Fusion of GDR-Net and\n  Edge Detection","summary":"  Object pose estimation of transparent objects remains a challenging task in\nthe field of robot vision due to the immense influence of lighting, background,\nand reflections. However, the edges of clear objects have the highest contrast,\nwhich leads to stable and prominent features. We propose a novel approach by\nincorporating edge detection in a pre-processing step for the tasks of object\ndetection and object pose estimation. We conducted experiments to investigate\nthe effect of edge detectors on transparent objects. We examine the performance\nof the state-of-the-art 6D object pose estimation pipeline GDR-Net and the\nobject detector YOLOX when applying different edge detectors as pre-processing\nsteps (i.e., Canny edge detection with and without color information, and\nholistically-nested edges (HED)). We evaluate the physically-based rendered\ndataset Trans6D-32 K of transparent objects with parameters proposed by the BOP\nChallenge. Our results indicate that applying edge detection as a\npre-processing enhances performance for certain objects.\n","authors":["Tessa Pulli","Peter H√∂nig","Stefan Thalhammer","Matthias Hirschmanner","Markus Vincze"],"pdf_url":"https://arxiv.org/pdf/2502.12027v1.pdf","comment":"accepted at First Austrian Symposium on AI, Robotics, and Vision\n  (AIROV 2024)"},{"id":"http://arxiv.org/abs/2501.18592v3","updated":"2025-02-17T16:54:39Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v3.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2502.12003v1","updated":"2025-02-17T16:41:46Z","published":"2025-02-17T16:41:46Z","title":"Predicting Next-Day Wildfire Spread with Time Series and Attention","summary":"  Recent research has demonstrated the potential of deep neural networks (DNNs)\nto accurately predict next-day wildfire spread, based upon the current extent\nof a fire and geospatial rasters of influential environmental covariates e.g.,\nvegetation, topography, climate, and weather. In this work, we investigate a\nrecent transformer-based model, termed the SwinUnet, for next-day wildfire\nprediction. We benchmark Swin-based models against several current\nstate-of-the-art models on WildfireSpreadTS (WFTS), a large public benchmark\ndataset of historical wildfire events. We consider two next-day fire prediction\nscenarios: when the model is given input of (i) a single previous day of data,\nor (ii) five previous days of data. We find that, with the proper\nmodifications, SwinUnet achieves state-of-the-art accuracy on next-day\nprediction for both the single-day and multi-day scenarios. SwinUnet's success\ndepends heavily upon utilizing pre-trained weights from ImageNet. Consistent\nwith prior work, we also found that models with multi-day-input always\noutperformed models with single-day input.\n","authors":["Saad Lahrichi","Jesse Johnson","Jordan Malof"],"pdf_url":"https://arxiv.org/pdf/2502.12003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12002v1","updated":"2025-02-17T16:40:23Z","published":"2025-02-17T16:40:23Z","title":"NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis\n  with Differential Digital Signal Processing","summary":"  Recent advancements in visual speech recognition (VSR) have promoted progress\nin lip-to-speech synthesis, where pre-trained VSR models enhance the\nintelligibility of synthesized speech by providing valuable semantic\ninformation. The success achieved by cascade frameworks, which combine\npseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the\ntranscribed text, highlights the benefits of leveraging VSR models. However,\nthese methods typically rely on mel-spectrograms as an intermediate\nrepresentation, which may introduce a key bottleneck: the domain gap between\nsynthetic mel-spectrograms, generated from inherently error-prone lip-to-speech\nmappings, and real mel-spectrograms used to train vocoders. This mismatch\ninevitably degrades synthesis quality. To bridge this gap, we propose Natural\nLip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic\ninductive biases with differentiable speech generation components.\nSpecifically, we introduce a fundamental frequency (F0) predictor to capture\nprosodic variations in synthesized speech. The predicted F0 then drives a\nDifferentiable Digital Signal Processing (DDSP) synthesizer to generate a\ncoarse signal which serves as prior information for subsequent speech\nsynthesis. Additionally, instead of relying on a reference speaker embedding as\nan auxiliary input, our approach achieves satisfactory performance on speaker\nsimilarity without explicitly modelling speaker characteristics. Both objective\nand subjective evaluation results demonstrate that NaturalL2S can effectively\nenhance the quality of the synthesized speech when compared to state-of-the-art\nmethods. Our demonstration page is accessible at\nhttps://yifan-liang.github.io/NaturalL2S/.\n","authors":["Yifan Liang","Fangkun Liu","Andong Li","Xiaodong Li","Chengshi Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.12002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09278v2","updated":"2025-02-17T16:37:49Z","published":"2025-02-13T12:49:25Z","title":"ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View\n  Gaussian Optimization","summary":"  Recent advances in diffusion models have significantly improved 3D\ngeneration, enabling the use of assets generated from an image for embodied AI\nsimulations. However, the one-to-many nature of the image-to-3D problem limits\ntheir use due to inconsistent content and quality across views. Previous models\noptimize a 3D model by sampling views from a view-conditioned diffusion prior,\nbut diffusion models cannot guarantee view consistency. Instead, we present\nConsistentDreamer, where we first generate a set of fixed multi-view prior\nimages and sample random views between them with another diffusion model\nthrough a score distillation sampling (SDS) loss. Thereby, we limit the\ndiscrepancies between the views guided by the SDS loss and ensure a consistent\nrough shape. In each iteration, we also use our generated multi-view prior\nimages for fine-detail reconstruction. To balance between the rough shape and\nthe fine-detail optimizations, we introduce dynamic task-dependent weights\nbased on homoscedastic uncertainty, updated automatically in each iteration.\nAdditionally, we employ opacity, depth distortion, and normal alignment losses\nto refine the surface for mesh extraction. Our method ensures better view\nconsistency and visual quality compared to the state-of-the-art.\n","authors":["Onat ≈ûahin","Mohammad Altillawi","George Eskandar","Carlos Carbone","Ziyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09278v2.pdf","comment":"Manuscript accepted by Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2502.11993v1","updated":"2025-02-17T16:33:59Z","published":"2025-02-17T16:33:59Z","title":"MultiFlow: A unified deep learning framework for multi-vessel\n  classification, segmentation and clustering of phase-contrast MRI validated\n  on a multi-site single ventricle patient cohort","summary":"  This study presents a unified deep learning (DL) framework, MultiFlowSeg, for\nclassification and segmentation of velocity-encoded phase-contrast magnetic\nresonance imaging data, and MultiFlowDTC for temporal clustering of flow\nphenotypes. Applied to the FORCE registry of Fontan procedure patients,\nMultiFlowSeg achieved 100% classification accuracy for the aorta, SVC, and IVC,\nand 94% for the LPA and RPA. It demonstrated robust segmentation with a median\nDice score of 0.91 (IQR: 0.86-0.93). The automated pipeline processed registry\ndata, achieving high segmentation success despite challenges like poor image\nquality and dextrocardia. Temporal clustering identified five distinct patient\nsubgroups, with significant differences in clinical outcomes, including\nejection fraction, exercise tolerance, liver disease, and mortality. These\nresults demonstrate the potential of combining DL and time-varying flow data\nfor improved CHD prognosis and personalized care.\n","authors":["Tina Yao","Nicole St. Clair","Gabriel F. Miller","FORCE Investigators","Jennifer A. Steeden","Rahul H. Rathod","Vivek Muthurangu"],"pdf_url":"https://arxiv.org/pdf/2502.11993v1.pdf","comment":"6 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2502.11992v1","updated":"2025-02-17T16:33:33Z","published":"2025-02-17T16:33:33Z","title":"On the Logic Elements Associated with Round-Off Errors and Gaussian Blur\n  in Image Registration: A Simple Case of Commingling","summary":"  Discrete image registration can be a strategy to reconstruct signals from\nsamples corrupted by blur and noise. We examine superresolution and discrete\nimage registration for one-dimensional spatially-limited piecewise constant\nfunctions which are subject to blur which is Gaussian or a mixture of Gaussians\nas well as to round-off errors. Previous approaches address the signal recovery\nproblem as an optimization problem. We focus on a regime with low blur and\nsuggest that the operations of blur, sampling, and quantization are not unlike\nthe operation of a computer program and have an abstraction that can be studied\nwith a type of logic. When the minimum distance between discontinuity points is\nbetween $1.5$ and 2 times the sampling interval, we can encounter the simplest\nform of a type of interference between discontinuity points that we call\n``commingling.'' We describe a way to reason about two sets of samples of the\nsame signal that will often result in the correct recovery of signal\namplitudes. We also discuss ways to estimate bounds on the distances between\ndiscontinuity points.\n","authors":["Serap A. Savari"],"pdf_url":"https://arxiv.org/pdf/2502.11992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11989v1","updated":"2025-02-17T16:28:15Z","published":"2025-02-17T16:28:15Z","title":"Characterizing Photorealism and Artifacts in Diffusion Model-Generated\n  Images","summary":"  Diffusion model-generated images can appear indistinguishable from authentic\nphotographs, but these images often contain artifacts and implausibilities that\nreveal their AI-generated provenance. Given the challenge to public trust in\nmedia posed by photorealistic AI-generated images, we conducted a large-scale\nexperiment measuring human detection accuracy on 450 diffusion-model generated\nimages and 149 real images. Based on collecting 749,828 observations and 34,675\ncomments from 50,444 participants, we find that scene complexity of an image,\nartifact types within an image, display time of an image, and human curation of\nAI-generated images all play significant roles in how accurately people\ndistinguish real from AI-generated images. Additionally, we propose a taxonomy\ncharacterizing artifacts often appearing in images generated by diffusion\nmodels. Our empirical observations and taxonomy offer nuanced insights into the\ncapabilities and limitations of diffusion models to generate photorealistic\nimages in 2024.\n","authors":["Negar Kamali","Karyn Nakamura","Aakriti Kumar","Angelos Chatzimparmpas","Jessica Hullman","Matthew Groh"],"pdf_url":"https://arxiv.org/pdf/2502.11989v1.pdf","comment":"26 pages, 24 Figures, Accepted by ACM CHI 2025"},{"id":"http://arxiv.org/abs/2502.11974v1","updated":"2025-02-17T16:20:48Z","published":"2025-02-17T16:20:48Z","title":"Image Inversion: A Survey from GANs to Diffusion and Beyond","summary":"  Image inversion is a fundamental task in generative models, aiming to map\nimages back to their latent representations to enable downstream applications\nsuch as editing, restoration, and style transfer. This paper provides a\ncomprehensive review of the latest advancements in image inversion techniques,\nfocusing on two main paradigms: Generative Adversarial Network (GAN) inversion\nand diffusion model inversion. We categorize these techniques based on their\noptimization methods. For GAN inversion, we systematically classify existing\nmethods into encoder-based approaches, latent optimization approaches, and\nhybrid approaches, analyzing their theoretical foundations, technical\ninnovations, and practical trade-offs. For diffusion model inversion, we\nexplore training-free strategies, fine-tuning methods, and the design of\nadditional trainable modules, highlighting their unique advantages and\nlimitations. Additionally, we discuss several popular downstream applications\nand emerging applications beyond image tasks, identifying current challenges\nand future research directions. By synthesizing the latest developments, this\npaper aims to provide researchers and practitioners with a valuable reference\nresource, promoting further advancements in the field of image inversion. We\nkeep track of the latest works at https://github.com/RyanChenYN/ImageInversion\n","authors":["Yinan Chen","Jiangning Zhang","Yali Bi","Xiaobin Hu","Teng Hu","Zhucun Xue","Ran Yi","Yong Liu","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2502.11974v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.11971v1","updated":"2025-02-17T16:18:57Z","published":"2025-02-17T16:18:57Z","title":"Robust 6DoF Pose Tracking Considering Contour and Interior\n  Correspondence Uncertainty for AR Assembly Guidance","summary":"  Augmented reality assembly guidance is essential for intelligent\nmanufacturing and medical applications, requiring continuous measurement of the\n6DoF poses of manipulated objects. Although current tracking methods have made\nsignificant advancements in accuracy and efficiency, they still face challenges\nin robustness when dealing with cluttered backgrounds, rotationally symmetric\nobjects, and noisy sequences. In this paper, we first propose a robust\ncontour-based pose tracking method that addresses error-prone contour\ncorrespondences and improves noise tolerance. It utilizes a fan-shaped search\nstrategy to refine correspondences and models local contour shape and noise\nuncertainty as mixed probability distribution, resulting in a highly robust\ncontour energy function. Secondly, we introduce a CPU-only strategy to better\ntrack rotationally symmetric objects and assist the contour-based method in\novercoming local minima by exploring sparse interior correspondences. This is\nachieved by pre-sampling interior points from sparse viewpoint templates\noffline and using the DIS optical flow algorithm to compute their\ncorrespondences during tracking. Finally, we formulate a unified energy\nfunction to fuse contour and interior information, which is solvable using a\nre-weighted least squares algorithm. Experiments on public datasets and real\nscenarios demonstrate that our method significantly outperforms\nstate-of-the-art monocular tracking methods and can achieve more than 100 FPS\nusing only a CPU.\n","authors":["Jixiang Chen","Jing Chen","Kai Liu","Haochen Chang","Shanfeng Fu","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11971v1.pdf","comment":"Submitted to IEEE Transactions on Instrumentation and Measurement"},{"id":"http://arxiv.org/abs/2502.11969v1","updated":"2025-02-17T16:18:07Z","published":"2025-02-17T16:18:07Z","title":"Learning Generalizable Prompt for CLIP with Class Similarity Knowledge","summary":"  In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.\n","authors":["Sehun Jung","Hyang-won Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v2","updated":"2025-02-17T16:11:44Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023","summary":"  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v2.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2502.11955v1","updated":"2025-02-17T16:05:31Z","published":"2025-02-17T16:05:31Z","title":"pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM","summary":"  pySLAM is an open-source Python framework for Visual SLAM, supporting\nmonocular, stereo, and RGB-D cameras. It provides a flexible interface for\nintegrating both classical and modern local features, making it adaptable to\nvarious SLAM tasks. The framework includes different loop closure methods, a\nvolumetric reconstruction pipeline, and support for depth prediction models.\nAdditionally, it offers a suite of tools for visual odometry and SLAM\napplications. Designed for both beginners and experienced researchers, pySLAM\nencourages community contributions, fostering collaborative development in the\nfield of Visual SLAM.\n","authors":["Luigi Freda"],"pdf_url":"https://arxiv.org/pdf/2502.11955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11925v1","updated":"2025-02-17T15:35:36Z","published":"2025-02-17T15:35:36Z","title":"GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs","summary":"  The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.\n","authors":["Yi Fang","Bowen Jin","Jiacheng Shen","Sirui Ding","Qiaoyu Tan","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2502.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11897v1","updated":"2025-02-17T15:22:31Z","published":"2025-02-17T15:22:31Z","title":"DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation","summary":"  In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a\ntraining-free paradigm that can make use of adaptive temporal compression in\nlatent space. While existing video generative models apply fixed compression\nrates via pretrained VAE, we observe that real-world video content exhibits\nsubstantial temporal non-uniformity, with high-motion segments containing more\ninformation than static scenes. Based on this insight, DLFR-VAE dynamically\nadjusts the latent frame rate according to the content complexity.\nSpecifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent\nFrame Rate Scheduler that partitions videos into temporal chunks and adaptively\ndetermines optimal frame rates based on information-theoretic content\ncomplexity, and (2) A training-free adaptation mechanism that transforms\npretrained VAE architectures into a dynamic VAE that can process features with\nvariable frame rates. Our simple but effective DLFR-VAE can function as a\nplug-and-play module, seamlessly integrating with existing video generation\nmodels and accelerating the video generation process.\n","authors":["Zhihang Yuan","Siyuan Wang","Rui Xie","Hanling Zhang","Tongcheng Fang","Yuzhang Shang","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11891v1","updated":"2025-02-17T15:17:08Z","published":"2025-02-17T15:17:08Z","title":"From Open-Vocabulary to Vocabulary-Free Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation enables models to identify novel object\ncategories beyond their training data. While this flexibility represents a\nsignificant advancement, current approaches still rely on manually specified\nclass names as input, creating an inherent bottleneck in real-world\napplications. This work proposes a Vocabulary-Free Semantic Segmentation\npipeline, eliminating the need for predefined class vocabularies. Specifically,\nwe address the chicken-and-egg problem where users need knowledge of all\npotential objects within a scene to identify them, yet the purpose of\nsegmentation is often to discover these objects. The proposed approach\nleverages Vision-Language Models to automatically recognize objects and\ngenerate appropriate class names, aiming to solve the challenge of class\nspecification and naming quality. Through extensive experiments on several\npublic datasets, we highlight the crucial role of the text encoder in model\nperformance, particularly when the image text classes are paired with generated\ndescriptions. Despite the challenges introduced by the sensitivity of the\nsegmentation text encoder to false negatives within the class tagging process,\nwhich adds complexity to the task, we demonstrate that our fully automated\npipeline significantly enhances vocabulary-free segmentation accuracy across\ndiverse real-world scenarios.\n","authors":["Klara Reichard","Giulia Rizzoli","Stefano Gasperini","Lukas Hoyer","Pietro Zanuttigh","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2502.11891v1.pdf","comment":"Submitted to: Pattern Recognition Letters, Klara Reichard and Giulia\n  Rizzoli equally contributed to this work"},{"id":"http://arxiv.org/abs/2501.15369v2","updated":"2025-02-17T15:09:31Z","published":"2025-01-26T02:34:58Z","title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","summary":"  We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.\n","authors":["Chuanyang Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.15369v2.pdf","comment":"Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer"},{"id":"http://arxiv.org/abs/2403.16998v3","updated":"2025-02-17T14:58:31Z","published":"2024-03-25T17:59:09Z","title":"Understanding Long Videos with Multimodal Language Models","summary":"  Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.\n","authors":["Kanchana Ranasinghe","Xiang Li","Kumara Kahatapitiya","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2403.16998v3.pdf","comment":"Code available at https://github.com/kahnchana/mvu"},{"id":"http://arxiv.org/abs/2502.11864v1","updated":"2025-02-17T14:56:25Z","published":"2025-02-17T14:56:25Z","title":"Does Knowledge About Perceptual Uncertainty Help an Agent in Automated\n  Driving?","summary":"  Agents in real-world scenarios like automated driving deal with uncertainty\nin their environment, in particular due to perceptual uncertainty. Although,\nreinforcement learning is dedicated to autonomous decision-making under\nuncertainty these algorithms are typically not informed about the uncertainty\ncurrently contained in their environment. On the other hand, uncertainty\nestimation for perception itself is typically directly evaluated in the\nperception domain, e.g., in terms of false positive detection rates or\ncalibration errors based on camera images. Its use for deciding on\ngoal-oriented actions remains largely unstudied. In this paper, we investigate\nhow an agent's behavior is influenced by an uncertain perception and how this\nbehavior changes if information about this uncertainty is available. Therefore,\nwe consider a proxy task, where the agent is rewarded for driving a route as\nfast as possible without colliding with other road users. For controlled\nexperiments, we introduce uncertainty in the observation space by perturbing\nthe perception of the given agent while informing the latter. Our experiments\nshow that an unreliable observation space modeled by a perturbed perception\nleads to a defensive driving behavior of the agent. Furthermore, when adding\nthe information about the current uncertainty directly to the observation\nspace, the agent adapts to the specific situation and in general accomplishes\nits task faster while, at the same time, accounting for risks.\n","authors":["Natalie Grabowsky","Annika M√ºtze","Joshua Wendland","Nils Jansen","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2502.11864v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.11859v1","updated":"2025-02-17T14:50:53Z","published":"2025-02-17T14:50:53Z","title":"Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics","summary":"  The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.\n","authors":["Wenrui Xu","Dalin Lyu","Weihang Wang","Jie Feng","Chen Gao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.11859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11858v1","updated":"2025-02-17T14:50:34Z","published":"2025-02-17T14:50:34Z","title":"Rethinking Audio-Visual Adversarial Vulnerability from Temporal and\n  Modality Perspectives","summary":"  While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency.\n","authors":["Zeliang Zhang","Susan Liang","Daiki Shimada","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11858v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11850v1","updated":"2025-02-17T14:44:12Z","published":"2025-02-17T14:44:12Z","title":"Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif\n  Discovery","summary":"  Time Series Motif Discovery (TSMD) identifies repeating patterns in time\nseries data, but its unsupervised nature might result in motifs that are not\ninteresting to the user. To address this, we propose a framework that allows\nthe user to impose constraints on the motifs to be discovered, where\nconstraints can easily be defined according to the properties of the desired\nmotifs in the application domain. We also propose an efficient implementation\nof the framework, the LoCoMotif-DoK algorithm. We demonstrate that\nLoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic\ndata, outperforming other TSMD techniques which only support a limited form of\ndomain knowledge.\n","authors":["Aras Yurtman","Daan Van Wesenbeeck","Wannes Meert","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2502.11850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00464v2","updated":"2025-02-17T14:44:05Z","published":"2025-02-01T15:48:20Z","title":"Evaluation of End-to-End Continuous Spanish Lipreading in Different Data\n  Conditions","summary":"  Visual speech recognition remains an open research problem where different\nchallenges must be considered by dispensing with the auditory sense, such as\nvisual ambiguities, the inter-personal variability among speakers, and the\ncomplex modeling of silence. Nonetheless, recent remarkable results have been\nachieved in the field thanks to the availability of large-scale databases and\nthe use of powerful attention mechanisms. Besides, multiple languages apart\nfrom English are nowadays a focus of interest. This paper presents noticeable\nadvances in automatic continuous lipreading for Spanish. First, an end-to-end\nsystem based on the hybrid CTC/Attention architecture is presented. Experiments\nare conducted on two corpora of disparate nature, reaching state-of-the-art\nresults that significantly improve the best performance obtained to date for\nboth databases. In addition, a thorough ablation study is carried out, where it\nis studied how the different components that form the architecture influence\nthe quality of speech recognition. Then, a rigorous error analysis is carried\nout to investigate the different factors that could affect the learning of the\nautomatic system. Finally, a new Spanish lipreading benchmark is consolidated.\nCode and trained models are available at\nhttps://github.com/david-gimeno/evaluating-end2end-spanish-lipreading.\n","authors":["David Gimeno-G√≥mez","Carlos-D. Mart√≠nez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2502.00464v2.pdf","comment":"Accepted in the \"Language Resources and Evaluation\" journal, Springer\n  Nature"},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11831v1","updated":"2025-02-17T14:27:14Z","published":"2025-02-17T14:27:14Z","title":"Intuitive physics understanding emerges from self-supervised pretraining\n  on natural videos","summary":"  We investigate the emergence of intuitive physics understanding in\ngeneral-purpose deep neural network models trained to predict masked regions in\nnatural videos. Leveraging the violation-of-expectation framework, we find that\nvideo prediction models trained to predict outcomes in a learned representation\nspace demonstrate an understanding of various intuitive physics properties,\nsuch as object permanence and shape consistency. In contrast, video prediction\nin pixel space and multimodal large language models, which reason through text,\nachieve performance closer to chance. Our comparisons of these architectures\nreveal that jointly learning an abstract representation space while predicting\nmissing parts of sensory input, akin to predictive coding, is sufficient to\nacquire an understanding of intuitive physics, and that even models trained on\none week of unique video achieve above chance performance. This challenges the\nidea that core knowledge -- a set of innate systems to help understand the\nworld -- needs to be hardwired to develop an understanding of intuitive\nphysics.\n","authors":["Quentin Garrido","Nicolas Ballas","Mahmoud Assran","Adrien Bardes","Laurent Najman","Michael Rabbat","Emmanuel Dupoux","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2502.11831v1.pdf","comment":"24 pages,14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2411.14467v2","updated":"2025-02-17T14:21:20Z","published":"2024-11-18T15:46:39Z","title":"Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device\n  Triggers for Insect Camera Traps","summary":"  Camera traps, combined with AI, have emerged as a way to achieve automated,\nscalable biodiversity monitoring. However, the passive infrared (PIR) sensors\nthat trigger camera traps are poorly suited for detecting small, fast-moving\nectotherms such as insects. Insects comprise over half of all animal species\nand are key components of ecosystems and agriculture. The need for an\nappropriate and scalable insect camera trap is critical in the wake of\nconcerning reports of declines in insect populations. This study proposes an\nalternative to the PIR trigger: ultra-lightweight convolutional neural networks\nrunning on low-powered hardware to detect insects in a continuous stream of\ncaptured images. We train a suite of models to distinguish insect images from\nbackgrounds. Our design achieves zero latency between trigger and image\ncapture. Our models are rigorously tested and achieve high accuracy ranging\nfrom 91.8% to 96.4% AUC on validation data and >87% AUC on data from\ndistributions unseen during training. The high specificity of our models\nensures minimal saving of false positive images, maximising deployment storage\nefficiency. High recall scores indicate a minimal false negative rate,\nmaximising insect detection. Further analysis with saliency maps shows the\nlearned representation of our models to be robust, with low reliance on\nspurious background features. Our system is also shown to operate deployed on\noff-the-shelf, low-powered microcontroller units, consuming a maximum power\ndraw of less than 300mW. This enables longer deployment times using cheap and\nreadily available battery components. Overall we offer a step change in the\ncost, efficiency and scope of insect monitoring. Solving the challenging\ntrigger problem, we demonstrate a system which can be deployed for far longer\nthan existing designs and budgets power and bandwidth effectively, moving\ntowards a generic insect camera trap.\n","authors":["Ross Gardiner","Sareh Rowands","Benno I. Simmons"],"pdf_url":"https://arxiv.org/pdf/2411.14467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11809v1","updated":"2025-02-17T13:54:02Z","published":"2025-02-17T13:54:02Z","title":"Revealing Bias Formation in Deep Neural Networks Through the Geometric\n  Mechanisms of Human Visual Decoupling","summary":"  Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.\n","authors":["Yanbiao Ma","Bowei Liu","Wei Dai","Jiayi Chen","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2502.11809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11801v1","updated":"2025-02-17T13:46:47Z","published":"2025-02-17T13:46:47Z","title":"3D Gaussian Inpainting with Depth-Guided Cross-View Consistency","summary":"  When performing 3D inpainting using novel-view rendering methods like Neural\nRadiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture\nand geometry consistency across camera views has been a challenge. In this\npaper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided\nCross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided\nby the rendered depth information from each training view, our 3DGIC exploits\nbackground pixels visible across different views for updating the inpainting\nmask, allowing us to refine the 3DGS for inpainting purposes.Through extensive\nexperiments on benchmark datasets, we confirm that our 3DGIC outperforms\ncurrent state-of-the-art 3D inpainting methods quantitatively and\nqualitatively.\n","authors":["Sheng-Yu Huang","Zi-Ting Chou","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07758v2","updated":"2025-02-17T13:44:46Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Drago≈ü Du≈üe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander R√∂lle","Christina C. Westhoff","B√©n√©dicte Lenoir","Niels Halama","Inka Z√∂rnig","Dirk J√§ger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v2.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.07173v2","updated":"2025-02-17T13:25:17Z","published":"2024-10-09T17:59:33Z","title":"Better Language Models Exhibit Higher Visual Alignment","summary":"  How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08474v2","updated":"2025-02-17T13:22:54Z","published":"2024-09-13T02:00:16Z","title":"Rethinking Meta-Learning from a Learning Lens","summary":"  Meta-learning has emerged as a powerful approach for leveraging knowledge\nfrom previous tasks to solve new tasks. The mainstream methods focus on\ntraining a well-generalized model initialization, which is then adapted to\ndifferent tasks with limited data and updates. However, it pushes the model\noverfitting on the training tasks. Previous methods mainly attributed this to\nthe lack of data and used augmentations to address this issue, but they were\nlimited by sufficient training and effective augmentation strategies. In this\nwork, we focus on the more fundamental learning to learn strategy of\nmeta-learning to explore what causes errors and how to eliminate these errors\nwithout changing the environment. Specifically, we first rethink the\nalgorithmic procedure of meta-learning from a learning lens. Through\ntheoretical and empirical analyses, we find that (i) this paradigm faces the\nrisk of both overfitting and underfitting and (ii) the model adapted to\ndifferent tasks promote each other where the effect is stronger if the tasks\nare more similar. Based on this insight, we propose using task relations to\ncalibrate the optimization process of meta-learning and propose a plug-and-play\nmethod called Task Relation Learner (TRLearner) to achieve this goal.\nSpecifically, it first obtains task relation matrices from the extracted\ntask-specific meta-data. Then, it uses the obtained matrices with\nrelation-aware consistency regularization to guide optimization. Extensive\ntheoretical and empirical analyses demonstrate the effectiveness of TRLearner.\n","authors":["Jingyao Wang","Wenwen Qiang","Chuxiong Sun","Changwen Zheng","Jiangmeng Li"],"pdf_url":"https://arxiv.org/pdf/2409.08474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11777v1","updated":"2025-02-17T13:11:35Z","published":"2025-02-17T13:11:35Z","title":"Deep Neural Networks for Accurate Depth Estimation with Latent Space\n  Features","summary":"  Depth estimation plays a pivotal role in advancing human-robot interactions,\nespecially in indoor environments where accurate 3D scene reconstruction is\nessential for tasks like navigation and object handling. Monocular depth\nestimation, which relies on a single RGB camera, offers a more affordable\nsolution compared to traditional methods that use stereo cameras or LiDAR.\nHowever, despite recent progress, many monocular approaches struggle with\naccurately defining depth boundaries, leading to less precise reconstructions.\nIn response to these challenges, this study introduces a novel depth estimation\nframework that leverages latent space features within a deep convolutional\nneural network to enhance the precision of monocular depth maps. The proposed\nmodel features dual encoder-decoder architecture, enabling both color-to-depth\nand depth-to-depth transformations. This structure allows for refined depth\nestimation through latent space encoding. To further improve the accuracy of\ndepth boundaries and local features, a new loss function is introduced. This\nfunction combines latent loss with gradient loss, helping the model maintain\nthe integrity of depth boundaries. The framework is thoroughly tested using the\nNYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in\ncomplex indoor scenarios. The results clearly show that this approach\neffectively reduces depth ambiguities and blurring, making it a promising\nsolution for applications in human-robot interaction and 3D scene\nreconstruction.\n","authors":["Siddiqui Muhammad Yasir","Hyunsik Ahn"],"pdf_url":"https://arxiv.org/pdf/2502.11777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11775v1","updated":"2025-02-17T13:07:40Z","published":"2025-02-17T13:07:40Z","title":"video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model","summary":"  While recent advancements in reasoning optimization have significantly\nenhanced the capabilities of large language models (LLMs), existing efforts to\nimprove reasoning have been limited to solving mathematical problems and\nfocusing on visual graphical inputs, neglecting broader applications in general\nvideo understanding.This paper proposes video-SALMONN-o1, the first open-source\nreasoning-enhanced audio-visual LLM designed for general video understanding\ntasks. To enhance its reasoning abilities, we develop a reasoning-intensive\ndataset featuring challenging audio-visual questions with step-by-step\nsolutions. We also propose process direct preference optimization (pDPO), which\nleverages contrastive step selection to achieve efficient step-level reward\nmodelling tailored for multimodal inputs. Additionally, we introduce RivaBench,\nthe first reasoning-intensive video understanding benchmark, featuring over\n4,000 high-quality, expert-curated question-answer pairs across scenarios such\nas standup comedy, academic presentations, and synthetic video detection.\nvideo-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision\nbaseline across different video reasoning benchmarks. Besides, pDPO achieves\n6-8% improvements compared to the supervised fine-tuning model on RivaBench.\nEnhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection\ncapabilities.\n","authors":["Guangzhi Sun","Yudong Yang","Jimin Zhuang","Changli Tang","Yixuan Li","Wei Li","Zejun MA","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08545v3","updated":"2025-02-17T12:59:13Z","published":"2025-01-15T03:11:33Z","title":"T2VEval: T2V-generated Videos Benchmark Dataset and Objective Evaluation\n  Method","summary":"  Recent advances in text-to-video (T2V) technology, as demonstrated by models\nsuch as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the\napplicability and popularity of the technology. This progress has created a\ngrowing demand for accurate quality assessment metrics to evaluate the\nperceptual quality of T2V-generated videos and optimize video generation\nmodels. However, assessing the quality of text-to-video outputs remain\nchallenging due to the presence of highly complex distortions, such as\nunnatural actions and phenomena that defy human cognition. To address these\nchallenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset\nfor text-to-video quality evaluation, comprising 148 textual prompts and 1,783\nvideos generated by 13 T2V models. To ensure a comprehensive evaluation, we\nscored each video on four dimensions in the subjective experiment, which are\noverall impression, text-video consistency, realness, and technical quality.\nBased on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for\nT2V quality evaluation. T2VEval assesses videos across three branches:\ntext-video consistency, realness, and technical quality. Using an\nattention-based fusion module, T2VEval effectively integrates features from\neach branch and predicts scores with the aid of a large language model.\nAdditionally, we implemented a progressive training strategy, enabling each\nbranch to learn targeted knowledge while maintaining synergy with the others.\nExperimental results demonstrate that T2VEval achieves state-of-the-art\nperformance across multiple metrics.\n","authors":["Zelu Qi","Ping Shi","Shuqi Wang","Zhaoyang Zhang","Fei Zhao","Zefeng Ying","Da Pan"],"pdf_url":"https://arxiv.org/pdf/2501.08545v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07560v2","updated":"2025-02-17T12:57:42Z","published":"2025-02-11T13:57:30Z","title":"Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning","summary":"  Class-incremental learning (CIL) seeks to enable a model to sequentially\nlearn new classes while retaining knowledge of previously learned ones.\nBalancing flexibility and stability remains a significant challenge,\nparticularly when the task ID is unknown. To address this, our study reveals\nthat the gap in feature distribution between novel and existing tasks is\nprimarily driven by differences in mean and covariance moments. Building on\nthis insight, we propose a novel semantic drift calibration method that\nincorporates mean shift compensation and covariance calibration. Specifically,\nwe calculate each class's mean by averaging its sample embeddings and estimate\ntask shifts using weighted embedding changes based on their proximity to the\nprevious mean, effectively capturing mean shifts for all learned classes with\neach new task. We also apply Mahalanobis distance constraint for covariance\ncalibration, aligning class-specific embedding covariances between old and\ncurrent networks to mitigate the covariance shift. Additionally, we integrate a\nfeature-level self-distillation approach to enhance generalization.\nComprehensive experiments on commonly used datasets demonstrate the\neffectiveness of our approach. The source code is available at\n\\href{https://github.com/fwu11/MACIL.git}{https://github.com/fwu11/MACIL.git}.\n","authors":["Fangwen Wu","Lechao Cheng","Shengeng Tang","Xiaofeng Zhu","Chaowei Fang","Dingwen Zhang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07560v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.11763v1","updated":"2025-02-17T12:55:41Z","published":"2025-02-17T12:55:41Z","title":"Lightweight Deepfake Detection Based on Multi-Feature Fusion","summary":"  Deepfake technology utilizes deep learning based face manipulation techniques\nto seamlessly replace faces in videos creating highly realistic but\nartificially generated content. Although this technology has beneficial\napplications in media and entertainment misuse of its capabilities may lead to\nserious risks including identity theft cyberbullying and false information. The\nintegration of DL with visual cognition has resulted in important technological\nimprovements particularly in addressing privacy risks caused by artificially\ngenerated deepfake images on digital media platforms. In this study we propose\nan efficient and lightweight method for detecting deepfake images and videos\nmaking it suitable for devices with limited computational resources. In order\nto reduce the computational burden usually associated with DL models our method\nintegrates machine learning classifiers in combination with keyframing\napproaches and texture analysis. Moreover the features extracted with a\nhistogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands\nwere integrated to evaluate using random forest extreme gradient boosting extra\ntrees and support vector classifier algorithms. Our findings show a\nfeature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and\n96% on FaceForensics++ and Celeb-DFv2 respectively.\n","authors":["Siddiqui Muhammad Yasir","Hyun Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08075v2","updated":"2025-02-17T12:53:00Z","published":"2025-02-12T02:37:16Z","title":"Knowledge Swapping via Learning and Unlearning","summary":"  We introduce \\textbf{Knowledge Swapping}, a novel task designed to\nselectively regulate knowledge of a pretrained model by enabling the forgetting\nof user\\-specified information, retaining essential knowledge, and acquiring\nnew knowledge simultaneously. By delving into the analysis of knock-on feature\nhierarchy, we find that incremental learning typically progresses from\nlow\\-level representations to higher\\-level semantics, whereas forgetting tends\nto occur in the opposite direction\\-starting from high-level semantics and\nmoving down to low-level features. Building upon this, we propose to benchmark\nthe knowledge swapping task with the strategy of \\textit{Learning Before\nForgetting}. Comprehensive experiments on various tasks like image\nclassification, object detection, and semantic segmentation validate the\neffectiveness of the proposed strategy. The source code is available at\n\\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.\n","authors":["Mingyu Xing","Lechao Cheng","Shengeng Tang","Yaxiong Wang","Zhun Zhong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08075v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.11756v1","updated":"2025-02-17T12:52:10Z","published":"2025-02-17T12:52:10Z","title":"On the Computation of the Fisher Information in Continual Learning","summary":"  One of the most popular methods for continual learning with deep neural\nnetworks is Elastic Weight Consolidation (EWC), which involves computing the\nFisher Information. The exact way in which the Fisher Information is computed\nis however rarely described, and multiple different implementations for it can\nbe found online. This blog post discusses and empirically compares several\noften-used implementations, which highlights that many currently reported\nresults for EWC could likely be improved by changing the way the Fisher\nInformation is computed.\n","authors":["Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2502.11756v1.pdf","comment":"To appear in the blogpost track at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11751v1","updated":"2025-02-17T12:47:00Z","published":"2025-02-17T12:47:00Z","title":"Language Models Can See Better: Visual Contrastive Decoding For LLM\n  Multimodal Reasoning","summary":"  Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https://github.com/Pbhgit/MVCD.\n","authors":["Yuqi Pang","Bowen Yang","Haoqin Tu","Yun Cao","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11751v1.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.11749v1","updated":"2025-02-17T12:43:04Z","published":"2025-02-17T12:43:04Z","title":"JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling\n  Network for Accelerating Dynamic MRI","summary":"  Joint low-rank and sparse unrolling networks have shown superior performance\nin dynamic MRI reconstruction. However, existing works mainly utilized matrix\nlow-rank priors, neglecting the tensor characteristics of dynamic MRI images,\nand only a global threshold is applied for the sparse constraint to the\nmulti-channel data, limiting the flexibility of the network. Additionally, most\nof them have inherently complex network structure, with intricate interactions\namong variables. In this paper, we propose a novel deep unrolling network,\nJotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank\nand attention-based sparse priors. Specifically, we utilize tensor low-rank\nprior to exploit the structural correlations in high-dimensional data.\nConvolutional neural networks are used to adaptively learn the low-rank and\nsparse transform domains. A novel attention-based soft thresholding operator is\nproposed to assign a unique learnable threshold to each channel of the data in\nthe CNN-learned sparse domain. The network is unrolled from the elaborately\ndesigned composite splitting algorithm and thus features a simple yet efficient\nparallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)\ndemonstrate the superior performance of JotlasNet in dynamic MRI\nreconstruction.\n","authors":["Yinghao Zhang","Haiyan Gui","Ningdi Yang","Yue Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11749v1.pdf","comment":"13 pages, 7 figures, accepted by Magnetic Resonance Imaging"},{"id":"http://arxiv.org/abs/2502.11748v1","updated":"2025-02-17T12:42:38Z","published":"2025-02-17T12:42:38Z","title":"ILIAS: Instance-Level Image retrieval At Scale","summary":"  This work introduces ILIAS, a new test dataset for Instance-Level Image\nretrieval At Scale. It is designed to evaluate the ability of current and\nfuture foundation models and retrieval techniques to recognize particular\nobjects. The key benefits over existing datasets include large scale, domain\ndiversity, accurate ground truth, and a performance that is far from saturated.\nILIAS includes query and positive images for 1,000 object instances, manually\ncollected to capture challenging conditions and diverse domains. Large-scale\nretrieval is conducted against 100 million distractor images from YFCC100M. To\navoid false negatives without extra annotation effort, we include only query\nobjects confirmed to have emerged after 2014, i.e. the compilation date of\nYFCC100M. An extensive benchmarking is performed with the following\nobservations: i) models fine-tuned on specific domains, such as landmarks or\nproducts, excel in that domain but fail on ILIAS ii) learning a linear\nadaptation layer using multi-domain class supervision results in performance\nimprovements, especially for vision-language models iii) local descriptors in\nretrieval re-ranking are still a key ingredient, especially in the presence of\nsevere background clutter iv) the text-to-image performance of the\nvision-language foundation models is surprisingly close to the corresponding\nimage-to-image case. website: https://vrg.fel.cvut.cz/ilias/\n","authors":["Giorgos Kordopatis-Zilos","Vladan Stojniƒá","Anna Manko","Pavel ≈†uma","Nikolaos-Antonios Ypsilantis","Nikos Efthymiadis","Zakaria Laskar","Ji≈ô√≠ Matas","Ond≈ôej Chum","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2502.11748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02401v7","updated":"2025-02-17T12:37:09Z","published":"2024-10-03T11:29:09Z","title":"SynCo: Synthetic Hard Negatives for Contrastive Visual Representation\n  Learning","summary":"  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning, but efficiently leveraging hard negatives, which are\nsamples closely resembling the anchor, remains challenging. We introduce SynCo\n(Synthetic negatives in Contrastive learning), a novel approach that improves\nmodel performance by generating synthetic hard negatives on the representation\nspace. Building on the MoCo framework, SynCo introduces six strategies for\ncreating diverse synthetic hard negatives on-the-fly with minimal computational\noverhead. SynCo achieves faster training and strong representation learning,\nsurpassing MoCo-v2 by +0.4% and MoCHI by +1.0% on ImageNet ILSVRC-2012 linear\nevaluation. It also transfers more effectively to detection tasks achieving\nstrong results on PASCAL VOC detection (57.2% AP) and significantly improving\nover MoCo-v2 on COCO detection (+1.0% AP) and instance segmentation (+0.8% AP).\nOur synthetic hard negative generation approach significantly enhances visual\nrepresentations learned through self-supervised contrastive learning.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.02401v7.pdf","comment":"Preprint. Code: https://github.com/giakoumoglou/synco, Supplementary:\n  https://giakoumoglou.com/src/synco_suppl.pdf"},{"id":"http://arxiv.org/abs/2502.05540v2","updated":"2025-02-17T12:36:11Z","published":"2025-02-08T12:10:02Z","title":"Demystifying Catastrophic Forgetting in Two-Stage Incremental Object\n  Detector","summary":"  Catastrophic forgetting is a critical chanllenge for incremental object\ndetection (IOD). Most existing methods treat the detector monolithically,\nrelying on instance replay or knowledge distillation without analyzing\ncomponent-specific forgetting. Through dissection of Faster R-CNN, we reveal a\nkey insight: Catastrophic forgetting is predominantly localized to the RoI Head\nclassifier, while regressors retain robustness across incremental stages. This\nfinding challenges conventional assumptions, motivating us to develop a\nframework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates\nclassifier forgetting via replay of two types of prototypes: coarse prototypes\nrepresent class-wise semantic centers of RoI features, while fine-grained\nprototypes model intra-class variations. Null Space Gradient Projection (NSGP)\nis further introduced to eliminate prototype-feature misalignment by updating\nthe feature extractor in directions orthogonal to subspace of old inputs via\ngradient projection, aligning RePRE with incremental learning dynamics. Our\nsimple yet effective design allows NSGP-RePRE to achieve state-of-the-art\nperformance on the Pascal VOC and MS COCO datasets under various settings. Our\nwork not only advances IOD methodology but also provide pivotal insights for\ncatastrophic forgetting mitigation in IOD. Code will be available soon.\n","authors":["Qirui Wu","Shizhou Zhang","De Cheng","Yinghui Xing","Di Xu","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05540v2.pdf","comment":"14 pages, 7 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.11744v1","updated":"2025-02-17T12:34:42Z","published":"2025-02-17T12:34:42Z","title":"FUNCTO: Function-Centric One-Shot Imitation Learning for Tool\n  Manipulation","summary":"  Learning tool use from a single human demonstration video offers a highly\nintuitive and efficient approach to robot teaching. While humans can\neffortlessly generalize a demonstrated tool manipulation skill to diverse tools\nthat support the same function (e.g., pouring with a mug versus a teapot),\ncurrent one-shot imitation learning (OSIL) methods struggle to achieve this. A\nkey challenge lies in establishing functional correspondences between\ndemonstration and test tools, considering significant geometric variations\namong tools with the same function (i.e., intra-function variations). To\naddress this challenge, we propose FUNCTO (Function-Centric OSIL for Tool\nManipulation), an OSIL method that establishes function-centric correspondences\nwith a 3D functional keypoint representation, enabling robots to generalize\ntool manipulation skills from a single human demonstration video to novel tools\nwith the same function despite significant intra-function variations. With this\nformulation, we factorize FUNCTO into three stages: (1) functional keypoint\nextraction, (2) function-centric correspondence establishment, and (3)\nfunctional keypoint-based action planning. We evaluate FUNCTO against exiting\nmodular OSIL methods and end-to-end behavioral cloning methods through\nreal-robot experiments on diverse tool manipulation tasks. The results\ndemonstrate the superiority of FUNCTO when generalizing to novel tools with\nintra-function geometric variations. More details are available at\nhttps://sites.google.com/view/functo.\n","authors":["Chao Tang","Anxing Xiao","Yuhong Deng","Tianrun Hu","Wenlong Dong","Hanbo Zhang","David Hsu","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11742v1","updated":"2025-02-17T12:29:26Z","published":"2025-02-17T12:29:26Z","title":"Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition","summary":"  Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a\nchallenging task where the query is an RGB image, and the database samples are\nLiDAR point clouds. Compared to single-modal VPR, this approach benefits from\nthe widespread availability of RGB cameras and the robustness of point clouds\nin providing accurate spatial geometry and distance information. However,\ncurrent methods rely on intermediate modalities that capture either the\nvertical or horizontal field of view, limiting their ability to fully exploit\nthe complementary information from both sensors. In this work, we propose an\ninnovative initial retrieval + re-rank method that effectively combines\ninformation from range (or RGB) images and Bird's Eye View (BEV) images. Our\napproach relies solely on a computationally efficient global descriptor\nsimilarity search process to achieve re-ranking. Additionally, we introduce a\nnovel similarity label supervision technique to maximize the utility of limited\ntraining data. Specifically, we employ points average distance to approximate\nappearance similarity and incorporate an adaptive margin, based on similarity\ndifferences, into the vanilla triplet loss. Experimental results on the KITTI\ndataset demonstrate that our method significantly outperforms state-of-the-art\napproaches.\n","authors":["Jianyi Peng","Fan Lu","Bin Li","Yuan Huang","Sanqing Qu","Guang Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11742v1.pdf","comment":"Submmitted to IEEE IV 2025"},{"id":"http://arxiv.org/abs/2502.11740v1","updated":"2025-02-17T12:26:34Z","published":"2025-02-17T12:26:34Z","title":"Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via\n  Modality-decoupled Gradient Descent","summary":"  Recent MLLMs have shown emerging visual understanding and reasoning abilities\nafter being pre-trained on large-scale multimodal datasets. Unlike\npre-training, where MLLMs receive rich visual-text alignment,\ninstruction-tuning is often text-driven with weaker visual supervision, leading\nto the degradation of pre-trained visual understanding and causing visual\nforgetting. Existing approaches, such as direct fine-tuning and continual\nlearning methods, fail to explicitly address this issue, often compressing\nvisual representations and prioritizing task alignment over visual retention,\nwhich further worsens visual forgetting. To overcome this limitation, we\nintroduce a novel perspective leveraging effective rank to quantify the\ndegradation of visual representation richness, interpreting this degradation\nthrough the information bottleneck principle as excessive compression that\nleads to the degradation of crucial pre-trained visual knowledge. Building on\nthis view, we propose a modality-decoupled gradient descent (MDGD) method that\nregulates gradient updates to maintain the effective rank of visual\nrepresentations while mitigating the over-compression effects described by the\ninformation bottleneck. By explicitly disentangling the optimization of visual\nunderstanding from task-specific alignment, MDGD preserves pre-trained visual\nknowledge while enabling efficient task adaptation. To enable lightweight\ninstruction-tuning, we further develop a memory-efficient fine-tuning approach\nusing gradient masking, which selectively updates a subset of model parameters\nto enable parameter-efficient fine-tuning (PEFT), reducing computational\noverhead while preserving rich visual representations. Extensive experiments\nacross various downstream tasks and backbone MLLMs demonstrate that MDGD\neffectively mitigates visual forgetting from pre-trained tasks while enabling\nstrong adaptation to new tasks.\n","authors":["Junda Wu","Yuxin Xiong","Xintong Li","Yu Xia","Ruoyu Wang","Yu Wang","Tong Yu","Sungchul Kim","Ryan A. Rossi","Lina Yao","Jingbo Shang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.11740v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.11731v1","updated":"2025-02-17T12:18:24Z","published":"2025-02-17T12:18:24Z","title":"GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs","summary":"  Accurately restoring topology is both challenging and crucial in tubular\nstructure extraction tasks, such as blood vessel segmentation and road network\nextraction. Diverging from traditional approaches based on pixel-level\nclassification, our proposed method, named GraphMorph, focuses on branch-level\nfeatures of tubular structures to achieve more topologically accurate\npredictions. GraphMorph comprises two main components: a Graph Decoder and a\nMorph Module. Utilizing multi-scale features extracted from an image patch by\nthe segmentation network, the Graph Decoder facilitates the learning of\nbranch-level features and generates a graph that accurately represents the\ntubular structure in this patch. The Morph Module processes two primary inputs:\nthe graph and the centerline probability map, provided by the Graph Decoder and\nthe segmentation network, respectively. Employing a novel SkeletonDijkstra\nalgorithm, the Morph Module produces a centerline mask that aligns with the\npredicted graph. Furthermore, we observe that employing centerline masks\npredicted by GraphMorph significantly reduces false positives in the\nsegmentation task, which is achieved by a simple yet effective post-processing\nstrategy. The efficacy of our method in the centerline extraction and\nsegmentation tasks has been substantiated through experimental evaluations\nacross various datasets. Source code will be released soon.\n","authors":["Zhao Zhang","Ziwei Zhao","Dong Wang","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11731v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.03517v2","updated":"2025-02-17T12:13:15Z","published":"2023-10-05T12:56:34Z","title":"PrototypeFormer: Learning to Explore Prototype Relationships for\n  Few-shot Image Classification","summary":"  Few-shot image classification has received considerable attention for\novercoming the challenge of limited classification performance with limited\nsamples in novel classes. Most existing works employ sophisticated learning\nstrategies and feature learning modules to alleviate this challenge. In this\npaper, we propose a novel method called PrototypeFormer, exploring the\nrelationships among category prototypes in the few-shot scenario. Specifically,\nwe utilize a transformer architecture to build a prototype extraction module,\naiming to extract class representations that are more discriminative for\nfew-shot classification. Besides, during the model training process, we propose\na contrastive learning-based optimization approach to optimize prototype\nfeatures in few-shot learning scenarios. Despite its simplicity, our method\nperforms remarkably well, with no bells and whistles. We have experimented with\nour approach on several popular few-shot image classification benchmark\ndatasets, which shows that our method outperforms all current state-of-the-art\nmethods. In particular, our method achieves 97.07\\% and 90.88\\% on 5-way 5-shot\nand 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art\nresults with accuracy of 0.57\\% and 6.84\\%, respectively. The code will be\nreleased later.\n","authors":["Meijuan Su","Feihong He","Fanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2310.03517v2.pdf","comment":"Submitted to Neurocomputing"},{"id":"http://arxiv.org/abs/2406.10469v2","updated":"2025-02-17T12:12:25Z","published":"2024-06-15T02:19:31Z","title":"Object-Attribute-Relation Representation Based Video Semantic\n  Communication","summary":"  With the rapid growth of multimedia data volume, there is an increasing need\nfor efficient video transmission in applications such as virtual reality and\nfuture video streaming services. Semantic communication is emerging as a vital\ntechnique for ensuring efficient and reliable transmission in low-bandwidth,\nhigh-noise settings. However, most current approaches focus on joint\nsource-channel coding (JSCC) that depends on end-to-end training. These methods\noften lack an interpretable semantic representation and struggle with\nadaptability to various downstream tasks. In this paper, we introduce the use\nof object-attribute-relation (OAR) as a semantic framework for videos to\nfacilitate low bit-rate coding and enhance the JSCC process for more effective\nvideo transmission. We utilize OAR sequences for both low bit-rate\nrepresentation and generative video reconstruction. Additionally, we\nincorporate OAR into the image JSCC model to prioritize communication resources\nfor areas more critical to downstream tasks. Our experiments on traffic\nsurveillance video datasets assess the effectiveness of our approach in terms\nof video transmission performance. The empirical findings demonstrate that our\nOAR-based video coding method not only outperforms H.265 coding at lower\nbit-rates but also synergizes with JSCC to deliver robust and efficient video\ntransmission.\n","authors":["Qiyuan Du","Yiping Duan","Qianqian Yang","Xiaoming Tao","M√©rouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2406.10469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11726v1","updated":"2025-02-17T12:11:56Z","published":"2025-02-17T12:11:56Z","title":"No-reference geometry quality assessment for colorless point clouds via\n  list-wise rank learning","summary":"  Geometry quality assessment (GQA) of colorless point clouds is crucial for\nevaluating the performance of emerging point cloud-based solutions (e.g.,\nwatermarking, compression, and 3-Dimensional (3D) reconstruction).\nUnfortunately, existing objective GQA approaches are traditional full-reference\nmetrics, whereas state-of-the-art learning-based point cloud quality assessment\n(PCQA) methods target both color and geometry distortions, neither of which are\nqualified for the no-reference GQA task. In addition, the lack of large-scale\nGQA datasets with subjective scores, which are always imprecise, biased, and\ninconsistent, also hinders the development of learning-based GQA metrics.\nDriven by these limitations, this paper proposes a no-reference geometry-only\nquality assessment approach based on list-wise rank learning, termed LRL-GQA,\nwhich comprises of a geometry quality assessment network (GQANet) and a\nlist-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the\nno-reference GQA as a list-wise rank problem, with the objective of directly\noptimizing the entire quality ordering. Specifically, a large dataset\ncontaining a variety of geometry-only distortions is constructed first, named\nLRL dataset, in which each sample is label-free but coupled with quality\nranking information. Then, the GQANet is designed to capture intrinsic\nmulti-scale patch-wise geometric features in order to predict a quality index\nfor each point cloud. After that, the LRLNet leverages the LRL dataset and a\nlikelihood loss to train the GQANet and ranks the input list of degraded point\nclouds according to their distortion levels. In addition, the pre-trained\nGQANet can be fine-tuned further to obtain absolute quality scores.\nExperimental results demonstrate the superior performance of the proposed\nno-reference LRL-GQA method compared with existing full-reference GQA metrics.\n","authors":["Zheng Li","Bingxu Xie","Chao Chu","Weiqing Li","Zhiyong Su"],"pdf_url":"https://arxiv.org/pdf/2502.11726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11725v1","updated":"2025-02-17T12:11:01Z","published":"2025-02-17T12:11:01Z","title":"Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual\n  Metrics","summary":"  Measuring perceptual similarity is a key tool in computer vision. In recent\nyears perceptual metrics based on features extracted from neural networks with\nlarge and diverse training sets, e.g. CLIP, have become popular. At the same\ntime, the metrics extracted from features of neural networks are not\nadversarially robust. In this paper we show that adversarially robust CLIP\nmodels, called R-CLIP$_\\textrm{F}$, obtained by unsupervised adversarial\nfine-tuning induce a better and adversarially robust perceptual metric that\noutperforms existing metrics in a zero-shot setting, and further matches the\nperformance of state-of-the-art metrics while being robust after fine-tuning.\nMoreover, our perceptual metric achieves strong performance on related tasks\nsuch as robust image-to-image retrieval, which becomes especially relevant when\napplied to \"Not Safe for Work\" (NSFW) content detection and dataset filtering.\nWhile standard perceptual metrics can be easily attacked by a small\nperturbation completely degrading NSFW detection, our robust perceptual metric\nmaintains high accuracy under an attack while having similar performance for\nunperturbed images. Finally, perceptual metrics induced by robust CLIP models\nhave higher interpretability: feature inversion can show which images are\nconsidered similar, while text inversion can find what images are associated to\na given prompt. This also allows us to visualize the very rich visual concepts\nlearned by a CLIP model, including memorized persons, paintings and complex\nqueries.\n","authors":["Francesco Croce","Christian Schlarmann","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2502.11725v1.pdf","comment":"This work has been accepted for publication in the IEEE Conference on\n  Secure and Trustworthy Machine Learning (SaTML). The final version will be\n  available on IEEE Xplore"},{"id":"http://arxiv.org/abs/2502.11724v1","updated":"2025-02-17T12:10:35Z","published":"2025-02-17T12:10:35Z","title":"Incomplete Modality Disentangled Representation for Ophthalmic Disease\n  Grading and Diagnosis","summary":"  Ophthalmologists typically require multimodal data sources to improve\ndiagnostic accuracy in clinical decisions. However, due to medical device\nshortages, low-quality data and data privacy concerns, missing data modalities\nare common in real-world scenarios. Existing deep learning methods tend to\naddress it by learning an implicit latent subspace representation for different\nmodality combinations. We identify two significant limitations of these\nmethods: (1) implicit representation constraints that hinder the model's\nability to capture modality-specific information and (2) modality\nheterogeneity, causing distribution gaps and redundancy in feature\nrepresentations. To address these, we propose an Incomplete Modality\nDisentangled Representation (IMDR) strategy, which disentangles features into\nexplicit independent modal-common and modal-specific features by guidance of\nmutual information, distilling informative knowledge and enabling it to\nreconstruct valuable missing semantics and produce robust multimodal\nrepresentations. Furthermore, we introduce a joint proxy learning module that\nassists IMDR in eliminating intra-modality redundancy by exploiting the\nextracted proxies from each class. Experiments on four ophthalmology multimodal\ndatasets demonstrate that the proposed IMDR outperforms the state-of-the-art\nmethods significantly.\n","authors":["Chengzhi Liu","Zile Huang","Zhe Chen","Feilong Tang","Yu Tian","Zhongxing Xu","Zihong Luo","Yalin Zheng","Yanda Meng"],"pdf_url":"https://arxiv.org/pdf/2502.11724v1.pdf","comment":"7 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.05952v2","updated":"2025-02-17T12:04:53Z","published":"2025-01-10T13:27:04Z","title":"Scalable Vision Language Model Training via High Quality Data Curation","summary":"  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning\nvia High QuaLity Data Curation), an open-source vision language model (VLM)\nseries achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.\nThe following three key improvements contribute to SAIL-VL's leading\nperformance: (1) Scalable high-quality visual understanding data construction:\nWe implement a data construction pipeline to enable hundred-million-scale\nhigh-quality recaption data annotation, and the resulted dataset SAIL-Caption\nis validated to be of the highest data quality compared with opensource\nalternatives. (2) Scalable Pretraining with High-Quality Visual Understanding\nData: We scale SAIL-VL's pretraining budget up to 655B tokens and show that\neven a 2B VLM benefits from scaled up training data sizes, exhibiting expected\ndata size scaling laws in visual understanding and instruction following\nperformance. (3) Scalable SFT via data quantity and complexity scaling: We\ncurate a high-quality SFT dataset collection which outperforms opensource\nalternatives in data quantity scaling effectiveness. We also demonstrate that\ntraining with progressively higher-complexity data surpasses baseline one-stage\ntraining by a large margin. SAIL-VL series models achieve the highest average\nscore in 18 widely used VLM benchmarks in our evaluation, with the 2B model\ntakes the top position over VLMs of comparable sizes on OpenCompass 2024\n(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust\nvisual comprehension abilities. SAIL-VL series models are released at\nHuggingFace (https://huggingface.co/BytedanceDouyinContent).\n","authors":["Hongyuan Dong","Zijian Kang","Weijie Yin","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"https://arxiv.org/pdf/2501.05952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11718v1","updated":"2025-02-17T12:02:23Z","published":"2025-02-17T12:02:23Z","title":"\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for\n  Large Vision Language Models","summary":"  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n","authors":["Jihao Gu","Yingyao Wang","Pi Bu","Chen Wang","Ziming Wang","Tengtao Song","Donglai Wei","Jiale Yuan","Yingxiu Zhao","Yancheng He","Shilong Li","Jiaheng Liu","Meng Cao","Jun Song","Yingshui Tan","Xiang Li","Wenbo Su","Zhicheng Zheng","Xiaoyong Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11718v1.pdf","comment":"24 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.08279v2","updated":"2025-02-17T12:01:02Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14585v2","updated":"2025-02-17T11:59:26Z","published":"2024-08-26T19:09:21Z","title":"Global-Local Distillation Network-Based Audio-Visual Speaker Tracking\n  with Incomplete Modalities","summary":"  In speaker tracking research, integrating and complementing multi-modal data\nis a crucial strategy for improving the accuracy and robustness of tracking\nsystems. However, tracking with incomplete modalities remains a challenging\nissue due to noisy observations caused by occlusion, acoustic noise, and sensor\nfailures. Especially when there is missing data in multiple modalities, the\nperformance of existing multi-modal fusion methods tends to decrease. To this\nend, we propose a Global-Local Distillation-based Tracker (GLDTracker) for\nrobust audio-visual speaker tracking. GLDTracker is driven by a teacher-student\ndistillation model, enabling the flexible fusion of incomplete information from\neach modality. The teacher network processes global signals captured by camera\nand microphone arrays, and the student network handles local information\nsubject to visual occlusion and missing audio channels. By transferring\nknowledge from teacher to student, the student network can better adapt to\ncomplex dynamic scenes with incomplete observations. In the student network, a\nglobal feature reconstruction module based on the generative adversarial\nnetwork is constructed to reconstruct global features from feature embedding\nwith missing local information. Furthermore, a multi-modal multi-level fusion\nattention is introduced to integrate the incomplete feature and the\nreconstructed feature, leveraging the complementarity and consistency of\naudio-visual and global-local features. Experimental results on the AV16.3\ndataset demonstrate that the proposed GLDTracker outperforms existing\nstate-of-the-art audio-visual trackers and achieves leading performance on both\nstandard and incomplete modalities datasets, highlighting its superiority and\nrobustness in complex conditions. The code and models will be available.\n","authors":["Yidi Li","Yihan Li","Yixin Guo","Bin Ren","Zhenhuan Xu","Hao Guo","Hong Liu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2408.14585v2.pdf","comment":"We request to withdraw our paper from arXiv due to unresolved author\n  disagreements about the data interpretation and study conclusions. To\n  maintain scientific integrity, we believe withdrawing the paper is necessary.\n  We regret any confusion caused"},{"id":"http://arxiv.org/abs/2502.11712v1","updated":"2025-02-17T11:54:43Z","published":"2025-02-17T11:54:43Z","title":"Component-aware Unsupervised Logical Anomaly Generation for Industrial\n  Anomaly Detection","summary":"  Anomaly detection is critical in industrial manufacturing for ensuring\nproduct quality and improving efficiency in automated processes. The scarcity\nof anomalous samples limits traditional detection methods, making anomaly\ngeneration essential for expanding the data repository. However, recent\ngenerative models often produce unrealistic anomalies increasing false\npositives, or require real-world anomaly samples for training. In this work, we\ntreat anomaly generation as a compositional problem and propose ComGEN, a\ncomponent-aware and unsupervised framework that addresses the gap in logical\nanomaly generation. Our method comprises a multi-component learning strategy to\ndisentangle visual components, followed by subsequent generation editing\nprocedures. Disentangled text-to-component pairs, revealing intrinsic logical\nconstraints, conduct attention-guided residual mapping and model training with\niteratively matched references across multiple scales. Experiments on the\nMVTecLOCO dataset confirm the efficacy of ComGEN, achieving the best AUROC\nscore of 91.2%. Additional experiments on the real-world scenario of Diesel\nEngine and widely-used MVTecAD dataset demonstrate significant performance\nimprovements when integrating simulated anomalies generated by ComGEN into\nautomated production workflows.\n","authors":["Xuan Tong","Yang Chang","Qing Zhao","Jiawen Yu","Boyang Wang","Junxiong Lin","Yuxuan Lin","Xinji Mai","Haoran Wang","Zeng Tao","Yan Wang","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03210v2","updated":"2025-02-17T11:53:44Z","published":"2024-12-04T10:55:44Z","title":"Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality\n  Assessment","summary":"  Human vision models are at the core of image processing. For instance,\nclassical approaches to the problem of image quality are based on models that\ninclude knowledge about human vision. However, nowadays, deep learning\napproaches have obtained competitive results by simply approaching this problem\nas regression of human decisions, and training an standard network on\nhuman-rated datasets. These approaches have the advantages of being easily\nadaptable to a particular problem and they fit very efficiently when data is\navailable. However, mainly due to the excess of parameters, they have the\nproblems of lack of interpretability, and over-fitting. Here we propose a\nvision model that combines the best of both worlds by using a parametric neural\nnetwork architecture. We parameterize the layers to have bioplausible\nfunctionality, and provide a set of bioplausible parameters. We analyzed\ndifferent versions of the model and compared it with the non-parametric\nversion. The parametric models achieve a three orders of magnitude reduction in\nthe number of parameters without suffering in regression performance.\nFurthermore, we show that the parametric models behave better during training\nand are easier to interpret as vision models. Interestingly, we find that, even\ninitialized with bioplausible trained for regression using human rated\ndatasets, which we call the feature-spreading problem. This suggests that the\ndeep learning approach is inherently flawed, and emphasizes the need to\nevaluate and train models beyond regression.\n","authors":["Jorge Vila-Tom√°s","Pablo Hern√°ndez-C√°mara","Valero Laparra","Jes√∫s Malo"],"pdf_url":"https://arxiv.org/pdf/2412.03210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11710v1","updated":"2025-02-17T11:50:42Z","published":"2025-02-17T11:50:42Z","title":"The Worse The Better: Content-Aware Viewpoint Generation Network for\n  Projection-related Point Cloud Quality Assessment","summary":"  Through experimental studies, however, we observed the instability of final\npredicted quality scores, which change significantly over different viewpoint\nsettings. Inspired by the \"wooden barrel theory\", given the default\ncontent-independent viewpoints of existing projection-related PCQA approaches,\nthis paper presents a novel content-aware viewpoint generation network (CAVGN)\nto learn better viewpoints by taking the distribution of geometric and\nattribute features of degraded point clouds into consideration. Firstly, the\nproposed CAVGN extracts multi-scale geometric and texture features of the\nentire input point cloud, respectively. Then, for each default\ncontent-independent viewpoint, the extracted geometric and texture features are\nrefined to focus on its corresponding visible part of the input point cloud.\nFinally, the refined geometric and texture features are concatenated to\ngenerate an optimized viewpoint. To train the proposed CAVGN, we present a\nself-supervised viewpoint ranking network (SSVRN) to select the viewpoint with\nthe worst quality projected image to construct a default-optimized viewpoint\ndataset, which consists of thousands of paired default viewpoints and\ncorresponding optimized viewpoints. Experimental results show that the\nprojection-related PCQA methods can achieve higher performance using the\nviewpoints generated by the proposed CAVGN.\n","authors":["Zhiyong Su","Bingxu Xie","Zheng Li","Jincan Wu","Weiqing Li"],"pdf_url":"https://arxiv.org/pdf/2502.11710v1.pdf","comment":"To be published in IEEE Transactions on Circuits and Systems for\n  Video Technology"},{"id":"http://arxiv.org/abs/2502.11697v1","updated":"2025-02-17T11:34:58Z","published":"2025-02-17T11:34:58Z","title":"MVTokenFlow: High-quality 4D Content Generation using Multiview Token\n  Flow","summary":"  In this paper, we present MVTokenFlow for high-quality 4D content creation\nfrom monocular videos. Recent advancements in generative models such as video\ndiffusion models and multiview diffusion models enable us to create videos or\n3D models. However, extending these generative models for dynamic 4D content\ncreation is still a challenging task that requires the generated content to be\nconsistent spatially and temporally. To address this challenge, MVTokenFlow\nutilizes the multiview diffusion model to generate multiview images on\ndifferent timesteps, which attains spatial consistency across different\nviewpoints and allows us to reconstruct a reasonable coarse 4D field. Then,\nMVTokenFlow further regenerates all the multiview images using the rendered 2D\nflows as guidance. The 2D flows effectively associate pixels from different\ntimesteps and improve the temporal consistency by reusing tokens in the\nregeneration process. Finally, the regenerated images are spatiotemporally\nconsistent and utilized to refine the coarse 4D field to get a high-quality 4D\nfield. Experiments demonstrate the effectiveness of our design and show\nsignificantly improved quality than baseline methods.\n","authors":["Hanzhuo Huang","Yuan Liu","Ge Zheng","Jiepeng Wang","Zhiyang Dou","Sibei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11697v1.pdf","comment":"ICLR 2025. Project page: https://soolab.github.io/MVTokenFlow"},{"id":"http://arxiv.org/abs/2406.01395v4","updated":"2025-02-17T11:24:49Z","published":"2024-06-03T14:58:49Z","title":"TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for\n  Traversability Estimation","summary":"  This paper presents TE-NeXt, a novel and efficient architecture for\nTraversability Estimation (TE) from sparse LiDAR point clouds based on a\nresidual convolution block. TE-NeXt block fuses notions of current trends such\nas attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate\nhigh capacity for generalisation in a variety of urban and natural\nenvironments, using well-known and accessible datasets such as SemanticKITTI,\nRellis-3D and SemanticUSL. Thus, the designed architecture ouperforms\nstate-of-the-art methods in the problem of semantic segmentation, demonstrating\nbetter results in unstructured environments and maintaining high reliability\nand robustness in urbans environments, which leads to better abstraction.\nImplementation is available in a open repository to the scientific community\nwith the aim of ensuring the reproducibility of results.\n","authors":["Antonio Santo","Juan J. Cabrera","David Valiente","Carlos Viegas","Arturo Gil"],"pdf_url":"https://arxiv.org/pdf/2406.01395v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22078v2","updated":"2025-02-17T11:08:04Z","published":"2024-10-29T14:36:03Z","title":"DINeuro: Distilling Knowledge from 2D Natural Images via Deformable\n  Tubular Transferring Strategy for 3D Neuron Reconstruction","summary":"  Reconstructing neuron morphology from 3D light microscope imaging data is\ncritical to aid neuroscientists in analyzing brain networks and neuroanatomy.\nWith the boost from deep learning techniques, a variety of learning-based\nsegmentation models have been developed to enhance the signal-to-noise ratio of\nraw neuron images as a pre-processing step in the reconstruction workflow.\nHowever, most existing models directly encode the latent representative\nfeatures of volumetric neuron data but neglect their intrinsic morphological\nknowledge. To address this limitation, we design a novel framework that\ndistills the prior knowledge from a 2D Vision Transformer pre-trained on\nextensive 2D natural images to facilitate neuronal morphological learning of\nour 3D Vision Transformer. To bridge the knowledge gap between the 2D natural\nimage and 3D microscopic morphologic domains, we propose a deformable tubular\ntransferring strategy that adapts the pre-trained 2D natural knowledge to the\ninherent tubular characteristics of neuronal structure in the latent embedding\nspace. The experimental results on the Janelia dataset of the BigNeuron project\ndemonstrate that our method achieves a segmentation performance improvement of\n4.53% in mean Dice and 3.56% in mean 95% Hausdorff distance.\n","authors":["Yik San Cheng","Runkai Zhao","Heng Wang","Hanchuan Peng","Yui Lo","Yuqian Chen","Lauren J. O'Donnell","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.22078v2.pdf","comment":"9 pages, 3 figures, and 2 tables. This work has been accepted to 2025\n  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2502.11663v1","updated":"2025-02-17T10:53:56Z","published":"2025-02-17T10:53:56Z","title":"MaskGWM: A Generalizable Driving World Model with Video Mask\n  Reconstruction","summary":"  World models that forecast environmental changes from actions are vital for\nautonomous driving models with strong generalization. The prevailing driving\nworld model mainly build on video prediction model. Although these models can\nproduce high-fidelity video sequences with advanced diffusion-based generator,\nthey are constrained by their predictive duration and overall generalization\ncapabilities. In this paper, we explore to solve this problem by combining\ngeneration loss with MAE-style feature-level context learning. In particular,\nwe instantiate this target with three key design: (1) A more scalable Diffusion\nTransformer (DiT) structure trained with extra mask construction task. (2) we\ndevise diffusion-related mask tokens to deal with the fuzzy relations between\nmask reconstruction and generative diffusion process. (3) we extend mask\nconstruction task to spatial-temporal domain by utilizing row-wise mask for\nshifted self-attention rather than masked self-attention in MAE. Then, we adopt\na row-wise cross-view module to align with this mask design. Based on above\nimprovement, we propose MaskGWM: a Generalizable driving World Model embodied\nwith Video Mask reconstruction. Our model contains two variants: MaskGWM-long,\nfocusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view\ngeneration. Comprehensive experiments on standard benchmarks validate the\neffectiveness of the proposed method, which contain normal validation of\nNuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot\nvalidation of Waymo dataset. Quantitative metrics on these datasets show our\nmethod notably improving state-of-the-art driving world model.\n","authors":["Jingcheng Ni","Yuxin Guo","Yichen Liu","Rui Chen","Lewei Lu","Zehuan Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11655v1","updated":"2025-02-17T10:46:47Z","published":"2025-02-17T10:46:47Z","title":"Object-Centric Image to Video Generation with Language Guidance","summary":"  Accurate and flexible world models are crucial for autonomous systems to\nunderstand their environment and predict future events. Object-centric models,\nwith structured latent spaces, have shown promise in modeling object dynamics\nand interactions, but often face challenges in scaling to complex datasets and\nincorporating external guidance, limiting their applicability in robotics. To\naddress these limitations, we propose TextOCVP, an object-centric model for\nimage-to-video generation guided by textual descriptions. TextOCVP parses an\nobserved scene into object representations, called slots, and utilizes a\ntext-conditioned transformer predictor to forecast future object states and\nvideo frames. Our approach jointly models object dynamics and interactions\nwhile incorporating textual guidance, thus leading to accurate and controllable\npredictions. Our method's structured latent space offers enhanced control over\nthe prediction process, outperforming several image-to-video generative\nbaselines. Additionally, we demonstrate that structured object-centric\nrepresentations provide superior controllability and interpretability,\nfacilitating the modeling of object dynamics and enabling more precise and\nunderstandable predictions. Videos and code are available at\nhttps://play-slot.github.io/TextOCVP/.\n","authors":["Angel Villar-Corrales","Gjergj Plepi","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2502.11655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11651v1","updated":"2025-02-17T10:43:38Z","published":"2025-02-17T10:43:38Z","title":"MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression","summary":"  Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https://github.com/linjiemu/MMXU}{https://github.com/linjiemu/MMXU}.\n","authors":["Linjie Mu","Zhongzhen Huang","Shengqian Qin","Yakun Zhu","Shaoting Zhang","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09269v2","updated":"2025-02-17T10:42:24Z","published":"2025-02-13T12:31:09Z","title":"Memory-based Ensemble Learning in CMR Semantic Segmentation","summary":"  Existing models typically segment either the entire 3D frame or 2D slices\nindependently to derive clinical functional metrics from ventricular\nsegmentation in cardiac cine sequences. While performing well overall, they\nstruggle at the end slices. To address this, we leverage spatial continuity to\nextract global uncertainty from segmentation variance and use it as memory in\nour ensemble learning method, Streaming, for classifier weighting, balancing\noverall and end-slice performance. Additionally, we introduce the End\nCoefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M&Ms\ndatasets show that our framework achieves near-state-of-the-art Dice Similarity\nCoefficient (DSC) and outperforms all models on end-slice performance,\nimproving patient-specific segmentation accuracy.\n","authors":["Yiwei Liu","Ziyi Wu","Liang Zhong","Lingyi Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07546v2","updated":"2025-02-17T10:41:01Z","published":"2024-11-12T04:50:10Z","title":"Contrastive Language Prompting to Ease False Positives in Medical\n  Anomaly Detection","summary":"  A pre-trained visual-language model, contrastive language-image pre-training\n(CLIP), successfully accomplishes various downstream tasks with text prompts,\nsuch as finding images or localizing regions within the image. Despite CLIP's\nstrong multi-modal data capabilities, it remains limited in specialized\nenvironments, such as medical applications. For this purpose, many CLIP\nvariants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives\nrelated to normal regions persist. Thus, we aim to present a simple yet\nimportant goal of reducing false positives in medical anomaly detection. We\nintroduce a Contrastive LAnguage Prompting (CLAP) method that leverages both\npositive and negative text prompts. This straightforward approach identifies\npotential lesion regions by visual attention to the positive prompts in the\ngiven image. To reduce false positives, we attenuate attention on normal\nregions using negative prompts. Extensive experiments with the BMAD dataset,\nincluding six biomedical benchmarks, demonstrate that CLAP method enhances\nanomaly detection performance. Our future plans include developing an automated\nfine prompting method for more practical usage.\n","authors":["YeongHyeon Park","Myung Jin Kim","Hyeong Seok Kim"],"pdf_url":"https://arxiv.org/pdf/2411.07546v2.pdf","comment":"4 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.11642v1","updated":"2025-02-17T10:36:36Z","published":"2025-02-17T10:36:36Z","title":"GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with\n  Pose Guidance from Text","summary":"  In this paper, we introduce GaussianMotion, a novel human rendering model\nthat generates fully animatable scenes aligned with textual descriptions using\nGaussian Splatting. Although existing methods achieve reasonable text-to-3D\ngeneration of human bodies using various 3D representations, they often face\nlimitations in fidelity and efficiency, or primarily focus on static models\nwith limited pose control. In contrast, our method generates fully animatable\n3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score\ndistillation, achieving high fidelity and efficient rendering for arbitrary\nposes. By densely generating diverse random poses during optimization, our\ndeformable 3D human model learns to capture a wide range of natural motions\ndistilled from a pose-conditioned diffusion model in an end-to-end manner.\nFurthermore, we propose Adaptive Score Distillation that effectively balances\nrealistic detail and smoothness to achieve optimal 3D results. Experimental\nresults demonstrate that our approach outperforms existing baselines by\nproducing high-quality textures in both static and animated results, and by\ngenerating diverse 3D human models from various textual inputs.\n","authors":["Gyumin Shim","Sangmin Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2502.11642v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.11638v1","updated":"2025-02-17T10:31:24Z","published":"2025-02-17T10:31:24Z","title":"Enhancing Out-of-Distribution Detection in Medical Imaging with\n  Normalizing Flows","summary":"  Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging\nto ensure reliability and safety by identifying inputs outside a model's\ntraining distribution. Existing methods often require retraining or\nmodifications to pre-trained models, which is impractical for clinical\napplications. This study introduces a post-hoc normalizing flow-based approach\nthat seamlessly integrates with pre-trained models. By leveraging normalizing\nflows, it estimates the likelihood of feature vectors extracted from\npre-trained models, capturing semantically meaningful representations without\nrelying on pixel-level statistics. The method was evaluated using the MedMNIST\nbenchmark and a newly curated MedOOD dataset simulating clinically relevant\ndistributional shifts. Performance was measured using standard OOD detection\nmetrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses\ncomparing it against ten baseline methods. On MedMNIST, the proposed model\nachieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD,\nit achieved an AUROC of 84.61%, demonstrating superior performance against\nother methods. Its post-hoc nature ensures compatibility with existing clinical\nworkflows, addressing the limitations of previous approaches. The model and\ncode to build OOD datasets are available at\nhttps://github.com/dlotfi/MedOODFlow.\n","authors":["Dariush Lotfi","Mohammad-Ali Nikouei Mahani","Mohamad Koohi-Moghadam","Kyongtae Ty Bae"],"pdf_url":"https://arxiv.org/pdf/2502.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12693v2","updated":"2025-02-17T10:28:00Z","published":"2024-12-17T09:10:55Z","title":"SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through\n  Hierarchical Evaluation","summary":"  Current vision-language models may grasp basic spatial cues and simple\ndirections (e.g. left, right, front, back), but struggle with the\nmulti-dimensional spatial reasoning necessary for human-like understanding and\nreal-world applications. To address this gap, we develop SPHERE (Spatial\nPerception and Hierarchical Evaluation of REasoning), a hierarchical evaluation\nframework supported by a new human-annotated dataset. SPHERE systematically\nprobes models across increasing levels of complexity, from fundamental skills\nto multi-skill integration and high-level reasoning that combines spatial,\nvisual, and logical understanding. Benchmark evaluation of state-of-the-art\nmodels reveals significant deficiencies, especially in reasoning about distance\nand proximity, understanding both egocentric and allocentric perspectives, and\napplying spatial logic in physical contexts. These findings expose critical\nblind spots in existing models and underscore the need for more advanced\nspatial reasoning techniques, driving the development of vision-language models\nthat align more closely with human spatial cognition. The dataset will be\nopen-sourced upon publication.\n","authors":["Wenyu Zhang","Wei En Ng","Lixin Ma","Yuwen Wang","Jungqi Zhao","Allison Koenecke","Boyang Li","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00463v2","updated":"2025-02-17T10:13:59Z","published":"2024-12-31T14:22:53Z","title":"SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion\n  Models with Self-Augmented Training","summary":"  The rapid proliferation of AI-generated images necessitates effective\nwatermarking techniques to protect intellectual property and detect fraudulent\ncontent. While existing training-based watermarking methods show promise, they\noften struggle with generalizing across diverse prompts and tend to introduce\nvisible artifacts. To this end, we propose a novel, provably generalizable\nimage watermarking approach for Latent Diffusion Models, termed Self-Augmented\nTraining (SAT-LDM). Our method aligns the training and testing phases through a\nfree generation distribution, thereby enhancing the watermarking module's\ngeneralization capabilities. We theoretically consolidate SAT-LDM by proving\nthat the free generation distribution contributes to its tight generalization\nbound, without the need for additional data collection. Extensive experiments\nshow that SAT-LDM not only achieves robust watermarking but also significantly\nimproves the quality of watermarked images across a wide range of prompts.\nMoreover, our experimental analyses confirm the strong generalization abilities\nof SAT-LDM. We hope that our method provides a practical and efficient solution\nfor securing high-fidelity AI-generated content.\n","authors":["Lu Zhang","Liang Zeng"],"pdf_url":"https://arxiv.org/pdf/2501.00463v2.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.18204v2","updated":"2025-02-17T10:01:36Z","published":"2024-12-24T06:20:01Z","title":"BoxMAC -- A Boxing Dataset for Multi-label Action Classification","summary":"  In competitive combat sports like boxing, analyzing a boxers's performance\nstatics is crucial for evaluating the quantity and variety of punches delivered\nduring bouts. These statistics provide valuable data and feedback, which are\nroutinely used for coaching and performance enhancement. We introduce BoxMAC, a\nreal-world boxing dataset featuring 15 professional boxers and encompassing 13\ndistinct action labels. Comprising over 60,000 frames, our dataset has been\nmeticulously annotated for multiple actions per frame with inputs from a boxing\ncoach. Since two boxers can execute different punches within a single\ntimestamp, this problem falls under the domain of multi-label action\nclassification. We propose a novel architecture for jointly recognizing\nmultiple actions in both individual images and videos. We investigate baselines\nusing deep neural network architectures to address both tasks. We believe that\nBoxMAC will enable researchers and practitioners to develop and evaluate more\nefficient models for performance analysis. With its realistic and diverse\nnature, BoxMAC can serve as a valuable resource for the advancement of boxing\nas a sport\n","authors":["Shashikanta Sahoo"],"pdf_url":"https://arxiv.org/pdf/2412.18204v2.pdf","comment":"Significant modifications are required to improve the clarity and\n  accuracy of the findings and This submission was made without the full\n  agreement of all co-authors. To ensure proper authorship attribution and\n  compliance with ethical guidelines, we are withdrawing this version. A\n  revised and more complete version will be submitted soon"},{"id":"http://arxiv.org/abs/2502.11619v1","updated":"2025-02-17T10:01:24Z","published":"2025-02-17T10:01:24Z","title":"Membership Inference Attacks for Face Images Against Fine-Tuned Latent\n  Diffusion Models","summary":"  The rise of generative image models leads to privacy concerns when it comes\nto the huge datasets used to train such models. This paper investigates the\npossibility of inferring if a set of face images was used for fine-tuning a\nLatent Diffusion Model (LDM). A Membership Inference Attack (MIA) method is\npresented for this task. Using generated auxiliary data for the training of the\nattack model leads to significantly better performance, and so does the use of\nwatermarks. The guidance scale used for inference was found to have a\nsignificant influence. If a LDM is fine-tuned for long enough, the text prompt\nused for inference has no significant influence. The proposed MIA is found to\nbe viable in a realistic black-box setup against LDMs fine-tuned on\nface-images.\n","authors":["Lauritz Christian Holme","Anton Mosquera Storgaard","Siavash Arjomand Bigdeli"],"pdf_url":"https://arxiv.org/pdf/2502.11619v1.pdf","comment":"In Proceedings of the 20th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP\n  2025) - Volume 2: VISAPP, pages 439-446"},{"id":"http://arxiv.org/abs/2502.11618v1","updated":"2025-02-17T10:01:13Z","published":"2025-02-17T10:01:13Z","title":"Real-time Neural Rendering of LiDAR Point Clouds","summary":"  Static LiDAR scanners produce accurate, dense, colored point clouds, but\noften contain obtrusive artifacts which makes them ill-suited for direct\ndisplay. We propose an efficient method to render photorealistic images of such\nscans without any expensive preprocessing or training of a scene-specific\nmodel. A naive projection of the point cloud to the output view using 1x1\npixels is fast and retains the available detail, but also results in\nunintelligible renderings as background points leak in between the foreground\npixels. The key insight is that these projections can be transformed into a\nrealistic result using a deep convolutional model in the form of a U-Net, and a\ndepth-based heuristic that prefilters the data. The U-Net also handles\nLiDAR-specific problems such as missing parts due to occlusion, color\ninconsistencies and varying point densities. We also describe a method to\ngenerate synthetic training data to deal with imperfectly-aligned ground truth\nimages. Our method achieves real-time rendering rates using an off-the-shelf\nGPU and outperforms the state-of-the-art in both speed and quality.\n","authors":["Joni Vanherck","Brent Zoomers","Tom Mertens","Lode Jorissen","Nick Michiels"],"pdf_url":"https://arxiv.org/pdf/2502.11618v1.pdf","comment":"6 pages, 3 figures, 1 table,"},{"id":"http://arxiv.org/abs/2502.08769v2","updated":"2025-02-17T09:54:11Z","published":"2025-02-12T20:17:10Z","title":"Cluster and Predict Latent Patches for Improved Masked Image Modeling","summary":"  Masked Image Modeling (MIM) offers a promising approach to self-supervised\nrepresentation learning, however existing MIM models still lag behind the\nstate-of-the-art. In this paper, we systematically analyze target\nrepresentations, loss functions, and architectures, to introduce CAPI - a novel\npure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and\nexhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%\naccuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,\nsubstantially outperforming previous MIM methods and approaching the\nperformance of the current state-of-the-art, DINOv2. We release all our code\nand models.\n","authors":["Timoth√©e Darcet","Federico Baldassarre","Maxime Oquab","Julien Mairal","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2502.08769v2.pdf","comment":"13 pages, 7 figures, submitted to TMLR"},{"id":"http://arxiv.org/abs/2502.11594v1","updated":"2025-02-17T09:28:31Z","published":"2025-02-17T09:28:31Z","title":"iMOVE: Instance-Motion-Aware Video Understanding","summary":"  Enhancing the fine-grained instance spatiotemporal motion perception\ncapabilities of Video Large Language Models is crucial for improving their\ntemporal and general video understanding. However, current models struggle to\nperceive detailed and complex instance motions. To address these challenges, we\nhave made improvements from both data and model perspectives. In terms of data,\nwe have meticulously curated iMOVE-IT, the first large-scale\ninstance-motion-aware video instruction-tuning dataset. This dataset is\nenriched with comprehensive instance motion annotations and spatiotemporal\nmutual-supervision tasks, providing extensive training for the model's\ninstance-motion-awareness. Building on this foundation, we introduce iMOVE, an\ninstance-motion-aware video foundation model that utilizes Event-aware\nSpatiotemporal Efficient Modeling to retain informative instance spatiotemporal\nmotion details while maintaining computational efficiency. It also incorporates\nRelative Spatiotemporal Position Tokens to ensure awareness of instance\nspatiotemporal positions. Evaluations indicate that iMOVE excels not only in\nvideo temporal understanding and general video understanding but also\ndemonstrates significant advantages in long-term video understanding.\n","authors":["Jiaze Li","Yaya Shi","Zongyang Ma","Haoran Xu","Feng Cheng","Huihui Xiao","Ruiwen Kang","Fan Yang","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11586v1","updated":"2025-02-17T09:18:06Z","published":"2025-02-17T09:18:06Z","title":"Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis\n  from Japanese Haiku","summary":"  In the era of the metaverse, where immersive technologies redefine human\nexperiences, translating abstract literary concepts into navigable 3D\nenvironments presents a fundamental challenge in preserving semantic and\nemotional fidelity. This research introduces HaikuVerse, a novel framework for\ntransforming poetic abstraction into spatial representation, with Japanese\nHaiku serving as an ideal test case due to its sophisticated encapsulation of\nprofound emotions and imagery within minimal text. While existing text-to-3D\nmethods struggle with nuanced interpretations, we present a literary-guided\napproach that synergizes traditional poetry analysis with advanced generative\ntechnologies. Our framework centers on two key innovations: (1) Hierarchical\nLiterary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both\nexplicit imagery and implicit emotional resonance through structured semantic\ndecomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage\npipeline that systematically transforms poetic elements into coherent 3D scenes\nthrough sequential diffusion processes, geometric optimization, and real-time\nenhancement. Extensive experiments demonstrate that HaikuVerse significantly\noutperforms conventional text-to-3D approaches in both literary fidelity and\nvisual quality, establishing a new paradigm for preserving cultural heritage in\nimmersive digital spaces. Project website at:\nhttps://syllables-to-scenes.github.io/\n","authors":["Chunan Yu","Yidong Han","Chaotao Ding","Ying Zang","Lanyun Zhu","Xinhao Chen","Zejian Li","Renjun Xu","Tianrun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11586v1.pdf","comment":"16 pages, 11 figures, submitted to IJCAI"},{"id":"http://arxiv.org/abs/2410.10167v3","updated":"2025-02-17T09:11:44Z","published":"2024-10-14T05:23:12Z","title":"X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing","summary":"  Human sensing, which employs various sensors and advanced deep learning\ntechnologies to accurately capture and interpret human body information, has\nsignificantly impacted fields like public security and robotics. However,\ncurrent human sensing primarily depends on modalities such as cameras and\nLiDAR, each of which has its own strengths and limitations. Furthermore,\nexisting multi-modal fusion solutions are typically designed for fixed modality\ncombinations, requiring extensive retraining when modalities are added or\nremoved for diverse scenarios. In this paper, we propose a modality-invariant\nfoundation model for all modalities, X-Fi, to address this issue. X-Fi enables\nthe independent or combinatory use of sensor modalities without additional\ntraining by utilizing a transformer structure to accommodate variable input\nsizes and incorporating a novel \"X-fusion\" mechanism to preserve\nmodality-specific features during multimodal integration. This approach not\nonly enhances adaptability but also facilitates the learning of complementary\nfeatures across modalities. Extensive experiments conducted on the MM-Fi and\nXRF55 datasets, employing six distinct modalities, demonstrate that X-Fi\nachieves state-of-the-art performance in human pose estimation (HPE) and human\nactivity recognition (HAR) tasks. The findings indicate that our proposed model\ncan efficiently support a wide range of human sensing applications, ultimately\ncontributing to the evolution of scalable, multimodal sensing technologies.\n","authors":["Xinyan Chen","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01867v3","updated":"2025-02-17T09:00:41Z","published":"2024-06-04T00:38:44Z","title":"MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by\n  Adversarial Training","summary":"  In text-to-motion generation, controllability as well as generation quality\nand speed has become increasingly critical. The controllability challenges\ninclude generating a motion of a length that matches the given textual\ndescription and editing the generated motions according to control signals,\nsuch as the start-end positions and the pelvis trajectory. In this paper, we\npropose MoLA, which provides fast, high-quality, variable-length motion\ngeneration and can also deal with multiple editing tasks in a single framework.\nOur approach revisits the motion representation used as inputs and outputs in\nthe model, incorporating an activation variable to enable variable-length\nmotion generation. Additionally, we integrate a variational autoencoder and a\nlatent diffusion model, further enhanced through adversarial training, to\nachieve high-quality and fast generation. Moreover, we apply a training-free\nguided generation framework to achieve various editing tasks with motion\ncontrol inputs. We quantitatively show the effectiveness of adversarial\nlearning in text-to-motion generation, and demonstrate the applicability of our\nediting framework to multiple editing tasks in the motion domain.\n","authors":["Kengo Uchida","Takashi Shibuya","Yuhta Takida","Naoki Murata","Julian Tanke","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2406.01867v3.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.11570v1","updated":"2025-02-17T08:59:59Z","published":"2025-02-17T08:59:59Z","title":"Towards a Trustworthy Anomaly Detection for Critical Applications\n  through Approximated Partial AUC Loss","summary":"  Anomaly Detection is a crucial step for critical applications such in the\nindustrial, medical or cybersecurity domains. These sectors share the same\nrequirement of handling differently the different types of classification\nerrors. Indeed, even if false positives are acceptable, false negatives are\nnot, because it would reflect a missed detection of a quality issue, a disease\nor a cyber threat. To fulfill this requirement, we propose a method that\ndynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A\nbinary classifier is trained to optimize the specific range of the AUC ROC\ncurve that prevents the True Positive Rate (TPR) to reach 100% while minimizing\nthe False Positive Rate (FPR). The optimal threshold that does not trigger any\nfalse negative is then kept and used at the test step. The results show a TPR\nof 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR\nimprovement of 4.3% for a FPR cost of 12.2% against other state-of-the-art\nmethods. The code is available at https://github.com/ArnaudBougaham/tapAUC.\n","authors":["Arnaud Bougaham","Beno√Æt Fr√©nay"],"pdf_url":"https://arxiv.org/pdf/2502.11570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10248v2","updated":"2025-02-17T08:58:33Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v2.pdf","comment":"36 pages, 14 figures"},{"id":"http://arxiv.org/abs/2306.16132v4","updated":"2025-02-17T08:43:25Z","published":"2023-06-28T12:01:51Z","title":"High-quality Unknown Object Instance Segmentation via Quadruple Boundary\n  Error Refinement","summary":"  Accurate and efficient segmentation of unknown objects in unstructured\nenvironments is essential for robotic manipulation. Unknown Object Instance\nSegmentation (UOIS), which aims to identify all objects in unknown categories\nand backgrounds, has become a key capability for various robotic tasks.\nHowever, existing methods struggle with over-segmentation and\nunder-segmentation, leading to failures in manipulation tasks such as grasping.\nTo address these challenges, we propose QuBER (Quadruple Boundary Error\nRefinement), a novel error-informed refinement approach for high-quality UOIS.\nQuBER first estimates quadruple boundary errors-true positive, true negative,\nfalse positive, and false negative pixels-at the instance boundaries of the\ninitial segmentation. It then refines the segmentation using an error-guided\nfusion mechanism, effectively correcting both fine-grained and instance-level\nsegmentation errors. Extensive evaluations on three public benchmarks\ndemonstrate that QuBER outperforms state-of-the-art methods and consistently\nimproves various UOIS methods while maintaining a fast inference time of less\nthan 0.1 seconds. Furthermore, we show that QuBER improves the success rate of\ngrasping target objects in cluttered environments. Code and supplementary\nmaterials are available at https://sites.google.com/view/uois-quber.\n","authors":["Seunghyeok Back","Sangbeom Lee","Kangmin Kim","Joosoon Lee","Sungho Shin","Jemo Maeng","Kyoobin Lee"],"pdf_url":"https://arxiv.org/pdf/2306.16132v4.pdf","comment":"8 pages, 7 figures, accepted at ICRA 2025, project website:\n  https://sites.google.com/view/uois-quber"},{"id":"http://arxiv.org/abs/2408.01014v2","updated":"2025-02-17T08:34:38Z","published":"2024-08-02T05:17:14Z","title":"Growth Inhibitors for Suppressing Inappropriate Image Concepts in\n  Diffusion Models","summary":"  Despite their remarkable image generation capabilities, text-to-image\ndiffusion models inadvertently learn inappropriate concepts from vast and\nunfiltered training data, which leads to various ethical and business risks.\nSpecifically, model-generated images may exhibit not safe for work (NSFW)\ncontent and style copyright infringements. The prompts that result in these\nproblems often do not include explicit unsafe words; instead, they contain\nobscure and associative terms, which are referred to as implicit unsafe\nprompts. Existing approaches directly fine-tune models under textual guidance\nto alter the cognition of the diffusion model, thereby erasing inappropriate\nconcepts. This not only requires concept-specific fine-tuning but may also\nincur catastrophic forgetting. To address these issues, we explore the\nrepresentation of inappropriate concepts in the image space and guide them\ntowards more suitable ones by injecting growth inhibitors, which are tailored\nbased on the identified features related to inappropriate concepts during the\ndiffusion process. Additionally, due to the varying degrees and scopes of\ninappropriate concepts, we train an adapter to infer the corresponding\nsuppression scale during the injection process. Our method effectively captures\nthe manifestation of subtle words at the image level, enabling direct and\nefficient erasure of target concepts without the need for fine-tuning. Through\nextensive experimentation, we demonstrate that our approach achieves superior\nerasure results with little effect on other concepts while preserving image\nquality and semantics.\n","authors":["Die Chen","Zhiwen Li","Mingyuan Fan","Cen Chen","Wenmeng Zhou","Yanhao Wang","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2408.01014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14317v4","updated":"2025-02-17T08:18:07Z","published":"2025-01-24T08:22:02Z","title":"Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation","summary":"  Triangle meshes are fundamental to 3D applications, enabling efficient\nmodification and rasterization while maintaining compatibility with standard\nrendering pipelines. However, current automatic mesh generation methods\ntypically rely on intermediate representations that lack the continuous surface\nquality inherent to meshes. Converting these representations into meshes\nproduces dense, suboptimal outputs. Although recent autoregressive approaches\ndemonstrate promise in directly modeling mesh vertices and faces, they are\nconstrained by the limitation in face count, scalability, and structural\nfidelity. To address these challenges, we propose Nautilus, a locality-aware\nautoencoder for artist-like mesh generation that leverages the local properties\nof manifold meshes to achieve structural fidelity and efficient representation.\nOur approach introduces a novel tokenization algorithm that preserves face\nproximity relationships and compresses sequence length through locally shared\nvertices and edges, enabling the generation of meshes with an unprecedented\nscale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point\nConditioner that provides multi-scale geometric guidance, ensuring global\nconsistency and local structural fidelity by capturing fine-grained geometric\nfeatures. Extensive experiments demonstrate that Nautilus significantly\noutperforms state-of-the-art methods in both fidelity and scalability. The\nproject page is at https://nautilusmeshgen.github.io.\n","authors":["Yuxuan Wang","Xuanyu Yi","Haohan Weng","Qingshan Xu","Xiaokang Wei","Xianghui Yang","Chunchao Guo","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.14317v4.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2407.18611v3","updated":"2025-02-17T08:15:19Z","published":"2024-07-26T09:11:25Z","title":"IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs","summary":"  Large-scale Neural Radiance Fields (NeRF) reconstructions are typically\nhindered by the requirement for extensive image datasets and substantial\ncomputational resources. This paper introduces IOVS4NeRF, a framework that\nemploys an uncertainty-guided incremental optimal view selection strategy\nadaptable to various NeRF implementations. Specifically, by leveraging a hybrid\nuncertainty model that combines rendering and positional uncertainties, the\nproposed method calculates the most informative view from among the candidates,\nthereby enabling incremental optimization of scene reconstruction. Our detailed\nexperiments demonstrate that IOVS4NeRF achieves high-fidelity NeRF\nreconstruction with minimal computational resources, making it suitable for\nlarge-scale scene applications.\n","authors":["Jingpeng Xie","Shiyu Tan","Yuanlei Wang","Tianle Du","Yifei Xue","Yizhen Lao"],"pdf_url":"https://arxiv.org/pdf/2407.18611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11534v1","updated":"2025-02-17T08:04:53Z","published":"2025-02-17T08:04:53Z","title":"SurgPose: a Dataset for Articulated Robotic Surgical Tool Pose\n  Estimation and Tracking","summary":"  Accurate and efficient surgical robotic tool pose estimation is of\nfundamental significance to downstream applications such as augmented reality\n(AR) in surgical training and learning-based autonomous manipulation. While\nsignificant advancements have been made in pose estimation for humans and\nanimals, it is still a challenge in surgical robotics due to the scarcity of\npublished data. The relatively large absolute error of the da Vinci end\neffector kinematics and arduous calibration procedure make calibrated\nkinematics data collection expensive. Driven by this limitation, we collected a\ndataset, dubbed SurgPose, providing instance-aware semantic keypoints and\nskeletons for visual surgical tool pose estimation and tracking. By marking\nkeypoints using ultraviolet (UV) reactive paint, which is invisible under white\nlight and fluorescent under UV light, we execute the same trajectory under\ndifferent lighting conditions to collect raw videos and keypoint annotations,\nrespectively. The SurgPose dataset consists of approximately 120k surgical\ninstrument instances (80k for training and 40k for validation) of 6 categories.\nEach instrument instance is labeled with 7 semantic keypoints. Since the videos\nare collected in stereo pairs, the 2D pose can be lifted to 3D based on\nstereo-matching depth. In addition to releasing the dataset, we test a few\nbaseline approaches to surgical instrument tracking to demonstrate the utility\nof SurgPose. More details can be found at surgpose.github.io.\n","authors":["Zijian Wu","Adam Schmidt","Randy Moore","Haoying Zhou","Alexandre Banks","Peter Kazanzides","Septimiu E. Salcudean"],"pdf_url":"https://arxiv.org/pdf/2502.11534v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2502.11532v1","updated":"2025-02-17T08:03:55Z","published":"2025-02-17T08:03:55Z","title":"Control-CLIP: Decoupling Category and Style Guidance in CLIP for\n  Specific-Domain Generation","summary":"  Text-to-image diffusion models have shown remarkable capabilities of\ngenerating high-quality images closely aligned with textual inputs. However,\nthe effectiveness of text guidance heavily relies on the CLIP text encoder,\nwhich is trained to pay more attention to general content but struggles to\ncapture semantics in specific domains like styles. As a result, generation\nmodels tend to fail on prompts like \"a photo of a cat in Pokemon style\" in\nterms of simply producing images depicting \"a photo of a cat\". To fill this\ngap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that\nenables the CLIP model to learn the meaning of category and style in a\ncomplement manner. With specially designed fine-tuning tasks on minimal data\nand a modified cross-attention mechanism, Control-CLIP can precisely guide the\ndiffusion model to a specific domain. Moreover, the parameters of the diffusion\nmodel remain unchanged at all, preserving the original generation performance\nand diversity. Experiments across multiple domains confirm the effectiveness of\nour approach, particularly highlighting its robust plug-and-play capability in\ngenerating content with various specific styles.\n","authors":["Zexi Jia","Chuanwei Huang","Hongyan Fei","Yeshuang Zhu","Zhiqiang Yuan","Jinchao Zhang","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05145v2","updated":"2025-02-17T07:54:08Z","published":"2023-06-08T12:12:02Z","title":"Variable Radiance Field for Real-World Category-Specific Reconstruction\n  from Single Image","summary":"  Reconstructing category-specific objects using Neural Radiance Field (NeRF)\nfrom a single image is a promising yet challenging task. Existing approaches\npredominantly rely on projection-based feature retrieval to associate 3D points\nin the radiance field with local image features from the reference image.\nHowever, this process is computationally expensive, dependent on known camera\nintrinsics, and susceptible to occlusions. To address these limitations, we\npropose Variable Radiance Field (VRF), a novel framework capable of efficiently\nreconstructing category-specific objects without requiring known camera\nintrinsics and demonstrating robustness against occlusions. First, we replace\nthe local feature retrieval with global latent representations, generated\nthrough a single feed-forward pass, which improves efficiency and eliminates\nreliance on camera intrinsics. Second, to tackle coordinate inconsistencies\ninherent in real-world dataset, we define a canonical space by introducing a\nlearnable, category-specific shape template and explicitly aligning each\ntraining object to this template using a learnable 3D transformation. This\napproach also reduces the complexity of geometry prediction to modeling\ndeformations from the template to individual instances. Finally, we employ a\nhyper-network-based method for efficient NeRF creation and enhance the\nreconstruction performance through a contrastive learning-based pretraining\nstrategy. Evaluations on the CO3D dataset demonstrate that VRF achieves\nstate-of-the-art performance in both reconstruction quality and computational\nefficiency.\n","authors":["Kun Wang","Zhiqiang Yan","Zhenyu Zhang","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2306.05145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10797v2","updated":"2025-02-17T07:48:07Z","published":"2024-06-16T03:45:45Z","title":"STAR: Scale-wise Text-conditioned AutoRegressive image generation","summary":"  We introduce STAR, a text-to-image model that employs a scale-wise\nauto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned\nsynthesis for images up to 256$\\times$256, STAR enables text-driven image\ngeneration up to 1024$\\times$1024 through three key designs. First, we\nintroduce a pre-trained text encoder to extract and adopt representations for\ntextual constraints, enhancing details and generalizability. Second, given the\ninherent structural correlation across different scales, we leverage 2D Rotary\nPositional Encoding (RoPE) and tweak it into a normalized version, ensuring\nconsistent interpretation of relative positions across token maps and\nstabilizing the training process. Third, we observe that simultaneously\nsampling all tokens within a single scale can disrupt inter-token\nrelationships, leading to structural instability, particularly in\nhigh-resolution generation. To address this, we propose a novel stable sampling\nmethod that incorporates causal relationships into the sampling process,\nensuring both rich details and stable structures. Compared to previous\ndiffusion models and auto-regressive models, STAR surpasses existing benchmarks\nin fidelity, text-image consistency, and aesthetic quality, requiring just\n2.21s for 1024$\\times$1024 images on A100. This highlights the potential of\nauto-regressive methods in high-quality image synthesis, offering new\ndirections for the text-to-image generation.\n","authors":["Xiaoxiao Ma","Mohan Zhou","Tao Liang","Yalong Bai","Tiejun Zhao","Biye Li","Huaian Chen","Yi Jin"],"pdf_url":"https://arxiv.org/pdf/2406.10797v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.10120v2","updated":"2025-02-17T07:35:28Z","published":"2025-02-14T12:40:37Z","title":"Compress image to patches for Vision Transformer","summary":"  The Vision Transformer (ViT) has made significant strides in the field of\ncomputer vision. However, as the depth of the model and the resolution of the\ninput images increase, the computational cost associated with training and\nrunning ViT models has surged dramatically. This paper proposes a hybrid model\nbased on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a\nmodule called CI2P, which utilizes the CompressAI encoder to compress images\nand subsequently generates a sequence of patches through a series of\nconvolutions. CI2P can replace the Patch Embedding component in the ViT model,\nenabling seamless integration into existing ViT models. Compared to ViT-B/16,\nCI2P-ViT has the number of patches input to the self-attention layer reduced to\na quarter of the original. This design not only significantly reduces the\ncomputational cost of the ViT model but also effectively enhances the model's\naccuracy by introducing the inductive bias properties of CNN. The ViT model's\nprecision is markedly enhanced. When trained from the ground up on the\nAnimals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing\na 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's\ncomputational operations, measured in floating-point operations per second\n(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in\ntraining velocity on identical hardware configurations.\n","authors":["Xinfeng Zhao","Yaoru Sun"],"pdf_url":"https://arxiv.org/pdf/2502.10120v2.pdf","comment":"15 pages,5 figures"},{"id":"http://arxiv.org/abs/2502.11515v1","updated":"2025-02-17T07:29:36Z","published":"2025-02-17T07:29:36Z","title":"SayAnything: Audio-Driven Lip Synchronization with Conditional Video\n  Diffusion","summary":"  Recent advances in diffusion models have led to significant progress in\naudio-driven lip synchronization. However, existing methods typically rely on\nconstrained audio-visual alignment priors or multi-stage learning of\nintermediate representations to force lip motion synthesis. This leads to\ncomplex training pipelines and limited motion naturalness. In this paper, we\npresent SayAnything, a conditional video diffusion framework that directly\nsynthesizes lip movements from audio input while preserving speaker identity.\nSpecifically, we propose three specialized modules including identity\npreservation module, audio guidance module, and editing control module. Our\nnovel design effectively balances different condition signals in the latent\nspace, enabling precise control over appearance, motion, and region-specific\ngeneration without requiring additional supervision signals or intermediate\nrepresentations. Extensive experiments demonstrate that SayAnything generates\nhighly realistic videos with improved lip-teeth coherence, enabling unseen\ncharacters to say anything, while effectively generalizing to animated\ncharacters.\n","authors":["Junxian Ma","Shiwen Wang","Jian Yang","Junyi Hu","Jian Liang","Guosheng Lin","Jingbo chen","Kai Li","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2502.11515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06155v2","updated":"2025-02-17T07:08:23Z","published":"2025-02-10T05:00:56Z","title":"Efficient-vDiT: Efficient Video Diffusion Transformers With Attention\n  Tile","summary":"  Despite the promise of synthesizing high-fidelity videos, Diffusion\nTransformers (DiTs) with 3D full attention suffer from expensive inference due\nto the complexity of attention computation and numerous sampling steps. For\nexample, the popular Open-Sora-Plan model consumes more than 9 minutes for\ngenerating a single video of 29 frames. This paper addresses the inefficiency\nissue from two aspects: 1) Prune the 3D full attention based on the redundancy\nwithin video data; We identify a prevalent tile-style repetitive pattern in the\n3D attention maps for video data, and advocate a new family of sparse 3D\nattention that holds a linear complexity w.r.t. the number of video frames. 2)\nShorten the sampling process by adopting existing multi-step consistency\ndistillation; We split the entire sampling trajectory into several segments and\nperform consistency distillation within each one to activate few-step\ngeneration capacities. We further devise a three-stage training pipeline to\nconjoin the low-complexity attention and few-step generation capacities.\nNotably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into\nan efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video\ngeneration with a marginal performance trade-off in VBench. In addition, we\ndemonstrate that our approach is amenable to distributed inference, achieving\nan additional 3.91x speedup when running on 4 GPUs with sequence parallelism.\n","authors":["Hangliang Ding","Dacheng Li","Runlong Su","Peiyuan Zhang","Zhijie Deng","Ion Stoica","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11501v1","updated":"2025-02-17T07:05:36Z","published":"2025-02-17T07:05:36Z","title":"Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?","summary":"  Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.\n","authors":["Zichen Wen","Yifeng Gao","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11501v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.11494v1","updated":"2025-02-17T06:56:28Z","published":"2025-02-17T06:56:28Z","title":"Stop Looking for Important Tokens in Multimodal Language Models:\n  Duplication Matters More","summary":"  Vision tokens in multimodal large language models often dominate huge\ncomputational overhead due to their excessive length compared to linguistic\nmodality. Abundant recent methods aim to solve this problem with token pruning,\nwhich first defines an importance criterion for tokens and then prunes the\nunimportant vision tokens during inference. However, in this paper, we show\nthat the importance is not an ideal indicator to decide whether a token should\nbe pruned. Surprisingly, it usually results in inferior performance than random\ntoken pruning and leading to incompatibility to efficient attention computation\noperators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),\nwhich prunes tokens based on its duplication with other tokens, leading to\nsignificant and training-free acceleration. Concretely, DART selects a small\nsubset of pivot tokens and then retains the tokens with low duplication to the\npivots, ensuring minimal information loss during token pruning. Experiments\ndemonstrate that DART can prune 88.9% vision tokens while maintaining\ncomparable performance, leading to a 1.99$\\times$ and 2.99$\\times$ speed-up in\ntotal time and prefilling stage, respectively, with good compatibility to\nefficient attention operators. Our codes are available at\nhttps://github.com/ZichenWen1/DART.\n","authors":["Zichen Wen","Yifeng Gao","Shaobo Wang","Junyuan Zhang","Qintong Zhang","Weijia Li","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11494v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.11492v1","updated":"2025-02-17T06:54:49Z","published":"2025-02-17T06:54:49Z","title":"Why Vision Language Models Struggle with Visual Arithmetic? Towards\n  Enhanced Chart and Geometry Understanding","summary":"  Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks.\n","authors":["Kung-Hsiang Huang","Can Qin","Haoyi Qiu","Philippe Laban","Shafiq Joty","Caiming Xiong","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08507v2","updated":"2025-02-17T06:52:15Z","published":"2024-07-11T13:45:50Z","title":"Bootstrapping Vision-language Models for Self-supervised Remote\n  Physiological Measurement","summary":"  Facial video-based remote physiological measurement is a promising research\narea for detecting human vital signs (e.g., heart rate, respiration frequency)\nin a non-contact way. Conventional approaches are mostly supervised learning,\nrequiring extensive collections of facial videos and synchronously recorded\nphotoplethysmography (PPG) signals. To tackle it, self-supervised learning has\nrecently gained attentions; due to the lack of ground truth PPG signals, its\nperformance is however limited. In this paper, we propose a novel\nself-supervised framework that successfully integrates the popular\nvision-language models (VLMs) into the remote physiological measurement task.\nGiven a facial video, we first augment its positive and negative video samples\nwith varying rPPG signal frequencies. Next, we introduce a frequency-oriented\nvision-text pair generation method by carefully creating contrastive\nspatio-temporal maps from positive and negative samples and designing proper\ntext prompts to describe their relative ratios of signal frequencies. A\npre-trained VLM is employed to extract features for these formed vision-text\npairs and estimate rPPG signals thereafter. We develop a series of generative\nand contrastive learning mechanisms to optimize the VLM, including the\ntext-guided visual map reconstruction task, the vision-text contrastive\nlearning task, and the frequency contrastive and ranking task. Overall, our\nmethod for the first time adapts VLMs to digest and align the frequency-related\nknowledge in vision and text modalities. Extensive experiments on four\nbenchmark datasets demonstrate that it significantly outperforms state of the\nart self-supervised methods.\n","authors":["Zijie Yue","Miaojing Shi","Hanli Wang","Shuai Ding","Qijun Chen","Shanlin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.08507v2.pdf","comment":"International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2405.13459v3","updated":"2025-02-17T06:46:11Z","published":"2024-05-22T09:01:56Z","title":"Adapting Multi-modal Large Language Model to Concept Drift From\n  Pre-training Onwards","summary":"  Multi-modal Large Language Models (MLLMs) frequently face challenges from\nconcept drift when dealing with real-world streaming data, wherein\ndistributions change unpredictably. This mainly includes gradual drift due to\nlong-tailed data and sudden drift from Out-Of-Distribution (OOD) data, both of\nwhich have increasingly drawn the attention of the research community. While\nthese issues have been extensively studied in the individual domain of vision\nor language, their impacts on MLLMs in concept drift settings remain largely\nunderexplored. In this paper, we reveal the susceptibility and vulnerability of\nVision-Language (VL) models to significant biases arising from gradual drift\nand sudden drift, particularly in the pre-training. To effectively address\nthese challenges, we propose a unified framework that extends concept drift\ntheory to the multi-modal domain, enhancing the adaptability of the VL model to\nunpredictable distribution changes. Additionally, a T-distribution based drift\nadapter is proposed to effectively mitigate the bias induced by the gradual\ndrift, which also facilitates the model in distinguishing sudden distribution\nchanges through explicit distribution modeling. Extensive experiments\ndemonstrate our method enhances the efficiency and accuracy of image-text\nalignment in the pre-training of VL models, particularly in the concept drift\nscenario. Moreover, various downstream tasks exhibit significant improvements\nin our model's ability to adapt to the long-tailed open world. Furthermore, we\ncreate a set of multi-modal datasets called OpenMMlo, specifically tailored for\nthe long-tailed open-world setting, to validate our findings. To foster the\ndevelopment of the multi-modal community, we have made both OpenMMlo datasets\nand our code publicly available at:\nhttps://github.com/XiaoyuYoung/ConceptDriftMLLMs.\n","authors":["Xiaoyu Yang","Jie Lu","En Yu"],"pdf_url":"https://arxiv.org/pdf/2405.13459v3.pdf","comment":"ICLR 2025 Poster"},{"id":"http://arxiv.org/abs/2502.11481v1","updated":"2025-02-17T06:35:37Z","published":"2025-02-17T06:35:37Z","title":"Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound\n  Videos","summary":"  The intersection of medical imaging and artificial intelligence has become an\nimportant research direction in intelligent medical treatment, particularly in\nthe analysis of medical images using deep learning for clinical diagnosis.\nDespite the advances, existing keyframe classification methods lack extraction\nof time series features, while ultrasonic video classification based on\nthree-dimensional convolution requires uniform frame numbers across patients,\nresulting in poor feature extraction efficiency and model classification\nperformance. This study proposes a novel video classification method based on\nCNN and LSTM, introducing NLP's long and short sentence processing scheme into\nvideo classification for the first time. The method reduces CNN-extracted image\nfeatures to 1x512 dimension, followed by sorting and compressing feature\nvectors for LSTM training. Specifically, feature vectors are sorted by patient\nvideo frame numbers and populated with padding value 0 to form variable\nbatches, with invalid padding values compressed before LSTM training to\nconserve computing resources. Experimental results demonstrate that our\nvariable-frame CNNLSTM method outperforms other approaches across all metrics,\nshowing improvements of 3-6% in F1 score and 1.5% in specificity compared to\nkeyframe methods. The variable-frame CNNLSTM also achieves better accuracy and\nprecision than equal-frame CNNLSTM. These findings validate the effectiveness\nof our approach in classifying variable-frame ultrasound videos and suggest\npotential applications in other medical imaging modalities.\n","authors":["Xiangxiang Cui","Zhongyu Li","Xiayue Fan","Peng Huang","Ying Wang","Meng Yang","Shi Chang","Jihua Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11477v1","updated":"2025-02-17T06:28:53Z","published":"2025-02-17T06:28:53Z","title":"Learning to Sample Effective and Diverse Prompts for Text-to-Image\n  Generation","summary":"  Recent advances in text-to-image diffusion models have achieved impressive\nimage generation capabilities. However, it remains challenging to control the\ngeneration process with desired properties (e.g., aesthetic quality, user\nintention), which can be expressed as black-box reward functions. In this\npaper, we focus on prompt adaptation, which refines the original prompt into\nmodel-preferred prompts to generate desired images. While prior work uses\nreinforcement learning (RL) to optimize prompts, we observe that applying RL\noften results in generating similar postfixes and deterministic behaviors. To\nthis end, we introduce \\textbf{P}rompt \\textbf{A}daptation with\n\\textbf{G}FlowNets (\\textbf{PAG}), a novel approach that frames prompt\nadaptation as a probabilistic inference problem. Our key insight is that\nleveraging Generative Flow Networks (GFlowNets) allows us to shift from reward\nmaximization to sampling from an unnormalized density function, enabling both\nhigh-quality and diverse prompt generation. However, we identify that a naive\napplication of GFlowNets suffers from mode collapse and uncovers a previously\noverlooked phenomenon: the progressive loss of neural plasticity in the model,\nwhich is compounded by inefficient credit assignment in sequential prompt\ngeneration. To address this critical challenge, we develop a systematic\napproach in PAG with flow reactivation, reward-prioritized sampling, and reward\ndecomposition for prompt adaptation. Extensive experiments validate that PAG\nsuccessfully learns to sample effective and diverse prompts for text-to-image\ngeneration. We also show that PAG exhibits strong robustness across various\nreward functions and transferability to different text-to-image models.\n","authors":["Taeyoung Yun","Dinghuai Zhang","Jinkyoo Park","Ling Pan"],"pdf_url":"https://arxiv.org/pdf/2502.11477v1.pdf","comment":"18 pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.11468v1","updated":"2025-02-17T05:57:57Z","published":"2025-02-17T05:57:57Z","title":"Semantically Robust Unsupervised Image Translation for Paired Remote\n  Sensing Images","summary":"  Image translation for change detection or classification in bi-temporal\nremote sensing images is unique. Although it can acquire paired images, it is\nstill unsupervised. Moreover, strict semantic preservation in translation is\nalways needed instead of multimodal outputs. In response to these problems,\nthis paper proposes a new method, SRUIT (Semantically Robust Unsupervised\nImage-to-image Translation), which ensures semantically robust translation and\nproduces deterministic output. Inspired by previous works, the method explores\nthe underlying characteristics of bi-temporal Remote Sensing images and designs\nthe corresponding networks. Firstly, we assume that bi-temporal Remote Sensing\nimages share the same latent space, for they are always acquired from the same\nland location. So SRUIT makes the generators share their high-level layers, and\nthis constraint will compel two domain mapping to fall into the same latent\nspace. Secondly, considering land covers of bi-temporal images could evolve\ninto each other, SRUIT exploits the cross-cycle-consistent adversarial networks\nto translate from one to the other and recover them. Experimental results show\nthat constraints of sharing weights and cross-cycle consistency enable\ntranslated images with both good perceptual image quality and semantic\npreservation for significant differences.\n","authors":["Sheng Fang","Kaiyu Li","Zhe Li","Jianli Zhao","Xingli Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05331v3","updated":"2025-02-17T05:54:20Z","published":"2024-12-05T07:44:40Z","title":"Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object\n  Detection and Motion Tracking","summary":"  This project aims to develop a robust video surveillance system, which can\nsegment videos into smaller clips based on the detection of activities. It uses\nCCTV footage, for example, to record only major events-like the appearance of a\nperson or a thief-so that storage is optimized and digital searches are easier.\nIt utilizes the latest techniques in object detection and tracking, including\nConvolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well\nas Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks\n(LSTMs), to achieve high accuracy in detection and capture temporal\ndependencies. The approach incorporates adaptive background modeling through\nGaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to\ndetect motions. Multi-scale and contextual analysis are used to improve\ndetection across different object sizes and environments. A hybrid motion\nsegmentation strategy combines statistical and deep learning models to manage\ncomplex movements, while optimizations for real-time processing ensure\nefficient computation. Tracking methods, such as Kalman Filters and Siamese\nnetworks, are employed to maintain smooth tracking even in cases of occlusion.\nDetection is improved on various-sized objects for multiple scenarios by\nmulti-scale and contextual analysis. Results demonstrate high precision and\nrecall in detecting and tracking objects, with significant improvements in\nprocessing times and accuracy due to real-time optimizations and\nillumination-invariant features. The impact of this research lies in its\npotential to transform video surveillance, reducing storage requirements and\nenhancing security through reliable and efficient object detection and\ntracking.\n","authors":["Shahran Rahman Alve"],"pdf_url":"https://arxiv.org/pdf/2412.05331v3.pdf","comment":"15 Pages, 7 Figures"},{"id":"http://arxiv.org/abs/2502.01101v2","updated":"2025-02-17T05:49:03Z","published":"2025-02-03T06:45:00Z","title":"VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion\n  Control","summary":"  With the advancement of generative artificial intelligence, previous studies\nhave achieved the task of generating aesthetic images from hand-drawn sketches,\nfulfilling the public's needs for drawing. However, these methods are limited\nto static images and lack the ability to control video animation generation\nusing hand-drawn sketches. To address this gap, we propose VidSketch, the first\nmethod capable of generating high-quality video animations directly from any\nnumber of hand-drawn sketches and simple text prompts, bridging the divide\nbetween ordinary users and professional artists. Specifically, our method\nintroduces a Level-Based Sketch Control Strategy to automatically adjust the\nguidance strength of sketches during the generation process, accommodating\nusers with varying drawing skills. Furthermore, a TempSpatial Attention\nmechanism is designed to enhance the spatiotemporal consistency of generated\nvideo animations, significantly improving the coherence across frames. You can\nfind more detailed cases on our official website.\n","authors":["Lifan Jiang","Shuang Chen","Boxi Wu","Xiaotong Guan","Jiahui Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.01101v2.pdf","comment":"17pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.09855v2","updated":"2025-02-17T05:35:12Z","published":"2024-10-13T14:28:16Z","title":"Text4Seg: Reimagining Image Segmentation as Text Generation","summary":"  Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks; however, effectively integrating image segmentation\ninto these models remains a significant challenge. In this paper, we introduce\nText4Seg, a novel text-as-mask paradigm that casts image segmentation as a text\ngeneration problem, eliminating the need for additional decoders and\nsignificantly simplifying the segmentation process. Our key innovation is\nsemantic descriptors, a new textual representation of segmentation masks where\neach image patch is mapped to its corresponding text label. This unified\nrepresentation allows seamless integration into the auto-regressive training\npipeline of MLLMs for easier optimization. We demonstrate that representing an\nimage with $16\\times16$ semantic descriptors yields competitive segmentation\nperformance. To enhance efficiency, we introduce the Row-wise Run-Length\nEncoding (R-RLE), which compresses redundant text sequences, reducing the\nlength of semantic descriptors by 74% and accelerating inference by $3\\times$,\nwithout compromising performance. Extensive experiments across various vision\ntasks, such as referring expression segmentation and comprehension, show that\nText4Seg achieves state-of-the-art performance on multiple datasets by\nfine-tuning different MLLM backbones. Our approach provides an efficient,\nscalable solution for vision-centric tasks within the MLLM framework.\n","authors":["Mengcheng Lan","Chaofeng Chen","Yue Zhou","Jiaxing Xu","Yiping Ke","Xinjiang Wang","Litong Feng","Wayne Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09855v2.pdf","comment":"ICLR 2025. Project page: https://mc-lan.github.io/Text4Seg/"},{"id":"http://arxiv.org/abs/2502.11456v1","updated":"2025-02-17T05:29:50Z","published":"2025-02-17T05:29:50Z","title":"Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning\n  Network for Semi-supervised 3D Medical Image Segmentation","summary":"  Semi-supervised 3D medical image segmentation aims to achieve accurate\nsegmentation using few labelled data and numerous unlabelled data. The main\nchallenge in the design of semi-supervised learning methods consists in the\neffective use of the unlabelled data for training. A promising solution\nconsists of ensuring consistent predictions across different views of the data,\nwhere the efficacy of this strategy depends on the accuracy of the\npseudo-labels generated by the model for this consistency learning strategy. In\nthis paper, we introduce a new methodology to produce high-quality\npseudo-labels for a consistency learning strategy to address semi-supervised 3D\nmedical image segmentation. The methodology has three important contributions.\nThe first contribution is the Cooperative Rectification Learning Network (CRLN)\nthat learns multiple prototypes per class to be used as external knowledge\npriors to adaptively rectify pseudo-labels at the voxel level. The second\ncontribution consists of the Dynamic Interaction Module (DIM) to facilitate\npairwise and cross-class interactions between prototypes and multi-resolution\nimage features, enabling the production of accurate voxel-level clues for\npseudo-label rectification. The third contribution is the Cooperative Positive\nSupervision (CPS), which optimises uncertain representations to align with\nunassertive representations of their class distributions, improving the model's\naccuracy in classifying uncertain regions. Extensive experiments on three\npublic 3D medical segmentation datasets demonstrate the effectiveness and\nsuperiority of our semi-supervised learning method.\n","authors":["Yanyan Wang","Kechen Song","Yuyuan Liu","Shuai Ma","Yunhui Yan","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2502.11456v1.pdf","comment":"Medical Image Analysis"},{"id":"http://arxiv.org/abs/2412.17042v3","updated":"2025-02-17T05:13:09Z","published":"2024-12-22T14:49:55Z","title":"Adapting Image-to-Video Diffusion Models for Large-Motion Frame\n  Interpolation","summary":"  With the development of video generation models has advanced significantly in\nrecent years, we adopt large-scale image-to-video diffusion models for video\nframe interpolation. We present a conditional encoder designed to adapt an\nimage-to-video model for large-motion frame interpolation. To enhance\nperformance, we integrate a dual-branch feature extractor and propose a\ncross-frame attention mechanism that effectively captures both spatial and\ntemporal information, enabling accurate interpolations of intermediate frames.\nOur approach demonstrates superior performance on the Fr\\'echet Video Distance\n(FVD) metric when evaluated against other state-of-the-art approaches,\nparticularly in handling large motion scenarios, highlighting advancements in\ngenerative-based methodologies.\n","authors":["Luoxu Jin","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2412.17042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18643v2","updated":"2025-02-17T05:11:30Z","published":"2025-01-29T04:18:51Z","title":"3D Reconstruction of Shoes for Augmented Reality","summary":"  This paper introduces a mobile-based solution that enhances online shoe\nshopping through 3D modeling and Augmented Reality (AR), leveraging the\nefficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D\nimages, the framework generates realistic 3D shoe models from 2D images,\nachieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables\nimmersive AR interactions via smartphones. A custom shoe segmentation dataset\nof 3120 images was created, with the best-performing segmentation model\nachieving an Intersection over Union (IoU) score of 0.95. This paper\ndemonstrates the potential of 3D modeling and AR to revolutionize online\nshopping by offering realistic virtual interactions, with applicability across\nbroader fashion categories.\n","authors":["Pratik Shrestha","Sujan Kapali","Swikar Gautam","Vishal Pokharel","Santosh Giri"],"pdf_url":"https://arxiv.org/pdf/2501.18643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11440v1","updated":"2025-02-17T04:54:47Z","published":"2025-02-17T04:54:47Z","title":"Medical Image Registration Meets Vision Foundation Model: Prototype\n  Learning and Contour Awareness","summary":"  Medical image registration is a fundamental task in medical image analysis,\naiming to establish spatial correspondences between paired images. However,\nexisting unsupervised deformable registration methods rely solely on\nintensity-based similarity metrics, lacking explicit anatomical knowledge,\nwhich limits their accuracy and robustness. Vision foundation models, such as\nthe Segment Anything Model (SAM), can generate high-quality segmentation masks\nthat provide explicit anatomical structure knowledge, addressing the\nlimitations of traditional methods that depend only on intensity similarity.\nBased on this, we propose a novel SAM-assisted registration framework\nincorporating prototype learning and contour awareness. The framework includes:\n(1) Explicit anatomical information injection, where SAM-generated segmentation\nmasks are used as auxiliary inputs throughout training and testing to ensure\nthe consistency of anatomical information; (2) Prototype learning, which\nleverages segmentation masks to extract prototype features and aligns\nprototypes to optimize semantic correspondences between images; and (3)\nContour-aware loss, a contour-aware loss is designed that leverages the edges\nof segmentation masks to improve the model's performance in fine-grained\ndeformation fields. Extensive experiments demonstrate that the proposed\nframework significantly outperforms existing methods across multiple datasets,\nparticularly in challenging scenarios with complex anatomical structures and\nambiguous boundaries. Our code is available at\nhttps://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration.\n","authors":["Hao Xu","Tengfei Xue","Jianan Fan","Dongnan Liu","Yuqian Chen","Fan Zhang","Carl-Fredrik Westin","Ron Kikinis","Lauren J. O'Donnell","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2502.11440v1.pdf","comment":"Accepted by Information Processing in Medical Imaging (IPMI) 2025"},{"id":"http://arxiv.org/abs/2502.11427v1","updated":"2025-02-17T04:38:12Z","published":"2025-02-17T04:38:12Z","title":"Do we Really Need Visual Instructions? Towards Visual Instruction-Free\n  Fine-tuning for Large Vision-Language Models","summary":"  Visual instruction tuning has become the predominant technology in eliciting\nthe multimodal task-solving capabilities of large vision-language models\n(LVLMs). Despite the success, as visual instructions require images as the\ninput, it would leave the gap in inheriting the task-solving capabilities from\nthe backbone LLMs, and make it costly to collect a large-scale dataset. To\naddress it, we propose ViFT, a visual instruction-free fine-tuning framework\nfor LVLMs. In ViFT, we only require the text-only instructions and image\ncaption data during training, to separately learn the task-solving and visual\nperception abilities. During inference, we extract and combine the\nrepresentations of the text and image inputs, for fusing the two abilities to\nfulfill multimodal tasks. Experimental results demonstrate that ViFT can\nachieve state-of-the-art performance on several visual reasoning and visual\ninstruction following benchmarks, with rather less training data. Our code and\ndata will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11427v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2502.11408v1","updated":"2025-02-17T03:49:18Z","published":"2025-02-17T03:49:18Z","title":"Precise GPS-Denied UAV Self-Positioning via Context-Enhanced Cross-View\n  Geo-Localization","summary":"  Image retrieval has been employed as a robust complementary technique to\naddress the challenge of Unmanned Aerial Vehicles (UAVs) self-positioning.\nHowever, most existing methods primarily focus on localizing objects captured\nby UAVs through complex part-based representations, often overlooking the\nunique challenges associated with UAV self-positioning, such as fine-grained\nspatial discrimination requirements and dynamic scene variations. To address\nthe above issues, we propose the Context-Enhanced method for precise UAV\nSelf-Positioning (CEUSP), specifically designed for UAV self-positioning tasks.\nCEUSP integrates a Dynamic Sampling Strategy (DSS) to efficiently select\noptimal negative samples, while the Rubik's Cube Attention (RCA) module,\ncombined with the Context-Aware Channel Integration (CACI) module, enhances\nfeature representation and discrimination by exploiting interdimensional\ninteractions, inspired by the rotational mechanics of a Rubik's Cube. Extensive\nexperimental validate the effectiveness of the proposed method, demonstrating\nnotable improvements in feature representation and UAV self-positioning\naccuracy within complex urban environments. Our approach achieves\nstate-of-the-art performance on the DenseUAV dataset, which is specifically\ndesigned for dense urban contexts, and also delivers competitive results on the\nwidely recognized University-1652 benchmark.\n","authors":["Yuanze Xu","Ming Dai","Wenxiao Cai","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11408v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.02795v3","updated":"2025-02-17T03:49:14Z","published":"2025-01-06T06:29:55Z","title":"InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion","summary":"  We introduce InfiFusion, an efficient training pipeline designed to integrate\nmultiple domain-specialized Large Language Models (LLMs) into a single pivot\nmodel, effectively harnessing the strengths of each source model. Traditional\nfusion methods either merge model parameters directly or rely on knowledge\ndistillation with rigid assumptions, limiting their flexibility and efficiency.\nInfiFusion overcomes these limitations by enhancing Universal Logit\nDistillation (ULD) with Top-K selection and Logits Standardization. We propose\ntwo fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source\nmodel knowledge is distilled individually into the pivot model followed by\nmerging and Unified Fusion (InfiFusion$_u$), where knowledge from all source\nmodels is distilled simultaneously into the pivot model. InfiFusion outperforms\nthe state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11\nwidely applied benchmarks covering reasoning, coding, mathematics, and\ninstruction-following tasks. Notably, InfiFusion achieves this superior\nperformance while significantly reduces computational costs, completing full\ntraining with only 160 H800 GPU hours compared to the millions typically\nrequired for traditional LLM training.\n","authors":["Zhaoyi Yan","Yiming Zhang","Baoyi He","Yuhao Fu","Qi Zhou","Zhijie Sang","Chunlin Ji","Shengyu Zhang","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2501.02795v3.pdf","comment":"Significant performance improvements over the previous version; under\n  review;"},{"id":"http://arxiv.org/abs/2502.11390v1","updated":"2025-02-17T03:12:16Z","published":"2025-02-17T03:12:16Z","title":"MARS: Mesh AutoRegressive Model for 3D Shape Detailization","summary":"  State-of-the-art methods for mesh detailization predominantly utilize\nGenerative Adversarial Networks (GANs) to generate detailed meshes from coarse\nones. These methods typically learn a specific style code for each category or\nsimilar categories without enforcing geometry supervision across different\nLevels of Detail (LODs). Consequently, such methods often fail to generalize\nacross a broader range of categories and cannot ensure shape consistency\nthroughout the detailization process. In this paper, we introduce MARS, a novel\napproach for 3D shape detailization. Our method capitalizes on a novel\nmulti-LOD, multi-category mesh representation to learn shape-consistent mesh\nrepresentations in latent space across different LODs. We further propose a\nmesh autoregressive model capable of generating such latent representations\nthrough next-LOD token prediction. This approach significantly enhances the\nrealism of the generated shapes. Extensive experiments conducted on the\nchallenging 3D Shape Detailization benchmark demonstrate that our proposed MARS\nmodel achieves state-of-the-art performance, surpassing existing methods in\nboth qualitative and quantitative assessments. Notably, the model's capability\nto generate fine-grained details while preserving the overall shape integrity\nis particularly commendable.\n","authors":["Jingnan Gao","Weizhe Liu","Weixuan Sun","Senbo Wang","Xibin Song","Taizhang Shang","Shenzhou Chen","Hongdong Li","Xiaokang Yang","Yichao Yan","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2502.11390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04772v3","updated":"2025-02-17T03:10:52Z","published":"2024-06-07T09:17:33Z","title":"REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning","summary":"  Recent rehearsal-free methods, guided by prompts, excel in vision-related\ncontinual learning (CL) with drifting data but lack resource efficiency, making\nreal-world deployment challenging. In this paper, we introduce\nResource-Efficient Prompting (REP), which improves the computational and memory\nefficiency of prompt-based rehearsal-free methods while minimizing accuracy\ntrade-offs. Our approach employs swift prompt selection to refine input data\nusing a carefully provisioned model and introduces adaptive token merging\n(AToM) and layer dropping (ALD) for efficient prompt updates. AToM and ALD\nselectively skip data and model layers while preserving task-specific features\nduring new-task learning. Extensive experiments on multiple image\nclassification datasets demonstrates REP's superior resource efficiency over\nstate-of-the-art ViT- and CNN-based methods.\n","authors":["Sungho Jeon","Xinyue Ma","Kwang In Kim","Myeongjae Jeon"],"pdf_url":"https://arxiv.org/pdf/2406.04772v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05823v2","updated":"2025-02-17T03:04:57Z","published":"2024-11-05T05:45:26Z","title":"FlexCAD: Unified and Versatile Controllable CAD Generation with\n  Fine-tuned Large Language Models","summary":"  Recently, there is a growing interest in creating computer-aided design (CAD)\nmodels based on user intent, known as controllable CAD generation. Existing\nwork offers limited controllability and needs separate models for different\ntypes of control, reducing efficiency and practicality. To achieve controllable\ngeneration across all CAD construction hierarchies, such as sketch-extrusion,\nextrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by\nfine-tuning large language models (LLMs). First, to enhance comprehension by\nLLMs, we represent a CAD model as a structured text by abstracting each\nhierarchy as a sequence of text tokens. Second, to address various controllable\ngeneration tasks in a unified model, we introduce a hierarchy-aware masking\nstrategy. Specifically, during training, we mask a hierarchy-aware field in the\nCAD text with a mask token. This field, composed of a sequence of tokens, can\nbe set flexibly to represent various hierarchies. Subsequently, we ask LLMs to\npredict this masked field. During inference, the user intent is converted into\na CAD text with a mask token replacing the part the user wants to modify, which\nis then fed into FlexCAD to generate new CAD models. Comprehensive experiments\non public dataset demonstrate the effectiveness of FlexCAD in both generation\nquality and controllability. Code will be available at\nhttps://github.com/microsoft/FlexCAD.\n","authors":["Zhanwei Zhang","Shizhao Sun","Wenxiao Wang","Deng Cai","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2411.05823v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11382v1","updated":"2025-02-17T02:54:14Z","published":"2025-02-17T02:54:14Z","title":"A Physics-Informed Blur Learning Framework for Imaging Systems","summary":"  Accurate blur estimation is essential for high-performance imaging across\nvarious applications. Blur is typically represented by the point spread\nfunction (PSF). In this paper, we propose a physics-informed PSF learning\nframework for imaging systems, consisting of a simple calibration followed by a\nlearning process. Our framework could achieve both high accuracy and universal\napplicability. Inspired by the Seidel PSF model for representing spatially\nvarying PSF, we identify its limitations in optimization and introduce a novel\nwavefront-based PSF model accompanied by an optimization strategy, both\nreducing optimization complexity and improving estimation accuracy. Moreover,\nour wavefront-based PSF model is independent of lens parameters, eliminate the\nneed for prior knowledge of the lens. To validate our approach, we compare it\nwith recent PSF estimation methods (Degradation Transfer and Fast Two-step)\nthrough a deblurring task, where all the estimated PSFs are used to train\nstate-of-the-art deblurring algorithms. Our approach demonstrates improvements\nin image quality in simulation and also showcases noticeable visual quality\nimprovements on real captured images.\n","authors":["Liqun Chen","Yuxuan Li","Jun Dai","Jinwei Gu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2502.11382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11381v1","updated":"2025-02-17T02:53:08Z","published":"2025-02-17T02:53:08Z","title":"Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for\n  UAV-View Geo-Localization","summary":"  UAV-View Geo-Localization (UVGL) aims to ascertain the precise location of a\nUAV by retrieving the most similar GPS-tagged satellite image. However,\nexisting methods predominantly rely on supervised learning paradigms that\nnecessitate annotated paired data for training, which incurs substantial\nannotation costs and impedes large-scale deployment. To overcome this\nlimitation, we propose the Dynamic Memory-Driven and Neighborhood Information\nLearning (DMNIL) network, a lightweight end-to-end self-supervised framework\nfor UAV-view geo-localization. The DMNIL framework utilizes a dual-path\nclustering-based contrastive learning architecture as its baseline to model\nintra-view structural relationships, enhancing feature consistency and\ndiscriminability. Additionally, a dynamic memory-driven hierarchical learning\nmodule is proposed to progressively mine local and global information,\nreinforcing multi-level feature associations to improve model robustness. To\nbridge the domain gap between UAV and satellite views, we design an\ninformation-consistent evolutionary learning mechanism that systematically\nexplores latent correlations within intra-view neighborhoods and across\ncross-view domains, ultimately constructing a unified cross-view feature\nrepresentation space. Extensive experiments on three benchmarks\n(University-1652, SUES-200, and DenseUAV) demonstrate that DMNIL achieves\ncompetitive performance against state-of-the-art supervised methods while\nmaintaining computational efficiency. Notably, this superiority is attained\nwithout relying on paired training data, underscoring the framework's\npracticality for real-world deployment. Codes will be released soon.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2502.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14750v2","updated":"2025-02-17T02:49:16Z","published":"2024-04-23T05:16:24Z","title":"Grounded Knowledge-Enhanced Medical Vision-Language Pre-training for\n  Chest X-Ray","summary":"  Medical foundation models have the potential to revolutionize healthcare by\nproviding robust and generalized representations of medical data. Medical\nvision-language pre-training has emerged as a promising approach for learning\ndomain-general representations of medical image and text. Current algorithms\nthat exploit global and local alignment between medical image and text could\nhowever be marred by redundant information in medical data. To address this\nissue, we propose a grounded knowledge-enhanced medical vision-language\npre-training (GK-MVLP) framework for chest X-ray. In this framework, medical\nknowledge was grounded to the appropriate anatomical regions by using a\ntransformer-based grounded knowledge-enhanced module for fine-grained alignment\nbetween textural features of medical knowledge and the corresponding anatomical\nregion-level visual features. The performance of GK-MVLP was competitive with\nor exceeded the state of the art on downstream image understanding tasks (chest\nX-ray disease classification, disease localization), generative task (report\ngeneration), and vision-language understanding task (medical visual\nquestion-answering). Our results demonstrate the advantage of incorporating\ngrounding mechanism to remove biases and improve the alignment between chest\nX-ray image and radiology report.\n","authors":["Qiao Deng","Zhongzhen Huang","Yunqi Wang","Zhichuan Wang","Zhao Wang","Xiaofan Zhang","Qi Dou","Yeung Yu Hui","Edward S. Hui"],"pdf_url":"https://arxiv.org/pdf/2404.14750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07887v3","updated":"2025-02-17T02:18:38Z","published":"2024-02-02T12:37:23Z","title":"Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot\n  Representations","summary":"  Several accounts of human cognition posit that our intelligence is rooted in\nour ability to form abstract composable concepts, ground them in our\nenvironment, and reason over these grounded entities. This trifecta of human\nthought has remained elusive in modern intelligent machines. In this work, we\ninvestigate whether slot representations extracted from visual scenes serve as\nappropriate compositional abstractions for grounding and reasoning. We present\nthe Neural Slot Interpreter (NSI), which learns to ground object semantics in\nslots. At the core of NSI is an XML-like schema that uses simple syntax rules\nto organize the object semantics of a scene into object-centric schema\nprimitives. Then, the NSI metric learns to ground primitives into slots through\na structured contrastive learning objective that reasons over the intermodal\nalignment. Experiments with a bi-modal object-property and scene retrieval task\ndemonstrate the grounding efficacy and interpretability of correspondences\nlearned by NSI. From a scene representation standpoint, we find that emergent\nNSI slots that move beyond the image grid by binding to spatial objects\nfacilitate improved visual grounding compared to conventional\nbounding-box-based approaches. From a data efficiency standpoint, we\nempirically validate that NSI learns more generalizable representations from a\nfixed amount of annotation data than the traditional approach. We also show\nthat the grounded slots surpass unsupervised slots in real-world object\ndiscovery and scale with scene complexity. Finally, we investigate the\nreasoning abilities of the grounded slots. Vision Transformers trained on\ngrounding-aware NSI tokenizers using as few as ten tokens outperform\npatch-based tokens on challenging few-shot classification tasks.\n","authors":["Bhishma Dedhia","Niraj K. Jha"],"pdf_url":"https://arxiv.org/pdf/2403.07887v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11360v1","updated":"2025-02-17T02:18:33Z","published":"2025-02-17T02:18:33Z","title":"GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder","summary":"  We introduce GeoDANO, a geometric vision-language model (VLM) with a\ndomain-agnostic vision encoder, for solving plane geometry problems. Although\nVLMs have been employed for solving geometry problems, their ability to\nrecognize geometric features remains insufficiently analyzed. To address this\ngap, we propose a benchmark that evaluates the recognition of visual geometric\nfeatures, including primitives such as dots and lines, and relations such as\northogonality. Our preliminary study shows that vision encoders often used in\ngeneral-purpose VLMs, e.g., OpenCLIP, fail to detect these features and\nstruggle to generalize across domains. We develop GeoCLIP, a CLIP based model\ntrained on synthetic geometric diagram-caption pairs to overcome the\nlimitation. Benchmark results show that GeoCLIP outperforms existing vision\nencoders in recognizing geometric features. We then propose our VLM, GeoDANO,\nwhich augments GeoCLIP with a domain adaptation strategy for unseen diagram\nstyles. GeoDANO outperforms specialized methods for plane geometry problems and\nGPT-4o on MathVerse.\n","authors":["Seunghyuk Cho","Zhenyue Qin","Yang Liu","Youngbin Choi","Seungbeom Lee","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11360v1.pdf","comment":"14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.07701v3","updated":"2025-02-17T02:02:08Z","published":"2025-02-11T16:58:15Z","title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute","summary":"  In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.\n","authors":["Hongwei Yi","Shitong Shao","Tian Ye","Jiantong Zhao","Qingyu Yin","Michael Lingelbach","Li Yuan","Yonghong Tian","Enze Xie","Daquan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07701v3.pdf","comment":"Serious updates are needed"},{"id":"http://arxiv.org/abs/2406.11288v3","updated":"2025-02-17T02:00:52Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from factuality due to inherent bias or\nincorrect inference. To address this issue, we introduce MFC-Bench, a rigorous\nand comprehensive benchmark designed to evaluate the factual accuracy of LVLMs\nacross three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v3.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.11338v1","updated":"2025-02-17T01:31:36Z","published":"2025-02-17T01:31:36Z","title":"WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld\n  Radiographic Testing","summary":"  Radiographic testing is a fundamental non-destructive evaluation technique\nfor identifying weld defects and assessing quality in industrial applications\ndue to its high-resolution imaging capabilities. Over the past decade, deep\nlearning techniques have significantly advanced weld defect identification in\nradiographic images. However, conventional approaches, which rely on training\nsmall-scale, task-specific models on single-scenario datasets, exhibit poor\ncross-scenario generalization. Recently, the Segment Anything Model (SAM), a\npre-trained visual foundation model trained on large-scale datasets, has\ndemonstrated exceptional zero-shot generalization capabilities. Fine-tuning SAM\nwith limited domain-specific data has yielded promising results in fields such\nas medical image segmentation and anomaly detection. To the best of our\nknowledge, this work is the first to introduce SAM-based segmentation for\ngeneral weld radiographic testing images. We propose WRT-SAM, a novel weld\nradiographic defect segmentation model that leverages SAM through an\nadapter-based integration with a specialized prompt generator architecture. To\nimprove adaptability to grayscale weld radiographic images, we introduce a\nfrequency prompt generator module, which enhances the model's sensitivity to\nfrequency-domain information. Furthermore, to address the multi-scale nature of\nweld defects, we incorporate a multi-scale prompt generator module, enabling\nthe model to effectively extract and encode defect information across varying\nscales. Extensive experimental evaluations demonstrate that WRT-SAM achieves a\nrecall of 78.87%, a precision of 84.04%, and an AUC of 0.9746, setting a new\nstate-of-the-art (SOTA) benchmark. Moreover, the model exhibits superior\nzero-shot generalization performance, highlighting its potential for practical\ndeployment in diverse radiographic testing scenarios.\n","authors":["Yunyi Zhou","Kun Shi","Gang Hao"],"pdf_url":"https://arxiv.org/pdf/2502.11338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11337v1","updated":"2025-02-17T01:27:35Z","published":"2025-02-17T01:27:35Z","title":"A Comparison of Human and Machine Learning Errors in Face Recognition","summary":"  Machine learning applications in high-stakes scenarios should always operate\nunder human oversight. Developing an optimal combination of human and machine\nintelligence requires an understanding of their complementarities, particularly\nregarding the similarities and differences in the way they make mistakes. We\nperform extensive experiments in the area of face recognition and compare two\nautomated face recognition systems against human annotators through a\ndemographically balanced user study. Our research uncovers important ways in\nwhich machine learning errors and human errors differ from each other, and\nsuggests potential strategies in which human-machine collaboration can improve\naccuracy in face recognition.\n","authors":["Marina Est√©vez-Almenzar","Ricardo Baeza-Yates","Carlos Castillo"],"pdf_url":"https://arxiv.org/pdf/2502.11337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09873v2","updated":"2025-02-17T01:24:21Z","published":"2024-05-16T07:49:24Z","title":"IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet\n  Transform Feature Modulation Model","summary":"  Infrared image super-resolution demands long-range dependency modeling and\nmulti-scale feature extraction to address challenges such as homogeneous\nbackgrounds, weak edges, and sparse textures. While Mamba-based state-space\nmodels (SSMs) excel in global dependency modeling with linear complexity, their\nblock-wise processing disrupts spatial consistency, limiting their\neffectiveness for IR image reconstruction. We propose IRSRMamba, a novel\nframework integrating wavelet transform feature modulation for multi-scale\nadaptation and an SSMs-based semantic consistency loss to restore fragmented\ncontextual information. This design enhances global-local feature fusion,\nstructural coherence, and fine-detail preservation while mitigating\nblock-induced artifacts. Experiments on benchmark datasets demonstrate that\nIRSRMamba outperforms state-of-the-art methods in PSNR, SSIM, and perceptual\nquality. This work establishes Mamba-based architectures as a promising\ndirection for high-fidelity IR image enhancement. Code are available at\nhttps://github.com/yongsongH/IRSRMamba.\n","authors":["Yongsong Huang","Tomo Miyazaki","Xiaofeng Liu","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2405.09873v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2411.12199v2","updated":"2025-02-17T01:10:00Z","published":"2024-11-19T03:30:44Z","title":"Rethinking Text-Promptable Surgical Instrument Segmentation with Robust\n  Framework","summary":"  Surgical instrument segmentation (SIS) is essential in computer-assisted\nsurgeries, with deep learning methods improving accuracy in complex\nenvironments. Recently, text-promptable segmentation methods have been\nintroduced, generating masks based on textual descriptions. However, they\nassume the text-described object is present and always generate an associated\nmask even when the object is absent. Existing methods address this by using\nprompts only for objects already known to exist in the scene, which relies on\ninaccessible information. To address this, we rethink text-promptable SIS and\nredefine it under robust conditions as Robust text-promptable SIS (R-SIS).\nUnlike previous approaches, R-SIS is a process that analyzes text prompts for\nall surgical instrument categories without relying on external knowledge,\nidentifies the instruments present in the scene, and segments them accordingly.\nBuilding on this, we propose Robust Surgical Instrument Segmentation (RoSIS),\nan optimized framework combining visual and language features for promptable\nsegmentation in the R-SIS setting. RoSIS employs an encoder-decoder\narchitecture with a Multi-Modal Fusion Block (MMFB) and a Selective Gate Block\n(SGB) for balanced integration of vision and language features. Additionally,\nan iterative refinement strategy enhances segmentation masks through a two-step\nprocess: an initial pass with name-based prompts, followed by refinement with\nlocation prompts. Experiments across multiple datasets and settings show that\nRoSIS outperforms existing vision-based and promptable segmentation methods\nunder robust conditions. By rethinking text-promptable SIS, our work\nestablishes a fair and effective approach to surgical instrument segmentation.\n","authors":["Tae-Min Choi","Juyoun Park"],"pdf_url":"https://arxiv.org/pdf/2411.12199v2.pdf","comment":"11 pages, 6 figures, 7 tables, submitted to IEEE Journal of\n  Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2502.11329v1","updated":"2025-02-17T01:04:47Z","published":"2025-02-17T01:04:47Z","title":"Differentially private fine-tuned NF-Net to predict GI cancer type","summary":"  Based on global genomic status, the cancer tumor is classified as\nMicrosatellite Instable (MSI) and Microsatellite Stable (MSS). Immunotherapy is\nused to diagnose MSI, whereas radiation and chemotherapy are used for MSS.\nTherefore, it is significant to classify a gastro-intestinal (GI) cancer tumor\ninto MSI vs. MSS to provide appropriate treatment. The existing literature\nshowed that deep learning could directly predict the class of GI cancer tumors\nfrom histological images. However, deep learning (DL) models are susceptible to\nvarious threats, including membership inference attacks, model extraction\nattacks, etc. These attacks render the use of DL models impractical in\nreal-world scenarios. To make the DL models useful and maintain privacy, we\nintegrate differential privacy (DP) with DL. In particular, this paper aims to\npredict the state of GI cancer while preserving the privacy of sensitive data.\nWe fine-tuned the Normalizer Free Net (NF-Net) model. We obtained an accuracy\nof 88.98\\% without DP to predict (GI) cancer status. When we fine-tuned the\nNF-Net using DP-AdamW and adaptive DP-AdamW, we got accuracies of 74.58% and\n76.48%, respectively. Moreover, we investigate the Weighted Random Sampler\n(WRS) and Class weighting (CW) to solve the data imbalance. We also evaluated\nand analyzed the DP algorithms in different settings.\n","authors":["Sai Venkatesh Chilukoti","Imran Hossen Md","Liqun Shan","Vijay Srinivas Tida","Xiali Hei"],"pdf_url":"https://arxiv.org/pdf/2502.11329v1.pdf","comment":"10 pages, 8 tables, 2 figures"},{"id":"http://arxiv.org/abs/2501.07957v2","updated":"2025-02-17T00:40:03Z","published":"2025-01-14T09:21:17Z","title":"AI Guide Dog: Egocentric Path Prediction on Smartphone","summary":"  This paper presents AI Guide Dog (AIGD), a lightweight egocentric\n(first-person) navigation system for visually impaired users, designed for\nreal-time deployment on smartphones. AIGD employs a vision-only multi-label\nclassification approach to predict directional commands, ensuring safe\nnavigation across diverse environments. We introduce a novel technique for\ngoal-based outdoor navigation by integrating GPS signals and high-level\ndirections, while also handling uncertain multi-path predictions for\ndestination-free indoor navigation. As the first navigation assistance system\nto handle both goal-oriented and exploratory navigation across indoor and\noutdoor settings, AIGD establishes a new benchmark in blind navigation. We\npresent methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems.\n","authors":["Aishwarya Jadhav","Jeffery Cao","Abhishree Shetty","Urvashi Priyam Kumar","Aditi Sharma","Ben Sukboontip","Jayant Sravan Tamarapalli","Jingyi Zhang","Anirudh Koul"],"pdf_url":"https://arxiv.org/pdf/2501.07957v2.pdf","comment":"Accepted at the AAAI 2025 Spring Symposium on Human-Compatible AI for\n  Well-being: Harnessing Potential of GenAI for AI-Powered Science"},{"id":"http://arxiv.org/abs/2403.12326v3","updated":"2025-02-17T00:34:04Z","published":"2024-03-18T23:42:04Z","title":"Hiding and Recovering Knowledge in Text-to-Image Diffusion Models via\n  Learnable Prompts","summary":"  Diffusion models have demonstrated remarkable capability in generating\nhigh-quality visual content from textual descriptions. However, since these\nmodels are trained on large-scale internet data, they inevitably learn\nundesirable concepts, such as sensitive content, copyrighted material, and\nharmful or unethical elements. While previous works focus on permanently\nremoving such concepts, this approach is often impractical, as it can degrade\nmodel performance and lead to irreversible loss of information. In this work,\nwe introduce a novel concept-hiding approach that makes unwanted concepts\ninaccessible to public users while allowing controlled recovery when needed.\nInstead of erasing knowledge from the model entirely, we incorporate a\nlearnable prompt into the cross-attention module, acting as a secure memory\nthat suppresses the generation of hidden concepts unless a secret key is\nprovided. This enables flexible access control -- ensuring that undesirable\ncontent cannot be easily generated while preserving the option to reinstate it\nunder restricted conditions. Our method introduces a new paradigm where concept\nsuppression and controlled recovery coexist, which was not feasible in prior\nworks. We validate its effectiveness on the Stable Diffusion model,\ndemonstrating that hiding concepts mitigate the risks of permanent removal\nwhile maintaining the model's overall capability.\n","authors":["Anh Bui","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2403.12326v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10973v3","updated":"2025-02-17T00:25:52Z","published":"2024-06-16T15:14:56Z","title":"ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision\n  Transformers under Domain Shifts","summary":"  Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation\n(LoRA) can effectively adapt large pre-trained foundation models to downstream\ntasks using only a small fraction (0.1%-10%) of the original trainable weights.\nAn under-explored question of PEFT is in extending the pre-training phase\nwithout supervised labels; that is, can we adapt a pre-trained foundation model\nto a new domain via efficient self-supervised pre-training on this new domain?\nIn this work, we introduce ExPLoRA, a highly effective technique to improve\ntransfer learning of pre-trained vision transformers (ViTs) under domain\nshifts. Initializing a ViT with pre-trained weights on large, natural-image\ndatasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised\npre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks\nand tuning all other layers with LoRA. We then fine-tune the resulting model\nonly with LoRA on this new domain for supervised learning. Our experiments\ndemonstrate state-of-the-art results on satellite imagery, even outperforming\nfully pre-training and fine-tuning ViTs. Using the DinoV2 training objective,\nwe demonstrate up to 8% improvement in linear probing top-1 accuracy on\ndownstream tasks while using <10% of the number of parameters that are used in\nprior fully-tuned state-of-the art approaches. Our ablation studies confirm the\nefficacy of our approach over other baselines, including PEFT and unfreezing\nmore ViT blocks. Code is available on the project website:\nhttps://samar-khanna.github.io/ExPLoRA/\n","authors":["Samar Khanna","Medhanie Irgau","David B. Lobell","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2406.10973v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.12145v1","updated":"2025-02-17T18:56:20Z","published":"2025-02-17T18:56:20Z","title":"Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented\n  Generation with Flexible User Control","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications.\n","authors":["Jinyan Su","Jennifer Healey","Preslav Nakov","Claire Cardie"],"pdf_url":"https://arxiv.org/pdf/2502.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12137v1","updated":"2025-02-17T18:53:42Z","published":"2025-02-17T18:53:42Z","title":"REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to\n  Enhance Wikipedia Tail Biographies through Personal Narratives","summary":"  Wikipedia is an invaluable resource for factual information about a wide\nrange of entities. However, the quality of articles on less-known entities\noften lags behind that of the well-known ones. This study proposes a novel\napproach to enhancing Wikipedia's B and C category biography articles by\nleveraging personal narratives such as autobiographies and biographies. By\nutilizing a multi-staged retrieval-augmented generation technique -- REVerSum\n-- we aim to enrich the informational content of these lesser-known articles.\nOur study reveals that personal narratives can significantly improve the\nquality of Wikipedia articles, providing a rich source of reliable information\nthat has been underutilized in previous studies. Based on crowd-based\nevaluation, REVerSum generated content outperforms the best performing baseline\nby 17% in terms of integrability to the original Wikipedia article and 28.5\\%\nin terms of informativeness. Code and Data are available at:\nhttps://github.com/sayantan11995/wikipedia_enrichment\n","authors":["Sayantan Adak","Pauras Mangesh Meher","Paramita Das","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.12137v1.pdf","comment":"Accepted at COLING2025 Industry Track"},{"id":"http://arxiv.org/abs/2410.21745v4","updated":"2025-02-17T16:26:20Z","published":"2024-10-29T05:18:34Z","title":"RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment","summary":"  Graph clustering is an essential aspect of network analysis that involves\ngrouping nodes into separate clusters. Recent developments in deep learning\nhave resulted in graph clustering, which has proven effective in many\napplications. Nonetheless, these methods often encounter difficulties when\ndealing with real-world graphs, particularly in the presence of noisy edges.\nAdditionally, many denoising graph clustering methods tend to suffer from lower\nperformance, training instability, and challenges in scaling to large datasets\ncompared to non-denoised models. To tackle these issues, we introduce a new\nframework called the Robust Deep Graph Clustering Framework via Dual Soft\nAssignment (RDSA). RDSA consists of three key components: (i) a node embedding\nmodule that effectively integrates the graph's topological features and node\nattributes; (ii) a structure-based soft assignment module that improves graph\nmodularity by utilizing an affinity matrix for node assignments; and (iii) a\nnode-based soft assignment module that identifies community landmarks and\nrefines node assignments to enhance the model's robustness. We assess RDSA on\nvarious real-world datasets, demonstrating its superior performance relative to\nexisting state-of-the-art methods. Our findings indicate that RDSA provides\nrobust clustering across different graph types, excelling in clustering\neffectiveness and robustness, including adaptability to noise, stability, and\nscalability.\n","authors":["Yang Xiang","Li Fan","Tulika Saha","Xiaoying Pang","Yushan Pan","Haiyang Zhang","Chengtao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.21745v4.pdf","comment":"Accepted by DASFAA 2025; Complete version"},{"id":"http://arxiv.org/abs/2502.11921v1","updated":"2025-02-17T15:33:28Z","published":"2025-02-17T15:33:28Z","title":"Joint Evaluation of Fairness and Relevance in Recommender Systems with\n  Pareto Frontier","summary":"  Fairness and relevance are two important aspects of recommender systems\n(RSs). Typically, they are evaluated either (i) separately by individual\nmeasures of fairness and relevance, or (ii) jointly using a single measure that\naccounts for fairness with respect to relevance. However, approach (i) often\ndoes not provide a reliable joint estimate of the goodness of the models, as it\nhas two different best models: one for fairness and another for relevance.\nApproach (ii) is also problematic because these measures tend to be ad-hoc and\ndo not relate well to traditional relevance measures, like NDCG. Motivated by\nthis, we present a new approach for jointly evaluating fairness and relevance\nin RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction\ndata, we compute their Pareto frontier for a pair of existing relevance and\nfairness measures, and then use the distance from the frontier as a measure of\nthe jointly achievable fairness and relevance. Our approach is modular and\nintuitive as it can be computed with existing measures. Experiments with 4 RS\nmodels, 3 re-ranking strategies, and 6 datasets show that existing metrics have\ninconsistent associations with our Pareto-optimal solution, making DPFR a more\nrobust and theoretically well-founded joint measure for assessing fairness and\nrelevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation\n","authors":["Theresia Veronika Rampisela","Tuukka Ruotsalo","Maria Maistro","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2502.11921v1.pdf","comment":"Accepted to TheWebConf/WWW 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.11883v1","updated":"2025-02-17T15:11:09Z","published":"2025-02-17T15:11:09Z","title":"FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information\n  Retrieval Algorithms","summary":"  In modern information retrieval (IR). achieving more than just accuracy is\nessential to sustaining a healthy ecosystem, especially when addressing\nfairness and diversity considerations. To meet these needs, various datasets,\nalgorithms, and evaluation frameworks have been introduced. However, these\nalgorithms are often tested across diverse metrics, datasets, and experimental\nsetups, leading to inconsistencies and difficulties in direct comparisons. This\nhighlights the need for a comprehensive IR toolkit that enables standardized\nevaluation of fairness- and diversity-aware algorithms across different IR\ntasks. To address this challenge, we present FairDiverse, an open-source and\nstandardized toolkit. FairDiverse offers a framework for integrating fair and\ndiverse methods, including pre-processing, in-processing, and post-processing\ntechniques, at different stages of the IR pipeline. The toolkit supports the\nevaluation of 28 fairness and diversity algorithms across 16 base models,\ncovering two core IR tasks (search and recommendation) thereby establishing a\ncomprehensive benchmark. Moreover, FairDiverse is highly extensible, providing\nmultiple APIs that empower IR researchers to swiftly develop and evaluate their\nown fairness and diversity aware models, while ensuring fair comparisons with\nexisting baselines. The project is open-sourced and available on\nhttps://github.com/XuChen0427/FairDiverse.\n","authors":["Chen Xu","Zhirui Deng","Clara Rus","Xiaopeng Ye","Yuanna Liu","Jun Xu","Zhicheng Dou","Ji-Rong Wen","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2502.11883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11747v1","updated":"2025-02-17T12:40:35Z","published":"2025-02-17T12:40:35Z","title":"Multi-Modal Retrieval Augmentation for Open-Ended and\n  Knowledge-Intensive Video Question Answering","summary":"  While current video question answering systems perform well on some tasks\nrequiring only direct visual understanding, they struggle with questions\ndemanding knowledge beyond what is immediately observable in the video content.\nWe refer to this challenging scenario as knowledge-intensive video question\nanswering (KI-VideoQA), where models must retrieve and integrate external\ninformation with visual understanding to generate accurate responses. This work\npresents the first attempt to (1) study multi-modal retrieval-augmented\ngeneration for KI-VideoQA, and (2) go beyond multi-choice questions by studying\nopen-ended questions in this task. Through an extensive empirical study of\nstate-of-the-art retrieval and vision language models in both zero-shot and\nfine-tuned settings, we explore how different retrieval augmentation strategies\ncan enhance knowledge integration in KI-VideoQA. We analyze three key aspects:\n(1) model's effectiveness across different information sources and modalities,\n(2) the impact of heterogeneous multi-modal context integration, and (3)\nmodel's effectiveness across different query formulation and retrieval result\nconsumption. Our results suggest that while retrieval augmentation generally\nimproves performance, its effectiveness varies significantly based on modality\nchoice and retrieval strategy. Additionally, we find that successful knowledge\nintegration often requires careful consideration of query formulation and\noptimal retrieval depth. Our exploration advances state-of-the-art accuracy for\nmultiple choice questions by over 17.5% on the KnowIT VQA dataset.\n","authors":["Md Zarif Ul Alam","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2502.11747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11721v1","updated":"2025-02-17T12:08:18Z","published":"2025-02-17T12:08:18Z","title":"Enhancing Recommendation Explanations through User-Centric Refinement","summary":"  Generating natural language explanations for recommendations has become\nincreasingly important in recommender systems. Traditional approaches typically\ntreat user reviews as ground truth for explanations and focus on improving\nreview prediction accuracy by designing various model architectures. However,\ndue to limitations in data scale and model capability, these explanations often\nfail to meet key user-centric aspects such as factuality, personalization, and\nsentiment coherence, significantly reducing their overall helpfulness to users.\nIn this paper, we propose a novel paradigm that refines initial explanations\ngenerated by existing explainable recommender models during the inference stage\nto enhance their quality in multiple aspects. Specifically, we introduce a\nmulti-agent collaborative refinement framework based on large language models.\nTo ensure alignment between the refinement process and user demands, we employ\na plan-then-refine pattern to perform targeted modifications. To enable\ncontinuous improvements, we design a hierarchical reflection mechanism that\nprovides feedback on the refinement process from both strategic and content\nperspectives. Extensive experiments on three datasets demonstrate the\neffectiveness of our framework.\n","authors":["Jingsen Zhang","Zihang Tian","Xueyang Feng","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v3","updated":"2025-02-17T11:34:41Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2502.05558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02061v2","updated":"2025-02-17T11:22:58Z","published":"2025-02-04T07:17:54Z","title":"Reason4Rec: Large Language Models for Recommendation with Deliberative\n  User Preference Alignment","summary":"  While recent advancements in aligning Large Language Models (LLMs) with\nrecommendation tasks have shown great potential and promising performance\noverall, these aligned recommendation LLMs still face challenges in complex\nscenarios. This is primarily due to the current alignment approach focusing on\noptimizing LLMs to generate user feedback directly, without incorporating\ndeliberation. To overcome this limitation and develop more reliable LLMs for\nrecommendations, we propose a new Deliberative Recommendation task, which\nincorporates explicit reasoning about user preferences as an additional\nalignment goal. We then introduce the Reasoning-powered Recommender framework\nfor deliberative user preference alignment, designed to enhance reasoning\ncapabilities by utilizing verbalized user feedback in a step-wise manner to\ntackle this task. The framework employs collaborative step-wise experts and\ntailored training strategies for each expert. Experimental results across three\nreal-world datasets demonstrate the rationality of the deliberative task\nformulation and the superior performance of the proposed framework in improving\nboth prediction accuracy and reasoning quality.\n","authors":["Yi Fang","Wenjie Wang","Yang Zhang","Fengbin Zhu","Qifan Wang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2502.02061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11610v1","updated":"2025-02-17T09:54:46Z","published":"2025-02-17T09:54:46Z","title":"Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an\n  LLM-Assisted Benchmark","summary":"  In quantitative SciSci (science of science) studies, accurately identifying\nindividual scholars is paramount for scientific data analysis. However, the\nvariability in how names are represented-due to commonality, abbreviations, and\ndifferent spelling conventions-complicates this task. While identifier systems\nlike ORCID are being developed, many scholars remain unregistered, and numerous\npublications are not included. Scholarly databases such as Clarivate and\nOpenAlex have introduced their own ID systems as preliminary name\ndisambiguation solutions. This study evaluates the effectiveness of these\nsystems across different groups to determine their suitability for various\napplication scenarios. We sampled authors from the top quartile (Q1) of Web of\nScience (WOS) journals based on country, discipline, and number of\ncorresponding author papers. For each group, we selected 100 scholars and\nmeticulously annotated all their papers using a Search-enhanced Large Language\nModel method. Using these annotations, we identified the corresponding IDs in\nOpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS\njournals, and calculated precision and recall by comparing against the\nannotated dataset.\n","authors":["Renyu Zhao","Yunxin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11624v4","updated":"2025-02-17T09:26:15Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Effective recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interactive relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant challenges: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations within behavior patterns on the target relation in recommender system\nscenarios. In this work, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interactive relations, and includes a relation chain representation\nlearner and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06% and 12.15% on average across all datasets in terms of\nRecall@10 and NDCG@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Yanwei Yu","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11571v1","updated":"2025-02-17T09:05:21Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v1.pdf","comment":"to appear in ACL 2025"},{"id":"http://arxiv.org/abs/2410.23166v2","updated":"2025-02-17T08:59:45Z","published":"2024-10-30T16:18:22Z","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","summary":"  The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.\n","authors":["Wenxiao Wang","Lihui Gu","Liye Zhang","Yunxiang Luo","Yi Dai","Chen Shen","Liang Xie","Binbin Lin","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.23166v2.pdf","comment":"20 pages, 5 figures, 12 tables. The code has been availabel:\n  https://github.com/cheerss/SciPIP"},{"id":"http://arxiv.org/abs/2306.13887v3","updated":"2025-02-17T08:44:49Z","published":"2023-06-24T07:27:43Z","title":"Cross-domain Recommender Systems via Multimodal Domain Adaptation","summary":"  Collaborative Filtering (CF) has emerged as one of the most prominent\nimplementation strategies for building recommender systems. The key idea is to\nexploit the usage patterns of individuals to generate personalized\nrecommendations. CF techniques, especially for newly launched platforms, often\nface a critical issue known as the data sparsity problem, which greatly limits\ntheir performance. Cross-domain CF alleviates the problem of data sparsity by\nfinding a common set of entities (users or items) across the domains, which\nthen act as a conduit for knowledge transfer. Nevertheless, most real-world\ndatasets are collected from different domains, so they often lack information\nabout anchor points or reference information for entity alignment. This paper\nintroduces a domain adaptation technique to align the embeddings of entities\nacross domains. Our approach first exploits the available textual and visual\ninformation to independently learn a multi-view latent representation for each\nentity in the auxiliary and target domains. The different representations of\nthe entity are then fused to generate the corresponding unified representation.\nA domain classifier is then trained to learn the embedding for the domain\nalignment by fixing the unified features as the anchor points. Experiments on\n\\AS{four} publicly available benchmark datasets indicate the effectiveness of\nour proposed approach.\n","authors":["Adamya Shyam","Ramya Kamani","Venkateswara Rao Kagita","Vikas Kumar"],"pdf_url":"https://arxiv.org/pdf/2306.13887v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2501.07365v3","updated":"2025-02-17T08:40:10Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commerce search interactions and is a key factor for customers at\nproduct explorations. However, its impact on semantic retrieval has not been\nwell studied yet. In this research, we build a multimodal representation for\nproduct items in e-commerce search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v3.pdf","comment":"Accepted at EReL@MIR WWW 2025"},{"id":"http://arxiv.org/abs/2502.09827v2","updated":"2025-02-17T08:34:43Z","published":"2025-02-13T23:53:40Z","title":"Data and Decision Traceability for SDA TAP Lab's Prototype Battle\n  Management System","summary":"  Space Protocol is applying the principles derived from MITRE and NIST's\nSupply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a\ncomplex multi party system to achieve introspection, auditing, and replay of\ndata and decisions that ultimately lead to a end decision. The core goal of\ndecision traceability is to ensure transparency, accountability, and integrity\nwithin the WA system. This is accomplished by providing a clear, auditable path\nfrom the system's inputs all the way to the final decision. This traceability\nenables the system to track the various algorithms and data flows that have\ninfluenced a particular outcome.\n","authors":["Latha Pratti","Samya Bagchi","Yasir Latif"],"pdf_url":"https://arxiv.org/pdf/2502.09827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11490v1","updated":"2025-02-17T06:49:34Z","published":"2025-02-17T06:49:34Z","title":"GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale\n  Recommendations","summary":"  Web recommendations provide personalized items from massive catalogs for\nusers, which rely heavily on retrieval stages to trade off the effectiveness\nand efficiency of selecting a small relevant set from billion-scale candidates\nin online digital platforms. As one of the largest Chinese search engine and\nnews feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based\nApproximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance\nestimation and efficient search for relevant items. However, current retrieval\nat Baidu fails in comprehensive user-item relational understanding due to\ndissected interaction modeling, and performs inefficiently in large-scale\ngraph-based ANNS because of suboptimal traversal navigation and the GPU\ncomputational bottleneck under high concurrency. To this end, we propose a\nGPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to\nachieve effective yet efficient retrieval in web-scale recommendations. First,\nwe propose a multi-relational user-item relevance metric learning method that\nunifies diverse user behaviors through multi-objective optimization and employs\na self-covariant loss to enhance pathfinding performance. Second, we develop a\nhierarchical parallel graph-based ANNS to boost graph retrieval throughput,\nwhich conducts breadth-depth-balanced searches on a large-scale item graph and\ncost-effectively handles irregular neural computation via adaptive aggregation\non GPUs. In addition, we integrate system optimization strategies in the\ndeployment of GMP-GR in Baidu. Extensive experiments demonstrate the\nsuperiority of GMP-GR in retrieval accuracy and efficiency. Deployed across\nmore than twenty applications at Baidu, GMP-GR serves hundreds of millions of\nusers with a throughput exceeding one hundred million requests per second.\n","authors":["Zhuoning Guo","Guangxing Chen","Qian Gao","Xiaochao Liao","Jianjia Zheng","Lu Shen","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11471v1","updated":"2025-02-17T06:02:59Z","published":"2025-02-17T06:02:59Z","title":"GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.\n","authors":["Kangyang Luo","Yuzhuo Bai","Cheng Gao","Shuzheng Si","Yingli Shen","Zhu Liu","Zhitong Wang","Cunliang Kong","Wenhao Li","Yufei Huang","Ye Tian","Xuantang Xiong","Lei Han","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09077v2","updated":"2025-02-17T05:32:59Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document Question Answering with\n  LLM-Enhanced Knowledge Graph Reasoning","summary":"  Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.\n","authors":["Zukang Yang","Zixuan Zhu","Xuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11442v1","updated":"2025-02-17T04:58:14Z","published":"2025-02-17T04:58:14Z","title":"Multi-Turn Multi-Modal Question Clarification for Enhanced\n  Conversational Understanding","summary":"  Conversational query clarification enables users to refine their search\nqueries through interactive dialogue, improving search effectiveness.\nTraditional approaches rely on text-based clarifying questions, which often\nfail to capture complex user preferences, particularly those involving visual\nattributes. While recent work has explored single-turn multi-modal\nclarification with images alongside text, such methods do not fully support the\nprogressive nature of user intent refinement over multiple turns. Motivated by\nthis, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,\nwhich combines text and visual modalities to refine user queries in a\nmulti-turn conversation. To facilitate this task, we create a large-scale\ndataset named ClariMM comprising over 13k multi-turn interactions and 33k\nquestion-answer pairs containing multi-modal clarifying questions. We propose\nMario, a retrieval framework that employs a two-phase ranking strategy: initial\nretrieval with BM25, followed by a multi-modal generative re-ranking model that\nintegrates textual and visual information from conversational history. Our\nexperiments show that multi-turn multi-modal clarification outperforms\nuni-modal and single-turn approaches, improving MRR by 12.88%. The gains are\nmost significant in longer interactions, demonstrating the value of progressive\nrefinement for complex queries.\n","authors":["Kimia Ramezan","Alireza Amiri Bavandpour","Yifei Yuan","Clemencia Siro","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.11442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11414v1","updated":"2025-02-17T03:55:51Z","published":"2025-02-17T03:55:51Z","title":"Unbiased Learning to Rank with Query-Level Click Propensity Estimation:\n  Beyond Pointwise Observation and Relevance","summary":"  Most existing unbiased learning-to-rank (ULTR) approaches are based on the\nuser examination hypothesis, which assumes that users will click a result only\nif it is both relevant and observed (typically modeled by position). However,\nin real-world scenarios, users often click only one or two results after\nexamining multiple relevant options, due to limited patience or because their\ninformation needs have already been satisfied. Motivated by this, we propose a\nquery-level click propensity model to capture the probability that users will\nclick on different result lists, allowing for non-zero probabilities that users\nmay not click on an observed relevant result. We hypothesize that this\npropensity increases when more potentially relevant results are present, and\nrefer to this user behavior as relevance saturation bias. Our method introduces\na Dual Inverse Propensity Weighting (DualIPW) mechanism -- combining\nquery-level and position-level IPW -- to address both relevance saturation and\nposition bias. Through theoretical derivation, we prove that DualIPW can learn\nan unbiased ranking model. Experiments on the real-world Baidu-ULTR dataset\ndemonstrate that our approach significantly outperforms state-of-the-art ULTR\nbaselines. The code and dataset information can be found at\nhttps://github.com/Trustworthy-Information-Access/DualIPW.\n","authors":["Lulu Yu","Keping Bi","Jiafeng Guo","Shihao Liu","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2502.11414v1.pdf","comment":"5 pages, 3 figures, accepted by The ACM Web Conference (WWW) 2025\n  Short Paper Track"},{"id":"http://arxiv.org/abs/2310.14483v4","updated":"2025-02-17T03:10:44Z","published":"2023-10-23T01:29:18Z","title":"Chain-of-Factors Paper-Reviewer Matching","summary":"  With the rapid increase in paper submissions to academic conferences, the\nneed for automated and accurate paper-reviewer matching is more critical than\never. Previous efforts in this area have considered various factors to assess\nthe relevance of a reviewer's expertise to a paper, such as the semantic\nsimilarity, shared topics, and citation connections between the paper and the\nreviewer's previous works. However, most of these studies focus on only one\nfactor, resulting in an incomplete evaluation of the paper-reviewer relevance.\nTo address this issue, we propose a unified model for paper-reviewer matching\nthat jointly considers semantic, topic, and citation factors. To be specific,\nduring training, we instruction-tune a contextualized language model shared\nacross all factors to capture their commonalities and characteristics; during\ninference, we chain the three factors to enable step-by-step, coarse-to-fine\nsearch for qualified reviewers given a submission. Experiments on four datasets\n(one of which is newly contributed by us) spanning various fields such as\nmachine learning, computer vision, information retrieval, and data mining\nconsistently demonstrate the effectiveness of our proposed Chain-of-Factors\nmodel in comparison with state-of-the-art paper-reviewer matching methods and\nscientific pre-trained language models.\n","authors":["Yu Zhang","Yanzhen Shen","SeongKu Kang","Xiusi Chen","Bowen Jin","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2310.14483v4.pdf","comment":"10 pages; Accepted to WWW 2025 (Code:\n  https://github.com/yuzhimanhua/CoF)"},{"id":"http://arxiv.org/abs/2410.10293v3","updated":"2025-02-17T02:51:14Z","published":"2024-10-14T08:47:21Z","title":"FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG","summary":"  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate the generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the widely used\nretrieval paradigm remains flat. It treats retrieval procedures as a one-off\ndeal with constant granularity. Despite effectiveness, we argue that they\nsuffer from two limitations: (1) flat retrieval exerts a significant burden on\none retriever; (2) constant granularity limits the ceiling of retrieval\nperformance. In this work, we propose a progressive retrieval paradigm with\ncoarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance\neffectiveness and efficiency. Specifically, FunnelRAG establishes a progressive\nretrieval pipeline by collaborating coarse-to-fine granularity, large-to-small\nquantity, and low-to-high capacity, which can relieve the burden on one\nretriever and also promote the ceiling of retrieval performance. Extensive\nexperiments manifest that FunnelRAG achieves comparable retrieval performance\nwhile the time overhead is reduced by nearly 40 percent.\n","authors":["Xinping Zhao","Yan Zhong","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Dongfang Li","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10293v3.pdf","comment":"18 pages, 6 figures, 13 tables. Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08346v3","updated":"2025-02-17T02:47:18Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11374v1","updated":"2025-02-17T02:41:11Z","published":"2025-02-17T02:41:11Z","title":"Leave No One Behind: Enhancing Diversity While Maintaining Accuracy in\n  Social Recommendation","summary":"  Social recommendation, a branch of algorithms that utilizes social connection\ninformation to construct recommender systems, has demonstrated its\neffectiveness in enhancing recommendation accuracy. However, apart from\naccuracy, the diversity of recommendations also plays a critical role in user\nengagement. Unfortunately, the impact of social recommendation models on\nrecommendation diversity remains largely unexplored. In this study, we\ninvestigate the dual performance of existing social recommendation algorithms\nin terms of accuracy and diversity. Our empirical findings highlight a\nconcerning trend: social recommendation models tend to decrease diversity,\ndespite their accuracy improvements. To address this issue, we propose a novel\napproach called Diversified Social Recommendation (DivSR), which leverages\nrelational knowledge distillation techniques to transfer high-diversity\nstructured knowledge from non-social recommendation models to social\nrecommendation models. DivSR is designed as a simple, model-agnostic framework\nthat integrates seamlessly with existing social recommendation architectures.\nExperimental results on three benchmark datasets demonstrate that DivSR\nsignificantly increases diversity without markedly compromising accuracy across\nvarious social recommendation backbones, achieving a better accuracy-diversity\ntrade-off. Our code and data are publicly available at:\nhttps://github.com/ll0ruc/DivSR\n","authors":["Lei Li","Xiao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11374v1.pdf","comment":"Accepted by DASFAA2025"},{"id":"http://arxiv.org/abs/2502.11371v1","updated":"2025-02-17T02:36:30Z","published":"2025-02-17T02:36:30Z","title":"RAG vs. GraphRAG: A Systematic Evaluation and Key Insights","summary":"  Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across\nvarious tasks by retrieving relevant information from external sources,\nparticularly on text-based data. For structured data, such as knowledge graphs,\nGraphRAG has been widely used to retrieve relevant information. However, recent\nstudies have revealed that structuring implicit knowledge from text into graphs\ncan benefit certain tasks, extending the application of GraphRAG from graph\ndata to general text-based data. Despite their successful extensions, most\napplications of GraphRAG for text data have been designed for specific tasks\nand datasets, lacking a systematic evaluation and comparison between RAG and\nGraphRAG on widely used text-based benchmarks. In this paper, we systematically\nevaluate RAG and GraphRAG on well-established benchmark tasks, such as Question\nAnswering and Query-based Summarization. Our results highlight the distinct\nstrengths of RAG and GraphRAG across different tasks and evaluation\nperspectives. Inspired by these observations, we investigate strategies to\nintegrate their strengths to improve downstream tasks. Additionally, we provide\nan in-depth discussion of the shortcomings of current GraphRAG approaches and\noutline directions for future research.\n","authors":["Haoyu Han","Harry Shomer","Yu Wang","Yongjia Lei","Kai Guo","Zhigang Hua","Bo Long","Hui Liu","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2502.11371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15005v5","updated":"2025-02-17T01:53:43Z","published":"2024-11-22T15:29:05Z","title":"Multi-granularity Interest Retrieval and Refinement Network for\n  Long-Term User Behavior Modeling in CTR Prediction","summary":"  Click-through Rate (CTR) prediction is crucial for online personalization\nplatforms. Recent advancements have shown that modeling rich user behaviors can\nsignificantly improve the performance of CTR prediction. Current long-term user\nbehavior modeling algorithms predominantly follow two cascading stages. The\nfirst stage retrieves subsequence related to the target item from the long-term\nbehavior sequence, while the second stage models the relationship between the\nsubsequence and the target item. Despite significant progress, these methods\nhave two critical flaws. First, the retrieval query typically includes only\ntarget item information, limiting the ability to capture the user's diverse\ninterests. Second, relational information, such as sequential and interactive\ninformation within the subsequence, is frequently overlooked. Therefore, it\nrequires to be further mined to more accurately model user interests.\n  To this end, we propose Multi-granularity Interest Retrieval and Refinement\nNetwork (MIRRN). Specifically, we first construct queries based on behaviors\nobserved at different time scales to obtain subsequences, each capturing users'\ninterest at various granularities. We then introduce an noval multi-head\nFourier transformer to efficiently learn sequential and interactive information\nwithin the subsequences, leading to more accurate modeling of user interests.\nFinally, we employ multi-head target attention to adaptively assess the impact\nof these multi-granularity interests on the target item. Extensive experiments\nhave demonstrated that MIRRN significantly outperforms state-of-the-art\nbaselines. Furthermore, an A/B test shows that MIRRN increases the average\nnumber of listening songs by 1.32% and the average time of listening songs by\n0.55% on the Huawei Music App. The implementation code is publicly available at\nhttps://github.com/USTC-StarTeam/MIRRN.\n","authors":["Xiang Xu","Hao Wang","Wei Guo","Luankang Zhang","Wanshan Yang","Runlong Yu","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15005v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2312.15490v4","updated":"2025-02-17T01:36:07Z","published":"2023-12-24T14:23:15Z","title":"Diffusion-EXR: Controllable Review Generation for Explainable\n  Recommendation via Diffusion Models","summary":"  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in\nimage and audio generation tasks. However, there exist few attempts to employ\nDDPM in the text generation, especially review generation under recommendation\nsystems. Fueled by the predicted reviews explainability that justifies\nrecommendations could assist users better understand the recommended items and\nincrease the transparency of recommendation system, we propose a Diffusion\nModel-based Review Generation towards EXplainable Recommendation named\nDiffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by\nincrementally introducing varied levels of Gaussian noise to the sequence of\nword embeddings and learns to reconstruct the original word representations in\nthe reverse process. The nature of DDPM enables our lightweight Transformer\nbackbone to perform excellently in the recommendation review generation task.\nExtensive experimental results have demonstrated that Diffusion-EXR can achieve\nstate-of-the-art review generation for recommendation on two publicly available\nbenchmark datasets.\n","authors":["Ling Li","Shaohua Li","Winda Marantika","Alex C. Kot","Huijing Zhan"],"pdf_url":"https://arxiv.org/pdf/2312.15490v4.pdf","comment":"We request to withdraw our paper from the archive due to significant\n  errors identified in the analysis and conclusions. Upon further review, we\n  realized that these errors undermine the validity of our findings. We plan to\n  conduct additional research to correct these issues and resubmit a revised\n  version in the future"},{"id":"http://arxiv.org/abs/2502.11335v1","updated":"2025-02-17T01:13:45Z","published":"2025-02-17T01:13:45Z","title":"Personalized Ranking on Cascading Behavior Graphs for Accurate\n  Multi-Behavior Recommendation","summary":"  Multi-behavior recommendation predicts items a user may purchase by analyzing\ndiverse behaviors like viewing, adding to a cart, and purchasing. Existing\nmethods fall into two categories: representation learning and graph ranking.\nRepresentation learning generates user and item embeddings to capture latent\ninteraction patterns, leveraging multi-behavior properties for better\ngeneralization. However, these methods often suffer from over-smoothing and\nbias toward frequent interactions, limiting their expressiveness. Graph ranking\nmethods, on the other hand, directly compute personalized ranking scores,\ncapturing user preferences more effectively. Despite their potential, graph\nranking approaches have been primarily explored in single-behavior settings and\nremain underutilized for multi-behavior recommendation. In this paper, we\npropose CascadingRank, a novel graph ranking method for multi-behavior\nrecommendation. It models the natural sequence of user behaviors (e.g.,\nviewing, adding to cart, and purchasing) through a cascading behavior graph. An\niterative algorithm computes ranking scores, ensuring smoothness, query\nfitting, and cascading alignment. Experiments on three real-world datasets\ndemonstrate that CascadingRank outperforms state-of-the-art methods, with up to\n9.56% and 7.16% improvements in HR@10 and NDCG@10, respectively. Furthermore,\nwe provide theoretical analysis highlighting its effectiveness, convergence,\nand scalability, showcasing the advantages of graph ranking in multi-behavior\nrecommendation.\n","authors":["Geonwoo Ko","Minseo Jeon","Jinhong Jung"],"pdf_url":"https://arxiv.org/pdf/2502.11335v1.pdf","comment":"26 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.12154v1","updated":"2025-02-17T18:59:50Z","published":"2025-02-17T18:59:50Z","title":"Diffusion Models without Classifier-free Guidance","summary":"  This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.\n","authors":["Zhicong Tang","Jianmin Bao","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2502.12154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12152v1","updated":"2025-02-17T18:59:06Z","published":"2025-02-17T18:59:06Z","title":"Learning Getting-Up Policies for Real-World Humanoid Robots","summary":"  Automatic fall recovery is a crucial prerequisite before humanoid robots can\nbe reliably deployed. Hand-designing controllers for getting up is difficult\nbecause of the varied configurations a humanoid can end up in after a fall and\nthe challenging terrains humanoid robots are expected to operate on. This paper\ndevelops a learning framework to produce controllers that enable humanoid\nrobots to get up from varying configurations on varying terrains. Unlike\nprevious successful applications of humanoid locomotion learning, the\ngetting-up task involves complex contact patterns, which necessitates\naccurately modeling the collision geometry and sparser rewards. We address\nthese challenges through a two-phase approach that follows a curriculum. The\nfirst stage focuses on discovering a good getting-up trajectory under minimal\nconstraints on smoothness or speed / torque limits. The second stage then\nrefines the discovered motions into deployable (i.e. smooth and slow) motions\nthat are robust to variations in initial configuration and terrains. We find\nthese innovations enable a real-world G1 humanoid robot to get up from two main\nsituations that we considered: a) lying face up and b) lying face down, both\ntested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass\nand snowfield). To the best of our knowledge, this is the first successful\ndemonstration of learned getting-up policies for human-sized humanoid robots in\nthe real world. Project page: https://humanoid-getup.github.io/\n","authors":["Xialin He","Runpei Dong","Zixuan Chen","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.12152v1.pdf","comment":"Project page: https://humanoid-getup.github.io/"},{"id":"http://arxiv.org/abs/2502.12147v1","updated":"2025-02-17T18:57:32Z","published":"2025-02-17T18:57:32Z","title":"Learning Smooth and Expressive Interatomic Potentials for Physical\n  Property Prediction","summary":"  Machine learning interatomic potentials (MLIPs) have become increasingly\neffective at approximating quantum mechanical calculations at a fraction of the\ncomputational cost. However, lower errors on held out test sets do not always\ntranslate to improved results on downstream physical property prediction tasks.\nIn this paper, we propose testing MLIPs on their practical ability to conserve\nenergy during molecular dynamic simulations. If passed, improved correlations\nare found between test errors and their performance on physical property\nprediction tasks. We identify choices which may lead to models failing this\ntest, and use these observations to improve upon highly-expressive models. The\nresulting model, eSEN, provides state-of-the-art results on a range of physical\nproperty prediction tasks, including materials stability prediction, thermal\nconductivity prediction, and phonon calculations.\n","authors":["Xiang Fu","Brandon M. Wood","Luis Barroso-Luque","Daniel S. Levine","Meng Gao","Misko Dzamba","C. Lawrence Zitnick"],"pdf_url":"https://arxiv.org/pdf/2502.12147v1.pdf","comment":"19 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.13697v2","updated":"2025-02-17T18:53:15Z","published":"2024-12-18T10:41:44Z","title":"Splitting criteria for ordinal decision trees: an experimental study","summary":"  Ordinal Classification (OC) is a machine learning field that addresses\nclassification tasks where the labels exhibit a natural order. Unlike nominal\nclassification, which treats all classes as equally distinct, OC takes the\nordinal relationship into account, producing more accurate and relevant\nresults. This is particularly critical in applications where the magnitude of\nclassification errors has implications. Despite this, OC problems are often\ntackled using nominal methods, leading to suboptimal solutions. Although\ndecision trees are one of the most popular classification approaches, ordinal\ntree-based approaches have received less attention when compared to other\nclassifiers. This work conducts an experimental study of tree-based\nmethodologies specifically designed to capture ordinal relationships. A\ncomprehensive survey of ordinal splitting criteria is provided, standardising\nthe notations used in the literature for clarity. Three ordinal splitting\ncriteria, Ordinal Gini (OGini), Weighted Information Gain (WIG), and Ranking\nImpurity (RI), are compared to the nominal counterparts of the first two (Gini\nand information gain), by incorporating them into a decision tree classifier.\nAn extensive repository considering 45 publicly available OC datasets is\npresented, supporting the first experimental comparison of ordinal and nominal\nsplitting criteria using well-known OC evaluation metrics. Statistical analysis\nof the results highlights OGini as the most effective ordinal splitting\ncriterion to date. Source code, datasets, and results are made available to the\nresearch community.\n","authors":["Rafael Ayll√≥n-Gavil√°n","Francisco Jos√© Mart√≠nez-Estudillo","David Guijo-Rubio","C√©sar Herv√°s-Mart√≠nez","Pedro Antonio Guti√©rrez"],"pdf_url":"https://arxiv.org/pdf/2412.13697v2.pdf","comment":"34 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.12128v1","updated":"2025-02-17T18:49:13Z","published":"2025-02-17T18:49:13Z","title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities","summary":"  Generative models are spearheading recent progress in deep learning, showing\nstrong promise for trajectory sampling in dynamical systems as well. However,\nwhile latent space modeling paradigms have transformed image and video\ngeneration, similar approaches are more difficult for most dynamical systems.\nSuch systems -- from chemical molecule structures to collective human behavior\n-- are described by interactions of entities, making them inherently linked to\nconnectivity patterns and the traceability of entities over time. Our approach,\nLaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked\nEntities), combines the advantages of graph neural networks, i.e., the\ntraceability of entities across time-steps, with the efficiency and scalability\nof recent advances in image and video generation, where pre-trained encoder and\ndecoder are frozen to enable generative modeling in the latent space. The core\nidea of LaM-SLidE is to introduce identifier representations (IDs) to allow for\nretrieval of entity properties, e.g., entity coordinates, from latent system\nrepresentations and thus enables traceability. Experimentally, across different\ndomains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,\nand generalizability. (Code is available at\nhttps://github.com/ml-jku/LaM-SLidE)\n","authors":["Florian Sestak","Artur Toshev","Andreas F√ºrst","G√ºnter Klambauer","Andreas Mayr","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.12128v1.pdf","comment":"Project page: https://ml-jku.github.io/LaM-SLidE/"},{"id":"http://arxiv.org/abs/2502.09606v2","updated":"2025-02-17T18:48:26Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency due to\nLLMs' disfavor.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20582v2","updated":"2025-02-17T18:48:09Z","published":"2024-05-31T02:28:41Z","title":"The Point of View of a Sentiment: Towards Clinician Bias Detection in\n  Psychiatric Notes","summary":"  Negative patient descriptions and stigmatizing language can contribute to\ngenerating healthcare disparities in two ways: (1) read by patients, they can\nharm their trust and engagement with the medical center; (2) read by\nphysicians, they may negatively influence their perspective of a future\npatient. In psychiatry, the patient-clinician therapeutic alliance is a major\ndeterminant of clinical outcomes. Therefore, language usage in psychiatric\nclinical notes may not only create healthcare disparities, but also perpetuate\nthem. Recent advances in NLP systems have facilitated the efforts to detect\ndiscriminatory language in healthcare. However, such attempts have only focused\non the perspectives of the medical center and its physicians. Considering both\nphysicians and non-physicians' point of view is a more translatable approach to\nidentifying potentially harmful language in clinical notes. By leveraging\npre-trained and large language models (PLMs and LLMs), this work aims to\ncharacterize potentially harmful language usage in psychiatric notes by\nidentifying the sentiment expressed in sentences describing patients based on\nthe reader's point of view. Extracting 39 sentences from the Mount Sinai Health\nSystem containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa,\nGatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and\nfew-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to\nclassify the sentiment of the sentences according to the physician or\nnon-physician point of view. Results showed that GPT-3.5 aligned best to\nphysician point of view and Mistral aligned best to non-physician point of\nview. These results underline the importance of recognizing the reader's point\nof view, not only for improving the note writing process, but also for the\nquantification, identification, and reduction of bias in computational systems\nfor downstream analyses.\n","authors":["Alissa A. Valentine","Lauren A. Lepow","Lili Chan","Alexander W. Charney","Isotta Landi"],"pdf_url":"https://arxiv.org/pdf/2405.20582v2.pdf","comment":"Oral presentation at NAACL 2024 Queer in AI Workshop"},{"id":"http://arxiv.org/abs/2502.12125v1","updated":"2025-02-17T18:47:01Z","published":"2025-02-17T18:47:01Z","title":"Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the\n  Lens of Class Hierarchy","summary":"  We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.\n","authors":["Roman Malashin","Valeria Yachnaya","Alexander Mullin"],"pdf_url":"https://arxiv.org/pdf/2502.12125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12123v1","updated":"2025-02-17T18:46:32Z","published":"2025-02-17T18:46:32Z","title":"On the Query Complexity of Verifier-Assisted Language Generation","summary":"  Recently, a plethora of works have proposed inference-time algorithms (e.g.\nbest-of-n), which incorporate verifiers to assist the generation process. Their\nquality-efficiency trade-offs have been empirically benchmarked on a variety of\nconstrained generation tasks, but the algorithmic design landscape is still\nlargely poorly understood. In this paper, we develop a mathematical framework\nfor reasoning about constrained generation using a pre-trained language model\ngenerator oracle and a process verifier--which can decide whether a prefix can\nbe extended to a string which satisfies the constraints of choice. We show that\neven in very simple settings, access to a verifier can render an intractable\nproblem (information-theoretically or computationally) to a tractable one. In\nfact, we show even simple algorithms, like tokenwise rejection sampling, can\nenjoy significant benefits from access to a verifier. Empirically, we show that\na natural modification of tokenwise rejection sampling, in which the sampler is\nallowed to \"backtrack\" (i.e., erase the final few generated tokens) has robust\nand substantive benefits over natural baselines (e.g. (blockwise) rejection\nsampling, nucleus sampling)--both in terms of computational efficiency,\naccuracy and diversity.\n","authors":["Edoardo Botta","Yuchen Li","Aashay Mehta","Jordan T. Ash","Cyril Zhang","Andrej Risteski"],"pdf_url":"https://arxiv.org/pdf/2502.12123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12122v1","updated":"2025-02-17T18:46:29Z","published":"2025-02-17T18:46:29Z","title":"Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty\n  Quantification for LoRA","summary":"  Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large\nlanguage models by decomposing weight updates into low-rank matrices,\nsignificantly reducing storage and computational overhead. While effective,\nstandard LoRA lacks mechanisms for uncertainty quantification, leading to\noverconfident and poorly calibrated models. Bayesian variants of LoRA address\nthis limitation, but at the cost of a significantly increased number of\ntrainable parameters, partially offsetting the original efficiency gains.\nAdditionally, these models are harder to train and may suffer from unstable\nconvergence.\n  In this work, we propose a novel parameter-efficient Bayesian LoRA,\ndemonstrating that effective uncertainty quantification can be achieved in very\nlow-dimensional parameter spaces. The proposed method achieves strong\nperformance with improved calibration and generalization while maintaining\ncomputational efficiency. Our empirical findings show that, with the\nappropriate projection of the weight space: (1) uncertainty can be effectively\nmodeled in a low-dimensional space, and (2) weight covariances exhibit low\nranks.\n","authors":["Patryk Marsza≈Çek","Klaudia Ba≈Çazy","Jacek Tabor","Tomasz Ku≈õmierczyk"],"pdf_url":"https://arxiv.org/pdf/2502.12122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12120v1","updated":"2025-02-17T18:45:25Z","published":"2025-02-17T18:45:25Z","title":"LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws","summary":"  Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.\n","authors":["Prasanna Mayilvahanan","Thadd√§us Wiedemer","Sayak Mallick","Matthias Bethge","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2502.12120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12118v1","updated":"2025-02-17T18:43:24Z","published":"2025-02-17T18:43:24Z","title":"Scaling Test-Time Compute Without Verification or RL is Suboptimal","summary":"  Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.\n","authors":["Amrith Setlur","Nived Rajaraman","Sergey Levine","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.12118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09782v2","updated":"2025-02-17T18:42:31Z","published":"2025-02-13T21:33:57Z","title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers\n  and Large Language Models","summary":"  The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios.\n","authors":["Jin Hyun Park","Seyyed Ali Ayati","Yichen Cai"],"pdf_url":"https://arxiv.org/pdf/2502.09782v2.pdf","comment":"We will reflect comments from the reviewers and re-submit"},{"id":"http://arxiv.org/abs/2502.12115v1","updated":"2025-02-17T18:41:16Z","published":"2025-02-17T18:41:16Z","title":"SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance\n  Software Engineering?","summary":"  We introduce SWE-Lancer, a benchmark of over 1,400 freelance software\nengineering tasks from Upwork, valued at \\$1 million USD total in real-world\npayouts. SWE-Lancer encompasses both independent engineering tasks--ranging\nfrom \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks,\nwhere models choose between technical implementation proposals. Independent\ntasks are graded with end-to-end tests triple-verified by experienced software\nengineers, while managerial decisions are assessed against the choices of the\noriginal hired engineering managers. We evaluate model performance and find\nthat frontier models are still unable to solve the majority of tasks. To\nfacilitate future research, we open-source a unified Docker image and a public\nevaluation split, SWE-Lancer Diamond\n(https://github.com/openai/SWELancer-Benchmark). By mapping model performance\nto monetary value, we hope SWE-Lancer enables greater research into the\neconomic impact of AI model development.\n","authors":["Samuel Miserendino","Michele Wang","Tejal Patwardhan","Johannes Heidecke"],"pdf_url":"https://arxiv.org/pdf/2502.12115v1.pdf","comment":"9 pages, 24 pages appendix"},{"id":"http://arxiv.org/abs/2406.11785v3","updated":"2025-02-17T18:37:13Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12108v1","updated":"2025-02-17T18:29:24Z","published":"2025-02-17T18:29:24Z","title":"Using the Path of Least Resistance to Explain Deep Networks","summary":"  Integrated Gradients (IG), a widely used axiomatic path-based attribution\nmethod, assigns importance scores to input features by integrating model\ngradients along a straight path from a baseline to the input. While effective\nin some cases, we show that straight paths can lead to flawed attributions. In\nthis paper, we identify the cause of these misattributions and propose an\nalternative approach that treats the input space as a Riemannian manifold,\ncomputing attributions by integrating gradients along geodesics. We call this\nmethod Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we\nintroduce two techniques: a k-Nearest Neighbours-based approach for smaller\nmodels and a Stochastic Variational Inference-based method for larger ones.\nAdditionally, we propose a new axiom, Strong Completeness, extending the axioms\nsatisfied by IG. We show that this property is desirable for attribution\nmethods and that GIG is the only method that satisfies it. Through experiments\non both synthetic and real-world data, we demonstrate that GIG outperforms\nexisting explainability methods, including IG.\n","authors":["Sina Salek","Joseph Enguehard"],"pdf_url":"https://arxiv.org/pdf/2502.12108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12089v1","updated":"2025-02-17T18:06:33Z","published":"2025-02-17T18:06:33Z","title":"How compositional generalization and creativity improve as diffusion\n  models are trained","summary":"  Natural data is often organized as a hierarchical composition of features.\nHow many samples do generative models need to learn the composition rules, so\nas to produce a combinatorial number of novel data? What signal in the data is\nexploited to learn? We investigate these questions both theoretically and\nempirically. Theoretically, we consider diffusion models trained on simple\nprobabilistic context-free grammars - tree-like graphical models used to\nrepresent the structure of data such as language and images. We demonstrate\nthat diffusion models learn compositional rules with the sample complexity\nrequired for clustering features with statistically similar context, a process\nsimilar to the word2vec algorithm. However, this clustering emerges\nhierarchically: higher-level, more abstract features associated with longer\ncontexts require more data to be identified. This mechanism leads to a sample\ncomplexity that scales polynomially with the said context size. As a result,\ndiffusion models trained on intermediate dataset size generate data coherent up\nto a certain scale, but that lacks global coherence. We test these predictions\nin different domains, and find remarkable agreement: both generated texts and\nimages achieve progressively larger coherence lengths as the training time or\ndataset size grows. We discuss connections between the hierarchical clustering\nmechanism we introduce here and the renormalization group in physics.\n","authors":["Alessandro Favero","Antonio Sclocchi","Francesco Cagnetta","Pascal Frossard","Matthieu Wyart"],"pdf_url":"https://arxiv.org/pdf/2502.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12088v1","updated":"2025-02-17T18:04:39Z","published":"2025-02-17T18:04:39Z","title":"Meta-Statistical Learning: Supervised Learning of Statistical Inference","summary":"  This work demonstrates that the tools and principles driving the success of\nlarge language models (LLMs) can be repurposed to tackle distribution-level\ntasks, where the goal is to predict properties of the data-generating\ndistribution rather than labels for individual datapoints. These tasks\nencompass statistical inference problems such as parameter estimation,\nhypothesis testing, or mutual information estimation. Framing these tasks\nwithin traditional machine learning pipelines is challenging, as supervision is\ntypically tied to individual datapoint. We propose meta-statistical learning, a\nframework inspired by multi-instance learning that reformulates statistical\ninference tasks as supervised learning problems. In this approach, entire\ndatasets are treated as single inputs to neural networks, which predict\ndistribution-level parameters. Transformer-based architectures, without\npositional encoding, provide a natural fit due to their permutation-invariance\nproperties. By training on large-scale synthetic datasets, meta-statistical\nmodels can leverage the scalability and optimization infrastructure of\nTransformer-based LLMs. We demonstrate the framework's versatility with\napplications in hypothesis testing and mutual information estimation, showing\nstrong performance, particularly for small datasets where traditional neural\nmethods struggle.\n","authors":["Maxime Peyrard","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2502.12088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12086v1","updated":"2025-02-17T18:01:07Z","published":"2025-02-17T18:01:07Z","title":"Unifying Explainable Anomaly Detection and Root Cause Analysis in\n  Dynamical Systems","summary":"  Dynamical systems, prevalent in various scientific and engineering domains,\nare susceptible to anomalies that can significantly impact their performance\nand reliability. This paper addresses the critical challenges of anomaly\ndetection, root cause localization, and anomaly type classification in\ndynamical systems governed by ordinary differential equations (ODEs). We define\ntwo categories of anomalies: cyber anomalies, which propagate through\ninterconnected variables, and measurement anomalies, which remain localized to\nindividual variables. To address these challenges, we propose the Interpretable\nCausality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic\nexplainable learning framework. ICODE leverages Neural ODEs for anomaly\ndetection while employing causality inference through an explanation channel to\nperform root cause analysis (RCA), elucidating why specific time periods are\nflagged as anomalous. ICODE is designed to simultaneously perform anomaly\ndetection, RCA, and anomaly type classification within a single, interpretable\nframework. Our approach is grounded in the hypothesis that anomalies alter the\nunderlying ODEs of the system, manifesting as changes in causal relationships\nbetween variables. We provide a theoretical analysis of how perturbations in\nlearned model parameters can be utilized to identify anomalies and their root\ncauses in time series data. Comprehensive experimental evaluations demonstrate\nthe efficacy of ICODE across various dynamical systems, showcasing its ability\nto accurately detect anomalies, classify their types, and pinpoint their\norigins.\n","authors":["Yue Sun","Rick S. Blum","Parv Venkitasubramaniam"],"pdf_url":"https://arxiv.org/pdf/2502.12086v1.pdf","comment":"Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS)"},{"id":"http://arxiv.org/abs/2502.12085v1","updated":"2025-02-17T17:59:56Z","published":"2025-02-17T17:59:56Z","title":"APB: Accelerating Distributed Long-Context Inference by Passing\n  Compressed Context Blocks across GPUs","summary":"  While long-context inference is crucial for advancing large language model\n(LLM) applications, its prefill speed remains a significant bottleneck. Current\napproaches, including sequence parallelism strategies and compute reduction\nthrough approximate attention mechanisms, still fall short of delivering\noptimal inference efficiency. This hinders scaling the inputs to longer\nsequences and processing long-context queries in a timely manner. To address\nthis, we introduce APB, an efficient long-context inference framework that\nleverages multi-host approximate attention to enhance prefill speed by reducing\ncompute and enhancing parallelism simultaneously. APB introduces a\ncommunication mechanism for essential key-value pairs within a sequence\nparallelism framework, enabling a faster inference speed while maintaining task\nperformance. We implement APB by incorporating a tailored FlashAttn kernel\nalongside optimized distribution strategies, supporting diverse models and\nparallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x\ncompared with FlashAttn, RingAttn, and StarAttn, respectively, without any\nobservable task performance degradation. We provide the implementation and\nexperiment code of APB in https://github.com/thunlp/APB.\n","authors":["Yuxiang Huang","Mingye Li","Xu Han","Chaojun Xiao","Weilin Zhao","Sun Ao","Hao Zhou","Jie Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.12085v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12082v1","updated":"2025-02-17T17:56:23Z","published":"2025-02-17T17:56:23Z","title":"AdaSplash: Adaptive Sparse Flash Attention","summary":"  The computational cost of softmax-based attention in transformers limits\ntheir applicability to long-context tasks. Adaptive sparsity, of which\n$\\alpha$-entmax attention is an example, offers a flexible data-dependent\nalternative, but existing implementations are inefficient and do not leverage\nthe sparsity to obtain runtime and memory gains. In this work, we propose\nAdaSplash, which combines the efficiency of GPU-optimized algorithms with the\nsparsity benefits of $\\alpha$-entmax. We first introduce a hybrid\nHalley-bisection algorithm, resulting in a 7-fold reduction in the number of\niterations needed to compute the $\\alpha$-entmax transformation. Then, we\nimplement custom Triton kernels to efficiently handle adaptive sparsity.\nExperiments with RoBERTa and ModernBERT for text classification and\nsingle-vector retrieval, along with GPT-2 for language modeling, show that our\nmethod achieves substantial improvements in runtime and memory efficiency\ncompared to existing $\\alpha$-entmax implementations. It approaches -- and in\nsome cases surpasses -- the efficiency of highly optimized softmax\nimplementations like FlashAttention-2, enabling long-context training while\nmaintaining strong task performance.\n","authors":["Nuno Gon√ßalves","Marcos Treviso","Andr√© F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2502.12082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09115v2","updated":"2025-02-17T17:50:21Z","published":"2024-12-12T09:49:16Z","title":"Vision CNNs trained to estimate spatial latents learned similar\n  ventral-stream-aligned representations","summary":"  Studies of the functional role of the primate ventral visual stream have\ntraditionally focused on object categorization, often ignoring -- despite much\nprior evidence -- its role in estimating \"spatial\" latents such as object\nposition and pose. Most leading ventral stream models are derived by optimizing\nnetworks for object categorization, which seems to imply that the ventral\nstream is also derived under such an objective. Here, we explore an alternative\nhypothesis: Might the ventral stream be optimized for estimating spatial\nlatents? And a closely related question: How different -- if at all -- are\nrepresentations learned from spatial latent estimation compared to\ncategorization? To ask these questions, we leveraged synthetic image datasets\ngenerated by a 3D graphic engine and trained convolutional neural networks\n(CNNs) to estimate different combinations of spatial and category latents. We\nfound that models trained to estimate just a few spatial latents achieve neural\nalignment scores comparable to those trained on hundreds of categories, and the\nspatial latent performance of models strongly correlates with their neural\nalignment. Spatial latent and category-trained models have very similar -- but\nnot identical -- internal representations, especially in their early and middle\nlayers. We provide evidence that this convergence is partly driven by\nnon-target latent variability in the training data, which facilitates the\nimplicit learning of representations of those non-target latents. Taken\ntogether, these results suggest that many training objectives, such as spatial\nlatents, can lead to similar models aligned neurally with the ventral stream.\nThus, one should not assume that the ventral stream is optimized for object\ncategorization only. As a field, we need to continue to sharpen our measures of\ncomparing models to brains to better understand the functional roles of the\nventral stream.\n","authors":["Yudi Xie","Weichen Huang","Esther Alter","Jeremy Schwartz","Joshua B. Tenenbaum","James J. DiCarlo"],"pdf_url":"https://arxiv.org/pdf/2412.09115v2.pdf","comment":"30 pages, 21 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12066v1","updated":"2025-02-17T17:35:42Z","published":"2025-02-17T17:35:42Z","title":"CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication\n  Facilities with Large Language Models","summary":"  Automating planning with LLMs presents transformative opportunities for\ntraditional industries, yet remains underexplored. In commercial construction,\nthe complexity of automated scheduling often requires manual intervention to\nensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to\noptimize construction schedules in complex projects like semiconductor\nfabrication. CONSTRUCTA addresses key challenges by: (1) integrating\nconstruction-specific knowledge through static RAG; (2) employing\ncontext-sampling techniques inspired by architectural expertise to provide\nrelevant input; and (3) deploying Construction DPO to align schedules with\nexpert preferences using RLHF. Experiments on proprietary data demonstrate\nperformance improvements of +42.3% in missing value prediction, +79.1% in\ndependency analysis, and +28.9% in automated planning compared to baseline\nmethods, showcasing its potential to revolutionize construction workflows and\ninspire domain-specific LLM advancements.\n","authors":["Yifan Zhang","Xue Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11427v2","updated":"2025-02-17T17:34:45Z","published":"2024-06-17T11:25:57Z","title":"DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without\n  Domain-Specific Factors","summary":"  Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.\n","authors":["Keon Lee","Dong Won Kim","Jaehyeon Kim","Seungjun Chung","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2406.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12063v1","updated":"2025-02-17T17:30:14Z","published":"2025-02-17T17:30:14Z","title":"Low-Rank Thinning","summary":"  The goal in thinning is to summarize a dataset using a small set of\nrepresentative points. Remarkably, sub-Gaussian thinning algorithms like Kernel\nHalving and Compress can match the quality of uniform subsampling while\nsubstantially reducing the number of summary points. However, existing\nguarantees cover only a restricted range of distributions and kernel-based\nquality measures and suffer from pessimistic dimension dependence. To address\nthese deficiencies, we introduce a new low-rank analysis of sub-Gaussian\nthinning that applies to any distribution and any kernel, guaranteeing\nhigh-quality compression whenever the kernel or data matrix is approximately\nlow-rank. To demonstrate the broad applicability of the techniques, we design\npractical sub-Gaussian thinning approaches that improve upon the best known\nguarantees for approximating attention in transformers, accelerating stochastic\ngradient training through reordering, and distinguishing distributions in\nnear-linear time.\n","authors":["Annabelle Michael Carrell","Albert Gong","Abhishek Shetty","Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2502.12063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12051v1","updated":"2025-02-17T17:20:41Z","published":"2025-02-17T17:20:41Z","title":"How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines","summary":"  Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.\n","authors":["Ayan Sengupta","Yash Goel","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2502.12051v1.pdf","comment":"20 pages, 8 tables, 4 figures"},{"id":"http://arxiv.org/abs/2502.12049v1","updated":"2025-02-17T17:16:42Z","published":"2025-02-17T17:16:42Z","title":"Classifying the Stoichiometry of Virus-like Particles with Interpretable\n  Machine Learning","summary":"  Virus-like particles (VLPs) are valuable for vaccine development due to their\nimmune-triggering properties. Understanding their stoichiometry, the number of\nprotein subunits to form a VLP, is critical for vaccine optimisation. However,\ncurrent experimental methods to determine stoichiometry are time-consuming and\nrequire highly purified proteins. To efficiently classify stoichiometry classes\nin proteins, we curate a new dataset and propose an interpretable, data-driven\npipeline leveraging linear machine learning models. We also explore the impact\nof feature encoding on model performance and interpretability, as well as\nmethods to identify key protein sequence features influencing classification.\nThe evaluation of our pipeline demonstrates that it can classify stoichiometry\nwhile revealing protein features that possibly influence VLP assembly. The data\nand code used in this work are publicly available at\nhttps://github.com/Shef-AIRE/StoicIML.\n","authors":["Jiayang Zhang","Xianyuan Liu","Wei Wu","Sina Tabakhi","Wenrui Fan","Shuo Zhou","Kang Lan Tee","Tuck Seng Wong","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2502.12049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12048v1","updated":"2025-02-17T17:16:41Z","published":"2025-02-17T17:16:41Z","title":"A Survey on Bridging EEG Signals and Generative AI: From Image and Text\n  to Beyond","summary":"  Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.\n","authors":["Shreya Shukla","Jose Torres","Abhijit Mishra","Jacek Gwizdka","Shounak Roychowdhury"],"pdf_url":"https://arxiv.org/pdf/2502.12048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05171v2","updated":"2025-02-17T17:14:04Z","published":"2025-02-07T18:55:02Z","title":"Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach","summary":"  We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.\n","authors":["Jonas Geiping","Sean McLeish","Neel Jain","John Kirchenbauer","Siddharth Singh","Brian R. Bartoldson","Bhavya Kailkhura","Abhinav Bhatele","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.05171v2.pdf","comment":"The model is available at\n  https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can\n  be found at https://github.com/seal-rg/recurrent-pretraining"},{"id":"http://arxiv.org/abs/2410.15777v2","updated":"2025-02-17T17:11:46Z","published":"2024-10-21T08:42:10Z","title":"Revisiting the Equivalence of Bayesian Neural Networks and Gaussian\n  Processes: On the Importance of Learning Activations","summary":"  Gaussian Processes (GPs) provide a convenient framework for specifying\nfunction-space priors, making them a natural choice for modeling uncertainty.\nIn contrast, Bayesian Neural Networks (BNNs) offer greater scalability and\nextendability but lack the advantageous properties of GPs. This motivates the\ndevelopment of BNNs capable of replicating GP-like behavior. However, existing\nsolutions are either limited to specific GP kernels or rely on heuristics.\n  We demonstrate that trainable activations are crucial for effective mapping\nof GP priors to wide BNNs. Specifically, we leverage the closed-form\n2-Wasserstein distance for efficient gradient-based optimization of\nreparameterized priors and activations. Beyond learned activations, we also\nintroduce trainable periodic activations that ensure global stationarity by\ndesign, and functional priors conditioned on GP hyperparameters to allow\nefficient model selection.\n  Empirically, our method consistently outperforms existing approaches or\nmatches performance of the heuristic methods, while offering stronger\ntheoretical foundations.\n","authors":["Marcin Sendera","Amin Sorkhei","Tomasz Ku≈õmierczyk"],"pdf_url":"https://arxiv.org/pdf/2410.15777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12033v1","updated":"2025-02-17T17:03:12Z","published":"2025-02-17T17:03:12Z","title":"The geometry of BERT","summary":"  Transformer neural networks, particularly Bidirectional Encoder\nRepresentations from Transformers (BERT), have shown remarkable performance\nacross various tasks such as classification, text summarization, and question\nanswering. However, their internal mechanisms remain mathematically obscure,\nhighlighting the need for greater explainability and interpretability. In this\ndirection, this paper investigates the internal mechanisms of BERT proposing a\nnovel perspective on the attention mechanism of BERT from a theoretical\nperspective. The analysis encompasses both local and global network behavior.\nAt the local level, the concept of directionality of subspace selection as well\nas a comprehensive study of the patterns emerging from the self-attention\nmatrix are presented. Additionally, this work explores the semantic content of\nthe information stream through data distribution analysis and global\nstatistical measures including the novel concept of cone index. A case study on\nthe classification of SARS-CoV-2 variants using RNA which resulted in a very\nhigh accuracy has been selected in order to observe these concepts in an\napplication. The insights gained from this analysis contribute to a deeper\nunderstanding of BERT's classification process, offering potential avenues for\nfuture architectural improvements in Transformer models and further analysis in\nthe training process.\n","authors":["Matteo Bonino","Giorgia Ghione","Giansalvo Cirrincione"],"pdf_url":"https://arxiv.org/pdf/2502.12033v1.pdf","comment":"28 pages, 13 figures"},{"id":"http://arxiv.org/abs/2501.18592v3","updated":"2025-02-17T16:54:39Z","published":"2025-01-30T18:59:36Z","title":"Advances in Multimodal Adaptation and Generalization: From Traditional\n  Approaches to Foundation Models","summary":"  In real-world scenarios, achieving domain adaptation and generalization poses\nsignificant challenges, as models must adapt to or generalize across unknown\ntarget distributions. Extending these capabilities to unseen multimodal\ndistributions, i.e., multimodal domain adaptation and generalization, is even\nmore challenging due to the distinct characteristics of different modalities.\nSignificant progress has been made over the years, with applications ranging\nfrom action recognition to semantic segmentation. Besides, the recent advent of\nlarge-scale pre-trained multimodal foundation models, such as CLIP, has\ninspired works leveraging these models to enhance adaptation and generalization\nperformances or adapting them to downstream tasks. This survey provides the\nfirst comprehensive review of recent advances from traditional approaches to\nfoundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal\ntest-time adaptation; (3) Multimodal domain generalization; (4) Domain\nadaptation and generalization with the help of multimodal foundation models;\nand (5) Adaptation of multimodal foundation models. For each topic, we formally\ndefine the problem and thoroughly review existing methods. Additionally, we\nanalyze relevant datasets and applications, highlighting open challenges and\npotential future research directions. We maintain an active repository that\ncontains up-to-date literature at\nhttps://github.com/donghao51/Awesome-Multimodal-Adaptation.\n","authors":["Hao Dong","Moru Liu","Kaiyang Zhou","Eleni Chatzi","Juho Kannala","Cyrill Stachniss","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2501.18592v3.pdf","comment":"Project page:\n  https://github.com/donghao51/Awesome-Multimodal-Adaptation"},{"id":"http://arxiv.org/abs/2412.15218v2","updated":"2025-02-17T16:54:24Z","published":"2024-12-03T16:53:34Z","title":"Investigating the importance of social vulnerability in opioid-related\n  mortality across the United States","summary":"  The opioid crisis remains a critical public health challenge in the United\nStates. Despite national efforts to reduce opioid prescribing rates by nearly\n45\\% between 2011 and 2021, opioid overdose deaths more than tripled during\nthis same period. This alarming trend reflects a major shift in the crisis,\nwith illegal opioids now driving the majority of overdose deaths instead of\nprescription opioids. Although much attention has been given to supply-side\nfactors fueling this transition, the underlying socioeconomic conditions that\nperpetuate and exacerbate opioid misuse remain less understood. Moreover, the\nCOVID-19 pandemic intensified the opioid crisis through widespread social\nisolation and record-high unemployment; consequently, understanding the\nsocioeconomic drivers of this epidemic has become even more crucial in recent\nyears. To address this need, our study examines the correlation between\nopioid-related mortality and thirteen components of the Social Vulnerability\nIndex (SVI). Leveraging a nationwide county-level dataset spanning consecutive\nyears from 2010 to 2022, this study integrates empirical insights from\nexploratory data analysis with feature importance metrics derived from machine\nlearning models. Our findings highlight critical social factors strongly\ncorrelated with opioid-related mortality, emphasizing their potential roles in\nworsening the epidemic when their levels are high and mitigating it when their\nlevels are low.\n","authors":["Andrew Deas","Adam Spannaus","Dakotah D. Maguire","Jodie Trafton","Anuj J. Kapadia","Vasileios Maroulas"],"pdf_url":"https://arxiv.org/pdf/2412.15218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12018v1","updated":"2025-02-17T16:52:42Z","published":"2025-02-17T16:52:42Z","title":"Atom of Thoughts for Markov LLM Test-Time Scaling","summary":"  Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning progress is often achieved by solving a\nsequence of independent subquestions, each being self-contained and verifiable.\nThese subquestions are essentially atomic questions, relying primarily on their\ncurrent state rather than accumulated history, similar to the memoryless\ntransitions in a Markov process. Based on this observation, we propose Atom of\nThoughts (AoT), where each state transition in the reasoning process consists\nof decomposing the current question into a dependency-based directed acyclic\ngraph and contracting its subquestions, forming a new atomic question state.\nThis iterative decomposition-contraction process continues until reaching\ndirectly solvable atomic questions, naturally realizing Markov transitions\nbetween question states. Furthermore, these atomic questions can be seamlessly\nintegrated into existing test-time scaling methods, enabling AoT to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of AoT both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and\nDeepSeek-R1 by 10.6%. The code will be available at\nhttps://github.com/qixucen/atom.\n","authors":["Fengwei Teng","Zhaoyang Yu","Quan Shi","Jiayi Zhang","Chenglin Wu","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2502.12018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12013v1","updated":"2025-02-17T16:48:16Z","published":"2025-02-17T16:48:16Z","title":"Unsupervised Structural-Counterfactual Generation under Domain Shift","summary":"  Motivated by the burgeoning interest in cross-domain learning, we present a\nnovel generative modeling challenge: generating counterfactual samples in a\ntarget domain based on factual observations from a source domain. Our approach\noperates within an unsupervised paradigm devoid of parallel or joint datasets,\nrelying exclusively on distinct observational samples and causal graphs for\neach domain. This setting presents challenges that surpass those of\nconventional counterfactual generation. Central to our methodology is the\ndisambiguation of exogenous causes into effect-intrinsic and domain-intrinsic\ncategories. This differentiation facilitates the integration of domain-specific\ncausal graphs into a unified joint causal graph via shared effect-intrinsic\nexogenous variables. We propose leveraging Neural Causal models within this\njoint framework to enable accurate counterfactual generation under standard\nidentifiability assumptions. Furthermore, we introduce a novel loss function\nthat effectively segregates effect-intrinsic from domain-intrinsic variables\nduring model training. Given a factual observation, our framework combines the\nposterior distribution of effect-intrinsic variables from the source domain\nwith the prior distribution of domain-intrinsic variables from the target\ndomain to synthesize the desired counterfactuals, adhering to Pearl's causal\nhierarchy. Intriguingly, when domain shifts are restricted to alterations in\ncausal mechanisms without accompanying covariate shifts, our training regimen\nparallels the resolution of a conditional optimal transport problem. Empirical\nevaluations on a synthetic dataset show that our framework generates\ncounterfactuals in the target domain that very closely resemble the ground\ntruth.\n","authors":["Krishn Vishwas Kher","Lokesh Venkata Siva Maruthi Badisa","Kusampudi Venkata Datta Sri Harsha","Chitneedi Geetha Sowmya","SakethaNath Jagarlapudi"],"pdf_url":"https://arxiv.org/pdf/2502.12013v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.12011v1","updated":"2025-02-17T16:46:15Z","published":"2025-02-17T16:46:15Z","title":"Reconfigurable Intelligent Surfaces-Assisted Integrated Access and\n  Backhaul","summary":"  In this paper, we study the impact of reconfigurable intelligent surfaces\n(RISs) on the coverage extension of integrated access and backhaul (IAB)\nnetworks. Particularly, using a finite stochastic geometry model, with random\ndistributions of user equipments (UEs) in a finite region, and planned\nhierachical architecture for IAB, we study the service coverage probability\ndefined as the probability of the event that the UEs' minimum rate requirements\nare satisfied. We present comparisons between different cases including\nIAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network\ncontrolled repeaters (NCRs). Our investigations focus on wide-area IAB assisted\nwith RIS through the lens of different design architectures and deployments,\nrevealing both conflicts and synergies for minimizing the effect of tree\nfoliage over seasonal changes. Our simulation results reveal both opportunities\nand challenges towards the implementation of RIS in IAB.\n","authors":["Charitha Madapatha","Behrooz Makki","Hao Guo","Tommy Svensson"],"pdf_url":"https://arxiv.org/pdf/2502.12011v1.pdf","comment":"Submitted to 2025 European Conference on Networks and Communications\n  (EuCNC) & 6G Summit, 2025, Poznan, Poland"},{"id":"http://arxiv.org/abs/2502.12001v1","updated":"2025-02-17T16:39:28Z","published":"2025-02-17T16:39:28Z","title":"Merging Language and Domain Specific Models: The Impact on Technical\n  Vocabulary Acquisition","summary":"  This paper investigates the integration of technical vocabulary in merged\nlanguage models. We explore the knowledge transfer mechanisms involved when\ncombining a general-purpose language-specific model with a domain-specific\nmodel, focusing on the resulting model's comprehension of technical jargon. Our\nexperiments analyze the impact of this merging process on the target model's\nproficiency in handling specialized terminology. We present a quantitative\nevaluation of the performance of the merged model, comparing it with that of\nthe individual constituent models. The findings offer insights into the\neffectiveness of different model merging methods for enhancing domain-specific\nknowledge and highlight potential challenges and future directions in\nleveraging these methods for cross-lingual knowledge transfer in Natural\nLanguage Processing.\n","authors":["Thibault Rousset","Taisei Kakibuchi","Yusuke Sasaki","Yoshihide Nomura"],"pdf_url":"https://arxiv.org/pdf/2502.12001v1.pdf","comment":"Presented at the 263rd IPSJ-NL Workshop"},{"id":"http://arxiv.org/abs/2407.02025v4","updated":"2025-02-17T16:36:37Z","published":"2024-07-02T07:48:22Z","title":"On the Expressive Power of Sparse Geometric MPNNs","summary":"  Motivated by applications in chemistry and other sciences, we study the\nexpressive power of message-passing neural networks for geometric graphs, whose\nnode features correspond to 3-dimensional positions. Recent work has shown that\nsuch models can separate generic pairs of non-isomorphic geometric graphs,\nthough they may fail to separate some rare and complicated instances. However,\nthese results assume a fully connected graph, where each node possesses\ncomplete knowledge of all other nodes. In contrast, often, in application,\nevery node only possesses knowledge of a small number of nearest neighbors.\n  This paper shows that generic pairs of non-isomorphic geometric graphs can be\nseparated by message-passing networks with rotation equivariant features as\nlong as the underlying graph is connected. When only invariant intermediate\nfeatures are allowed, generic separation is guaranteed for generically globally\nrigid graphs. We introduce a simple architecture, EGENNET, which achieves our\ntheoretical guarantees and compares favorably with alternative architecture on\nsynthetic and chemical benchmarks. Our code is available at\nhttps://github.com/yonatansverdlov/E-GenNet.\n","authors":["Yonatan Sverdlov","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2407.02025v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06665v2","updated":"2025-02-17T16:34:29Z","published":"2024-10-09T08:19:31Z","title":"Revisiting Multi-Permutation Equivariance through the Lens of\n  Irreducible Representations","summary":"  This paper explores the characterization of equivariant linear layers for\nrepresentations of permutations and related groups. Unlike traditional\napproaches, which address these problems using parameter-sharing, we consider\nan alternative methodology based on irreducible representations and Schur's\nlemma. Using this methodology, we obtain an alternative derivation for existing\nmodels like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space\n(DWS) networks. The derivation for DWS networks is significantly simpler than\nthat of previous results.\n  Next, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant\nlayers are Siamese. In contrast, we give a full characterization of layers in\nthis case and show that there is a vast number of additional non-Siamese layers\nin some settings. We also show empirically that these additional non-Siamese\nlayers can improve performance in tasks like graph anomaly detection, weight\nspace alignment, and learning Wasserstein distances. Our code is available at\n\\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.\n","authors":["Yonatan Sverdlov","Ido Springer","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2410.06665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09969v2","updated":"2025-02-17T16:26:47Z","published":"2025-02-14T07:55:47Z","title":"Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning","summary":"  Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.\n","authors":["Ishika Agarwal","Dilek Hakkani-T√ºr"],"pdf_url":"https://arxiv.org/pdf/2502.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21745v4","updated":"2025-02-17T16:26:20Z","published":"2024-10-29T05:18:34Z","title":"RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment","summary":"  Graph clustering is an essential aspect of network analysis that involves\ngrouping nodes into separate clusters. Recent developments in deep learning\nhave resulted in graph clustering, which has proven effective in many\napplications. Nonetheless, these methods often encounter difficulties when\ndealing with real-world graphs, particularly in the presence of noisy edges.\nAdditionally, many denoising graph clustering methods tend to suffer from lower\nperformance, training instability, and challenges in scaling to large datasets\ncompared to non-denoised models. To tackle these issues, we introduce a new\nframework called the Robust Deep Graph Clustering Framework via Dual Soft\nAssignment (RDSA). RDSA consists of three key components: (i) a node embedding\nmodule that effectively integrates the graph's topological features and node\nattributes; (ii) a structure-based soft assignment module that improves graph\nmodularity by utilizing an affinity matrix for node assignments; and (iii) a\nnode-based soft assignment module that identifies community landmarks and\nrefines node assignments to enhance the model's robustness. We assess RDSA on\nvarious real-world datasets, demonstrating its superior performance relative to\nexisting state-of-the-art methods. Our findings indicate that RDSA provides\nrobust clustering across different graph types, excelling in clustering\neffectiveness and robustness, including adaptability to noise, stability, and\nscalability.\n","authors":["Yang Xiang","Li Fan","Tulika Saha","Xiaoying Pang","Yushan Pan","Haiyang Zhang","Chengtao Ji"],"pdf_url":"https://arxiv.org/pdf/2410.21745v4.pdf","comment":"Accepted by DASFAA 2025; Complete version"},{"id":"http://arxiv.org/abs/2502.11986v1","updated":"2025-02-17T16:26:05Z","published":"2025-02-17T16:26:05Z","title":"Selective Task Group Updates for Multi-Task Optimization","summary":"  Multi-task learning enables the acquisition of task-generic knowledge by\ntraining multiple tasks within a unified architecture. However, training all\ntasks together in a single architecture can lead to performance degradation,\nknown as negative transfer, which is a main concern in multi-task learning.\nPrevious works have addressed this issue by optimizing the multi-task network\nthrough gradient manipulation or weighted loss adjustments. However, their\noptimization strategy focuses on addressing task imbalance in shared\nparameters, neglecting the learning of task-specific parameters. As a result,\nthey show limitations in mitigating negative transfer, since the learning of\nshared space and task-specific information influences each other during\noptimization. To address this, we propose a different approach to enhance\nmulti-task performance by selectively grouping tasks and updating them for each\nbatch during optimization. We introduce an algorithm that adaptively determines\nhow to effectively group tasks and update them during the learning process. To\ntrack inter-task relations and optimize multi-task networks simultaneously, we\npropose proximal inter-task affinity, which can be measured during the\noptimization process. We provide a theoretical analysis on how dividing tasks\ninto multiple groups and updating them sequentially significantly affects\nmulti-task performance by enhancing the learning of task-specific parameters.\nOur methods substantially outperform previous multi-task optimization\napproaches and are scalable to different architectures and various numbers of\ntasks.\n","authors":["Wooseong Jeong","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2502.11986v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11981v1","updated":"2025-02-17T16:22:46Z","published":"2025-02-17T16:22:46Z","title":"Machine Learning Should Maximize Welfare, Not (Only) Accuracy","summary":"  Decades of research in machine learning have given us powerful tools for\nmaking accurate predictions. But when used in social settings and on human\ninputs, better accuracy does not immediately translate to better social\noutcomes. This may not be surprising given that conventional learning\nframeworks are not designed to express societal preferences -- let alone\npromote them. This position paper argues that machine learning is currently\nmissing, and can gain much from incorporating, a proper notion of social\nwelfare. The field of welfare economics asks: how should we allocate limited\nresources to self-interested agents in a way that maximizes social benefit? We\nargue that this perspective applies to many modern applications of machine\nlearning in social contexts, and advocate for its adoption. Rather than\ndisposing of prediction, we aim to leverage this forte of machine learning for\npromoting social welfare. We demonstrate this idea by proposing a conceptual\nframework that gradually transitions from accuracy maximization (with awareness\nto welfare) to welfare maximization (via accurate prediction). We detail\napplications and use-cases for which our framework can be effective, identify\ntechnical challenges and practical opportunities, and highlight future avenues\nworth pursuing.\n","authors":["Nir Rosenfeld","Haifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11969v1","updated":"2025-02-17T16:18:07Z","published":"2025-02-17T16:18:07Z","title":"Learning Generalizable Prompt for CLIP with Class Similarity Knowledge","summary":"  In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.\n","authors":["Sehun Jung","Hyang-won Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11968v1","updated":"2025-02-17T16:18:00Z","published":"2025-02-17T16:18:00Z","title":"Theoretical Barriers in Bellman-Based Reinforcement Learning","summary":"  Reinforcement Learning algorithms designed for high-dimensional spaces often\nenforce the Bellman equation on a sampled subset of states, relying on\ngeneralization to propagate knowledge across the state space. In this paper, we\nidentify and formalize a fundamental limitation of this common approach.\nSpecifically, we construct counterexample problems with a simple structure that\nthis approach fails to exploit. Our findings reveal that such algorithms can\nneglect critical information about the problems, leading to inefficiencies.\nFurthermore, we extend this negative result to another approach from the\nliterature: Hindsight Experience Replay learning state-to-state reachability.\n","authors":["Brieuc Pinon","Rapha√´l Jungers","Jean-Charles Delvenne"],"pdf_url":"https://arxiv.org/pdf/2502.11968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03540v3","updated":"2025-02-17T16:07:09Z","published":"2025-02-05T19:00:52Z","title":"Path Planning for Masked Diffusion Model Sampling","summary":"  In this paper, we explore how token unmasking order influences generative\nquality in masked diffusion models (MDMs). We derive an expanded evidence lower\nbound (ELBO) that introduces a planner to select which tokens to unmask at each\nstep. Our analysis reveals that alternative unmasking strategies can enhance\ngeneration performance. Building on this, we propose Path Planning (P2), a\nsampling framework that uses a pre-trained BERT model or the denoiser itself to\nguide unmasking decisions. P2 generalizes all known MDM sampling strategies and\nsignificantly improves performance across diverse domains, including language\ngeneration (in-context learning, code generation, story infilling, mathematical\nreasoning, reverse curse correction) and biological sequence generation\n(protein and RNA sequences).\n","authors":["Fred Zhangzhi Peng","Zachary Bezemek","Sawan Patel","Jarrid Rector-Brooks","Sherwood Yao","Alexander Tong","Pranam Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2502.03540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11953v1","updated":"2025-02-17T16:05:14Z","published":"2025-02-17T16:05:14Z","title":"Refined PAC-Bayes Bounds for Offline Bandits","summary":"  In this paper, we present refined probabilistic bounds on empirical reward\nestimates for off-policy learning in bandit problems. We build on the\nPAC-Bayesian bounds from Seldin et al. (2010) and improve on their results\nusing a new parameter optimization approach introduced by Rodr\\'iguez et al.\n(2024). This technique is based on a discretization of the space of possible\nevents to optimize the \"in probability\" parameter. We provide two\nparameter-free PAC-Bayes bounds, one based on Hoeffding-Azuma's inequality and\nthe other based on Bernstein's inequality. We prove that our bounds are almost\noptimal as they recover the same rate as would be obtained by setting the \"in\nprobability\" parameter after the realization of the data.\n","authors":["Amaury Gouverneur","Tobias J. Oechtering","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2502.11953v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.11951v1","updated":"2025-02-17T16:04:04Z","published":"2025-02-17T16:04:04Z","title":"Qubit-Based Framework for Quantum Machine Learning: Bridging Classical\n  Data and Quantum Algorithms","summary":"  This paper dives into the exciting and rapidly growing field of quantum\ncomputing, explaining its core ideas, current progress, and how it could\nrevolutionize the way we solve complex problems. It starts by breaking down the\nbasics, like qubits, quantum circuits, and how principles like superposition\nand entanglement make quantum computers fundamentally different-and far more\npowerful for certain tasks-than the classical computers we use today. We also\nexplore how quantum computing deals with complex problems and why it is\nuniquely suited for challenges classical systems struggle to handle. A big part\nof this paper focuses on Quantum Machine Learning (QML), where the strengths of\nquantum computing meet the world of artificial intelligence. By processing\nmassive datasets and optimizing intricate algorithms, quantum systems offer new\npossibilities for machine learning. We highlight different approaches to\ncombining quantum and classical computing, showing how they can work together\nto produce faster and more accurate results. Additionally, we explore the tools\nand platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are\nhelping researchers and developers bring these theories to life. Of course,\nquantum computing has its hurdles. Challenges like scaling up hardware,\ncorrecting errors, and keeping qubits stable are significant roadblocks. Yet,\nwith rapid advancements in cloud-based platforms and innovative technologies,\nthe potential of quantum computing feels closer than ever. This paper aims to\noffer readers a clear and comprehensive introduction to quantum computing, its\nrole in machine learning, and the immense possibilities it holds for the future\nof technology.\n","authors":["Bhavna Bose","Saurav Verma"],"pdf_url":"https://arxiv.org/pdf/2502.11951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11949v1","updated":"2025-02-17T16:02:54Z","published":"2025-02-17T16:02:54Z","title":"Massively Scaling Explicit Policy-conditioned Value Functions","summary":"  We introduce a scaling strategy for Explicit Policy-Conditioned Value\nFunctions (EPVFs) that significantly improves performance on challenging\ncontinuous-control tasks. EPVFs learn a value function V({\\theta}) that is\nexplicitly conditioned on the policy parameters, enabling direct gradient-based\nupdates to the parameters of any policy. However, EPVFs at scale struggle with\nunrestricted parameter growth and efficient exploration in the policy parameter\nspace. To address these issues, we utilize massive parallelization with\nGPU-based simulators, big batch sizes, weight clipping and scaled peturbations.\nOur results show that EPVFs can be scaled to solve complex tasks, such as a\ncustom Ant environment, and can compete with state-of-the-art Deep\nReinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO)\nand Soft Actor-Critic (SAC). We further explore action-based policy parameter\nrepresentations from previous work and specialized neural network architectures\nto efficiently handle weight-space features, which have not been used in the\ncontext of DRL before.\n","authors":["Nico Bohlinger","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.11949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11942v1","updated":"2025-02-17T15:56:07Z","published":"2025-02-17T15:56:07Z","title":"Sharp-PINNs: staggered hard-constrained physics-informed neural networks\n  for phase field modelling of corrosion","summary":"  Physics-informed neural networks have shown significant potential in solving\npartial differential equations (PDEs) across diverse scientific fields.\nHowever, their performance often deteriorates when addressing PDEs with\nintricate and strongly coupled solutions. In this work, we present a novel\nSharp-PINN framework to tackle complex phase field corrosion problems. Instead\nof minimizing all governing PDE residuals simultaneously, the Sharp-PINNs\nintroduce a staggered training scheme that alternately minimizes the residuals\nof Allen-Cahn and Cahn-Hilliard equations, which govern the corrosion system.\nTo further enhance its efficiency and accuracy, we design an advanced neural\nnetwork architecture that integrates random Fourier features as coordinate\nembeddings, employs a modified multi-layer perceptron as the primary backbone,\nand enforces hard constraints in the output layer. This framework is\nbenchmarked through simulations of corrosion problems with multiple pits, where\nthe staggered training scheme and network architecture significantly improve\nboth the efficiency and accuracy of PINNs. Moreover, in three-dimensional\ncases, our approach is 5-10 times faster than traditional finite element\nmethods while maintaining competitive accuracy, demonstrating its potential for\nreal-world engineering applications in corrosion prediction.\n","authors":["Nanxi Chen","Chuanjie Cui","Rujin Ma","Airong Chen","Sifan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05816v4","updated":"2025-02-17T15:55:26Z","published":"2024-06-09T15:08:00Z","title":"Attention as a Hypernetwork","summary":"  Transformers can under some circumstances generalize to novel problem\ninstances whose constituent parts might have been encountered during training,\nbut whose compositions have not. What mechanisms underlie this ability for\ncompositional generalization? By reformulating multi-head attention as a\nhypernetwork, we reveal that a composable, low-dimensional latent code\nspecifies key-query specific operations. We find empirically that this latent\ncode is predictive of the subtasks the network performs on unseen task\ncompositions, revealing that latent codes acquired during training are reused\nto solve unseen problem instances. To further examine the hypothesis that the\nintrinsic hypernetwork of multi-head attention supports compositional\ngeneralization, we ablate whether making the hypernetwork-generated linear\nvalue network nonlinear strengthens compositionality. We find that this\nmodification improves compositional generalization on abstract reasoning tasks.\nIn particular, we introduce a symbolic version of the Raven's Progressive\nMatrices human intelligence test, which gives us precise control over the\nproblem compositions encountered during training and evaluation. We demonstrate\non this task how scaling model size and data enables compositional\ngeneralization in transformers and gives rise to a functionally structured\nlatent space.\n","authors":["Simon Schug","Seijin Kobayashi","Yassir Akram","Jo√£o Sacramento","Razvan Pascanu"],"pdf_url":"https://arxiv.org/pdf/2406.05816v4.pdf","comment":"ICLR 2025 (Oral); Code available at\n  https://github.com/smonsays/hypernetwork-attention"},{"id":"http://arxiv.org/abs/2412.18547v4","updated":"2025-02-17T15:55:08Z","published":"2024-12-24T16:55:45Z","title":"Token-Budget-Aware LLM Reasoning","summary":"  Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.\n","authors":["Tingxu Han","Zhenting Wang","Chunrong Fang","Shiyu Zhao","Shiqing Ma","Zhenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18547v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11941v1","updated":"2025-02-17T15:52:22Z","published":"2025-02-17T15:52:22Z","title":"Deep Spatio-Temporal Neural Network for Air Quality Reanalysis","summary":"  Air quality prediction is key to mitigating health impacts and guiding\ndecisions, yet existing models tend to focus on temporal trends while\noverlooking spatial generalization. We propose AQ-Net, a spatiotemporal\nreanalysis model for both observed and unobserved stations in the near future.\nAQ-Net utilizes the LSTM and multi-head attention for the temporal regression.\nWe also propose a cyclic encoding technique to ensure continuous time\nrepresentation. To learn fine-grained spatial air quality estimation, we\nincorporate AQ-Net with the neural kNN to explore feature-based interpolation,\nsuch that we can fill the spatial gaps given coarse observation stations. To\ndemonstrate the efficiency of our model for spatiotemporal reanalysis, we use\ndata from 2013-2017 collected in northern China for PM2.5 analysis. Extensive\nexperiments show that AQ-Net excels in air quality reanalysis, highlighting the\npotential of hybrid spatio-temporal models to better capture environmental\ndynamics, especially in urban areas where both spatial and temporal variability\nare critical.\n","authors":["Ammar Kheder","Benjamin Foreback","Lili Wang","Zhi-Song Liu","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2502.11941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11937v1","updated":"2025-02-17T15:48:46Z","published":"2025-02-17T15:48:46Z","title":"FitLight: Federated Imitation Learning for Plug-and-Play Autonomous\n  Traffic Signal Control","summary":"  Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)\nmethods have been extensively studied, their practical applications still raise\nsome serious issues such as high learning cost and poor generalizability. This\nis because the ``trial-and-error'' training style makes RL agents extremely\ndependent on the specific traffic environment, which also requires a long\nconvergence time. To address these issues, we propose a novel Federated\nImitation Learning (FIL)-based framework for multi-intersection TSC, named\nFitLight, which allows RL agents to plug-and-play for any traffic environment\nwithout additional pre-training cost. Unlike existing imitation learning\napproaches that rely on pre-training RL agents with demonstrations, FitLight\nallows real-time imitation learning and seamless transition to reinforcement\nlearning. Due to our proposed knowledge-sharing mechanism and novel hybrid\npressure-based agent design, RL agents can quickly find a best control policy\nwith only a few episodes. Moreover, for resource-constrained TSC scenarios,\nFitLight supports model pruning and heterogeneous model aggregation, such that\nRL agents can work on a micro-controller with merely 16{\\it KB} RAM and 32{\\it\nKB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art\nmethods, FitLight not only provides a superior starting point but also\nconverges to a better final solution on both real-world and synthetic datasets,\neven under extreme resource limitations.\n","authors":["Yutong Ye","Yingbo Zhou","Zhusen Liu","Xiao Du","Hao Zhou","Xiang Lian","Mingsong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06079v2","updated":"2025-02-17T15:44:24Z","published":"2025-02-10T00:27:54Z","title":"Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo","summary":"  Discrete diffusion models are a class of generative models that produce\nsamples from an approximated data distribution within a discrete state space.\nOften, there is a need to target specific regions of the data distribution.\nCurrent guidance methods aim to sample from a distribution with mass\nproportional to $p_0(x_0) p(\\zeta|x_0)^\\alpha$ but fail to achieve this in\npractice. We introduce a Sequential Monte Carlo algorithm that generates\nunbiasedly from this target distribution, utilising the learnt unconditional\nand guided process. We validate our approach on low-dimensional distributions,\ncontrolled images and text generations. For text generation, our method\nprovides strong control while maintaining low perplexity compared to\nguidance-based approaches.\n","authors":["Cheuk Kit Lee","Paul Jeha","Jes Frellsen","Pietro Lio","Michael Samuel Albergo","Francisco Vargas"],"pdf_url":"https://arxiv.org/pdf/2502.06079v2.pdf","comment":"29 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.11927v1","updated":"2025-02-17T15:40:13Z","published":"2025-02-17T15:40:13Z","title":"Continual Learning Should Move Beyond Incremental Classification","summary":"  Continual learning (CL) is the sub-field of machine learning concerned with\naccumulating knowledge in dynamic environments. So far, CL research has mainly\nfocused on incremental classification tasks, where models learn to classify new\ncategories while retaining knowledge of previously learned ones. Here, we argue\nthat maintaining such a focus limits both theoretical development and practical\napplicability of CL methods. Through a detailed analysis of concrete examples -\nincluding multi-target classification, robotics with constrained output spaces,\nlearning in continuous task domains, and higher-level concept memorization - we\ndemonstrate how current CL approaches often fail when applied beyond standard\nclassification. We identify three fundamental challenges: (C1) the nature of\ncontinuity in learning problems, (C2) the choice of appropriate spaces and\nmetrics for measuring similarity, and (C3) the role of learning objectives\nbeyond classification. For each challenge, we provide specific recommendations\nto help move the field forward, including formalizing temporal dynamics through\ndistribution processes, developing principled approaches for continuous task\nspaces, and incorporating density estimation and generative objectives. In so\ndoing, this position paper aims to broaden the scope of CL research while\nstrengthening its theoretical foundations, making it more applicable to\nreal-world problems.\n","authors":["Rupert Mitchell","Antonio Alliegro","Raffaello Camoriano","Dustin Carri√≥n-Ojeda","Antonio Carta","Georgia Chalvatzaki","Nikhil Churamani","Carlo D'Eramo","Samin Hamidi","Robin Hesse","Fabian Hinder","Roshni Ramanna Kamath","Vincenzo Lomonaco","Subarnaduti Paul","Francesca Pistilli","Tinne Tuytelaars","Gido M van de Ven","Kristian Kersting","Simone Schaub-Meyer","Martin Mundt"],"pdf_url":"https://arxiv.org/pdf/2502.11927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17884v4","updated":"2025-02-17T15:37:58Z","published":"2024-04-27T12:43:02Z","title":"Generalization capabilities and robustness of hybrid models grounded in\n  physics compared to purely deep learning models","summary":"  This study investigates the generalization capabilities and robustness of\npurely deep learning (DL) models and hybrid models based on physical principles\nin fluid dynamics applications, specifically focusing on iteratively\nforecasting the temporal evolution of flow dynamics. Three autoregressive\nmodels were compared: a hybrid model (POD-DL) that combines proper orthogonal\ndecomposition (POD) with a long-short term memory (LSTM) layer, a convolutional\nautoencoder combined with a convolutional LSTM (ConvLSTM) layer and a\nvariational autoencoder (VAE) combined with a ConvLSTM layer. These models were\ntested on two high-dimensional, nonlinear datasets representing the velocity\nfield of flow past a circular cylinder in both laminar and turbulent regimes.\nThe study used latent dimension methods, enabling a bijective reduction of\nhigh-dimensional dynamics into a lower-order space to facilitate future\npredictions. While the VAE and ConvLSTM models accurately predicted laminar\nflow, the hybrid POD-DL model outperformed the others across both laminar and\nturbulent flow regimes. This success is attributed to the model's ability to\nincorporate modal decomposition, reducing the dimensionality of the data, by a\nnon-parametric method, and simplifying the forecasting component. By leveraging\nPOD, the model not only gained insight into the underlying physics, improving\nprediction accuracy with less training data, but also reduce the number of\ntrainable parameters as POD is non-parametric. The findings emphasize the\npotential of hybrid models, particularly those integrating modal decomposition\nand deep learning, in predicting complex flow dynamics.\n","authors":["Rodrigo Abad√≠a-Heredia","Adri√°n Corrochano","Manuel Lopez-Martin","Soledad Le Clainche"],"pdf_url":"https://arxiv.org/pdf/2404.17884v4.pdf","comment":"24 pages, two column, 26 figures and 11 tables"},{"id":"http://arxiv.org/abs/2502.11925v1","updated":"2025-02-17T15:35:36Z","published":"2025-02-17T15:35:36Z","title":"GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs","summary":"  The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.\n","authors":["Yi Fang","Bowen Jin","Jiacheng Shen","Sirui Ding","Qiaoyu Tan","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2502.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11918v1","updated":"2025-02-17T15:32:14Z","published":"2025-02-17T15:32:14Z","title":"VLP: Vision-Language Preference Learning for Embodied Manipulation","summary":"  Reward engineering is one of the key challenges in Reinforcement Learning\n(RL). Preference-based RL effectively addresses this issue by learning from\nhuman feedback. However, it is both time-consuming and expensive to collect\nhuman preference labels. In this paper, we propose a novel\n\\textbf{V}ision-\\textbf{L}anguage \\textbf{P}reference learning framework, named\n\\textbf{VLP}, which learns a vision-language preference model to provide\npreference feedback for embodied manipulation tasks. To achieve this, we define\nthree types of language-conditioned preferences and construct a vision-language\npreference dataset, which contains versatile implicit preference orders without\nhuman annotations. The preference model learns to extract language-related\nfeatures, and then serves as a preference annotator in various downstream\ntasks. The policy can be learned according to the annotated preferences via\nreward learning or direct policy optimization. Extensive empirical results on\nsimulated embodied manipulation tasks demonstrate that our method provides\naccurate preferences and generalizes to unseen tasks and unseen language\ninstructions, outperforming the baselines by a large margin.\n","authors":["Runze Liu","Chenjia Bai","Jiafei Lyu","Shengjie Sun","Yali Du","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2502.11918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11913v1","updated":"2025-02-17T15:30:17Z","published":"2025-02-17T15:30:17Z","title":"PreAdaptFWI: Pretrained-Based Adaptive Residual Learning for\n  Full-Waveform Inversion Without Dataset Dependency","summary":"  Full-waveform inversion (FWI) is a method that utilizes seismic data to\ninvert the physical parameters of subsurface media by minimizing the difference\nbetween simulated and observed waveforms. Due to its ill-posed nature, FWI is\nsusceptible to getting trapped in local minima. Consequently, various research\nefforts have attempted to combine neural networks with FWI to stabilize the\ninversion process. This study presents a simple yet effective training\nframework that is independent of dataset reliance and requires only moderate\npre-training on a simple initial model to stabilize network outputs. During the\ntransfer learning phase, the conventional FWI gradients will simultaneously\nupdate both the neural network and the proposed adaptive residual learning\nmodule, which learns the residual mapping of large-scale distribution features\nin the network's output, rather than directly fitting the target mapping.\nThrough this synergistic training paradigm, the proposed algorithm effectively\ninfers the physically-informed prior knowledge into a global representation of\nstratigraphic distribution, as well as capturing subtle variations in\ninter-layer velocities within local details, thereby escaping local optima.\nEvaluating the method on two benchmark models under various conditions,\nincluding absent low-frequency data, noise interference, and differing initial\nmodels, along with corresponding ablation experiments, consistently\ndemonstrates the superiority of the proposed approach.\n","authors":["Xintong Dong","Zhengyi Yuan","Jun Lin","Shiqi Dong","Xunqian Tong","Yue Li"],"pdf_url":"https://arxiv.org/pdf/2502.11913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11910v1","updated":"2025-02-17T15:28:40Z","published":"2025-02-17T15:28:40Z","title":"Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives","summary":"  Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes.\n","authors":["Leo Schwinn","Yan Scholten","Tom Wollschl√§ger","Sophie Xhonneux","Stephen Casper","Stephan G√ºnnemann","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2502.11910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11909v1","updated":"2025-02-17T15:28:19Z","published":"2025-02-17T15:28:19Z","title":"Neural Guided Diffusion Bridges","summary":"  We propose a novel method for simulating conditioned diffusion processes\n(diffusion bridges) in Euclidean spaces. By training a neural network to\napproximate bridge dynamics, our approach eliminates the need for\ncomputationally intensive Markov Chain Monte Carlo (MCMC) methods or\nreverse-process modeling. Compared to existing methods, it offers greater\nrobustness across various diffusion specifications and conditioning scenarios.\nThis applies in particular to rare events and multimodal distributions, which\npose challenges for score-learning- and MCMC-based approaches. We propose a\nflexible variational family for approximating the diffusion bridge path measure\nwhich is partially specified by a neural network. Once trained, it enables\nefficient independent sampling at a cost comparable to sampling the\nunconditioned (forward) process.\n","authors":["Gefan Yang","Frank van der Meulen","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2502.11909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11900v1","updated":"2025-02-17T15:23:59Z","published":"2025-02-17T15:23:59Z","title":"Ansatz-free Hamiltonian learning with Heisenberg-limited scaling","summary":"  Learning the unknown interactions that govern a quantum system is crucial for\nquantum information processing, device benchmarking, and quantum sensing. The\nproblem, known as Hamiltonian learning, is well understood under the assumption\nthat interactions are local, but this assumption may not hold for arbitrary\nHamiltonians. Previous methods all require high-order inverse polynomial\ndependency with precision, unable to surpass the standard quantum limit and\nreach the gold standard Heisenberg-limited scaling. Whether Heisenberg-limited\nHamiltonian learning is possible without prior assumptions about the\ninteraction structures, a challenge we term \\emph{ansatz-free Hamiltonian\nlearning}, remains an open question. In this work, we present a quantum\nalgorithm to learn arbitrary sparse Hamiltonians without any structure\nconstraints using only black-box queries of the system's real-time evolution\nand minimal digital controls to attain Heisenberg-limited scaling in estimation\nerror. Our method is also resilient to state-preparation-and-measurement\nerrors, enhancing its practical feasibility. Moreover, we establish a\nfundamental trade-off between total evolution time and quantum control on\nlearning arbitrary interactions, revealing the intrinsic interplay between\ncontrollability and total evolution time complexity for any learning algorithm.\nThese results pave the way for further exploration into Heisenberg-limited\nHamiltonian learning in complex quantum systems under minimal assumptions,\npotentially enabling new benchmarking and verification protocols.\n","authors":["Hong-Ye Hu","Muzhou Ma","Weiyuan Gong","Qi Ye","Yu Tong","Steven T. Flammia","Susanne F. Yelin"],"pdf_url":"https://arxiv.org/pdf/2502.11900v1.pdf","comment":"5 pages, 1 figure with Supplementary Materials (17 pages, 1 figure).\n  HYH and MM contributed equally"},{"id":"http://arxiv.org/abs/2502.11896v1","updated":"2025-02-17T15:22:19Z","published":"2025-02-17T15:22:19Z","title":"CAMEL: Continuous Action Masking Enabled by Large Language Models for\n  Reinforcement Learning","summary":"  Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.\n","authors":["Yanxiao Zhao","Yangge Qian","Jingyang Shan","Xiaolin Qin"],"pdf_url":"https://arxiv.org/pdf/2502.11896v1.pdf","comment":"Accepted at RLDM 2025"},{"id":"http://arxiv.org/abs/2502.11895v1","updated":"2025-02-17T15:21:11Z","published":"2025-02-17T15:21:11Z","title":"Continual Quantization-Aware Pre-Training: When to transition from\n  16-bit to 1.58-bit pre-training for BitNet language models?","summary":"  Large language models (LLMs) require immense resources for training and\ninference. Quantization, a technique that reduces the precision of model\nparameters, offers a promising solution for improving LLM efficiency and\nsustainability. While post-training quantization methods typically achieve 4-8\nbits per parameter, recent research suggests that training LLMs with 1.58 bits\nper weight parameter from scratch can maintain model accuracy while greatly\nreducing memory requirements and energy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-aware pre-training, where the\nmodels are first trained with 16-bit precision and then transition into\n1.58-bit quantization-aware training. Our results on 11 downstream tasks show\nthat this 16-to-1.58-bit training strategy is preferable over full 1.58-bit\ntraining and leaves models closer to those which have undergone 16-bit\ntraining. We further investigate the effects of retaining the optimizer state\nat the transition point and gradually phasing in quantization strength --\nfinding that both techniques alleviate the magnitude of loss spikes, but also\nthat these effects can be compensated through further training.\n","authors":["Jacob Nielsen","Peter Schneider-Kamp","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2502.11895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11893v1","updated":"2025-02-17T15:20:04Z","published":"2025-02-17T15:20:04Z","title":"Rethinking Benign Overfitting in Two-Layer Neural Networks","summary":"  Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed\na sharp phase transition from benign to harmful overfitting when the\nnoise-to-feature ratio exceeds a threshold-a situation common in long-tailed\ndata distributions where atypical data is prevalent. However, harmful\noverfitting rarely happens in overparameterized neural networks. Further\nexperimental results suggested that memorization is necessary for achieving\nnear-optimal generalization error in long-tailed data distributions (Feldman &\nZhang, 2020). We argue that this discrepancy between theoretical predictions\nand empirical observations arises because previous feature-noise data models\noverlook the heterogeneous nature of noise across different data classes. In\nthis paper, we refine the feature-noise data model by incorporating\nclass-dependent heterogeneous noise and re-examine the overfitting phenomenon\nin neural networks. Through a comprehensive analysis of the training dynamics,\nwe establish test loss bounds for the refined model. Our findings reveal that\nneural networks can leverage \"data noise\", previously deemed harmful, to learn\nimplicit features that improve the classification accuracy for long-tailed\ndata. Experimental validation on both synthetic and real-world datasets\nsupports our theoretical results.\n","authors":["Ruichen Xu","Kexin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11886v1","updated":"2025-02-17T15:13:29Z","published":"2025-02-17T15:13:29Z","title":"LIMR: Less is More for RL Scaling","summary":"  In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.\n","authors":["Xuefeng Li","Haoyang Zou","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11886v1.pdf","comment":"6pages"},{"id":"http://arxiv.org/abs/2502.11882v1","updated":"2025-02-17T15:09:45Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2501.15369v2","updated":"2025-02-17T15:09:31Z","published":"2025-01-26T02:34:58Z","title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","summary":"  We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\n\\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.\n","authors":["Chuanyang Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.15369v2.pdf","comment":"Accepted to ICLR 2025. Code:\n  https://github.com/ChuanyangZheng/iFormer"},{"id":"http://arxiv.org/abs/2502.11880v1","updated":"2025-02-17T15:06:28Z","published":"2025-02-17T15:06:28Z","title":"Bitnet.cpp: Efficient Edge Inference for Ternary LLMs","summary":"  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shijie Cao","Yan Xia","Ting Cao","Jianyu Wei","Shuming Ma","Hongyu Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.11880v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.11877v1","updated":"2025-02-17T15:03:54Z","published":"2025-02-17T15:03:54Z","title":"JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs","summary":"  We introduce a simple method for probabilistic predictions on tabular data\nbased on Large Language Models (LLMs) called JoLT (Joint LLM Process for\nTabular data). JoLT uses the in-context learning capabilities of LLMs to define\njoint distributions over tabular data conditioned on user-specified side\ninformation about the problem, exploiting the vast repository of latent\nproblem-relevant knowledge encoded in LLMs. JoLT defines joint distributions\nfor multiple target variables with potentially heterogeneous data types without\nany data conversion, data preprocessing, special handling of missing data, or\nmodel training, making it accessible and efficient for practitioners. Our\nexperiments show that JoLT outperforms competitive methods on low-shot\nsingle-target and multi-target tabular classification and regression tasks.\nFurthermore, we show that JoLT can automatically handle missing data and\nperform data imputation by leveraging textual side information. We argue that\ndue to its simplicity and generality, JoLT is an effective approach for a wide\nvariety of real prediction problems.\n","authors":["Aliaksandra Shysheya","John Bronskill","James Requeima","Shoaib Ahmed Siddiqui","Javier Gonzalez","David Duvenaud","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2502.11877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11863v1","updated":"2025-02-17T14:55:46Z","published":"2025-02-17T14:55:46Z","title":"FedEAT: A Robustness Optimization Framework for Federated LLMs","summary":"  Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.\n","authors":["Yahao Pang","Xingyuan Wu","Xiaojin Zhang","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2502.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11854v1","updated":"2025-02-17T14:46:58Z","published":"2025-02-17T14:46:58Z","title":"Enhanced Anomaly Detection in IoMT Networks using Ensemble AI Models on\n  the CICIoMT2024 Dataset","summary":"  The rapid proliferation of Internet of Medical Things (IoMT) devices in\nhealthcare has introduced unique cybersecurity challenges, primarily due to the\ndiverse communication protocols and critical nature of these devices This\nresearch aims to develop an advanced, real-time anomaly detection framework\ntailored for IoMT network traffic, leveraging AI/ML models and the CICIoMT2024\ndataset By integrating multi-protocol (MQTT, WiFi), attack-specific (DoS,\nDDoS), time-series (active/idle states), and device-specific (Bluetooth) data,\nour study captures a comprehensive range of IoMT interactions As part of our\ndata analysis, various machine learning techniques are employed which include\nan ensemble model using XGBoost for improved performance against specific\nattack types, sequential models comprised of LSTM and CNN-LSTM that leverage\ntime dependencies, and unsupervised models such as Autoencoders and Isolation\nForest that are good in general anomaly detection The results of the experiment\nprove with an ensemble model lowers false positive rates and reduced\ndetections.\n","authors":["Prathamesh Chandekar","Mansi Mehta","Swet Chandan"],"pdf_url":"https://arxiv.org/pdf/2502.11854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11853v1","updated":"2025-02-17T14:46:38Z","published":"2025-02-17T14:46:38Z","title":"StructTransform: A Scalable Attack Surface for Safety-Aligned Large\n  Language Models","summary":"  In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g. SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing\n\\textit{content transformations}, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware, and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection.\n","authors":["Shehel Yoosuf","Temoor Ali","Ahmed Lekssays","Mashael AlSabah","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2502.11853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11850v1","updated":"2025-02-17T14:44:12Z","published":"2025-02-17T14:44:12Z","title":"Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif\n  Discovery","summary":"  Time Series Motif Discovery (TSMD) identifies repeating patterns in time\nseries data, but its unsupervised nature might result in motifs that are not\ninteresting to the user. To address this, we propose a framework that allows\nthe user to impose constraints on the motifs to be discovered, where\nconstraints can easily be defined according to the properties of the desired\nmotifs in the application domain. We also propose an efficient implementation\nof the framework, the LoCoMotif-DoK algorithm. We demonstrate that\nLoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic\ndata, outperforming other TSMD techniques which only support a limited form of\ndomain knowledge.\n","authors":["Aras Yurtman","Daan Van Wesenbeeck","Wannes Meert","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2502.11850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11844v1","updated":"2025-02-17T14:37:47Z","published":"2025-02-17T14:37:47Z","title":"BaxBench: Can LLMs Generate Correct and Secure Backends?","summary":"  The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.\n","authors":["Mark Vero","Niels M√ºndler","Victor Chibotaru","Veselin Raychev","Maximilian Baader","Nikola Jovanoviƒá","Jingxuan He","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2502.11844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11840v1","updated":"2025-02-17T14:35:16Z","published":"2025-02-17T14:35:16Z","title":"ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition","summary":"  Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.\n","authors":["Muhammad Waseem Akram","Stefano Dettori","Valentina Colla","Giorgio Carlo Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2502.11840v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11836v1","updated":"2025-02-17T14:31:00Z","published":"2025-02-17T14:31:00Z","title":"Model Generalization on Text Attribute Graphs: Principles with Large\n  Language Models","summary":"  Large language models (LLMs) have recently been introduced to graph learning,\naiming to extend their zero-shot generalization success to tasks where labeled\ngraph data is scarce. Among these applications, inference over text-attributed\ngraphs (TAGs) presents unique challenges: existing methods struggle with LLMs'\nlimited context length for processing large node neighborhoods and the\nmisalignment between node embeddings and the LLM token space. To address these\nissues, we establish two key principles for ensuring generalization and derive\nthe framework LLM-BP accordingly: (1) Unifying the attribute space with\ntask-adaptive embeddings, where we leverage LLM-based encoders and task-aware\nprompting to enhance generalization of the text attribute embeddings; (2)\nDeveloping a generalizable graph information aggregation mechanism, for which\nwe adopt belief propagation with LLM-estimated parameters that adapt across\ngraphs. Evaluations on 11 real-world TAG benchmarks demonstrate that LLM-BP\nsignificantly outperforms existing approaches, achieving 8.10% improvement with\ntask-conditional embeddings and an additional 1.71% gain from adaptive\naggregation.\n","authors":["Haoyu Wang","Shikun Liu","Rongzhe Wei","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2502.11836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11828v1","updated":"2025-02-17T14:25:33Z","published":"2025-02-17T14:25:33Z","title":"Intersectional Fairness in Reinforcement Learning with Large State and\n  Constraint Spaces","summary":"  In traditional reinforcement learning (RL), the learner aims to solve a\nsingle objective optimization problem: find the policy that maximizes expected\nreward. However, in many real-world settings, it is important to optimize over\nmultiple objectives simultaneously. For example, when we are interested in\nfairness, states might have feature annotations corresponding to multiple\n(intersecting) demographic groups to whom reward accrues, and our goal might be\nto maximize the reward of the group receiving the minimal reward. In this work,\nwe consider a multi-objective optimization problem in which each objective is\ndefined by a state-based reweighting of a single scalar reward function. This\ngeneralizes the problem of maximizing the reward of the minimum reward group.\nWe provide oracle-efficient algorithms to solve these multi-objective RL\nproblems even when the number of objectives is exponentially large-for tabular\nMDPs, as well as for large MDPs when the group functions have additional\nstructure. Finally, we experimentally validate our theoretical results and\ndemonstrate applications on a preferential attachment graph MDP.\n","authors":["Eric Eaton","Marcel Hussing","Michael Kearns","Aaron Roth","Sikata Bela Sengupta","Jessica Sorrell"],"pdf_url":"https://arxiv.org/pdf/2502.11828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14467v2","updated":"2025-02-17T14:21:20Z","published":"2024-11-18T15:46:39Z","title":"Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device\n  Triggers for Insect Camera Traps","summary":"  Camera traps, combined with AI, have emerged as a way to achieve automated,\nscalable biodiversity monitoring. However, the passive infrared (PIR) sensors\nthat trigger camera traps are poorly suited for detecting small, fast-moving\nectotherms such as insects. Insects comprise over half of all animal species\nand are key components of ecosystems and agriculture. The need for an\nappropriate and scalable insect camera trap is critical in the wake of\nconcerning reports of declines in insect populations. This study proposes an\nalternative to the PIR trigger: ultra-lightweight convolutional neural networks\nrunning on low-powered hardware to detect insects in a continuous stream of\ncaptured images. We train a suite of models to distinguish insect images from\nbackgrounds. Our design achieves zero latency between trigger and image\ncapture. Our models are rigorously tested and achieve high accuracy ranging\nfrom 91.8% to 96.4% AUC on validation data and >87% AUC on data from\ndistributions unseen during training. The high specificity of our models\nensures minimal saving of false positive images, maximising deployment storage\nefficiency. High recall scores indicate a minimal false negative rate,\nmaximising insect detection. Further analysis with saliency maps shows the\nlearned representation of our models to be robust, with low reliance on\nspurious background features. Our system is also shown to operate deployed on\noff-the-shelf, low-powered microcontroller units, consuming a maximum power\ndraw of less than 300mW. This enables longer deployment times using cheap and\nreadily available battery components. Overall we offer a step change in the\ncost, efficiency and scope of insect monitoring. Solving the challenging\ntrigger problem, we demonstrate a system which can be deployed for far longer\nthan existing designs and budgets power and bandwidth effectively, moving\ntowards a generic insect camera trap.\n","authors":["Ross Gardiner","Sareh Rowands","Benno I. Simmons"],"pdf_url":"https://arxiv.org/pdf/2411.14467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11817v1","updated":"2025-02-17T14:09:51Z","published":"2025-02-17T14:09:51Z","title":"AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling","summary":"  Knowledge Tracing (KT) aims to predict students' future performances based on\ntheir former exercises and additional information in educational settings. KT\nhas received significant attention since it facilitates personalized\nexperiences in educational situations. Simultaneously, the autoregressive\nmodeling on the sequence of former exercises has been proven effective for this\ntask. One of the primary challenges in autoregressive modeling for Knowledge\nTracing is effectively representing the anterior (pre-response) and posterior\n(post-response) states of learners across exercises. Existing methods often\nemploy complex model architectures to update learner states using question and\nresponse records. In this study, we propose a novel perspective on knowledge\ntracing task by treating it as a generative process, consistent with the\nprinciples of autoregressive models. We demonstrate that knowledge states can\nbe directly represented through autoregressive encodings on a question-response\nalternate sequence, where model generate the most probable representation in\nhidden state space by analyzing history interactions. This approach underpins\nour framework, termed Alternate Autoregressive Knowledge Tracing (AAKT).\nAdditionally, we incorporate supplementary educational information, such as\nquestion-related skills, into our framework through an auxiliary task, and\ninclude extra exercise details, like response time, as additional inputs. Our\nproposed framework is implemented using advanced autoregressive technologies\nfrom Natural Language Generation (NLG) for both training and prediction.\nEmpirical evaluations on four real-world KT datasets indicate that AAKT\nconsistently outperforms all baseline models in terms of AUC, ACC, and RMSE.\nFurthermore, extensive ablation studies and visualized analysis validate the\neffectiveness of key components in AAKT.\n","authors":["Hao Zhou","Wenge Rong","Jianfei Zhang","Qing Sun","Yuanxin Ouyang","Zhang Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11816v1","updated":"2025-02-17T14:06:36Z","published":"2025-02-17T14:06:36Z","title":"IMTS-Mixer: Mixer-Networks for Irregular Multivariate Time Series\n  Forecasting","summary":"  Forecasting Irregular Multivariate Time Series (IMTS) has recently emerged as\na distinct research field, necessitating specialized models to address its\nunique challenges. While most forecasting literature assumes regularly spaced\nobservations without missing values, many real-world datasets - particularly in\nhealthcare, climate research, and biomechanics - violate these assumptions.\nTime Series (TS)-mixer models have achieved remarkable success in regular\nmultivariate time series forecasting. However, they remain unexplored for IMTS\ndue to their requirement for complete and evenly spaced observations. To bridge\nthis gap, we introduce IMTS-Mixer, a novel forecasting architecture designed\nspecifically for IMTS. Our approach retains the core principles of TS mixer\nmodels while introducing innovative methods to transform IMTS into fixed-size\nmatrix representations, enabling their seamless integration with mixer modules.\nWe evaluate IMTS-Mixer on a benchmark of four real-world datasets from various\ndomains. Our results demonstrate that IMTS-Mixer establishes a new\nstate-of-the-art in forecasting accuracy while also improving computational\nefficiency.\n","authors":["Christian Kl√∂tergens","Tim Dernedde","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2502.11816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v1","updated":"2025-02-17T13:59:41Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2411.08133v3","updated":"2025-02-17T13:50:54Z","published":"2024-11-12T19:24:42Z","title":"Impactful Bit-Flip Search on Full-precision Models","summary":"  Neural networks have shown remarkable performance in various tasks, yet they\nremain susceptible to subtle changes in their input or model parameters. One\nparticularly impactful vulnerability arises through the Bit-Flip Attack (BFA),\nwhere flipping a small number of critical bits in a model's parameters can\nseverely degrade its performance. A common technique for inducing bit flips in\nDRAM is the Row-Hammer attack, which exploits frequent uncached memory accesses\nto alter data. Identifying susceptible bits can be achieved through exhaustive\nsearch or progressive layer-by-layer analysis, especially in quantized\nnetworks. In this work, we introduce Impactful Bit-Flip Search (IBS), a novel\nmethod for efficiently pinpointing and flipping critical bits in full-precision\nnetworks. Additionally, we propose a Weight-Stealth technique that\nstrategically modifies the model's parameters in a way that maintains the float\nvalues within the original distribution, thereby bypassing simple range checks\noften used in tamper detection.\n","authors":["Nadav Benedek","Matan Levy","Mahmood Sharif"],"pdf_url":"https://arxiv.org/pdf/2411.08133v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23918v3","updated":"2025-02-17T13:50:17Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10020v2","updated":"2025-02-17T13:48:42Z","published":"2025-02-14T09:01:12Z","title":"Improved Online Confidence Bounds for Multinomial Logistic Bandits","summary":"  In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $ for sufficiently large\n$T$, where $\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$\nis the dimension of the contexts, and $T$ is the total number of rounds.\nFurthermore, we introduce a Maximum Likelihood Estimation (MLE)-based\nalgorithm, OFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O\n\\Big( d \\log (BT) \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2502.10020v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.11801v1","updated":"2025-02-17T13:46:47Z","published":"2025-02-17T13:46:47Z","title":"3D Gaussian Inpainting with Depth-Guided Cross-View Consistency","summary":"  When performing 3D inpainting using novel-view rendering methods like Neural\nRadiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture\nand geometry consistency across camera views has been a challenge. In this\npaper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided\nCross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided\nby the rendered depth information from each training view, our 3DGIC exploits\nbackground pixels visible across different views for updating the inpainting\nmask, allowing us to refine the 3DGS for inpainting purposes.Through extensive\nexperiments on benchmark datasets, we confirm that our 3DGIC outperforms\ncurrent state-of-the-art 3D inpainting methods quantitatively and\nqualitatively.\n","authors":["Sheng-Yu Huang","Zi-Ting Chou","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07930v2","updated":"2025-02-17T13:45:03Z","published":"2024-10-10T13:57:27Z","title":"Cost-aware simulation-based inference","summary":"  Simulation-based inference (SBI) is the preferred framework for estimating\nparameters of intractable models in science and engineering. A significant\nchallenge in this context is the large computational cost of simulating data\nfrom complex models, and the fact that this cost often depends on parameter\nvalues. We therefore propose \\textit{cost-aware SBI methods} which can\nsignificantly reduce the cost of existing sampling-based SBI methods, such as\nneural SBI and approximate Bayesian computation. This is achieved through a\ncombination of rejection and self-normalised importance sampling, which\nsignificantly reduces the number of expensive simulations needed. Our approach\nis studied extensively on models from epidemiology to telecommunications\nengineering, where we obtain significant reductions in the overall cost of\ninference.\n","authors":["Ayush Bharti","Daolang Huang","Samuel Kaski","Fran√ßois-Xavier Briol"],"pdf_url":"https://arxiv.org/pdf/2410.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07758v2","updated":"2025-02-17T13:44:46Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Drago≈ü Du≈üe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander R√∂lle","Christina C. Westhoff","B√©n√©dicte Lenoir","Niels Halama","Inka Z√∂rnig","Dirk J√§ger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v2.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.07739v2","updated":"2025-02-17T13:39:51Z","published":"2025-02-11T17:59:35Z","title":"HRP: High-Rank Preheating for Superior LoRA Initialization","summary":"  This paper studies the crucial impact of initialization on the convergence\nproperties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that\nrandom initialization, a widely used schema, will likely lead LoRA to random\nlow-rank results, rather than the best low-rank result. While this issue can be\nmitigated by adjusting initialization towards a well-informed direction, it\nrelies on prior knowledge of the target, which is typically unknown in\nreal-world scenarios. To approximate this well-informed initial direction, we\npropose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few\nsteps and uses the singular value decomposition of the preheated result as a\nsuperior initialization. HRP initialization is theory-supported to combine the\nconvergence strengths of high-rank LoRA and the generalization strengths of\nlow-rank LoRA. Extensive experiments demonstrate that HRP significantly\nenhances LoRA's effectiveness across various models and tasks, achieving\nperformance comparable to full-parameter fine-tuning and outperforming other\ninitialization strategies.\n","authors":["Yuzhu Chen","Yingjie Wang","Shi Fu","Li Shen","Yongcheng Jing","Xinmei Tian","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.07739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01434v2","updated":"2025-02-17T13:30:15Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions through\nsubnetworks that can be composed to perform more complex tasks. Recent advances\nin mechanistic interpretability have made progress in identifying\n$\\textit{circuits}$, which represent the minimal computational subgraphs\nresponsible for a model's behavior on specific tasks. However, most studies\nfocus on identifying circuits for individual tasks without investigating how\nfunctionally similar circuits $\\textit{relate}$ to each other. To address this\ngap, we study the modularity of neural networks by analyzing circuits for\nhighly compositional subtasks within a transformer-based language model.\nSpecifically, given a probabilistic context-free grammar, we identify and\ncompare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through set operations to represent more\ncomplex functional model capabilities.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v2.pdf","comment":"22 pages, 21 figures"},{"id":"http://arxiv.org/abs/2501.07824v2","updated":"2025-02-17T13:26:52Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08474v2","updated":"2025-02-17T13:22:54Z","published":"2024-09-13T02:00:16Z","title":"Rethinking Meta-Learning from a Learning Lens","summary":"  Meta-learning has emerged as a powerful approach for leveraging knowledge\nfrom previous tasks to solve new tasks. The mainstream methods focus on\ntraining a well-generalized model initialization, which is then adapted to\ndifferent tasks with limited data and updates. However, it pushes the model\noverfitting on the training tasks. Previous methods mainly attributed this to\nthe lack of data and used augmentations to address this issue, but they were\nlimited by sufficient training and effective augmentation strategies. In this\nwork, we focus on the more fundamental learning to learn strategy of\nmeta-learning to explore what causes errors and how to eliminate these errors\nwithout changing the environment. Specifically, we first rethink the\nalgorithmic procedure of meta-learning from a learning lens. Through\ntheoretical and empirical analyses, we find that (i) this paradigm faces the\nrisk of both overfitting and underfitting and (ii) the model adapted to\ndifferent tasks promote each other where the effect is stronger if the tasks\nare more similar. Based on this insight, we propose using task relations to\ncalibrate the optimization process of meta-learning and propose a plug-and-play\nmethod called Task Relation Learner (TRLearner) to achieve this goal.\nSpecifically, it first obtains task relation matrices from the extracted\ntask-specific meta-data. Then, it uses the obtained matrices with\nrelation-aware consistency regularization to guide optimization. Extensive\ntheoretical and empirical analyses demonstrate the effectiveness of TRLearner.\n","authors":["Jingyao Wang","Wenwen Qiang","Chuxiong Sun","Changwen Zheng","Jiangmeng Li"],"pdf_url":"https://arxiv.org/pdf/2409.08474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01220v2","updated":"2025-02-17T13:20:37Z","published":"2025-02-03T10:24:55Z","title":"Language Models Struggle to Achieve a Consistent Temporal Representation\n  of Facts","summary":"  Language Models (LMs) have shown substantial improvements in handling factual\nknowledge, yet their capability to consistently represent temporal facts, which\nare valid only within specific timeframes, remains underexplored. To\ninvestigate this, we introduce TimeStress, a novel dataset comprising 521K\nstatements on 2003 of the most popular temporal facts in Wikidata. Each\nstatement contextualizes a fact with correct and incorrect dates across three\nprecisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to\ndiscern between correct and incorrect temporal statements based on their\nprobability of being generated. We assess 18 LMs across various architectures\nusing two metrics: the win rate, indicating how often correct dates outperform\nincorrect ones, and robustness, reflecting consistent performance across all\ndates. Our findings reveal that while some LMs achieve a win rate exceeding\n80\\%, robustness remains low, with the best model achieving only 6\\%.\nFurthermore, robust knowledge at one date precision does not reliably transfer\nto others, highlighting a significant generalization gap. These results\nunderscore the struggle of LMs to maintain a consistent temporal\nrepresentation, supporting their limitations as reliable sources of temporal\nknowledge. We provide all data and code for further research.\n","authors":["Hichem Ammar Khodja","Fr√©d√©ric B√©chet","Quentin Brabant","Alexis Nasr","Gw√©nol√© Lecorv√©"],"pdf_url":"https://arxiv.org/pdf/2502.01220v2.pdf","comment":"preprint v2"},{"id":"http://arxiv.org/abs/2405.15506v3","updated":"2025-02-17T13:19:33Z","published":"2024-05-24T12:51:23Z","title":"Learning to Discretize Denoising Diffusion ODEs","summary":"  Diffusion Probabilistic Models (DPMs) are generative models showing\ncompetitive performance in various domains, including image synthesis and 3D\npoint cloud generation. Sampling from pre-trained DPMs involves multiple neural\nfunction evaluations (NFEs) to transform Gaussian noise samples into images,\nresulting in higher computational costs compared to single-step generative\nmodels such as GANs or VAEs. Therefore, reducing the number of NFEs while\npreserving generation quality is crucial. To address this, we propose LD3, a\nlightweight framework designed to learn the optimal time discretization for\nsampling. LD3 can be combined with various samplers and consistently improves\ngeneration quality without having to retrain resource-intensive neural\nnetworks. We demonstrate analytically and empirically that LD3 improves\nsampling efficiency with much less computational overhead. We evaluate our\nmethod with extensive experiments on 7 pre-trained models, covering\nunconditional and conditional sampling in both pixel-space and latent-space\nDPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional\nCIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient\napproach to sampling from pre-trained diffusion models. Code is available at\nhttps://github.com/vinhsuhi/LD3.\n","authors":["Vinh Tong","Trung-Dung Hoang","Anji Liu","Guy Van den Broeck","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2405.15506v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11778v1","updated":"2025-02-17T13:13:16Z","published":"2025-02-17T13:13:16Z","title":"Private Synthetic Graph Generation and Fused Gromov-Wasserstein Distance","summary":"  Networks are popular for representing complex data. In particular,\ndifferentially private synthetic networks are much in demand for method and\nalgorithm development. The network generator should be easy to implement and\nshould come with theoretical guarantees. Here we start with complex data as\ninput and jointly provide a network representation as well as a synthetic\nnetwork generator. Using a random connection model, we devise an effective\nalgorithmic approach for generating attributed synthetic graphs which is\n$\\epsilon$-differentially private at the vertex level, while preserving utility\nunder an appropriate notion of distance which we develop. We provide\ntheoretical guarantees for the accuracy of the private synthetic graphs using\nthe fused Gromov-Wasserstein distance, which extends the Wasserstein metric to\nstructured data. Our method draws inspiration from the PSMM method of\n\\citet{he2023}.\n","authors":["Leoni Carla Wirth","Gholamali Aminian","Gesine Reinert"],"pdf_url":"https://arxiv.org/pdf/2502.11778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06650v3","updated":"2025-02-17T13:11:43Z","published":"2024-11-11T01:34:10Z","title":"Quantum Policy Gradient in Reproducing Kernel Hilbert Space","summary":"  Parametrised quantum circuits offer expressive and data-efficient\nrepresentations for machine learning. Due to quantum states residing in a\nhigh-dimensional Hilbert space, parametrised quantum circuits have a natural\ninterpretation in terms of kernel methods. The representation of quantum\ncircuits in terms of quantum kernels has been studied widely in quantum\nsupervised learning, but has been overlooked in the context of quantum RL. This\npaper proposes parametric and non-parametric policy gradient and actor-critic\nalgorithms with quantum kernel policies in quantum environments. This approach,\nimplemented with both numerical and analytical quantum policy gradient\ntechniques, allows exploiting the many advantages of kernel methods, including\navailable analytic forms for the gradient of the policy and tunable\nexpressiveness. The proposed approach is suitable for vector-valued action\nspaces and each of the formulations demonstrates a quadratic reduction in query\ncomplexity compared to their classical counterparts. Two actor-critic\nalgorithms, one based on stochastic policy gradient and one based on\ndeterministic policy gradient (comparable to the popular DDPG algorithm),\ndemonstrate additional query complexity reductions compared to quantum policy\ngradient algorithms under favourable conditions.\n","authors":["David M. Bossens","Kishor Bharti","Jayne Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.06650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11774v1","updated":"2025-02-17T13:07:37Z","published":"2025-02-17T13:07:37Z","title":"Interpretable Machine Learning for Kronecker Coefficients","summary":"  We analyze the saliency of neural networks and employ interpretable machine\nlearning models to predict whether the Kronecker coefficients of the symmetric\ngroup are zero or not. Our models use triples of partitions as input features,\nas well as b-loadings derived from the principal component of an embedding that\ncaptures the differences between partitions. Across all approaches, we achieve\nan accuracy of approximately 83% and derive explicit formulas for a decision\nfunction in terms of b-loadings. Additionally, we develop transformer-based\nmodels for prediction, achieving the highest reported accuracy of over 99%.\n","authors":["Giorgi Butbaia","Kyu-Hwan Lee","Fabian Ruehle"],"pdf_url":"https://arxiv.org/pdf/2502.11774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10434v2","updated":"2025-02-17T13:07:21Z","published":"2023-02-21T04:15:34Z","title":"Kernel-Based Distributed Q-Learning: A Scalable Reinforcement Learning\n  Approach for Dynamic Treatment Regimes","summary":"  In recent years, large amounts of electronic health records (EHRs) concerning\nchronic diseases have been collected to facilitate medical diagnosis. Modeling\nthe dynamic properties of EHRs related to chronic diseases can be efficiently\ndone using dynamic treatment regimes (DTRs). While reinforcement learning (RL)\nis a widely used method for creating DTRs, there is ongoing research in\ndeveloping RL algorithms that can effectively handle large amounts of data. In\nthis paper, we present a scalable kernel-based distributed Q-learning algorithm\nfor generating DTRs. We perform both theoretical assessments and numerical\nanalysis for the proposed approach. The results demonstrate that our algorithm\nsignificantly reduces the computational complexity associated with the\nstate-of-the-art deep reinforcement learning methods, while maintaining\ncomparable generalization performance in terms of accumulated rewards across\nstages, such as survival time or cumulative survival probability.\n","authors":["Di Wang","Yao Wang","Shao-Bo Lin"],"pdf_url":"https://arxiv.org/pdf/2302.10434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04064v2","updated":"2025-02-17T13:04:24Z","published":"2024-10-05T07:25:56Z","title":"Text2Chart31: Instruction Tuning for Chart Generation with Automatic\n  Feedback","summary":"  Large language models (LLMs) have demonstrated strong capabilities across\nvarious language tasks, notably through instruction-tuning methods. However,\nLLMs face challenges in visualizing complex, real-world data through charts and\nplots. Firstly, existing datasets rarely cover a full range of chart types,\nsuch as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning\nmethods do not fully leverage the intricate relationships within rich datasets,\nincluding text, code, and figures. To address these challenges, we propose a\nhierarchical pipeline and a new dataset for chart generation. Our dataset,\nText2Chart31, includes 31 unique plot types referring to the Matplotlib\nlibrary, with 11.1K tuples of descriptions, code, data tables, and plots.\nMoreover, we introduce a reinforcement learning-based instruction tuning\ntechnique for chart generation tasks without requiring human feedback. Our\nexperiments show that this approach significantly enhances the model\nperformance, enabling smaller models to outperform larger open-source models\nand be comparable to state-of-the-art proprietary models in data visualization\ntasks. We make the code and dataset available at\nhttps://github.com/fatemehpesaran310/Text2Chart31.\n","authors":["Fatemeh Pesaran Zadeh","Juyeon Kim","Jin-Hwa Kim","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04064v2.pdf","comment":"EMNLP 2024 Main Oral. Code and dataset are released at\n  https://github.com/fatemehpesaran310/Text2Chart31"},{"id":"http://arxiv.org/abs/2502.11767v1","updated":"2025-02-17T12:58:17Z","published":"2025-02-17T12:58:17Z","title":"From Selection to Generation: A Survey of LLM-based Active Learning","summary":"  Active Learning (AL) has been a powerful paradigm for improving model\nefficiency and performance by selecting the most informative data points for\nlabeling and training. In recent active learning frameworks, Large Language\nModels (LLMs) have been employed not only for selection but also for generating\nentirely new data instances and providing more cost-effective annotations.\nMotivated by the increasing importance of high-quality data and efficient model\ntraining in the era of LLMs, we present a comprehensive survey on LLM-based\nActive Learning. We introduce an intuitive taxonomy that categorizes these\ntechniques and discuss the transformative roles LLMs can play in the active\nlearning loop. We further examine the impact of AL on LLM learning paradigms\nand its applications across various domains. Finally, we identify open\nchallenges and propose future research directions. This survey aims to serve as\nan up-to-date resource for researchers and practitioners seeking to gain an\nintuitive understanding of LLM-based AL techniques and deploy them to new\napplications.\n","authors":["Yu Xia","Subhojyoti Mukherjee","Zhouhang Xie","Junda Wu","Xintong Li","Ryan Aponte","Hanjia Lyu","Joe Barrow","Hongjie Chen","Franck Dernoncourt","Branislav Kveton","Tong Yu","Ruiyi Zhang","Jiuxiang Gu","Nesreen K. Ahmed","Yu Wang","Xiang Chen","Hanieh Deilamsalehy","Sungchul Kim","Zhengmian Hu","Yue Zhao","Nedim Lipka","Seunghyun Yoon","Ting-Hao Kenneth Huang","Zichao Wang","Puneet Mathur","Soumyabrata Pal","Koyel Mukherjee","Zhehao Zhang","Namyong Park","Thien Huu Nguyen","Jiebo Luo","Ryan A. Rossi","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11756v1","updated":"2025-02-17T12:52:10Z","published":"2025-02-17T12:52:10Z","title":"On the Computation of the Fisher Information in Continual Learning","summary":"  One of the most popular methods for continual learning with deep neural\nnetworks is Elastic Weight Consolidation (EWC), which involves computing the\nFisher Information. The exact way in which the Fisher Information is computed\nis however rarely described, and multiple different implementations for it can\nbe found online. This blog post discusses and empirically compares several\noften-used implementations, which highlights that many currently reported\nresults for EWC could likely be improved by changing the way the Fisher\nInformation is computed.\n","authors":["Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2502.11756v1.pdf","comment":"To appear in the blogpost track at ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01053v4","updated":"2025-02-17T12:50:31Z","published":"2024-05-02T07:15:23Z","title":"On the Universality of Self-Supervised Representation Learning","summary":"  In this paper, we investigate the characteristics that define a good\nrepresentation or model. We propose that such a representation or model should\npossess universality, characterized by: (i) discriminability: performing well\non training samples; (ii) generalization: performing well on unseen datasets;\nand (iii) transferability: performing well on unseen tasks with distribution\nshifts. Despite its importance, current self-supervised learning (SSL) methods\nlack explicit modeling of universality, and theoretical analysis remains\nunderexplored. To address these issues, we aim to explore and incorporate\nuniversality into SSL. Specifically, we first revisit SSL from a task\nperspective and find that each mini-batch can be viewed as a multi-class\nclassification task. We then propose that a universal SSL model should achieve:\n(i) learning universality by minimizing loss across all training samples, and\n(ii) evaluation universality by learning causally invariant representations\nthat generalize well to unseen tasks. To quantify this, we introduce a\n$\\sigma$-measurement that assesses the gap between the performance of SSL model\nand optimal task-specific models. Furthermore, to model universality, we\npropose the GeSSL framework. It first learns task-specific models by minimizing\nSSL loss, then incorporates future updates to enhance discriminability, and\nfinally integrates these models to learn from multiple tasks. Theoretical and\nempirical evidence supports the effectiveness of GeSSL.\n","authors":["Wenwen Qiang","Jingyao Wang","Lingyu Si","Chuxiong Sun","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2405.01053v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11743v1","updated":"2025-02-17T12:30:05Z","published":"2025-02-17T12:30:05Z","title":"Robust Partial-Label Learning by Leveraging Class Activation Values","summary":"  Real-world training data is often noisy; for example, human annotators assign\nconflicting class labels to the same instances. Partial-label learning (PLL) is\na weakly supervised learning paradigm that allows training classifiers in this\ncontext without manual data cleaning. While state-of-the-art methods have good\npredictive performance, their predictions are sensitive to high noise levels,\nout-of-distribution data, and adversarial perturbations. We propose a novel PLL\nmethod based on subjective logic, which explicitly represents uncertainty by\nleveraging the magnitudes of the underlying neural network's class activation\nvalues. Thereby, we effectively incorporate prior knowledge about the class\nlabels by using a novel label weight re-distribution strategy that we prove to\nbe optimal. We empirically show that our method yields more robust predictions\nin terms of predictive performance under high PLL noise levels, handling\nout-of-distribution examples, and handling adversarial perturbations on the\ntest instances.\n","authors":["Tobias Fuchs","Florian Kalinke"],"pdf_url":"https://arxiv.org/pdf/2502.11743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11740v1","updated":"2025-02-17T12:26:34Z","published":"2025-02-17T12:26:34Z","title":"Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via\n  Modality-decoupled Gradient Descent","summary":"  Recent MLLMs have shown emerging visual understanding and reasoning abilities\nafter being pre-trained on large-scale multimodal datasets. Unlike\npre-training, where MLLMs receive rich visual-text alignment,\ninstruction-tuning is often text-driven with weaker visual supervision, leading\nto the degradation of pre-trained visual understanding and causing visual\nforgetting. Existing approaches, such as direct fine-tuning and continual\nlearning methods, fail to explicitly address this issue, often compressing\nvisual representations and prioritizing task alignment over visual retention,\nwhich further worsens visual forgetting. To overcome this limitation, we\nintroduce a novel perspective leveraging effective rank to quantify the\ndegradation of visual representation richness, interpreting this degradation\nthrough the information bottleneck principle as excessive compression that\nleads to the degradation of crucial pre-trained visual knowledge. Building on\nthis view, we propose a modality-decoupled gradient descent (MDGD) method that\nregulates gradient updates to maintain the effective rank of visual\nrepresentations while mitigating the over-compression effects described by the\ninformation bottleneck. By explicitly disentangling the optimization of visual\nunderstanding from task-specific alignment, MDGD preserves pre-trained visual\nknowledge while enabling efficient task adaptation. To enable lightweight\ninstruction-tuning, we further develop a memory-efficient fine-tuning approach\nusing gradient masking, which selectively updates a subset of model parameters\nto enable parameter-efficient fine-tuning (PEFT), reducing computational\noverhead while preserving rich visual representations. Extensive experiments\nacross various downstream tasks and backbone MLLMs demonstrate that MDGD\neffectively mitigates visual forgetting from pre-trained tasks while enabling\nstrong adaptation to new tasks.\n","authors":["Junda Wu","Yuxin Xiong","Xintong Li","Yu Xia","Ruoyu Wang","Yu Wang","Tong Yu","Sungchul Kim","Ryan A. Rossi","Lina Yao","Jingbo Shang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.11740v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2309.06089v3","updated":"2025-02-17T12:17:35Z","published":"2023-09-12T09:37:08Z","title":"Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms:\n  Exploring Tuning Strategies","summary":"  The cross-lingual transfer is a promising technique to solve tasks in\nless-resourced languages. In this empirical study, we compare two fine-tuning\napproaches combined with zero-shot and full-shot learning approaches for large\nlanguage models in a cross-lingual setting. As fine-tuning strategies, we\ncompare parameter-efficient adapter methods with fine-tuning of all parameters.\nAs cross-lingual transfer strategies, we compare the intermediate-training\n(\\textit{IT}) that uses each language sequentially and cross-lingual validation\n(\\textit{CLV}) that uses a target language already in the validation phase of\nfine-tuning. We assess the success of transfer and the extent of catastrophic\nforgetting in a source language due to cross-lingual transfer, i.e., how much\npreviously acquired knowledge is lost when we learn new information in a\ndifferent language. The results on two different classification problems, hate\nspeech detection and product reviews, each containing datasets in several\nlanguages, show that the \\textit{IT} cross-lingual strategy outperforms\n\\textit{CLV} for the target language. Our findings indicate that, in the\nmajority of cases, the \\textit{CLV} strategy demonstrates superior retention of\nknowledge in the base language (English) compared to the \\textit{IT} strategy,\nwhen evaluating catastrophic forgetting in multiple cross-lingual transfers.\n","authors":["Boshko Koloski","Bla≈æ ≈†krlj","Marko Robnik-≈†ikonja","Senja Pollak"],"pdf_url":"https://arxiv.org/pdf/2309.06089v3.pdf","comment":"Accepted to IEEE Access"},{"id":"http://arxiv.org/abs/2502.11725v1","updated":"2025-02-17T12:11:01Z","published":"2025-02-17T12:11:01Z","title":"Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual\n  Metrics","summary":"  Measuring perceptual similarity is a key tool in computer vision. In recent\nyears perceptual metrics based on features extracted from neural networks with\nlarge and diverse training sets, e.g. CLIP, have become popular. At the same\ntime, the metrics extracted from features of neural networks are not\nadversarially robust. In this paper we show that adversarially robust CLIP\nmodels, called R-CLIP$_\\textrm{F}$, obtained by unsupervised adversarial\nfine-tuning induce a better and adversarially robust perceptual metric that\noutperforms existing metrics in a zero-shot setting, and further matches the\nperformance of state-of-the-art metrics while being robust after fine-tuning.\nMoreover, our perceptual metric achieves strong performance on related tasks\nsuch as robust image-to-image retrieval, which becomes especially relevant when\napplied to \"Not Safe for Work\" (NSFW) content detection and dataset filtering.\nWhile standard perceptual metrics can be easily attacked by a small\nperturbation completely degrading NSFW detection, our robust perceptual metric\nmaintains high accuracy under an attack while having similar performance for\nunperturbed images. Finally, perceptual metrics induced by robust CLIP models\nhave higher interpretability: feature inversion can show which images are\nconsidered similar, while text inversion can find what images are associated to\na given prompt. This also allows us to visualize the very rich visual concepts\nlearned by a CLIP model, including memorized persons, paintings and complex\nqueries.\n","authors":["Francesco Croce","Christian Schlarmann","Naman Deep Singh","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2502.11725v1.pdf","comment":"This work has been accepted for publication in the IEEE Conference on\n  Secure and Trustworthy Machine Learning (SaTML). The final version will be\n  available on IEEE Xplore"},{"id":"http://arxiv.org/abs/2408.08590v2","updated":"2025-02-17T12:09:50Z","published":"2024-08-16T07:47:39Z","title":"A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive\n  Language Models","summary":"  Recent studies on logical reasoning in Language Models (LMs) have sparked a\ndebate on whether they can learn systematic reasoning principles during\npre-training or merely exploit superficial patterns in the training data. This\npaper presents a mechanistic interpretation of syllogistic reasoning in LMs to\nadvance the understanding of internal dynamics. Specifically, we present a\nmethodology for circuit discovery aimed at interpreting content-independent\nreasoning mechanisms. Through two distinct intervention methods, we uncover a\nsufficient and necessary circuit involving middle-term suppression that\nelucidates how LMs transfer information to derive valid conclusions from\npremises. Furthermore, we investigate how belief biases manifest in syllogistic\nreasoning, finding evidence of partial contamination from additional attention\nheads responsible for encoding commonsense and contextualized knowledge.\nFinally, we explore the generalization of the discovered mechanisms across\nvarious syllogistic schemes, model sizes and architectures, finding that the\nidentified circuit is sufficient and necessary for the schemes on which the\nmodels achieve high downstream accuracy (> 60%), and that the activation\npatterns apply to models of different families. Overall, our findings suggest\nthat LMs indeed learn transferable content-independent reasoning mechanisms,\nbut that, at the same time, such mechanisms do not involve generalizable and\nabstract logical primitives, being susceptible to contamination by the same\nworld knowledge acquired during pre-training.\n","authors":["Geonhee Kim","Marco Valentino","Andr√© Freitas"],"pdf_url":"https://arxiv.org/pdf/2408.08590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11715v1","updated":"2025-02-17T12:00:28Z","published":"2025-02-17T12:00:28Z","title":"Proactive Depot Discovery: A Generative Framework for Flexible\n  Location-Routing","summary":"  The Location-Routing Problem (LRP), which combines the challenges of facility\n(depot) locating and vehicle route planning, is critically constrained by the\nreliance on predefined depot candidates, limiting the solution space and\npotentially leading to suboptimal outcomes. Previous research on LRP without\npredefined depots is scant and predominantly relies on heuristic algorithms\nthat iteratively attempt depot placements across a planar area. Such approaches\nlack the ability to proactively generate depot locations that meet specific\ngeographic requirements, revealing a notable gap in current research landscape.\nTo bridge this gap, we propose a data-driven generative DRL framework, designed\nto proactively generate depots for LRP without predefined depot candidates,\nsolely based on customer requests data which include geographic and demand\ninformation. It can operate in two distinct modes: direct generation of exact\ndepot locations, and the creation of a multivariate Gaussian distribution for\nflexible depots sampling. By extracting depots' geographic pattern from\ncustomer requests data, our approach can dynamically respond to logistical\nneeds, identifying high-quality depot locations that further reduce total\nrouting costs compared to traditional methods. Extensive experiments\ndemonstrate that, for a same group of customer requests, compared with those\ndepots identified through random attempts, our framework can proactively\ngenerate depots that lead to superior solution routes with lower routing cost.\nThe implications of our framework potentially extend into real-world\napplications, particularly in emergency medical rescue and disaster relief\nlogistics, where rapid establishment and adjustment of depot locations are\nparamount, showcasing its potential in addressing LRP for dynamic and\nunpredictable environments.\n","authors":["Site Qu","Guoqiang Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04453v3","updated":"2025-02-17T11:58:32Z","published":"2024-03-07T12:45:51Z","title":"Efficient Off-Policy Learning for High-Dimensional Action Spaces","summary":"  Existing off-policy reinforcement learning algorithms often rely on an\nexplicit state-action-value function representation, which can be problematic\nin high-dimensional action spaces due to the curse of dimensionality. This\nreliance results in data inefficiency as maintaining a state-action-value\nfunction in such spaces is challenging. We present an efficient approach that\nutilizes only a state-value function as the critic for off-policy deep\nreinforcement learning. This approach, which we refer to as Vlearn, effectively\ncircumvents the limitations of existing methods by eliminating the necessity\nfor an explicit state-action-value function. To this end, we leverage a\nweighted importance sampling loss for learning deep value functions from\noff-policy data. While this is common for linear methods, it has not been\ncombined with deep value function networks. This transfer to deep methods is\nnot straightforward and requires novel design choices such as robust policy\nupdates, twin value function networks to avoid an optimization bias, and\nimportance weight clipping. We also present a novel analysis of the variance of\nour estimate compared to commonly used importance sampling estimators such as\nV-trace. Our approach improves sample complexity as well as final performance\nand ensures consistent and robust performance across various benchmark tasks.\nEliminating the state-action-value function in Vlearn facilitates a streamlined\nlearning process, yielding high-return agents.\n","authors":["Fabian Otto","Philipp Becker","Ngo Anh Vien","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2403.04453v3.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11711v1","updated":"2025-02-17T11:53:58Z","published":"2025-02-17T11:53:58Z","title":"Knowledge-aware contrastive heterogeneous molecular graph learning","summary":"  Molecular representation learning is pivotal in predicting molecular\nproperties and advancing drug design. Traditional methodologies, which\npredominantly rely on homogeneous graph encoding, are limited by their\ninability to integrate external knowledge and represent molecular structures\nacross different levels of granularity. To address these limitations, we\npropose a paradigm shift by encoding molecular graphs into heterogeneous\nstructures, introducing a novel framework: Knowledge-aware Contrastive\nHeterogeneous Molecular Graph Learning (KCHML). This approach leverages\ncontrastive learning to enrich molecular representations with embedded external\nknowledge. KCHML conceptualizes molecules through three distinct graph\nviews-molecular, elemental, and pharmacological-enhanced by heterogeneous\nmolecular graphs and a dual message-passing mechanism. This design offers a\ncomprehensive representation for property prediction, as well as for downstream\ntasks such as drug-drug interaction (DDI) prediction. Extensive benchmarking\ndemonstrates KCHML's superiority over state-of-the-art molecular property\nprediction models, underscoring its ability to capture intricate molecular\nfeatures.\n","authors":["Mukun Chen","Jia Wu","Shirui Pan","Fu Lin","Bo Du","Xiuwen Gong","Wenbin Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10365v2","updated":"2025-02-17T11:45:52Z","published":"2025-02-14T18:43:22Z","title":"AffinityFlow: Guided Flows for Antibody Affinity Maturation","summary":"  Antibodies are widely used as therapeutics, but their development requires\ncostly affinity maturation, involving iterative mutations to enhance binding\naffinity.This paper explores a sequence-only scenario for affinity maturation,\nusing solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold\nwithin flow matching to generate diverse protein structures, enabling a\nsequence-conditioned generative model of structure. Building on this, we\npropose an alternating optimization framework that (1) fixes the sequence to\nguide structure generation toward high binding affinity using a structure-based\naffinity predictor, then (2) applies inverse folding to create sequence\nmutations, refined by a sequence-based affinity predictor for post selection. A\nkey challenge is the lack of labeled data for training both predictors. To\naddress this, we develop a co-teaching module that incorporates valuable\ninformation from noisy biophysical energies into predictor refinement. The\nsequence-based predictor selects consensus samples to teach the structure-based\npredictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art\nperformance in affinity maturation experiments. We plan to open-source our code\nafter acceptance.\n","authors":["Can Chen","Karla-Luise Herpoldt","Chenchao Zhao","Zichen Wang","Marcus Collins","Shang Shang","Ron Benson"],"pdf_url":"https://arxiv.org/pdf/2502.10365v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.11705v1","updated":"2025-02-17T11:44:11Z","published":"2025-02-17T11:44:11Z","title":"LLM Agents Making Agent Tools","summary":"  Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains which demand\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, a novel agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a short task\ndescription and a repository URL, ToolMaker autonomously installs required\ndependencies and generates code to perform the task, using a closed-loop\nself-correction mechanism to iteratively diagnose and rectify errors. To\nevaluate our approach, we introduce a benchmark comprising 15 diverse and\ncomplex computational tasks spanning both medical and non-medical domains with\nover 100 unit tests to objectively assess tool correctness and robustness.\nToolMaker correctly implements 80% of the tasks, substantially outperforming\ncurrent state-of-the-art software engineering agents. ToolMaker therefore is a\nstep towards fully autonomous agent-based scientific workflows.\n","authors":["Georg W√∂lflein","Dyke Ferber","Daniel Truhn","Ognjen Arandjeloviƒá","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2502.11705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03297v2","updated":"2025-02-17T11:42:24Z","published":"2025-02-05T15:56:26Z","title":"IRIS: An Immersive Robot Interaction System","summary":"  This paper introduces IRIS, an immersive Robot Interaction System leveraging\nExtended Reality (XR), designed for robot data collection and interaction\nacross multiple simulators, benchmarks, and real-world scenarios. While\nexisting XR-based data collection systems provide efficient and intuitive\nsolutions for large-scale data collection, they are often challenging to\nreproduce and reuse. This limitation arises because current systems are highly\ntailored to simulator-specific use cases and environments. IRIS is a novel,\neasily extendable framework that already supports multiple simulators,\nbenchmarks, and even headsets. Furthermore, IRIS is able to include additional\ninformation from real-world sensors, such as point clouds captured through\ndepth cameras. A unified scene specification is generated directly from\nsimulators or real-world sensors and transmitted to XR headsets, creating\nidentical scenes in XR. This specification allows IRIS to support any of the\nobjects, assets, and robots provided by the simulators. In addition, IRIS\nintroduces shared spatial anchors and a robust communication protocol that\nlinks simulations between multiple XR headsets. This feature enables multiple\nXR headsets to share a synchronized scene, facilitating collaborative and\nmulti-user data collection. IRIS can be deployed on any device that supports\nthe Unity Framework, encompassing the vast majority of commercially available\nheadsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and\nthe HoloLens 2. IRIS showcased its versatility across a wide range of\nreal-world and simulated scenarios, using current popular robot simulators such\nas MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study\nevaluates IRIS on a data collection task for the LIBERO benchmark. The study\nshows that IRIS significantly outperforms the baseline in both objective and\nsubjective metrics.\n","authors":["Xinkai Jiang","Qihao Yuan","Enes Ulas Dincer","Hongyi Zhou","Ge Li","Xueyin Li","Julius Haag","Nicolas Schreiber","Kailai Li","Gerhard Neumann","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2502.03297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11687v1","updated":"2025-02-17T11:25:28Z","published":"2025-02-17T11:25:28Z","title":"ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks\n  using Machine Unlearning","summary":"  Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.\n","authors":["Manaar Alam","Hithem Lamri","Michail Maniatakos"],"pdf_url":"https://arxiv.org/pdf/2502.11687v1.pdf","comment":"This paper is accepted at 62nd Design Automation Conference (DAC)\n  2025"},{"id":"http://arxiv.org/abs/2405.06368v4","updated":"2025-02-17T11:23:32Z","published":"2024-05-10T10:10:37Z","title":"DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under\n  Differentially Private Federated Learning using Dynamic Low-Rank Adaptation","summary":"  Federated learning (FL) allows clients to collaboratively train a global\nmodel without sharing their local data with a server. However, clients'\ncontributions to the server can still leak sensitive information. Differential\nprivacy (DP) addresses such leakage by providing formal privacy guarantees,\nwith mechanisms that add randomness to the clients' contributions. The\nrandomness makes it infeasible to train large transformer-based models, common\nin modern federated learning systems. In this work, we empirically evaluate the\npracticality of fine-tuning large scale on-device transformer-based models with\ndifferential privacy in a federated learning system. We conduct comprehensive\nexperiments on various system properties for tasks spanning a multitude of\ndomains: speech recognition, computer vision (CV) and natural language\nunderstanding (NLU). Our results show that full fine-tuning under\ndifferentially private federated learning (DP-FL) generally leads to huge\nperformance degradation which can be alleviated by reducing the dimensionality\nof contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks\nof existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA)\nconsistently outperforms other methods. An even more promising approach,\nDyLoRA, which makes the low rank variable, when naively combined with FL would\nstraightforwardly break differential privacy. We therefore propose an\nadaptation method that can be combined with differential privacy and call it\nDP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word\nerror rate (WER) increase due to DP to less than 2% and 7% respectively with 1\nmillion clients and a stringent privacy budget of $\\epsilon=2$.\n","authors":["Jie Xu","Karthikeyan Saravanan","Rogier van Dalen","Haaris Mehmood","David Tuckey","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2405.06368v4.pdf","comment":"16 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2408.16599v2","updated":"2025-02-17T11:20:46Z","published":"2024-08-29T15:09:04Z","title":"sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper\n  Limb Multi-Joint Movement Dynamics","summary":"  Exoskeletons and rehabilitation systems have the potential to improve human\nstrength and recovery by using adaptive human-machine interfaces. Achieving\nprecise and responsive control in these systems depends on accurately\nestimating joint movement dynamics, such as joint angle, velocity,\nacceleration, external mass, and torque. While machine learning (ML) approaches\nhave been employed to predict joint kinematics from surface electromyography\n(sEMG) data, traditional ML models often struggle to generalize across dynamic\nmovements. In contrast, physics-informed neural networks integrate\nbiomechanical principles, but their effectiveness in predicting full movement\ndynamics has not been thoroughly explored. To address this, we introduce the\nPhysics-informed Gated Recurrent Network (PiGRN), a novel model designed to\npredict multi-joint movement dynamics from sEMG data. PiGRN uses a Gated\nRecurrent Unit (GRU) to process time-series sEMG inputs, estimate multi-joint\nkinematics and external loads, and predict joint torque while incorporating\nphysics-based constraints during training. Experimental validation, using sEMG\ndata from five participants performing elbow flexion-extension tasks with 0 kg,\n2 kg, and 4 kg loads, showed that PiGRN accurately predicted joint torques for\n10 novel movements. RMSE values ranged from 4.02\\% to 11.40\\%, with correlation\ncoefficients between 0.87 and 0.98. These results underscore PiGRN's potential\nfor real-time applications in exoskeletons and rehabilitation. Future work will\nfocus on expanding datasets, improving musculoskeletal models, and\ninvestigating unsupervised learning approaches.\n","authors":["Rajnish Kumar","Anand Gupta","Suriya Prakash Muthukrishnan","Lalan Kumar","Sitikantha Roy"],"pdf_url":"https://arxiv.org/pdf/2408.16599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11682v1","updated":"2025-02-17T11:16:21Z","published":"2025-02-17T11:16:21Z","title":"Double Momentum and Error Feedback for Clipping with Fast Rates and\n  Differential Privacy","summary":"  Strong Differential Privacy (DP) and Optimization guarantees are two\ndesirable properties for a method in Federated Learning (FL). However, existing\nalgorithms do not achieve both properties at once: they either have optimal DP\nguarantees but rely on restrictive assumptions such as bounded\ngradients/bounded data heterogeneity, or they ensure strong optimization\nperformance but lack DP guarantees. To address this gap in the literature, we\npropose and analyze a new method called Clip21-SGD2M based on a novel\ncombination of clipping, heavy-ball momentum, and Error Feedback. In\nparticular, for non-convex smooth distributed problems with clients having\narbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal\nconvergence rate and also near optimal (local-)DP neighborhood. Our numerical\nexperiments on non-convex logistic regression and training of neural networks\nhighlight the superiority of Clip21-SGD2M over baselines in terms of the\noptimization performance for a given DP-budget.\n","authors":["Rustem Islamov","Samuel Horvath","Aurelien Lucchi","Peter Richtarik","Eduard Gorbunov"],"pdf_url":"https://arxiv.org/pdf/2502.11682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11680v1","updated":"2025-02-17T11:13:39Z","published":"2025-02-17T11:13:39Z","title":"Spectral structure learning for clinical time series","summary":"  We develop and evaluate a structure learning algorithm for clinical time\nseries. Clinical time series are multivariate time series observed in multiple\npatients and irregularly sampled, challenging existing structure learning\nalgorithms. We assume that our times series are realizations of StructGP, a\nk-dimensional multi-output or multi-task stationary Gaussian process (GP), with\nindependent patients sharing the same covariance function. StructGP encodes\nordered conditional relations between time series, represented in a directed\nacyclic graph. We implement an adapted NOTEARS algorithm, which based on a\ndifferentiable definition of acyclicity, recovers the graph by solving a series\nof continuous optimization problems. Simulation results show that up to mean\ndegree 3 and 20 tasks, we reach a median recall of 0.93% [IQR, 0.86, 0.97]\nwhile keeping a median precision of 0.71% [0.57-0.84], for recovering directed\nedges. We further show that the regularization path is key to identifying the\ngraph. With StructGP, we proposed a model of time series dependencies, that\nflexibly adapt to different time series regularity, while enabling us to learn\nthese dependencies from observations.\n","authors":["Ivan Lerner","Anita Burgun","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2502.11680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14157v2","updated":"2025-02-17T11:10:46Z","published":"2024-10-18T03:48:53Z","title":"Beyond Autoregression: Discrete Diffusion for Complex Reasoning and\n  Planning","summary":"  Autoregressive language models, despite their impressive capabilities,\nstruggle with complex reasoning and long-term planning tasks. We introduce\ndiscrete diffusion models as a novel solution to these challenges. Through the\nlens of subgoal imbalance, we demonstrate how diffusion models effectively\nlearn difficult subgoals that elude autoregressive approaches. We propose\nMulti-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on\ndifficulty during learning. On complex tasks like Countdown, Sudoku, and\nBoolean Satisfiability Problems, MDM significantly outperforms autoregressive\nmodels without using search techniques. For instance, MDM achieves 91.5\\% and\n100\\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\\% and\n20.7\\% for autoregressive models. Our work highlights the potential of\ndiffusion-based approaches in advancing AI capabilities for sophisticated\nlanguage understanding and problem-solving tasks.\n","authors":["Jiacheng Ye","Jiahui Gao","Shansan Gong","Lin Zheng","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.14157v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11673v1","updated":"2025-02-17T11:04:01Z","published":"2025-02-17T11:04:01Z","title":"Best of Both Worlds: Regret Minimization versus Minimax Play","summary":"  In this paper, we investigate the existence of online learning algorithms\nwith bandit feedback that simultaneously guarantee $O(1)$ regret compared to a\ngiven comparator strategy, and $O(\\sqrt{T})$ regret compared to the best\nstrategy in hindsight, where $T$ is the number of rounds. We provide the first\naffirmative answer to this question. In the context of symmetric zero-sum\ngames, both in normal- and extensive form, we show that our results allow us to\nguarantee to risk at most $O(1)$ loss while being able to gain $\\Omega(T)$ from\nexploitable opponents, thereby combining the benefits of both no-regret\nalgorithms and minimax play.\n","authors":["Adrian M√ºller","Jon Schneider","Stratis Skoulakis","Luca Viano","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2502.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11672v1","updated":"2025-02-17T11:01:03Z","published":"2025-02-17T11:01:03Z","title":"Exact Upper and Lower Bounds for the Output Distribution of Neural\n  Networks with Random Inputs","summary":"  We derive exact upper and lower bounds for the cumulative distribution\nfunction (cdf) of the output of a neural network over its entire support\nsubject to noisy (stochastic) inputs. The upper and lower bounds converge to\nthe true cdf over its domain as the resolution increases. Our method applies to\nany feedforward NN using continuous monotonic piecewise differentiable\nactivation functions (e.g., ReLU, tanh and softmax) and convolutional NNs,\nwhich were beyond the scope of competing approaches. The novelty and an\ninstrumental tool of our approach is to bound general NNs with ReLU NNs. The\nReLU NN based bounds are then used to derive upper and lower bounds of the cdf\nof the NN output. Experiments demonstrate that our method delivers guaranteed\nbounds of the predictive output distribution over its support, thus providing\nexact error guarantees, in contrast to competing approaches.\n","authors":["Andrey Kofnov","Daniel Kapla","Ezio Bartocci","Efstathia Bura"],"pdf_url":"https://arxiv.org/pdf/2502.11672v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.11671v1","updated":"2025-02-17T11:00:40Z","published":"2025-02-17T11:00:40Z","title":"Diversity-Oriented Data Augmentation with Large Language Models","summary":"  Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.\n","authors":["Zaitian Wang","Jinghan Zhang","Xinhao Zhang","Kunpeng Liu","Pengfei Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11669v1","updated":"2025-02-17T10:57:53Z","published":"2025-02-17T10:57:53Z","title":"Deep Subspace Learning for Surface Anomaly Classification Based on 3D\n  Point Cloud Data","summary":"  Surface anomaly classification is critical for manufacturing system fault\ndiagnosis and quality control. However, the following challenges always hinder\naccurate anomaly classification in practice: (i) Anomaly patterns exhibit\nintra-class variation and inter-class similarity, presenting challenges in the\naccurate classification of each sample. (ii) Despite the predefined classes,\nnew types of anomalies can occur during production that require to be detected\naccurately. (iii) Anomalous data is rare in manufacturing processes, leading to\nlimited data for model learning. To tackle the above challenges simultaneously,\nthis paper proposes a novel deep subspace learning-based 3D anomaly\nclassification model. Specifically, starting from a lightweight encoder to\nextract the latent representations, we model each class as a subspace to\naccount for the intra-class variation, while promoting distinct subspaces of\ndifferent classes to tackle the inter-class similarity. Moreover, the explicit\nmodeling of subspaces offers the capability to detect out-of-distribution\nsamples, i.e., new types of anomalies, and the regularization effect with much\nfewer learnable parameters of our proposed subspace classifier, compared to the\npopular Multi-Layer Perceptions (MLPs). Extensive numerical experiments\ndemonstrate our method achieves better anomaly classification results than\nbenchmark methods, and can effectively identify the new types of anomalies.\n","authors":["Xuanming Cao","Chengyu Tao","Juan Du"],"pdf_url":"https://arxiv.org/pdf/2502.11669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11665v1","updated":"2025-02-17T10:54:01Z","published":"2025-02-17T10:54:01Z","title":"On the kernel learning problem","summary":"  The classical kernel ridge regression problem aims to find the best fit for\nthe output $Y$ as a function of the input data $X\\in \\mathbb{R}^d$, with a\nfixed choice of regularization term imposed by a given choice of a reproducing\nkernel Hilbert space, such as a Sobolev space. Here we consider a\ngeneralization of the kernel ridge regression problem, by introducing an extra\nmatrix parameter $U$, which aims to detect the scale parameters and the feature\nvariables in the data, and thereby improve the efficiency of kernel ridge\nregression. This naturally leads to a nonlinear variational problem to optimize\nthe choice of $U$. We study various foundational mathematical aspects of this\nvariational problem, and in particular how this behaves in the presence of\nmultiscale structures in the data.\n","authors":["Yang Li","Feng Ruan"],"pdf_url":"https://arxiv.org/pdf/2502.11665v1.pdf","comment":"61 pages"},{"id":"http://arxiv.org/abs/2502.11657v1","updated":"2025-02-17T10:48:26Z","published":"2025-02-17T10:48:26Z","title":"How does ion temperature gradient turbulence depend on magnetic\n  geometry? Insights from data and machine learning","summary":"  Magnetic geometry has a significant effect on the level of turbulent\ntransport in fusion plasmas. Here, we model and analyze this dependence using\nmultiple machine learning methods and a dataset of > 200,000 nonlinear\nsimulations of ion-temperature-gradient turbulence in diverse non-axisymmetric\ngeometries. The dataset is generated using a large collection of both optimized\nand randomly generated stellarator equilibria. At fixed gradients, the\nturbulent heat flux varies between geometries by several orders of magnitude.\nTrends are apparent among the configurations with particularly high or low heat\nflux. Regression and classification techniques from machine learning are then\napplied to extract patterns in the dataset. Due to a symmetry of the\ngyrokinetic equation, the heat flux and regressions thereof should be invariant\nto translations of the raw features in the parallel coordinate, similar to\ntranslation invariance in computer vision applications. Multiple regression\nmodels including convolutional neural networks (CNNs) and decision trees can\nachieve reasonable predictive power for the heat flux in held-out test\nconfigurations, with highest accuracy for the CNNs. Using Spearman correlation,\nsequential feature selection, and Shapley values to measure feature importance,\nit is consistently found that the most important geometric lever on the heat\nflux is the flux surface compression in regions of bad curvature. The second\nmost important feature relates to the magnitude of geodesic curvature. These\ntwo features align remarkably with surrogates that have been proposed based on\ntheory, while the methods here allow a natural extension to more features for\nincreased accuracy. The dataset, released with this publication, may also be\nused to test other proposed surrogates, and we find many previously published\nproxies do correlate well with both the heat flux and stability boundary.\n","authors":["Matt Landreman","Jong Youl Choi","Caio Alves","Prasanna Balaprakash","R. Michael Churchill","Rory Conlin","Gareth Roberg-Clark"],"pdf_url":"https://arxiv.org/pdf/2502.11657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11646v1","updated":"2025-02-17T10:39:11Z","published":"2025-02-17T10:39:11Z","title":"Hyperspherical Energy Transformer with Recurrent Depth","summary":"  Transformer-based foundation models have achieved unprecedented success with\na gigantic amount of parameters and computational resources. Yet, the core\nbuilding blocks of these models, the Transformer layers, and how they are\narranged and configured are primarily engineered from the bottom up and driven\nby heuristics. For advancing next-generation architectures, it demands\nexploring a prototypical model that is amenable to high interpretability and of\npractical competence. To this end, we take a step from the top-down view and\ndesign neural networks from an energy minimization perspective. Specifically,\nto promote isotropic token distribution on the sphere, we formulate a modified\nHopfield energy function on the subspace-embedded hypersphere, based on which\nTransformer layers with symmetric structures are designed as the iterative\noptimization for the energy function. By integrating layers with the same\nparameters, we propose \\textit{Hyper-Spherical Energy Transformer} (Hyper-SET),\nan alternative to the vanilla Transformer with recurrent depth. This design\ninherently provides greater interpretability and allows for scaling to deeper\nlayers without a significant increase in the number of parameters. We also\nempirically demonstrate that Hyper-SET achieves comparable or even superior\nperformance on both synthetic and real-world tasks, such as solving Sudoku and\nmasked image modeling, while utilizing fewer parameters.\n","authors":["Yunzhe Hu","Difan Zou","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11646v1.pdf","comment":"20 pages, 13 figures, 12 tables"},{"id":"http://arxiv.org/abs/2502.11639v1","updated":"2025-02-17T10:33:24Z","published":"2025-02-17T10:33:24Z","title":"Neural Interpretable Reasoning","summary":"  We formalize a novel modeling framework for achieving interpretability in\ndeep learning, anchored in the principle of inference equivariance. While the\ndirect verification of interpretability scales exponentially with the number of\nvariables of the system, we show that this complexity can be mitigated by\ntreating interpretability as a Markovian property and employing neural\nre-parametrization techniques. Building on these insights, we propose a new\nmodeling paradigm -- neural generation and interpretable execution -- that\nenables scalable verification of equivariance. This paradigm provides a general\napproach for designing Neural Interpretable Reasoners that are not only\nexpressive but also transparent.\n","authors":["Pietro Barbiero","Giuseppe Marra","Gabriele Ciravegna","David Debot","Francesco De Santis","Michelangelo Diligenti","Mateo Espinosa Zarlenga","Francesco Giannini"],"pdf_url":"https://arxiv.org/pdf/2502.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02892v2","updated":"2025-02-17T10:22:52Z","published":"2024-10-03T18:30:47Z","title":"The Role of Deductive and Inductive Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning tasks, yet their reliance on static prompt structures and limited\nadaptability to complex scenarios remains a significant challenge. In this\npaper, we propose the Deductive and InDuctive(DID) method, a novel framework\nthat enhances LLM reasoning by dynamically integrating both deductive and\ninductive reasoning approaches. Drawing from cognitive science principles, DID\nimplements a dual-metric complexity evaluation system that combines Littlestone\ndimension and information entropy to precisely assess task difficulty and guide\ndecomposition strategies. DID enables the model to progressively adapt its\nreasoning pathways based on problem complexity, mirroring human cognitive\nprocesses. We evaluate DID's effectiveness across multiple benchmarks,\nincluding the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset\nfor temporal reasoning. Our results demonstrate significant improvements in\nreasoning quality and solution accuracy - achieving 70.3% accuracy on AIW\n(compared to 62.2% for Tree of Thought) while maintaining lower computational\ncosts. The success of DID in improving LLM performance while preserving\ncomputational efficiency suggests promising directions for developing more\ncognitively aligned and capable language models. Our work contributes a\ntheoretically grounded, input-centric approach to enhancing LLM reasoning\ncapabilities, offering an efficient alternative to traditional\noutput-exploration methods.\n","authors":["Chengkun Cai","Xu Zhao","Haoliang Liu","Zhongyu Jiang","Tianfang Zhang","Zongkai Wu","Jenq-Neng Hwang","Serge Belongie","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.02892v2.pdf","comment":"4 figures"},{"id":"http://arxiv.org/abs/2501.00463v2","updated":"2025-02-17T10:13:59Z","published":"2024-12-31T14:22:53Z","title":"SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion\n  Models with Self-Augmented Training","summary":"  The rapid proliferation of AI-generated images necessitates effective\nwatermarking techniques to protect intellectual property and detect fraudulent\ncontent. While existing training-based watermarking methods show promise, they\noften struggle with generalizing across diverse prompts and tend to introduce\nvisible artifacts. To this end, we propose a novel, provably generalizable\nimage watermarking approach for Latent Diffusion Models, termed Self-Augmented\nTraining (SAT-LDM). Our method aligns the training and testing phases through a\nfree generation distribution, thereby enhancing the watermarking module's\ngeneralization capabilities. We theoretically consolidate SAT-LDM by proving\nthat the free generation distribution contributes to its tight generalization\nbound, without the need for additional data collection. Extensive experiments\nshow that SAT-LDM not only achieves robust watermarking but also significantly\nimproves the quality of watermarked images across a wide range of prompts.\nMoreover, our experimental analyses confirm the strong generalization abilities\nof SAT-LDM. We hope that our method provides a practical and efficient solution\nfor securing high-fidelity AI-generated content.\n","authors":["Lu Zhang","Liang Zeng"],"pdf_url":"https://arxiv.org/pdf/2501.00463v2.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.11617v1","updated":"2025-02-17T10:00:24Z","published":"2025-02-17T10:00:24Z","title":"In-Context Parametric Inference: Point or Distribution Estimators?","summary":"  Bayesian and frequentist inference are two fundamental paradigms in\nstatistical estimation. Bayesian methods treat hypotheses as random variables,\nincorporating priors and updating beliefs via Bayes' theorem, whereas\nfrequentist methods assume fixed but unknown hypotheses, relying on estimators\nlike maximum likelihood. While extensive research has compared these\napproaches, the frequentist paradigm of obtaining point estimates has become\npredominant in deep learning, as Bayesian inference is challenging due to the\ncomputational complexity and the approximation gap of posterior estimation\nmethods. However, a good understanding of trade-offs between the two approaches\nis lacking in the regime of amortized estimators, where in-context learners are\ntrained to estimate either point values via maximum likelihood or maximum a\nposteriori estimation, or full posteriors using normalizing flows, score-based\ndiffusion samplers, or diagonal Gaussian approximations, conditioned on\nobservations. To help resolve this, we conduct a rigorous comparative analysis\nspanning diverse problem settings, from linear models to shallow neural\nnetworks, with a robust evaluation framework assessing both in-distribution and\nout-of-distribution generalization on tractable tasks. Our experiments indicate\nthat amortized point estimators generally outperform posterior inference,\nthough the latter remain competitive in some low-dimensional problems, and we\nfurther discuss why this might be the case.\n","authors":["Sarthak Mittal","Yoshua Bengio","Nikolay Malkin","Guillaume Lajoie"],"pdf_url":"https://arxiv.org/pdf/2502.11617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11612v1","updated":"2025-02-17T09:55:58Z","published":"2025-02-17T09:55:58Z","title":"Maximum Entropy Reinforcement Learning with Diffusion Policy","summary":"  The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a\nmainstream implementation for realizing the Maximum Entropy Reinforcement\nLearning (MaxEnt RL) objective, which incorporates entropy maximization to\nencourage exploration and enhance policy robustness. While the Gaussian policy\nperforms well on simpler tasks, its exploration capacity and potential\nperformance in complex multi-goal RL environments are limited by its inherent\nunimodality. In this paper, we employ the diffusion model, a powerful\ngenerative model capable of capturing complex multimodal distributions, as the\npolicy representation to fulfill the MaxEnt RL objective, developing a method\nnamed MaxEnt RL with Diffusion Policy (MaxEntDP). Our method enables efficient\nexploration and brings the policy closer to the optimal MaxEnt policy.\nExperimental results on Mujoco benchmarks show that MaxEntDP outperforms the\nGaussian policy and other generative models within the MaxEnt RL framework, and\nperforms comparably to other state-of-the-art diffusion-based online RL\nalgorithms. Our code is available at https://github.com/diffusionyes/MaxEntDP.\n","authors":["Xiaoyi Dong","Jian Cheng","Xi Sheryl Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11612v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.16176v2","updated":"2025-02-17T09:53:43Z","published":"2024-06-23T18:01:56Z","title":"GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large\n  Language Models on Graph Datasets","summary":"  Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to manipulate, program, and reason about\nstructured data, especially graphs. We introduce GraphEval36K, the first\ncomprehensive graph dataset, comprising 40 graph coding problems and 36,900\ntest cases to evaluate the ability of LLMs on graph problem-solving. Our\ndataset is categorized into eight primary and four sub-categories to ensure a\nthorough evaluation across different types of graphs. We benchmark ten LLMs,\nfinding that private models outperform open-source ones, though the gap is\nnarrowing. We also analyze the performance of LLMs across directed vs\nundirected graphs, different kinds of graph concepts, and network models.\nFurthermore, to improve the usability of our evaluation framework, we propose\nStructured Symbolic Decomposition (SSD), an instruction-based method designed\nto enhance LLM performance on complex graph tasks. Results show that SSD\nimproves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and\nClaude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.\n","authors":["Qiming Wu","Zichen Chen","Will Corcoran","Misha Sra","Ambuj K. Singh"],"pdf_url":"https://arxiv.org/pdf/2406.16176v2.pdf","comment":"The first two authors contributed equally to this work. This paper\n  has been accepted by NAACL 2025. GraphEval36K is available at\n  https://grapheval36k.github.io/"},{"id":"http://arxiv.org/abs/2502.11609v1","updated":"2025-02-17T09:52:19Z","published":"2025-02-17T09:52:19Z","title":"Exploiting Task Relationships for Continual Learning Using\n  Transferability-Aware Task Embeddings","summary":"  Continual learning (CL) has been an essential topic in the contemporary\napplication of deep neural networks, where catastrophic forgetting (CF) can\nimpede a model's ability to acquire knowledge progressively. Existing CL\nstrategies primarily address CF by regularizing model updates or separating\ntask-specific and shared components. However, these methods focus on task model\nelements while overlooking the potential of leveraging inter-task relationships\nfor learning enhancement. To address this, we propose a transferability-aware\ntask embedding named H-embedding and train a hypernet under its guidance to\nlearn task-conditioned model weights for CL tasks. Particularly, H-embedding is\nintroduced based on an information theoretical transferability measure and is\ndesigned to be online and easy to compute. The framework is also characterized\nby notable practicality, which only requires storing a low-dimensional task\nembedding for each task, and can be efficiently trained in an end-to-end way.\nExtensive evaluations and experimental analyses on datasets including Permuted\nMNIST, Cifar10/100, and ImageNet-R demonstrate that our framework performs\nprominently compared to various baseline methods, displaying great potential in\nexploiting intrinsic task relationships.\n","authors":["Yanru Wu","Xiangyu Chen","Jianning Wang","Enming Zhang","Hanbing Liu","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2502.11609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11607v1","updated":"2025-02-17T09:50:41Z","published":"2025-02-17T09:50:41Z","title":"GraphThought: Graph Combinatorial Optimization with Thought Generation","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, especially in text processing and generative tasks. Recent\nadvancements in the reasoning capabilities of state-of-the-art LLMs, such as\nOpenAI-o1, have significantly broadened their applicability, particularly in\ncomplex problem-solving and logical inference. However, most existing LLMs\nstruggle with notable limitations in handling graph combinatorial optimization\n(GCO) problems. To bridge this gap, we formally define the Optimal Thoughts\nDesign (OTD) problem, including its state and action thought space. We then\nintroduce a novel framework, GraphThought, designed to generate high-quality\nthought datasets for GCO problems. Leveraging these datasets, we fine-tune the\nLlama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact\n8B-parameter architecture, Llama-GT matches the performance of state-of-the-art\nLLMs on the GraphArena benchmark. Experimental results show that our approach\noutperforms both proprietary and open-source models, even rivaling specialized\nmodels like o1-mini. This work sets a new state-of-the-art benchmark while\nchallenging the prevailing notion that model scale is the primary driver of\nreasoning capability.\n","authors":["Zixiao Huang","Lifeng Guo","Junjie Sheng","Haosheng Chen","Wenhao Li","Bo Jin","Changhong Lu","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11607v1.pdf","comment":"41 pages, 5 figures, 13 tables"},{"id":"http://arxiv.org/abs/2404.19397v2","updated":"2025-02-17T09:49:46Z","published":"2024-04-30T09:42:40Z","title":"Can humans teach machines to code?","summary":"  The goal of inductive program synthesis is for a machine to automatically\ngenerate a program from user-supplied examples. A key underlying assumption is\nthat humans can provide sufficient examples to teach a concept to a machine. To\nevaluate the validity of this assumption, we conduct a study where human\nparticipants provide examples for six programming concepts, such as finding the\nmaximum element of a list. We evaluate the generalisation performance of five\nprogram synthesis systems trained on input-output examples (i) from non-expert\nhumans, (ii) from a human expert, and (iii) randomly sampled. Our results\nsuggest that non-experts typically do not provide sufficient examples for a\nprogram synthesis system to learn an accurate program.\n","authors":["C√©line Hocquette","Johannes Langer","Andrew Cropper","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2404.19397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10995v2","updated":"2025-02-17T09:49:15Z","published":"2024-09-17T08:58:33Z","title":"SynthSOD: Developing an Heterogeneous Dataset for Orchestra Music Source\n  Separation","summary":"  Recent advancements in music source separation have significantly progressed,\nparticularly in isolating vocals, drums, and bass elements from mixed tracks.\nThese developments owe much to the creation and use of large-scale, multitrack\ndatasets dedicated to these specific components. However, the challenge of\nextracting similarly sounding sources from orchestra recordings has not been\nextensively explored, largely due to a scarcity of comprehensive and clean (i.e\nbleed-free) multitrack datasets. In this paper, we introduce a novel multitrack\ndataset called SynthSOD, developed using a set of simulation techniques to\ncreate a realistic (i.e. using high-quality soundfonts), musically motivated,\nand heterogeneous training set comprising different dynamics, natural tempo\nchanges, styles, and conditions. Moreover, we demonstrate the application of a\nwidely used baseline music separation model trained on our synthesized dataset\nw.r.t to the well-known EnsembleSet, and evaluate its performance under both\nsynthetic and real-world conditions.\n","authors":["Jaime Garcia-Martinez","David Diaz-Guerra","Archontis Politis","Tuomas Virtanen","Julio J. Carabias-Orti","Pedro Vera-Candeas"],"pdf_url":"https://arxiv.org/pdf/2409.10995v2.pdf","comment":"The SynthSOD dataset can be downloaded from\n  https://doi.org/10.5281/zenodo.13759492"},{"id":"http://arxiv.org/abs/2502.11604v1","updated":"2025-02-17T09:44:23Z","published":"2025-02-17T09:44:23Z","title":"An Actor-Critic Algorithm with Function Approximation for Risk Sensitive\n  Cost Markov Decision Processes","summary":"  In this paper, we consider the risk-sensitive cost criterion with\nexponentiated costs for Markov decision processes and develop a model-free\npolicy gradient algorithm in this setting. Unlike additive cost criteria such\nas average or discounted cost, the risk-sensitive cost criterion is less\nstudied due to the complexity resulting from the multiplicative structure of\nthe resulting Bellman equation. We develop an actor-critic algorithm with\nfunction approximation in this setting and provide its asymptotic convergence\nanalysis. We also show the results of numerical experiments that demonstrate\nthe superiority in performance of our algorithm over other recent algorithms in\nthe literature.\n","authors":["Soumyajit Guin","Vivek S. Borkar","Shalabh Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2502.11604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18351v2","updated":"2025-02-17T09:34:52Z","published":"2024-06-26T13:52:47Z","title":"Reinforcement Learning with Intrinsically Motivated Feedback Graph for\n  Lost-sales Inventory Control","summary":"  Reinforcement learning (RL) has proven to be well-performed and\ngeneral-purpose in the inventory control (IC). However, further improvement of\nRL algorithms in the IC domain is impeded due to two limitations of online\nexperience. First, online experience is expensive to acquire in real-world\napplications. With the low sample efficiency nature of RL algorithms, it would\ntake extensive time to train the RL policy to convergence. Second, online\nexperience may not reflect the true demand due to the lost sales phenomenon\ntypical in IC, which makes the learning process more challenging. To address\nthe above challenges, we propose a decision framework that combines\nreinforcement learning with feedback graph (RLFG) and intrinsically motivated\nexploration (IME) to boost sample efficiency. In particular, we first take\nadvantage of the inherent properties of lost-sales IC problems and design the\nfeedback graph (FG) specially for lost-sales IC problems to generate abundant\nside experiences aid RL updates. Then we conduct a rigorous theoretical\nanalysis of how the designed FG reduces the sample complexity of RL methods.\nBased on the theoretical insights, we design an intrinsic reward to direct the\nRL agent to explore to the state-action space with more side experiences,\nfurther exploiting FG's power. Experimental results demonstrate that our method\ngreatly improves the sample efficiency of applying RL in IC. Our code is\navailable at https://anonymous.4open.science/r/RLIMFG4IC-811D/\n","authors":["Zifan Liu","Xinran Li","Shibo Chen","Gen Li","Jiashuo Jiang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11596v1","updated":"2025-02-17T09:28:51Z","published":"2025-02-17T09:28:51Z","title":"LLM Embeddings for Deep Learning on Tabular Data","summary":"  Tabular deep-learning methods require embedding numerical and categorical\ninput features into high-dimensional spaces before processing them. Existing\nmethods deal with this heterogeneous nature of tabular data by employing\nseparate type-specific encoding approaches. This limits the cross-table\ntransfer potential and the exploitation of pre-trained knowledge. We propose a\nnovel approach that first transforms tabular data into text, and then leverages\npre-trained representations from LLMs to encode this data, resulting in a\nplug-and-play solution to improv ing deep-learning tabular methods. We\ndemonstrate that our approach improves accuracy over competitive models, such\nas MLP, ResNet and FT-Transformer, by validating on seven classification\ndatasets.\n","authors":["Boshko Koloski","Andrei Margeloiu","Xiangjian Jiang","Bla≈æ ≈†krlj","Nikola Simidjievski","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2502.11596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11624v4","updated":"2025-02-17T09:26:15Z","published":"2024-03-18T09:56:00Z","title":"Dual-Channel Multiplex Graph Neural Networks for Recommendation","summary":"  Effective recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interactive relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant challenges: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations within behavior patterns on the target relation in recommender system\nscenarios. In this work, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interactive relations, and includes a relation chain representation\nlearner and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06% and 12.15% on average across all datasets in terms of\nRecall@10 and NDCG@10 respectively.\n","authors":["Xiang Li","Chaofan Fu","Zhongying Zhao","Guanjie Zheng","Chao Huang","Yanwei Yu","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15046v3","updated":"2025-02-17T09:20:06Z","published":"2024-11-22T16:31:36Z","title":"On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning","summary":"  In multi-agent systems, agent behavior is driven by utility functions that\nencapsulate their individual goals and interactions. Inverse Reinforcement\nLearning (IRL) seeks to uncover these utilities by analyzing expert behavior,\noffering insights into the underlying decision-making processes. However,\nmulti-agent settings pose significant challenges, particularly when rewards are\ninferred from equilibrium observations. A key obstacle is that single (Nash)\nequilibrium observations often fail to adequately capture critical game\nproperties, leading to potential misrepresentations. This paper offers a\nrigorous analysis of the feasible reward set in multi-agent IRL and addresses\nthese limitations by introducing entropy-regularized games, ensuring\nequilibrium uniqueness and enhancing interpretability. Furthermore, we examine\nthe effects of estimation errors and present the first sample complexity\nresults for multi-agent IRL across diverse scenarios.\n","authors":["Till Freihaut","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2411.15046v3.pdf","comment":"Currently under review"},{"id":"http://arxiv.org/abs/2502.11583v1","updated":"2025-02-17T09:16:25Z","published":"2025-02-17T09:16:25Z","title":"Distributional autoencoders know the score","summary":"  This work presents novel and desirable properties of a recently introduced\nclass of autoencoders -- the Distributional Principal Autoencoder (DPA) -- that\ncombines distributionally correct reconstruction with principal components-like\ninterpretability of the encodings.\n  First, we show that the level sets of the encoder orient themselves exactly\nwith regard to the score of the data distribution. This both explains the\nmethod's often remarkable performance in disentangling the the factors of\nvariation of the data, as well as opens up possibilities of recovering its\ndistribution while having access to samples only. In settings where the score\nitself has physical meaning -- such as when the data obey the Boltzmann\ndistribution -- we demonstrate that the method can recover scientifically\nimportant quantities such as the \\textit{minimum free energy path}.\n  Second, we show that if the data lie on a manifold that can be approximated\nby the encoder, the optimal encoder's components beyond the dimension of the\nmanifold will carry absolutely no additional information about the data\ndistribution. This promises new ways of determining the number of relevant\ndimensions of the data beyond common heuristics such as the scree plot.\n  Finally, the fact that the method is learning the score means that it could\nhave promise as a generative model, potentially rivaling approaches such as\ndiffusion, which similarly attempts to approximate the score of the data\ndistribution.\n","authors":["Andrej Leban"],"pdf_url":"https://arxiv.org/pdf/2502.11583v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.11571v1","updated":"2025-02-17T09:05:21Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v1.pdf","comment":"to appear in ACL 2025"},{"id":"http://arxiv.org/abs/2407.16205v4","updated":"2025-02-17T09:00:28Z","published":"2024-07-23T06:14:41Z","title":"LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.\n","authors":["Shi Lin","Hongming Yang","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11570v1","updated":"2025-02-17T08:59:59Z","published":"2025-02-17T08:59:59Z","title":"Towards a Trustworthy Anomaly Detection for Critical Applications\n  through Approximated Partial AUC Loss","summary":"  Anomaly Detection is a crucial step for critical applications such in the\nindustrial, medical or cybersecurity domains. These sectors share the same\nrequirement of handling differently the different types of classification\nerrors. Indeed, even if false positives are acceptable, false negatives are\nnot, because it would reflect a missed detection of a quality issue, a disease\nor a cyber threat. To fulfill this requirement, we propose a method that\ndynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A\nbinary classifier is trained to optimize the specific range of the AUC ROC\ncurve that prevents the True Positive Rate (TPR) to reach 100% while minimizing\nthe False Positive Rate (FPR). The optimal threshold that does not trigger any\nfalse negative is then kept and used at the test step. The results show a TPR\nof 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR\nimprovement of 4.3% for a FPR cost of 12.2% against other state-of-the-art\nmethods. The code is available at https://github.com/ArnaudBougaham/tapAUC.\n","authors":["Arnaud Bougaham","Beno√Æt Fr√©nay"],"pdf_url":"https://arxiv.org/pdf/2502.11570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23166v2","updated":"2025-02-17T08:59:45Z","published":"2024-10-30T16:18:22Z","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","summary":"  The rapid advancement of large language models (LLMs) has opened new\npossibilities for automating the proposal of innovative scientific ideas. This\nprocess involves two key phases: literature retrieval and idea generation.\nHowever, existing approaches often fall short due to their reliance on\nkeyword-based search tools during the retrieval phase, which neglects crucial\nsemantic information and frequently results in incomplete retrieval outcomes.\nSimilarly, in the idea generation phase, current methodologies tend to depend\nsolely on the internal knowledge of LLMs or metadata from retrieved papers,\nthereby overlooking significant valuable insights contained within the full\ntexts. To address these limitations, we introduce SciPIP, an innovative\nframework designed to enhance the LLM-based proposal of scientific ideas\nthrough improvements in both literature retrieval and idea generation. Our\napproach begins with the construction of a comprehensive literature database\nthat supports advanced retrieval based not only on keywords but also on\nsemantics and citation relationships. This is complemented by the introduction\nof a multi-granularity retrieval algorithm aimed at ensuring more thorough and\nexhaustive retrieval results. For the idea generation phase, we propose a\ndual-path framework that effectively integrates both the content of retrieved\npapers and the extensive internal knowledge of LLMs. This integration\nsignificantly boosts the novelty, feasibility, and practical value of proposed\nideas. Our experiments, conducted across various domains such as natural\nlanguage processing and computer vision, demonstrate SciPIP's capability to\ngenerate a multitude of innovative and useful ideas. These findings underscore\nSciPIP's potential as a valuable tool for researchers seeking to advance their\nfields with groundbreaking concepts.\n","authors":["Wenxiao Wang","Lihui Gu","Liye Zhang","Yunxiang Luo","Yi Dai","Chen Shen","Liang Xie","Binbin Lin","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.23166v2.pdf","comment":"20 pages, 5 figures, 12 tables. The code has been availabel:\n  https://github.com/cheerss/SciPIP"},{"id":"http://arxiv.org/abs/2502.11569v1","updated":"2025-02-17T08:59:16Z","published":"2025-02-17T08:59:16Z","title":"Towards Reasoning Ability of Small Language Models","summary":"  Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks.\n","authors":["Gaurav Srivastava","Shuxiang Cao","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11564v1","updated":"2025-02-17T08:54:29Z","published":"2025-02-17T08:54:29Z","title":"Continuous Diffusion Model for Language Modeling","summary":"  Diffusion models have emerged as a promising alternative to autoregressive\nmodels in modeling discrete categorical data. Yet diffusion models that\ndirectly work on discrete data space do not fully exploit the power of\niterative refinement, as the signals are lost during the transition between\ndiscrete states. Existing continuous diffusion models for discrete data have\nlimited performance compared to discrete approaches, and the unclear link\nbetween them restricts the development of diffusion models for discrete data.\nIn this work, we propose a continuous diffusion model for language modeling\nthat incorporates the geometry of the underlying categorical distribution. We\nestablish a connection between the discrete diffusion and continuous flow on\nthe statistical manifold, and building on the analogy, we introduce a simple\ndesign for the diffusion process that generalizes previous discrete diffusion\nmodels. We further propose a simulation-free training framework based on radial\nsymmetry and a simple technique to address the high dimensionality of the\nmanifold. Comprehensive experiments on language modeling benchmarks and other\nmodalities show that our method outperforms existing discrete diffusion models\nand approaches the performance of autoregressive models. Codes available at\n\\href{https://github.com/harryjo97/RDLM}{https://github.com/harryjo97/RDLM}.\n","authors":["Jaehyeong Jo","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.11564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04814v2","updated":"2025-02-17T08:53:28Z","published":"2024-10-07T07:54:53Z","title":"Learning Interpretable Hierarchical Dynamical Systems Models from Time\n  Series Data","summary":"  In science, we are often interested in obtaining a generative model of the\nunderlying system dynamics from observed time series. While powerful methods\nfor dynamical systems reconstruction (DSR) exist when data come from a single\ndomain, how to best integrate data from multiple dynamical regimes and leverage\nit for generalization is still an open question. This becomes particularly\nimportant when individual time series are short, and group-level information\nmay help to fill in for gaps in single-domain data. Here we introduce a\nhierarchical framework that enables to harvest group-level (multi-domain)\ninformation while retaining all single-domain characteristics, and showcase it\non popular DSR benchmarks, as well as on neuroscience and medical data. In\naddition to faithful reconstruction of all individual dynamical regimes, our\nunsupervised methodology discovers common low-dimensional feature spaces in\nwhich datasets with similar dynamics cluster. The features spanning these\nspaces were further dynamically highly interpretable, surprisingly in often\nlinear relation to control parameters that govern the dynamics of the\nunderlying system. Finally, we illustrate transfer learning and generalization\nto new parameter regimes, paving the way toward DSR foundation models.\n","authors":["Manuel Brenner","Elias Weber","Georgia Koppe","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2410.04814v2.pdf","comment":"Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2502.11560v1","updated":"2025-02-17T08:48:07Z","published":"2025-02-17T08:48:07Z","title":"A Survey of Automatic Prompt Engineering: An Optimization Perspective","summary":"  The rise of foundation models has shifted focus from resource-intensive\nfine-tuning to prompt engineering, a paradigm that steers model behavior\nthrough input design rather than weight updates. While manual prompt\nengineering faces limitations in scalability, adaptability, and cross-modal\nalignment, automated methods, spanning foundation model (FM) based\noptimization, evolutionary methods, gradient-based optimization, and\nreinforcement learning, offer promising solutions. Existing surveys, however,\nremain fragmented across modalities and methodologies. This paper presents the\nfirst comprehensive survey on automated prompt engineering through a unified\noptimization-theoretic lens. We formalize prompt optimization as a maximization\nproblem over discrete, continuous, and hybrid prompt spaces, systematically\norganizing methods by their optimization variables (instructions, soft prompts,\nexemplars), task-specific objectives, and computational frameworks. By bridging\ntheoretical formulation with practical implementations across text, vision, and\nmultimodal domains, this survey establishes a foundational framework for both\nresearchers and practitioners, while highlighting underexplored frontiers in\nconstrained optimization and agent-oriented prompt design.\n","authors":["Wenwu Li","Xiangfeng Wang","Wenhao Li","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2502.11560v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08696v2","updated":"2025-02-17T08:41:58Z","published":"2025-02-12T18:59:55Z","title":"Scalable Discrete Diffusion Samplers: Combinatorial Optimization and\n  Statistical Physics","summary":"  Learning to sample from complex unnormalized distributions over discrete\ndomains emerged as a promising research direction with applications in\nstatistical physics, variational inference, and combinatorial optimization.\nRecent work has demonstrated the potential of diffusion models in this domain.\nHowever, existing methods face limitations in memory scaling and thus the\nnumber of attainable diffusion steps since they require backpropagation through\nthe entire generative process. To overcome these limitations we introduce two\nnovel training methods for discrete diffusion samplers, one grounded in the\npolicy gradient theorem and the other one leveraging Self-Normalized Neural\nImportance Sampling (SN-NIS). These methods yield memory-efficient training and\nachieve state-of-the-art results in unsupervised combinatorial optimization.\nNumerous scientific applications additionally require the ability of unbiased\nsampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte\nCarlo that enable for the first time the application of discrete diffusion\nmodels to this problem. We validate our methods on Ising model benchmarks and\nfind that they outperform popular autoregressive approaches. Our work opens new\navenues for applying diffusion models to a wide range of scientific\napplications in discrete domains that were hitherto restricted to exact\nlikelihood models.\n","authors":["Sebastian Sanokowski","Wilhelm Berghammer","Martin Ennemoser","Haoyu Peter Wang","Sepp Hochreiter","Sebastian Lehner"],"pdf_url":"https://arxiv.org/pdf/2502.08696v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2501.07365v3","updated":"2025-02-17T08:40:10Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commerce search interactions and is a key factor for customers at\nproduct explorations. However, its impact on semantic retrieval has not been\nwell studied yet. In this research, we build a multimodal representation for\nproduct items in e-commerce search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v3.pdf","comment":"Accepted at EReL@MIR WWW 2025"},{"id":"http://arxiv.org/abs/2310.12746v3","updated":"2025-02-17T08:39:05Z","published":"2023-10-19T13:50:56Z","title":"TabuLa: Harnessing Language Models for Tabular Data Synthesis","summary":"  Tabular data synthesis is crucial for addressing privacy and security\nconcerns in industries reliant on tabular data. While recent advancements adopt\nlarge language models (LLMs) for realistic tabular data generation, their long\ntraining times and limited reusability hinder practical applications. In this\npaper, we propose Tabula, a tabular data synthesizer that leverages the\nstructure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data\nsynthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained\nweights originally designed for natural language tasks, focusing instead on a\ntailored approach for tabular data. In addition, Tabula introduces a token\nsequence compression strategy that significantly reduces training time while\nmaintaining data quality, alongside a novel token padding method that improves\nsequence alignment across training batches. Experiments on six datasets show\nthat Tabula achieves superior synthetic data utility compared to current SOTA\nmethods. Additionally, the results demonstrate that Tabula model trained on\ntabular datasets serves effectively as a foundational model for synthesizing\nnew tabular datasets. Furthermore, the proposed padding method outperforms the\nconventional left and right padding strategies. Finally, the results highlight\nthat Tabula averagely reduces training time per epoch by 46.2% compared to\nstate-of-the-art LLM approaches while achieving higher data utility. Our code\nis available at https://github.com/zhao-zilong/Tabula\n","authors":["Zilong Zhao","Robert Birke","Lydia Chen"],"pdf_url":"https://arxiv.org/pdf/2310.12746v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15490v2","updated":"2025-02-17T08:36:54Z","published":"2024-06-18T13:01:30Z","title":"Causal Discovery Inspired Unsupervised Domain Adaptation for\n  Emotion-Cause Pair Extraction","summary":"  This paper tackles the task of emotion-cause pair extraction in the\nunsupervised domain adaptation setting. The problem is challenging as the\ndistributions of the events causing emotions in target domains are dramatically\ndifferent than those in source domains, despite the distributions of emotional\nexpressions between domains are overlapped. Inspired by causal discovery, we\npropose a novel deep latent model in the variational autoencoder (VAE)\nframework, which not only captures the underlying latent structures of data but\nalso utilizes the easily transferable knowledge of emotions as the bridge to\nlink the distributions of events in different domains. To facilitate knowledge\ntransfer across domains, we also propose a novel variational posterior\nregularization technique to disentangle the latent representations of emotions\nfrom those of events in order to mitigate the damage caused by the spurious\ncorrelations related to the events in source domains. Through extensive\nexperiments, we demonstrate that our model outperforms the strongest baseline\nby approximately 11.05\\% on a Chinese benchmark and 2.45\\% on a English\nbenchmark in terms of weighted-average F1 score. We have released our source\ncode and the generated dataset publicly at:\nhttps://github.com/tk1363704/CAREL-VAE.\n","authors":["Yuncheng Hua","Yujin Huang","Shuo Huang","Tao Feng","Lizhen Qu","Chris Bain","Richard Bassed","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.15490v2.pdf","comment":"18 pages, 6 figures, 5 tables. The paper has been published in the\n  Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2412.05270v4","updated":"2025-02-17T08:27:58Z","published":"2024-12-06T18:55:34Z","title":"APOLLO: SGD-like Memory, AdamW-level Performance","summary":"  Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization.\n","authors":["Hanqing Zhu","Zhenyu Zhang","Wenyan Cong","Xi Liu","Sem Park","Vikas Chandra","Bo Long","David Z. Pan","Zhangyang Wang","Jinwon Lee"],"pdf_url":"https://arxiv.org/pdf/2412.05270v4.pdf","comment":"Accepted to MLSys 2025; the newest version with new experiments"},{"id":"http://arxiv.org/abs/2410.03138v2","updated":"2025-02-17T08:23:11Z","published":"2024-10-04T04:25:36Z","title":"Can LLMs Generate Diverse Molecules? Towards Alignment with Structural\n  Diversity","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nimpressive performance in molecular generation, which offers potential to\naccelerate drug discovery. However, the current LLMs overlook a critical\nrequirement for drug discovery: proposing a diverse set of molecules. This\ndiversity is essential for improving the chances of finding a viable drug, as\nit provides alternative molecules that may succeed where others fail in\nreal-world validations. Nevertheless, the LLMs often output structurally\nsimilar molecules. While decoding schemes like diverse beam search may enhance\ntextual diversity, this often does not align with molecular structural\ndiversity. In response, we propose a new method for fine-tuning molecular\ngenerative LLMs to autoregressively generate a set of structurally diverse\nmolecules, where each molecule is generated by conditioning on the previously\ngenerated molecules. Our approach consists of two stages: (1) supervised\nfine-tuning to adapt LLMs to autoregressively generate molecules in a sequence\nand (2) reinforcement learning to maximize structural diversity within the\ngenerated molecules. Our experiments show that the proposed approach enables\nLLMs to generate diverse molecules better than existing approaches for diverse\nsequence generation.\n","authors":["Hyosoon Jang","Yunhui Jang","Jaehyung Kim","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2410.03138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12522v2","updated":"2025-02-17T08:09:21Z","published":"2024-10-16T13:02:02Z","title":"MING: A Functional Approach to Learning Molecular Generative Models","summary":"  Traditional molecule generation methods often rely on sequence- or\ngraph-based representations, which can limit their expressive power or require\ncomplex permutation-equivariant architectures. This paper introduces a novel\nparadigm for learning molecule generative models based on functional\nrepresentations. Specifically, we propose Molecular Implicit Neural Generation\n(MING), a diffusion-based model that learns molecular distributions in the\nfunction space. Unlike standard diffusion processes in the data space, MING\nemploys a novel functional denoising probabilistic process, which jointly\ndenoises information in both the function's input and output spaces by\nleveraging an expectation-maximization procedure for latent implicit neural\nrepresentations of data. This approach enables a simple yet effective model\ndesign that accurately captures underlying function distributions. Experimental\nresults on molecule-related datasets demonstrate MING's superior performance\nand ability to generate plausible molecular samples, surpassing\nstate-of-the-art data-space methods while offering a more streamlined\narchitecture and significantly faster generation times. The code is available\nat https://github.com/v18nguye/MING.\n","authors":["Van Khoa Nguyen","Maciej Falkiewicz","Giangiacomo Mercatali","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2410.12522v2.pdf","comment":"AISTATS 2025"},{"id":"http://arxiv.org/abs/2502.11537v1","updated":"2025-02-17T08:06:10Z","published":"2025-02-17T08:06:10Z","title":"$\\text{M}^{\\text{3}}$: A Modular World Model over Streams of Tokens","summary":"  Token-based world models emerged as a promising modular framework, modeling\ndynamics over token streams while optimizing tokenization separately. While\nsuccessful in visual environments with discrete actions (e.g., Atari games),\ntheir broader applicability remains uncertain. In this paper, we introduce\n$\\text{M}^{\\text{3}}$, a $\\textbf{m}$odular $\\textbf{w}$orld $\\textbf{m}$odel\nthat extends this framework, enabling flexible combinations of observation and\naction modalities through independent modality-specific components.\n$\\text{M}^{\\text{3}}$ integrates several improvements from existing literature\nto enhance agent performance. Through extensive empirical evaluation across\ndiverse benchmarks, $\\text{M}^{\\text{3}}$ achieves state-of-the-art sample\nefficiency for planning-free world models. Notably, among these methods, it is\nthe first to reach a human-level median score on Atari 100K, with superhuman\nperformance on 13 games. We\n$\\href{https://github.com/leor-c/M3}{\\text{open-source our code and weights}}$.\n","authors":["Lior Cohen","Kaixin Wang","Bingyi Kang","Uri Gadot","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.11537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02663v2","updated":"2025-02-17T08:04:31Z","published":"2024-06-04T18:00:00Z","title":"Demystifying Spectral Bias on Real-World Data","summary":"  Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental\ntools in statistics and machine learning, with recent applications to highly\nover-parameterized deep neural networks. The ability of these tools to learn a\ntarget function is directly related to the eigenvalues of their kernel sampled\non the input data distribution. Targets that have support on higher eigenvalues\nare more learnable. However, solving such eigenvalue problems on real-world\ndata remains a challenge. Here, we consider cross-dataset learnability and show\nthat one may use eigenvalues and eigenfunctions associated with highly\nidealized data measures to reveal spectral bias on complex datasets and bound\nlearnability on real-world data. This allows us to leverage various symmetries\nthat realistic kernels manifest to unravel their spectral bias.\n","authors":["Itay Lavie","Zohar Ringel"],"pdf_url":"https://arxiv.org/pdf/2406.02663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09566v2","updated":"2025-02-17T08:04:06Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v2.pdf","comment":"13 pages, 4 figures, 4 tables (updated version, fixed typos and\n  formatting)"},{"id":"http://arxiv.org/abs/2405.14135v3","updated":"2025-02-17T07:52:16Z","published":"2024-05-23T03:19:02Z","title":"Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs","summary":"  Regional socioeconomic indicators are critical across various domains, yet\ntheir acquisition can be costly. Inferring global socioeconomic indicators from\na limited number of regional samples is essential for enhancing management and\nsustainability in urban areas and human settlements. Current inference methods\ntypically rely on spatial interpolation based on the assumption of spatial\ncontinuity, which does not adequately address the complex variations present\nwithin regional spaces. In this paper, we present GeoHG, the first space-aware\nsocioeconomic indicator inference method that utilizes a heterogeneous\ngraph-based structure to represent geospace for non-continuous inference.\nExtensive experiments demonstrate the effectiveness of GeoHG in comparison to\nexisting methods, achieving an $R^2$ score exceeding 0.8 under extreme data\nscarcity with a masked ratio of 95\\%.\n","authors":["Xingchen Zou","Jiani Huang","Xixuan Hao","Yuhao Yang","Haomin Wen","Yibo Yan","Chao Huang","Chao Chen","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2405.14135v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01222v2","updated":"2025-02-17T07:49:58Z","published":"2025-01-02T12:12:02Z","title":"Classification of Operational Records in Aviation Using Deep Learning\n  Approaches","summary":"  Ensuring safety in the aviation industry is critical, even minor anomalies\ncan lead to severe consequences. This study evaluates the performance of four\ndifferent models for DP (deep learning), including: Bidirectional Long\nShort-Term Memory (BLSTM), Convolutional Neural Networks (CNN), Long Short-Term\nMemory (LSTM), and Simple Recurrent Neural Networks (sRNN), on a multi-class\nclassification task involving Commercial, Military, and Private categories\nusing the Socrata aviation dataset of 4,864 records. The models were assessed\nusing a classification report, confusion matrix analysis, accuracy metrics,\nvalidation loss and accuracy curves. Among the models, BLSTM achieved the\nhighest overall accuracy of 72%, demonstrating superior performance in\nstability and balanced classification, while LSTM followed closely with 71%,\nexcelling in recall for the Commercial class. CNN and sRNN exhibited lower\naccuracies of 67% and 69%, with significant misclassifications in the Private\nclass. While the results highlight the strengths of BLSTM and LSTM in handling\nsequential dependencies and complex classification tasks, all models faced\nchallenges with class imbalance, particularly in predicting the Military and\nPrivate categories. Addressing these limitations through data augmentation,\nadvanced feature engineering, and ensemble learning techniques could enhance\nclassification accuracy and robustness. This study underscores the importance\nof selecting appropriate architectures for domain specific tasks\n","authors":["Aziida Nanyonga","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.01222v2.pdf","comment":"conference paper; aviation safety, NLP, DL, operational record\n  classification, Socrata"},{"id":"http://arxiv.org/abs/2502.11518v1","updated":"2025-02-17T07:39:34Z","published":"2025-02-17T07:39:34Z","title":"Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review","summary":"  Embodied multi-agent systems (EMAS) have attracted growing attention for\ntheir potential to address complex, real-world challenges in areas such as\nlogistics and robotics. Recent advances in foundation models pave the way for\ngenerative agents capable of richer communication and adaptive problem-solving.\nThis survey provides a systematic examination of how EMAS can benefit from\nthese generative capabilities. We propose a taxonomy that categorizes EMAS by\nsystem architectures and embodiment modalities, emphasizing how collaboration\nspans both physical and virtual contexts. Central building blocks, perception,\nplanning, communication, and feedback, are then analyzed to illustrate how\ngenerative techniques bolster system robustness and flexibility. Through\nconcrete examples, we demonstrate the transformative effects of integrating\nfoundation models into embodied, multi-agent frameworks. Finally, we discuss\nchallenges and future directions, underlining the significant promise of EMAS\nto reshape the landscape of AI-driven collaboration.\n","authors":["Di Wu","Xian Wei","Guang Chen","Hao Shen","Xiangfeng Wang","Wenhao Li","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2502.11518v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2502.11517v1","updated":"2025-02-17T07:39:16Z","published":"2025-02-17T07:39:16Z","title":"Learning to Keep a Promise: Scaling Language Model Decoding Parallelism\n  with Learned Asynchronous Decoding","summary":"  Decoding with autoregressive large language models (LLMs) traditionally\noccurs sequentially, generating one token after another. An emerging line of\nwork explored parallel decoding by identifying and simultaneously generating\nsemantically independent chunks of LLM responses. However, these techniques\nrely on hand-crafted heuristics tied to syntactic structures like lists and\nparagraphs, making them rigid and imprecise. We present PASTA, a learning-based\nsystem that teaches LLMs to identify semantic independence and express parallel\ndecoding opportunities in their own responses. At its core are PASTA-LANG and\nits interpreter: PASTA-LANG is an annotation language that enables LLMs to\nexpress semantic independence in their own responses; the language interpreter\nacts on these annotations to orchestrate parallel decoding on-the-fly at\ninference time. Through a two-stage finetuning process, we train LLMs to\ngenerate PASTA-LANG annotations that optimize both response quality and\ndecoding speed. Evaluation on AlpacaEval, an instruction following benchmark,\nshows that our approach Pareto-dominates existing methods in terms of decoding\nspeed and response quality; our results demonstrate geometric mean speedups\nranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to\n-7.1%, measured by length-controlled win rates against sequential decoding\nbaseline.\n","authors":["Tian Jin","Ellie Y. Cheng","Zack Ankner","Nikunj Saunshi","Blake M. Elias","Amir Yazdanbakhsh","Jonathan Ragan-Kelley","Suvinay Subramanian","Michael Carbin"],"pdf_url":"https://arxiv.org/pdf/2502.11517v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.22967v3","updated":"2025-02-17T07:35:14Z","published":"2024-10-30T12:26:02Z","title":"Adaptive NAD: Online and Self-adaptive Unsupervised Network Anomaly\n  Detector","summary":"  The widespread usage of the Internet of Things (IoT) has raised the risks of\ncyber threats, thus developing Anomaly Detection Systems (ADSs) that can adapt\nto evolving or new attacks is critical. Previous studies primarily focused on\noffline unsupervised learning methods to safeguard ADSs, which is not\napplicable in practical real-world applications. Besides, most of them strongly\nrely on assumptions of known legitimates and fail to satisfy the interpretable\nrequirements in security applications, creating barriers to the adoption in\npractice. In this paper, we design Adaptive NAD, a general framework to improve\nand interpret online unsupervised anomaly detection in security domains. An\ninterpretable two-layer anomaly detection strategy is proposed to generate\nreliable high-confidence pseudo-labels. Then, an online learning scheme is\nintroduced to update Adaptive NAD by a novel threshold calculation technique to\nadapt to new threats. Experimental results demonstrate that Adaptive NAD\nachieves more than 5.4%, 23.0%, and 3.2% improvements in SPAUC compared with\nstate-of-the-art solutions on the CIC-Darknet2020, CIC-DoHBrw-2020, and\nEdge-IIoTset datasets, respectively. The code is released at\nhttps://github.com/MyLearnCodeSpace/Adaptive-NAD.\n","authors":["Yachao Yuan","Yu Huang","Yali Yuan","Jin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.22967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10727v3","updated":"2025-02-17T07:31:30Z","published":"2024-02-16T14:40:22Z","title":"From Risk to Uncertainty: Generating Predictive Uncertainty Measures via\n  Bayesian Estimation","summary":"  There are various measures of predictive uncertainty in the literature, but\ntheir relationships to each other remain unclear. This paper uses a\ndecomposition of statistical pointwise risk into components, associated with\ndifferent sources of predictive uncertainty, namely aleatoric uncertainty\n(inherent data variability) and epistemic uncertainty (model-related\nuncertainty). Together with Bayesian methods, applied as an approximation, we\nbuild a framework that allows one to generate different predictive uncertainty\nmeasures.\n  We validate our method on image datasets by evaluating its performance in\ndetecting out-of-distribution and misclassified instances using the AUROC\nmetric. The experimental results confirm that the measures derived from our\nframework are useful for the considered downstream tasks.\n","authors":["Nikita Kotelevskii","Vladimir Kondratyev","Martin Tak√°ƒç","√âric Moulines","Maxim Panov"],"pdf_url":"https://arxiv.org/pdf/2402.10727v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11513v1","updated":"2025-02-17T07:28:52Z","published":"2025-02-17T07:28:52Z","title":"MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of\n  Large Language Models","summary":"  Large language models have demonstrated exceptional capabilities across\ndiverse tasks, but their fine-tuning demands significant memory, posing\nchallenges for resource-constrained environments. Zeroth-order (ZO)\noptimization provides a memory-efficient alternative by eliminating the need\nfor backpropagation. However, ZO optimization suffers from high gradient\nvariance, and prior research has largely focused on single-task learning,\nleaving its application to multi-task learning unexplored. Multi-task learning\nis crucial for leveraging shared knowledge across tasks to improve\ngeneralization, yet it introduces unique challenges under ZO settings, such as\namplified gradient variance and collinearity. In this paper, we present MaZO,\nthe first framework specifically designed for multi-task LLM fine-tuning under\nZO optimization. MaZO tackles these challenges at the parameter level through\ntwo key innovations: a weight importance metric to identify critical parameters\nand a multi-task weight update mask to selectively update these parameters,\nreducing the dimensionality of the parameter space and mitigating task\nconflicts. Experiments demonstrate that MaZO achieves state-of-the-art\nperformance, surpassing even multi-task learning methods designed for\nfirst-order optimization.\n","authors":["Zhen Zhang","Yifan Yang","Kai Zhen","Nathan Susanj","Athanasios Mouchtaris","Siegfried Kunzmann","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11513v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.12480v2","updated":"2025-02-17T07:23:58Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema matching (SM) and entity matching (EM) tasks are crucial for data\nintegration. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. This study presents the Knowledge-Compliant Matching Framework\n(KcMF), an LLM-based approach that addresses these issues without the need for\ndomain-specific fine-tuning. KcMF employs a once-and-for-all pseudo-code-based\ntask decomposition strategy to adopt natural language statements that guide LLM\nreasoning and reduce confusion across various task types. We also propose two\nmechanisms, Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build\ndomain knowledge sets when unstructured domain knowledge is lacking. Moreover,\nwe introduce a result-ensemble strategy to leverage multiple knowledge sources\nand suppress badly formatted outputs. Extensive evaluations confirm that KcMF\nclearly enhances five LLM backbones in both SM and EM tasks while outperforming\nthe non-LLM competitors by an average F1-score of 17.93%.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v2.pdf","comment":"under reveiw; new results and analysis added, typos corrected"},{"id":"http://arxiv.org/abs/2404.02115v3","updated":"2025-02-17T07:23:38Z","published":"2024-04-02T17:18:48Z","title":"GINopic: Topic Modeling with Graph Isomorphism Network","summary":"  Topic modeling is a widely used approach for analyzing and exploring large\ndocument collections. Recent research efforts have incorporated pre-trained\ncontextualized language models, such as BERT embeddings, into topic modeling.\nHowever, they often neglect the intrinsic informational value conveyed by\nmutual dependencies between words. In this study, we introduce GINopic, a topic\nmodeling framework based on graph isomorphism networks to capture the\ncorrelation between words. By conducting intrinsic (quantitative as well as\nqualitative) and extrinsic evaluations on diverse benchmark datasets, we\ndemonstrate the effectiveness of GINopic compared to existing topic models and\nhighlight its potential for advancing topic modeling.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2404.02115v3.pdf","comment":"Accepted as a long paper for NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2502.11509v1","updated":"2025-02-17T07:17:37Z","published":"2025-02-17T07:17:37Z","title":"DifCluE: Generating Counterfactual Explanations with Diffusion\n  Autoencoders and modal clustering","summary":"  Generating multiple counterfactual explanations for different modes within a\nclass presents a significant challenge, as these modes are distinct yet\nconverge under the same classification. Diffusion probabilistic models (DPMs)\nhave demonstrated a strong ability to capture the underlying modes of data\ndistributions. In this paper, we harness the power of a Diffusion Autoencoder\nto generate multiple distinct counterfactual explanations. By clustering in the\nlatent space, we uncover the directions corresponding to the different modes\nwithin a class, enabling the generation of diverse and meaningful\ncounterfactuals. We introduce a novel methodology, DifCluE, which consistently\nidentifies these modes and produces more reliable counterfactual explanations.\nOur experimental results demonstrate that DifCluE outperforms the current\nstate-of-the-art in generating multiple counterfactual explanations, offering a\nsignificant advance- ment in model interpretability.\n","authors":["Suparshva Jain","Amit Sangroya","Lovekesh Vig"],"pdf_url":"https://arxiv.org/pdf/2502.11509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22954v2","updated":"2025-02-17T07:16:40Z","published":"2024-10-30T12:09:29Z","title":"Retrieval-Augmented Generation with Estimation of Source Reliability","summary":"  Retrieval-augmented generation (RAG) addresses key limitations of large\nlanguage models (LLMs), such as hallucinations and outdated knowledge, by\nincorporating external databases. These databases typically consult multiple\nsources to encompass up-to-date and various information. However, standard RAG\nmethods often overlook the heterogeneous source reliability in the multi-source\ndatabase and retrieve documents solely based on relevance, making them prone to\npropagating misinformation. To address this, we propose Reliability-Aware RAG\n(RA-RAG) which estimates the reliability of multiple sources and incorporates\nthis information into both retrieval and aggregation processes. Specifically,\nit iteratively estimates source reliability and true answers for a set of\nqueries with no labelling. Then, it selectively retrieves relevant documents\nfrom a few of reliable sources and aggregates them using weighted majority\nvoting, where the selective retrieval ensures scalability while not\ncompromising the performance. We also introduce a benchmark designed to reflect\nreal-world scenarios with heterogeneous source reliability and demonstrate the\neffectiveness of RA-RAG compared to a set of baselines.\n","authors":["Jeongyeon Hwang","Junyoung Park","Hyejin Park","Sangdon Park","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2410.22954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11506v1","updated":"2025-02-17T07:14:30Z","published":"2025-02-17T07:14:30Z","title":"Learning Surrogate Potential Mean Field Games via Gaussian Processes: A\n  Data-Driven Approach to Ill-Posed Inverse Problems","summary":"  Mean field games (MFGs) describe the collective behavior of large populations\nof interacting agents. In this work, we tackle ill-posed inverse problems in\npotential MFGs, aiming to recover the agents' population, momentum, and\nenvironmental setup from limited, noisy measurements and partial observations.\nThese problems are ill-posed because multiple MFG configurations can explain\nthe same data, or different parameters can yield nearly identical observations.\nNonetheless, they remain crucial in practice for real-world scenarios where\ndata are inherently sparse or noisy, or where the MFG structure is not fully\ndetermined. Our focus is on finding surrogate MFGs that accurately reproduce\nthe observed data despite these challenges. We propose two Gaussian process\n(GP)-based frameworks: an inf-sup formulation and a bilevel approach. The\nchoice between them depends on whether the unknown parameters introduce\nconcavity in the objective. In the inf-sup framework, we use the linearity of\nGPs and their parameterization structure to maintain convex-concave properties,\nallowing us to apply standard convex optimization algorithms. In the bilevel\nframework, we employ a gradient-descent-based algorithm and introduce two\nmethods for computing the outer gradient. The first method leverages an\nexisting solver for the inner potential MFG and applies automatic\ndifferentiation, while the second adopts an adjoint-based strategy that\ncomputes the outer gradient independently of the inner solver. Our numerical\nexperiments show that when sufficient prior information is available, the\nunknown parameters can be accurately recovered. Otherwise, if prior information\nis limited, the inverse problem is ill-posed, but our frameworks can still\nproduce surrogate MFG models that closely match observed data.\n","authors":["Jingguo Zhang","Xianjin Yang","Chenchen Mou","Chao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11506v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2502.11505v1","updated":"2025-02-17T07:12:39Z","published":"2025-02-17T07:12:39Z","title":"A GNN-based Spectral Filtering Mechanism for Imbalance Classification in\n  Network Digital Twin","summary":"  Graph Neural Networks are gaining attention in Fifth-Generation (5G) core\nnetwork digital twins, which are data-driven complex systems with numerous\ncomponents. Analyzing these data can be challenging due to rare failure types,\nleading to imbalanced classification in multiclass settings. Digital twins of\n5G networks increasingly employ graph classification as the main method for\nidentifying failure types. However, the skewed distribution of failure\noccurrences is a major class imbalance issue that prevents effective graph data\nmining. Previous studies have not sufficiently tackled this complex problem. In\nthis paper, we propose Class-Fourier Graph Neural Network (CF-GNN) introduces a\nclass-oriented spectral filtering mechanism that ensures precise classification\nby estimating a unique spectral filter for each class. We employ eigenvalue and\neigenvector spectral filtering to capture and adapt to variations in the\nminority classes, ensuring accurate class-specific feature discrimination, and\nadept at graph representation learning for complex local structures among\nneighbors in an end-to-end setting. Extensive experiments have demonstrated\nthat the proposed CF-GNN could help with both the creation of new techniques\nfor enhancing classifiers and the investigation of the characteristics of the\nmulti-class imbalanced data in a network digital twin system.\n","authors":["Abubakar Isah","Ibrahim Aliyu","Sulaiman Muhammad Rashid","Jaehyung Park","Minsoo Hahn","Jinsul Kim"],"pdf_url":"https://arxiv.org/pdf/2502.11505v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.06595"},{"id":"http://arxiv.org/abs/2502.11504v1","updated":"2025-02-17T07:11:46Z","published":"2025-02-17T07:11:46Z","title":"Accelerated Gradient-based Design Optimization Via Differentiable\n  Physics-Informed Neural Operator: A Composites Autoclave Processing Case\n  Study","summary":"  Simulation and optimization are crucial for advancing the engineering design\nof complex systems and processes. Traditional optimization methods require\nsubstantial computational time and effort due to their reliance on\nresource-intensive simulations, such as finite element analysis, and the\ncomplexity of rigorous optimization algorithms. Data-agnostic AI-based\nsurrogate models, such as Physics-Informed Neural Operators (PINOs), offer a\npromising alternative to these conventional simulations, providing drastically\nreduced inference time, unparalleled data efficiency, and zero-shot\nsuper-resolution capability. However, the predictive accuracy of these models\nis often constrained to small, low-dimensional design spaces or systems with\nrelatively simple dynamics. To address this, we introduce a novel\nPhysics-Informed DeepONet (PIDON) architecture, which extends the capabilities\nof conventional neural operators to effectively model the nonlinear behavior of\ncomplex engineering systems across high-dimensional design spaces and a wide\nrange of dynamic design configurations. This new architecture outperforms\nexisting SOTA models, enabling better predictions across broader design spaces.\nLeveraging PIDON's differentiability, we integrate a gradient-based\noptimization approach using the Adam optimizer to efficiently determine optimal\ndesign variables. This forms an end-to-end gradient-based optimization\nframework that accelerates the design process while enhancing scalability and\nefficiency. We demonstrate the effectiveness of this framework in the\noptimization of aerospace-grade composites curing processes achieving a 3x\nspeedup in obtaining optimal design variables compared to gradient-free\nmethods. Beyond composites processing, the proposed model has the potential to\nbe used as a scalable and efficient optimization tool for broader applications\nin advanced engineering and digital twin systems.\n","authors":["Janak M. Patel","Milad Ramezankhani","Anirudh Deodhar","Dagnachew Birru"],"pdf_url":"https://arxiv.org/pdf/2502.11504v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.15225v2","updated":"2025-02-17T07:09:37Z","published":"2024-04-23T16:54:56Z","title":"PHLP: Sole Persistent Homology for Link Prediction - Interpretable\n  Feature Extraction","summary":"  Link prediction (LP), inferring the connectivity between nodes, is a\nsignificant research area in graph data, where a link represents essential\ninformation on relationships between nodes. Although graph neural network\n(GNN)-based models have achieved high performance in LP, understanding why they\nperform well is challenging because most comprise complex neural networks. We\nemploy persistent homology (PH), a topological data analysis method that helps\nanalyze the topological information of graphs, to interpret the features used\nfor prediction. We propose a novel method that employs PH for LP (PHLP)\nfocusing on how the presence or absence of target links influences the overall\ntopology. The PHLP utilizes the angle hop subgraph and new node labeling called\ndegree double radius node labeling (Degree DRNL), distinguishing the\ninformation of graphs better than DRNL. Using only a classifier, PHLP performs\nsimilarly to state-of-the-art (SOTA) models on most benchmark datasets.\nIncorporating the outputs calculated using PHLP into the existing GNN-based\nSOTA models improves performance across all benchmark datasets. To the best of\nour knowledge, PHLP is the first method of applying PH to LP without GNNs. The\nproposed approach, employing PH while not relying on neural networks, enables\nthe identification of crucial factors for improving performance.\n","authors":["Junwon You","Eunwoo Heo","Jae-Hun Jung"],"pdf_url":"https://arxiv.org/pdf/2404.15225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00201v3","updated":"2025-02-17T06:57:41Z","published":"2022-12-31T13:48:42Z","title":"Exploring Singularities in point clouds with the graph Laplacian: An\n  explicit approach","summary":"  We develop theory and methods that use the graph Laplacian to analyze the\ngeometry of the underlying manifold of datasets. Our theory provides\ntheoretical guarantees and explicit bounds on the functional forms of the graph\nLaplacian when it acts on functions defined close to singularities of the\nunderlying manifold. We use these explicit bounds to develop tests for\nsingularities and propose methods that can be used to estimate geometric\nproperties of singularities in the datasets.\n","authors":["Martin Andersson","Benny Avelin"],"pdf_url":"https://arxiv.org/pdf/2301.00201v3.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.11490v1","updated":"2025-02-17T06:49:34Z","published":"2025-02-17T06:49:34Z","title":"GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale\n  Recommendations","summary":"  Web recommendations provide personalized items from massive catalogs for\nusers, which rely heavily on retrieval stages to trade off the effectiveness\nand efficiency of selecting a small relevant set from billion-scale candidates\nin online digital platforms. As one of the largest Chinese search engine and\nnews feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based\nApproximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance\nestimation and efficient search for relevant items. However, current retrieval\nat Baidu fails in comprehensive user-item relational understanding due to\ndissected interaction modeling, and performs inefficiently in large-scale\ngraph-based ANNS because of suboptimal traversal navigation and the GPU\ncomputational bottleneck under high concurrency. To this end, we propose a\nGPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to\nachieve effective yet efficient retrieval in web-scale recommendations. First,\nwe propose a multi-relational user-item relevance metric learning method that\nunifies diverse user behaviors through multi-objective optimization and employs\na self-covariant loss to enhance pathfinding performance. Second, we develop a\nhierarchical parallel graph-based ANNS to boost graph retrieval throughput,\nwhich conducts breadth-depth-balanced searches on a large-scale item graph and\ncost-effectively handles irregular neural computation via adaptive aggregation\non GPUs. In addition, we integrate system optimization strategies in the\ndeployment of GMP-GR in Baidu. Extensive experiments demonstrate the\nsuperiority of GMP-GR in retrieval accuracy and efficiency. Deployed across\nmore than twenty applications at Baidu, GMP-GR serves hundreds of millions of\nusers with a throughput exceeding one hundred million requests per second.\n","authors":["Zhuoning Guo","Guangxing Chen","Qian Gao","Xiaochao Liao","Jianjia Zheng","Lu Shen","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11484v1","updated":"2025-02-17T06:38:43Z","published":"2025-02-17T06:38:43Z","title":"Dictionary-Learning-Based Data Pruning for System Identification","summary":"  System identification is normally involved in augmenting time series data by\ntime shifting and nonlinearisation (via polynomial basis), which introduce\nredundancy both feature-wise and sample-wise. Many research works focus on\nreducing redundancy feature-wise, while less attention is paid to sample-wise\nredundancy. This paper proposes a novel data pruning method, called\n(mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary\nlearning. Time series data is represented by some representative samples,\ncalled atoms, via dictionary learning. The useful samples are selected based on\ntheir correlation with the atoms. The method is tested on one simulated dataset\nand two benchmark datasets. The R-squared between the coefficients of models\ntrained on the full and the coefficients of models trained on pruned datasets\nis adopted to evaluate the performance of data pruning methods. It is found\nthat the proposed method significantly outperforms the random pruning method.\n","authors":["Tingna Wang","Sikai Zhang","Limin Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11483v1","updated":"2025-02-17T06:36:45Z","published":"2025-02-17T06:36:45Z","title":"No-regret incentive-compatible online learning under exact truthfulness\n  with non-myopic experts","summary":"  We study an online forecasting setting in which, over $T$ rounds, $N$\nstrategic experts each report a forecast to a mechanism, the mechanism selects\none forecast, and then the outcome is revealed. In any given round, each expert\nhas a belief about the outcome, but the expert wishes to select its report so\nas to maximize the total number of times it is selected. The goal of the\nmechanism is to obtain low belief regret: the difference between its cumulative\nloss (based on its selected forecasts) and the cumulative loss of the best\nexpert in hindsight (as measured by the experts' beliefs). We consider exactly\ntruthful mechanisms for non-myopic experts, meaning that truthfully reporting\nits belief strictly maximizes the expert's subjective probability of being\nselected in any future round. Even in the full-information setting, it is an\nopen problem to obtain the first no-regret exactly truthful mechanism in this\nsetting. We develop the first no-regret mechanism for this setting via an\nonline extension of the Independent-Event Lotteries Forecasting Competition\nMechanism (I-ELF). By viewing this online I-ELF as a novel instance of Follow\nthe Perturbed Leader (FPL) with noise based on random walks with loss-dependent\nperturbations, we obtain $\\tilde{O}(\\sqrt{T N})$ regret. Our results are fueled\nby new tail bounds for Poisson binomial random variables that we develop. We\nextend our results to the bandit setting, where we give an exactly truthful\nmechanism obtaining $\\tilde{O}(T^{2/3} N^{1/3})$ regret; this is the first\nno-regret result even among approximately truthful mechanisms.\n","authors":["Junpei Komiyama","Nishant A. Mehta","Ali Mortazavi"],"pdf_url":"https://arxiv.org/pdf/2502.11483v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2502.11482v1","updated":"2025-02-17T06:35:42Z","published":"2025-02-17T06:35:42Z","title":"DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free\n  Continual Learning","summary":"  Continual learning (CL) is essential for Large Language Models (LLMs) to\nadapt to evolving real-world demands, yet they are susceptible to catastrophic\nforgetting (CF). While traditional CF solutions rely on expensive data\nrehearsal, recent rehearsal-free methods employ model-based and\nregularization-based strategies to address this issue. However, these\napproaches often neglect the model's plasticity, which is crucial to achieving\noptimal performance on newly learned tasks. Consequently, a key challenge in CL\nis striking a balance between preserving plasticity and mitigating CF. To\ntackle this challenge, we propose the $\\textbf{D}$ecomposed\n$\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which\nexplicitly decouples and learns both task-specific and task-shared knowledge\nusing high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA\ndynamically adjusts the weights of adapters of different ranks based on their\nrelevance and distinction from previous tasks, allowing the model to acquire\nnew task-specific skills while effectively retaining previously learned\nknowledge. Specifically, we implement a decomposed component weighting strategy\ncomprising learnable components that collectively generate attention-based\nweights, allowing the model to integrate and utilize diverse knowledge from\neach DATA. Extensive experiments on three widely used benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance. Notably, our\napproach significantly enhances model plasticity and mitigates CF by extending\nlearnable components and employing stochastic restoration during training\niterations.\n","authors":["Huanxuan Liao","Shizhu He","Yupu Hao","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11480v1","updated":"2025-02-17T06:34:58Z","published":"2025-02-17T06:34:58Z","title":"Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian\n  Optimization Perspective","summary":"  Offline model-based reinforcement learning (MBRL) serves as a competitive\nframework that can learn well-performing policies solely from pre-collected\ndata with the help of learned dynamics models. To fully unleash the power of\noffline MBRL, model selection plays a pivotal role in determining the dynamics\nmodel utilized for downstream policy learning. However, offline MBRL\nconventionally relies on validation or off-policy evaluation, which are rather\ninaccurate due to the inherent distribution shift in offline RL. To tackle\nthis, we propose BOMS, an active model selection framework that enhances model\nselection in offline MBRL with only a small online interaction budget, through\nthe lens of Bayesian optimization (BO). Specifically, we recast model selection\nas BO and enable probabilistic inference in BOMS by proposing a novel\nmodel-induced kernel, which is theoretically grounded and computationally\nefficient. Through extensive experiments, we show that BOMS improves over the\nbaseline methods with a small amount of online interaction comparable to only\n$1\\%$-$2.5\\%$ of offline training data on various RL tasks.\n","authors":["Yu-Wei Yang","Yun-Ming Chan","Wei Hung","Xi Liu","Ping-Chun Hsieh"],"pdf_url":"https://arxiv.org/pdf/2502.11480v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.11478v1","updated":"2025-02-17T06:29:11Z","published":"2025-02-17T06:29:11Z","title":"TAPS: Throat and Acoustic Paired Speech Dataset for Deep Learning-Based\n  Speech Enhancement","summary":"  In high-noise environments such as factories, subways, and busy streets,\ncapturing clear speech is challenging due to background noise. Throat\nmicrophones provide a solution with their noise-suppressing properties,\nreducing the noise while recording speech. However, a significant limitation\nremains: high-frequency information is attenuated as sound waves pass through\nskin and tissue, reducing speech clarity. Recent deep learning approaches have\nshown promise in enhancing throat microphone recordings, but further progress\nis constrained by the absence of standardized dataset. We introduce a throat\nand acoustic paired speech dataset (TAPS), a collection of paired utterances\nrecorded from 60 native Korean speakers using throat and acoustic microphones.\nTo demonstrate the TAPS's utility, we tested three baseline deep learning\nmodels and identified the mapping-based approach as superior in improving\nspeech quality and restoring content. Additionally, we propose an optimal\nmethod to mitigate the signal mismatch between throat and acoustic microphones,\nensuring model performance. These results highlight the potential of TAPS to\nserve as a standardized dataset and advance research in throat microphone-based\nspeech enhancement.\n","authors":["Yunsik Kim","Yonghun Song","Yoonyoung Chung"],"pdf_url":"https://arxiv.org/pdf/2502.11478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03226v3","updated":"2025-02-17T06:27:16Z","published":"2025-01-06T18:59:13Z","title":"BoostStep: Boosting mathematical capability of Large Language Models via\n  improved single-step reasoning","summary":"  Large language models (LLMs) have demonstrated impressive ability in solving\ncomplex mathematical problems with multi-step reasoning and can be further\nenhanced with well-designed in-context learning (ICL) examples. However, this\npotential is often constrained by two major challenges in ICL: granularity\nmismatch and irrelevant information. We observe that while LLMs excel at\ndecomposing mathematical problems, they often struggle with reasoning errors in\nfine-grained steps. Moreover, ICL examples retrieved at the question level may\nomit critical steps or even mislead the model with irrelevant details. To\naddress this issue, we propose BoostStep, a method that enhances reasoning\naccuracy through step-aligned ICL, a novel mechanism that carefully aligns\nretrieved reference steps with the corresponding reasoning steps. Additionally,\nBoostStep incorporates an effective \"first-try\" strategy to deliver exemplars\nhighly relevant to the current state of reasoning. BoostStep is a flexible and\npowerful method that integrates seamlessly with chain-of-thought (CoT) and tree\nsearch algorithms, refining both candidate selection and decision-making.\nEmpirical results show that BoostStep improves GPT-4o's CoT performance by 4.6%\nacross mathematical benchmarks, significantly surpassing traditional few-shot\nlearning's 1.2%. Moreover, it can achieve an additional 7.5\\% gain combined\nwith tree search. Surprisingly, it enhances state-of-the-art LLMs to solve\nchallenging math problems using simpler examples. It improves\nDeepSeek-R1-671B's performance on AIME by 2.2%, leveraging simple examples only\nfrom the MATH dataset.\n","authors":["Beichen Zhang","Yuhong Liu","Xiaoyi Dong","Yuhang Zang","Pan Zhang","Haodong Duan","Yuhang Cao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.03226v3.pdf","comment":"Codes and Data are available at\n  https://github.com/beichenzbc/BoostStep"},{"id":"http://arxiv.org/abs/2403.16883v5","updated":"2025-02-17T06:06:11Z","published":"2024-03-25T15:53:32Z","title":"GLAD: Improving Latent Graph Generative Modeling with Simple\n  Quantization","summary":"  Learning graph generative models over latent spaces has received less\nattention compared to models that operate on the original data space and has so\nfar demonstrated lacklustre performance. We present GLAD a latent space graph\ngenerative model. Unlike most previous latent space graph generative models,\nGLAD operates on a discrete latent space that preserves to a significant extent\nthe discrete nature of the graph structures making no unnatural assumptions\nsuch as latent space continuity. We learn the prior of our discrete latent\nspace by adapting diffusion bridges to its structure. By operating over an\nappropriately constructed latent space we avoid relying on decompositions that\nare often used in models that operate in the original data space. We present\nexperiments on a series of graph benchmark datasets that demonstrates GLAD as\nthe first equivariant latent graph generative method achieves competitive\nperformance with the state of the art baselines.\n","authors":["Van Khoa Nguyen","Yoann Boget","Frantzeska Lavda","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2403.16883v5.pdf","comment":"AAAI 2025 (Previously accepted at SPIGM ICML 2024 workshop)"},{"id":"http://arxiv.org/abs/2502.11467v1","updated":"2025-02-17T05:56:11Z","published":"2025-02-17T05:56:11Z","title":"Approximation of Permutation Invariant Polynomials by Transformers:\n  Efficient Construction in Column-Size","summary":"  Transformers are a type of neural network that have demonstrated remarkable\nperformance across various domains, particularly in natural language processing\ntasks. Motivated by this success, research on the theoretical understanding of\ntransformers has garnered significant attention. A notable example is the\nmathematical analysis of their approximation power, which validates the\nempirical expressive capability of transformers. In this study, we investigate\nthe ability of transformers to approximate column-symmetric polynomials, an\nextension of symmetric polynomials that take matrices as input. Consequently,\nwe establish an explicit relationship between the size of the transformer\nnetwork and its approximation capability, leveraging the parameter efficiency\nof transformers and their compatibility with symmetry by focusing on the\nalgebraic properties of symmetric polynomials.\n","authors":["Naoki Takeshita","Masaaki Imaizumi"],"pdf_url":"https://arxiv.org/pdf/2502.11467v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2502.11466v1","updated":"2025-02-17T05:52:44Z","published":"2025-02-17T05:52:44Z","title":"GiFT: Gibbs Fine-Tuning for Code Generation","summary":"  Training Large Language Models (LLMs) with synthetic data is a prevalent\npractice in code generation. A key approach is self-training, where LLMs are\niteratively trained on self-generated correct code snippets. In this case, the\nself-generated codes are drawn from a conditional distribution, conditioned on\na specific seed description. However, the seed description is not the only\nvalid representation that aligns with its intended meaning. With all valid\ndescriptions and codes forming a joint space, codes drawn from the conditional\ndistribution would lead to an underrepresentation of the full description-code\nspace. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training\nmethod inspired by Gibbs sampling. GiFT allows self-generated data to be drawn\nfrom the marginal distribution of the joint space, thereby mitigating the\nbiases inherent in conditional sampling. We provide a theoretical analysis\ndemonstrating the potential benefits of fine-tuning LLMs with code derived from\nthe marginal distribution. Furthermore, we propose a perplexity-based code\nselection method to mitigate the imbalanced long-tail distribution of the\nself-generated codes. Empirical evaluation of two LLMs across four datasets\ndemonstrates that GiFT achieves superior performance, particularly on more\nchallenging benchmarks.\n","authors":["Haochen Li","Wanjin Feng","Xin Zhou","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2502.11466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11465v1","updated":"2025-02-17T05:52:09Z","published":"2025-02-17T05:52:09Z","title":"All Models Are Miscalibrated, But Some Less So: Comparing Calibration\n  with Conditional Mean Operators","summary":"  When working in a high-risk setting, having well calibrated probabilistic\npredictive models is a crucial requirement. However, estimators for calibration\nerror are not always able to correctly distinguish which model is better\ncalibrated. We propose the \\emph{conditional kernel calibration error} (CKCE)\nwhich is based on the Hilbert-Schmidt norm of the difference between\nconditional mean operators. By working directly with the definition of strong\ncalibration as the distance between conditional distributions, which we\nrepresent by their embeddings in reproducing kernel Hilbert spaces, the CKCE is\nless sensitive to the marginal distribution of predictive models. This makes it\nmore effective for relative comparisons than previously proposed calibration\nmetrics. Our experiments, using both synthetic and real data, show that CKCE\nprovides a more consistent ranking of models by their calibration error and is\nmore robust against distribution shift.\n","authors":["Peter Moskvichev","Dino Sejdinovic"],"pdf_url":"https://arxiv.org/pdf/2502.11465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11462v1","updated":"2025-02-17T05:42:03Z","published":"2025-02-17T05:42:03Z","title":"LMFCA-Net: A Lightweight Model for Multi-Channel Speech Enhancement with\n  Efficient Narrow-Band and Cross-Band Attention","summary":"  Deep learning based end-to-end multi-channel speech enhancement methods have\nachieved impressive performance by leveraging sub-band, cross-band, and spatial\ninformation. However, these methods often demand substantial computational\nresources, limiting their practicality on terminal devices. This paper presents\na lightweight multi-channel speech enhancement network with decoupled fully\nconnected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis\ndecoupled fully-connected attention (T-FCA) and frequency-axis decoupled\nfully-connected attention (F-FCA) mechanisms to effectively capture long-range\nnarrow-band and cross-band information without recurrent units. Experimental\nresults show that LMFCA-Net performs comparably to state-of-the-art methods\nwhile significantly reducing computational complexity and latency, making it a\npromising solution for practical applications.\n","authors":["Yaokai Zhang","Hanchen Pei","Wanqi Wang","Gongping Huang"],"pdf_url":"https://arxiv.org/pdf/2502.11462v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.11458v1","updated":"2025-02-17T05:33:11Z","published":"2025-02-17T05:33:11Z","title":"Towards Efficient Pre-training: Exploring FP4 Precision in Large\n  Language Models","summary":"  The burgeoning computational demands for training large language models\n(LLMs) necessitate efficient methods, including quantized training, which\nleverages low-bit arithmetic operations to reduce costs. While FP8 precision\nhas shown potential, leveraging FP4 remains challenging due to inherent\nquantization errors and limited representation capability. Based on the\nTransformer architecture, we present an FP4 training scheme for LLMs,\novercoming these obstacles through mixed-precision quantization strategies\ntailed for different modules and training stages. This allows us to apply the\nprecision level suitable to distinct components within the model, ensuring that\nmulti-head attention and linear layers are handled appropriately. Our\npretraining recipe ensures stability in backpropagation by incorporating\nfine-grained quantization methods with a target precision training schedule.\nExperimental results demonstrate that our FP4 training scheme achieves accuracy\ncomparable to BF16 and FP8, with smaller theoretical computational cost. With\nthe advent of next-generation hardware supporting FP4, our method sets the\nfoundation for efficient ultra-low precision training.\n","authors":["Jiecheng Zhou","Ding Tang","Rong Fu","Boni Hu","Haoran Xu","Yi Wang","Zhilin Pei","Zhongling Su","Liang Liu","Xingcheng Zhang","Weiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11458v1.pdf","comment":"8 pages, 2 figure"},{"id":"http://arxiv.org/abs/2404.09077v2","updated":"2025-02-17T05:32:59Z","published":"2024-04-13T20:43:46Z","title":"CuriousLLM: Elevating Multi-Document Question Answering with\n  LLM-Enhanced Knowledge Graph Reasoning","summary":"  Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.\n","authors":["Zukang Yang","Zixuan Zhu","Xuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.09077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11453v1","updated":"2025-02-17T05:28:04Z","published":"2025-02-17T05:28:04Z","title":"Connector-S: A Survey of Connectors in Multi-modal Large Language Models","summary":"  With the rapid advancements in multi-modal large language models (MLLMs),\nconnectors play a pivotal role in bridging diverse modalities and enhancing\nmodel performance. However, the design and evolution of connectors have not\nbeen comprehensively analyzed, leaving gaps in understanding how these\ncomponents function and hindering the development of more powerful connectors.\nIn this survey, we systematically review the current progress of connectors in\nMLLMs and present a structured taxonomy that categorizes connectors into atomic\noperations (mapping, compression, mixture of experts) and holistic designs\n(multi-layer, multi-encoder, multi-modal scenarios), highlighting their\ntechnical contributions and advancements. Furthermore, we discuss several\npromising research frontiers and challenges, including high-resolution input,\ndynamic compression, guide information selection, combination strategy, and\ninterpretability. This survey is intended to serve as a foundational reference\nand a clear roadmap for researchers, providing valuable insights into the\ndesign and optimization of next-generation connectors to enhance the\nperformance and adaptability of MLLMs.\n","authors":["Xun Zhu","Zheng Zhang","Xi Chen","Yiming Shi","Miao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11450v1","updated":"2025-02-17T05:22:23Z","published":"2025-02-17T05:22:23Z","title":"Fishing For Cheap And Efficient Pruners At Initialization","summary":"  Pruning offers a promising solution to mitigate the associated costs and\nenvironmental impact of deploying large deep neural networks (DNNs).\nTraditional approaches rely on computationally expensive trained models or\ntime-consuming iterative prune-retrain cycles, undermining their utility in\nresource-constrained settings. To address this issue, we build upon the\nestablished principles of saliency (LeCun et al., 1989) and connection\nsensitivity (Lee et al., 2018) to tackle the challenging problem of one-shot\npruning neural networks (NNs) before training (PBT) at initialization. We\nintroduce Fisher-Taylor Sensitivity (FTS), a computationally cheap and\nefficient pruning criterion based on the empirical Fisher Information Matrix\n(FIM) diagonal, offering a viable alternative for integrating first- and\nsecond-order information to identify a model's structurally important\nparameters. Although the FIM-Hessian equivalency only holds for convergent\nmodels that maximize the likelihood, recent studies (Karakida et al., 2019)\nsuggest that, even at initialization, the FIM captures essential geometric\ninformation of parameters in overparameterized NNs, providing the basis for our\nmethod. Finally, we demonstrate empirically that layer collapse, a critical\nlimitation of data-dependent pruning methodologies, is easily overcome by\npruning within a single training epoch after initialization. We perform\nexperiments on ResNet18 and VGG19 with CIFAR-10 and CIFAR-100, widely used\nbenchmarks in pruning research. Our method achieves competitive performance\nagainst state-of-the-art techniques for one-shot PBT, even under extreme\nsparsity conditions. Our code is made available to the public.\n","authors":["Ivo Gollini Navarrete","Nicolas Mauricio Cuadrado","Jose Renato Restom","Martin Tak√°ƒç","Samuel Horv√°th"],"pdf_url":"https://arxiv.org/pdf/2502.11450v1.pdf","comment":"8 pages of main content (excluding references), 2 figures, 2 tables,\n  1 algorithm, and 11 pages of appendix. Code available at\n  https://github.com/Gollini/Fisher_Taylor_Sensitivity"},{"id":"http://arxiv.org/abs/2305.00660v2","updated":"2025-02-17T05:12:27Z","published":"2023-05-01T05:16:07Z","title":"An Iterative Algorithm for Rescaled Hyperbolic Functions Regression","summary":"  Large language models (LLMs) have numerous real-life applications across\nvarious domains, such as natural language translation, sentiment analysis,\nlanguage modeling, chatbots and conversational agents, creative writing, text\nclassification, summarization, and generation. LLMs have shown great promise in\nimproving the accuracy and efficiency of these tasks, and have the potential to\nrevolutionize the field of natural language processing (NLP) in the years to\ncome. Exponential function based attention unit is a fundamental element in\nLLMs. Several previous works have studied the convergence of exponential\nregression and softmax regression.\n  In this paper, we propose an iterative algorithm to solve a rescaled version\nof the slightly different formulation of the softmax regression problem that\narises in attention mechanisms of large language models. Specifically, we\nconsider minimizing the squared loss between a certain function, which can be\neither the exponential function, hyperbolic sine function, or hyperbolic cosine\nfunction, and its inner product with a target $n$-dimensional vector $b$,\nscaled by the normalization term. This ``rescaled softmax regression'' differs\nfrom classical softmax regression in the location of the normalization factor.\n  The efficiency and generalizability of this framework to multiple hyperbolic\nfunctions make it relevant for optimizing attention mechanisms. The analysis\nalso leads to a corollary bounding solution changes under small perturbations\nfor in-context learning. Limitations and societal impact are discussed.\n","authors":["Yeqi Gao","Zhao Song","Junze Yin"],"pdf_url":"https://arxiv.org/pdf/2305.00660v2.pdf","comment":"AISTATS 2025"},{"id":"http://arxiv.org/abs/2502.11447v1","updated":"2025-02-17T05:09:46Z","published":"2025-02-17T05:09:46Z","title":"Does Editing Provide Evidence for Localization?","summary":"  A basic aspiration for interpretability research in large language models is\nto \"localize\" semantically meaningful behaviors to particular components within\nthe LLM. There are various heuristics for finding candidate locations within\nthe LLM. Once a candidate localization is found, it can be assessed by editing\nthe internal representations at the corresponding localization and checking\nwhether this induces model behavior that is consistent with the semantic\ninterpretation of the localization. The question we address here is: how strong\nis the evidence provided by such edits? To assess localization, we want to\nassess the effect of the optimal intervention at a particular location. The key\nnew technical tool is a way of adapting LLM alignment techniques to find such\noptimal localized edits. With this tool in hand, we give an example where the\nedit-based evidence for localization appears strong, but where localization\nclearly fails. Indeed, we find that optimal edits at random localizations can\nbe as effective as aligning the full model. In aggregate, our results suggest\nthat merely observing that localized edits induce targeted changes in behavior\nprovides little to no evidence that these locations actually encode the target\nbehavior.\n","authors":["Zihao Wang","Victor Veitch"],"pdf_url":"https://arxiv.org/pdf/2502.11447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11442v1","updated":"2025-02-17T04:58:14Z","published":"2025-02-17T04:58:14Z","title":"Multi-Turn Multi-Modal Question Clarification for Enhanced\n  Conversational Understanding","summary":"  Conversational query clarification enables users to refine their search\nqueries through interactive dialogue, improving search effectiveness.\nTraditional approaches rely on text-based clarifying questions, which often\nfail to capture complex user preferences, particularly those involving visual\nattributes. While recent work has explored single-turn multi-modal\nclarification with images alongside text, such methods do not fully support the\nprogressive nature of user intent refinement over multiple turns. Motivated by\nthis, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,\nwhich combines text and visual modalities to refine user queries in a\nmulti-turn conversation. To facilitate this task, we create a large-scale\ndataset named ClariMM comprising over 13k multi-turn interactions and 33k\nquestion-answer pairs containing multi-modal clarifying questions. We propose\nMario, a retrieval framework that employs a two-phase ranking strategy: initial\nretrieval with BM25, followed by a multi-modal generative re-ranking model that\nintegrates textual and visual information from conversational history. Our\nexperiments show that multi-turn multi-modal clarification outperforms\nuni-modal and single-turn approaches, improving MRR by 12.88%. The gains are\nmost significant in longer interactions, demonstrating the value of progressive\nrefinement for complex queries.\n","authors":["Kimia Ramezan","Alireza Amiri Bavandpour","Yifei Yuan","Clemencia Siro","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2502.11442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11439v1","updated":"2025-02-17T04:54:42Z","published":"2025-02-17T04:54:42Z","title":"An Efficient Row-Based Sparse Fine-Tuning","summary":"  Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning (SFT)\nand Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SFT framework, based on ideas from neural network pruning. At\na high level, we first identify \"important\" neurons/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Using experiments on common language tasks, we\ndemonstrate that our method significantly improves the memory efficiency of SFT\nwithout increasing training time complexity and implementation complexity,\nwhile achieving accuracy comparable to state-of-the-art methods such as LoRA\nand its variants.\n","authors":["Cen-Jhih Li","Aditya Bhaskara"],"pdf_url":"https://arxiv.org/pdf/2502.11439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11436v1","updated":"2025-02-17T04:50:41Z","published":"2025-02-17T04:50:41Z","title":"ADO: Automatic Data Optimization for Inputs in LLM Prompts","summary":"  This study explores a novel approach to enhance the performance of Large\nLanguage Models (LLMs) through the optimization of input data within prompts.\nWhile previous research has primarily focused on refining instruction\ncomponents and augmenting input data with in-context examples, our work\ninvestigates the potential benefits of optimizing the input data itself. We\nintroduce a two-pronged strategy for input data optimization: content\nengineering and structural reformulation. Content engineering involves imputing\nmissing values, removing irrelevant attributes, and enriching profiles by\ngenerating additional information inferred from existing attributes. Subsequent\nto content engineering, structural reformulation is applied to optimize the\npresentation of the modified content to LLMs, given their sensitivity to input\nformat. Our findings suggest that these optimizations can significantly improve\nthe performance of LLMs in various tasks, offering a promising avenue for\nfuture research in prompt engineering. The source code is available at\nhttps://anonymous.4open.science/r/ADO-6BC5/\n","authors":["Sam Lin","Wenyue Hua","Lingyao Li","Zhenting Wang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11435v1","updated":"2025-02-17T04:50:37Z","published":"2025-02-17T04:50:37Z","title":"SMART: Self-Aware Agent for Tool Overuse Mitigation","summary":"  Current Large Language Model (LLM) agents demonstrate strong reasoning and\ntool use capabilities, but often lack self-awareness, failing to balance these\napproaches effectively. This imbalance leads to Tool Overuse, where models\nunnecessarily rely on external tools for tasks solvable with parametric\nknowledge, increasing computational overhead. Inspired by human metacognition,\nwe introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm\nthat enhances an agent's self-awareness to optimize task handling and reduce\ntool overuse. To support this paradigm, we introduce SMART-ER, a dataset\nspanning three domains, where reasoning alternates between parametric knowledge\nand tool-dependent steps, with each step enriched by rationales explaining when\ntools are necessary. Through supervised training, we develop SMARTAgent, a\nfamily of models that dynamically balance parametric knowledge and tool use.\nEvaluations show that SMARTAgent reduces tool use by 24% while improving\nperformance by over 37%, enabling 7B-scale models to match its 70B counterpart\nand GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test\ndata like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool\ncalls. These highlight the potential of strategic tool use to enhance\nreasoning, mitigate overuse, and bridge the gap between model size and\nperformance, advancing intelligent and resource-efficient agent designs.\n","authors":["Cheng Qian","Emre Can Acikgoz","Hongru Wang","Xiusi Chen","Avirup Sil","Dilek Hakkani-T√ºr","Gokhan Tur","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2502.11435v1.pdf","comment":"18 pages, 8 tables, 7 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.03823v2","updated":"2025-02-17T18:29:13Z","published":"2024-11-06T10:44:15Z","title":"Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination","summary":"  The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.\n","authors":["Dingjie Song","Sicheng Lai","Shunian Chen","Lichao Sun","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03823v2.pdf","comment":"Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect"},{"id":"http://arxiv.org/abs/2502.12096v1","updated":"2025-02-17T18:14:18Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Unified Framework for Cross-modal Context-aware\n  Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a unified\nframework to leverage cross-modal context information in generative semantic\ncommunications (GenSC). TokCom is a new paradigm, motivated by the recent\nsuccess of generative foundation models and multimodal large language models\n(GFM/MLLMs), where the communication units are tokens, enabling efficient\ntransformer-based token processing at the transmitter and receiver. In this\npaper, we introduce the potential opportunities and challenges of leveraging\ncontext in GenSC, explore how to integrate GFM/MLLMs-based token processing\ninto semantic communication systems to leverage cross-modal context\neffectively, present the key principles for efficient TokCom at various layers\nin future wireless networks. We demonstrate the corresponding TokCom benefits\nin a GenSC setup for image, leveraging cross-modal context information, which\nincreases the bandwidth efficiency by 70.8% with negligible loss of\nsemantic/perceptual quality. Finally, the potential research directions are\nidentified to facilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19651v2","updated":"2025-02-17T15:29:40Z","published":"2024-07-29T02:32:44Z","title":"Bridging Compressed Image Latents and Multimodal Large Language Models","summary":"  This paper presents the first-ever study of adapting compressed image latents\nto suit the needs of downstream vision tasks that adopt Multimodal Large\nLanguage Models (MLLMs). MLLMs have extended the success of large language\nmodels to modalities (e.g. images) beyond text, but their billion scale hinders\ndeployment on resource-constrained end devices. While cloud-hosted MLLMs could\nbe available, transmitting raw, uncompressed images captured by end devices to\nthe cloud requires an efficient image compression system. To address this, we\nfocus on emerging neural image compression and propose a novel framework with a\nlightweight transform-neck and a surrogate loss to adapt compressed image\nlatents for MLLM-based vision tasks. Given the huge scale of MLLMs, our\nframework excludes the entire downstream MLLM except part of its visual encoder\nfrom training our system. This stands out from most existing coding for machine\napproaches that involve downstream networks in training and thus could be\nimpractical when the networks are MLLMs. The proposed framework is general in\nthat it is applicable to various MLLMs, neural image codecs, and multiple\napplication scenarios, where the neural image codec can be (1) pre-trained for\nhuman perception without updating, (2) fully updated for joint human and\nmachine perception, or (3) fully updated for only machine perception. Extensive\nexperiments on different neural image codecs and various MLLMs show that our\nmethod achieves great rate-accuracy performance with much less complexity.\n","authors":["Chia-Hao Kao","Cheng Chien","Yu-Jen Tseng","Yi-Hsin Chen","Alessandro Gnutti","Shao-Yuan Lo","Wen-Hsiao Peng","Riccardo Leonardi"],"pdf_url":"https://arxiv.org/pdf/2407.19651v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2406.10469v2","updated":"2025-02-17T12:12:25Z","published":"2024-06-15T02:19:31Z","title":"Object-Attribute-Relation Representation Based Video Semantic\n  Communication","summary":"  With the rapid growth of multimedia data volume, there is an increasing need\nfor efficient video transmission in applications such as virtual reality and\nfuture video streaming services. Semantic communication is emerging as a vital\ntechnique for ensuring efficient and reliable transmission in low-bandwidth,\nhigh-noise settings. However, most current approaches focus on joint\nsource-channel coding (JSCC) that depends on end-to-end training. These methods\noften lack an interpretable semantic representation and struggle with\nadaptability to various downstream tasks. In this paper, we introduce the use\nof object-attribute-relation (OAR) as a semantic framework for videos to\nfacilitate low bit-rate coding and enhance the JSCC process for more effective\nvideo transmission. We utilize OAR sequences for both low bit-rate\nrepresentation and generative video reconstruction. Additionally, we\nincorporate OAR into the image JSCC model to prioritize communication resources\nfor areas more critical to downstream tasks. Our experiments on traffic\nsurveillance video datasets assess the effectiveness of our approach in terms\nof video transmission performance. The empirical findings demonstrate that our\nOAR-based video coding method not only outperforms H.265 coding at lower\nbit-rates but also synergizes with JSCC to deliver robust and efficient video\ntransmission.\n","authors":["Qiyuan Du","Yiping Duan","Qianqian Yang","Xiaoming Tao","M√©rouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2406.10469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08334v2","updated":"2025-02-17T01:49:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v2.pdf","comment":"preprint"}]},"2025-02-16T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.09921v2","updated":"2025-02-16T23:43:14Z","published":"2024-09-16T01:39:50Z","title":"Towards Real-Time Generation of Delay-Compensated Video Feeds for\n  Outdoor Mobile Robot Teleoperation","summary":"  Teleoperation is an important technology to enable supervisors to control\nagricultural robots remotely. However, environmental factors in dense crop rows\nand limitations in network infrastructure hinder the reliability of data\nstreamed to teleoperators. These issues result in delayed and variable frame\nrate video feeds that often deviate significantly from the robot's actual\nviewpoint. We propose a modular learning-based vision pipeline to generate\ndelay-compensated images in real-time for supervisors. Our extensive offline\nevaluations demonstrate that our method generates more accurate images compared\nto state-of-the-art approaches in our setting. Additionally, ours is one of the\nfew works to evaluate a delay-compensation method in outdoor field environments\nwith complex terrain on data from a real robot in real-time. Resulting videos\nand code are provided at https://sites.google.com/illinois.edu/comp-teleop.\n","authors":["Neeloy Chakraborty","Yixiao Fang","Andre Schreiber","Tianchen Ji","Zhe Huang","Aganze Mihigo","Cassidy Wall","Abdulrahman Almana","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2409.09921v2.pdf","comment":"Accepted to IEEE ICRA 2025; 8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.19483v4","updated":"2025-02-16T23:31:28Z","published":"2024-09-28T23:10:37Z","title":"MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation","summary":"  Segmentation of anatomical structures and pathological regions in medical\nimages is essential for modern clinical diagnosis, disease research, and\ntreatment planning. While significant advancements have been made in deep\nlearning-based segmentation techniques, many of these methods still suffer from\nlimitations in data efficiency, generalizability, and interactivity. As a\nresult, developing precise segmentation methods that require fewer labeled\ndatasets remains a critical challenge in medical image analysis. Recently, the\nintroduction of foundation models like CLIP and Segment-Anything-Model (SAM),\nwith robust cross-domain representations, has paved the way for interactive and\nuniversal image segmentation. However, further exploration of these models for\ndata-efficient segmentation in medical imaging is still needed and highly\nrelevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that\nintegrates the CLIP and SAM models to perform segmentation on clinical scans\nusing text prompts, in both zero-shot and weakly supervised settings. Our\napproach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard\nNegative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the\nMulti-modal Information Bottleneck (M2IB) to create visual prompts for\ngenerating segmentation masks from SAM in the zero-shot setting. We also\ninvestigate using zero-shot segmentation labels within a weakly supervised\nparadigm to enhance segmentation quality further. Extensive testing across four\ndiverse segmentation tasks and medical imaging modalities (breast tumor\nultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high\naccuracy of our proposed framework. Our code is available at\nhttps://github.com/HealthX-Lab/MedCLIP-SAMv2.\n","authors":["Taha Koleilat","Hojat Asgariandehkordi","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.19483v4.pdf","comment":"10 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.11307v1","updated":"2025-02-16T23:10:57Z","published":"2025-02-16T23:10:57Z","title":"Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly\n  Detection","summary":"  Anomaly detection (AD) in 3D point clouds is crucial in a wide range of\nindustrial applications, especially in various forms of precision\nmanufacturing. Considering the industrial demand for reliable 3D AD, several\nmethods have been developed. However, most of these approaches typically\nrequire training separate models for each category, which is memory-intensive\nand lacks flexibility. In this paper, we propose a novel Point-Language model\nwith dual-prompts for 3D ANomaly dEtection (PLANE). The approach leverages\nmulti-modal prompts to extend the strong generalization capabilities of\npre-trained Point-Language Models (PLMs) to the domain of 3D point cloud AD,\nachieving impressive detection performance across multiple categories using a\nsingle model. Specifically, we propose a dual-prompt learning method,\nincorporating both text and point cloud prompts. The method utilizes a dynamic\nprompt creator module (DPCM) to produce sample-specific dynamic prompts, which\nare then integrated with class-specific static prompts for each modality,\neffectively driving the PLMs. Additionally, based on the characteristics of\npoint cloud data, we propose a pseudo 3D anomaly generation method (Ano3D) to\nimprove the model's detection capabilities in an unsupervised setting.\nExperimental results demonstrate that the proposed method, which is under the\nmulti-class-one-model paradigm, achieves a +8.7%/+17% gain on anomaly detection\nand localization performance as compared to the state-of-the-art\none-class-one-model methods for the Anomaly-ShapeNet dataset, and obtains\n+4.3%/+4.1% gain for the Real3D-AD dataset. Code will be available upon\npublication.\n","authors":["Jiaxiang Wang","Haote Xu","Xiaolu Chen","Haodi Xu","Yue Huang","Xinghao Ding","Xiaotong Tu"],"pdf_url":"https://arxiv.org/pdf/2502.11307v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.11304v1","updated":"2025-02-16T23:03:26Z","published":"2025-02-16T23:03:26Z","title":"Leveraging Multimodal-LLMs Assisted by Instance Segmentation for\n  Intelligent Traffic Monitoring","summary":"  A robust and efficient traffic monitoring system is essential for smart\ncities and Intelligent Transportation Systems (ITS), using sensors and cameras\nto track vehicle movements, optimize traffic flow, reduce congestion, enhance\nroad safety, and enable real-time adaptive traffic control. Traffic monitoring\nmodels must comprehensively understand dynamic urban conditions and provide an\nintuitive user interface for effective management. This research leverages the\nLLaVA visual grounding multimodal large language model (LLM) for traffic\nmonitoring tasks on the real-time Quanser Interactive Lab simulation platform,\ncovering scenarios like intersections, congestion, and collisions. Cameras\nplaced at multiple urban locations collect real-time images from the\nsimulation, which are fed into the LLaVA model with queries for analysis. An\ninstance segmentation model integrated into the cameras highlights key elements\nsuch as vehicles and pedestrians, enhancing training and throughput. The system\nachieves 84.3% accuracy in recognizing vehicle locations and 76.4% in\ndetermining steering direction, outperforming traditional models.\n","authors":["Murat Arda Onsu","Poonam Lohan","Burak Kantarci","Aisha Syed","Matthew Andrews","Sean Kennedy"],"pdf_url":"https://arxiv.org/pdf/2502.11304v1.pdf","comment":"6 pages, 7 figures, submitted to 30th IEEE International Symposium on\n  Computers and Communications (ISCC) 2025"},{"id":"http://arxiv.org/abs/2502.11300v1","updated":"2025-02-16T22:54:44Z","published":"2025-02-16T22:54:44Z","title":"CORDIAL: Can Multimodal Large Language Models Effectively Understand\n  Coherence Relationships?","summary":"  Multimodal Large Language Models (MLLMs) are renowned for their superior\ninstruction-following and reasoning capabilities across diverse problem\ndomains. However, existing benchmarks primarily focus on assessing factual and\nlogical correctness in downstream tasks, with limited emphasis on evaluating\nMLLMs' ability to interpret pragmatic cues and intermodal relationships. To\naddress this gap, we assess the competency of MLLMs in performing Multimodal\nDiscourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL,\nencompasses a broad spectrum of Coherence Relations across 3 different\ndiscourse domains at varying levels of granularity. Through our experiments on\n10+ MLLMs employing different prompting strategies, we show that even top\nmodels like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple\nclassifier-based baselines. This study emphasizes the need to move beyond\nsimilarity-based metrics and adopt a discourse-driven framework for evaluating\nMLLMs, providing a more nuanced assessment of their capabilities. The benchmark\nand code are available at: https://github.com/aashish2000/CORDIAL.\n","authors":["Aashish Anantha Ramakrishnan","Aadarsh Anantha Ramakrishnan","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09564v2","updated":"2025-02-16T22:42:41Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v2.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2410.02098v3","updated":"2025-02-16T22:27:11Z","published":"2024-10-02T23:39:10Z","title":"EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing","summary":"  Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability.\n","authors":["Haotian Sun","Tao Lei","Bowen Zhang","Yanghao Li","Haoshuo Huang","Ruoming Pang","Bo Dai","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2410.02098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11287v1","updated":"2025-02-16T22:03:03Z","published":"2025-02-16T22:03:03Z","title":"MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for\n  Traffic Monitoring","summary":"  Single camera 3D perception for traffic monitoring faces significant\nchallenges due to occlusion and limited field of view. Moreover, fusing\ninformation from multiple cameras at the image feature level is difficult\nbecause of different view angles. Further, the necessity for practical\nimplementation and compatibility with existing traffic infrastructure compounds\nthese challenges. To address these issues, this paper introduces a novel\nBird's-Eye-View road occupancy detection framework that leverages multiple\nroadside cameras to overcome the aforementioned limitations. To facilitate the\nframework's development and evaluation, a synthetic dataset featuring diverse\nscenes and varying camera configurations is generated using the CARLA\nsimulator. A late fusion and three early fusion methods were implemented within\nthe proposed framework, with performance further enhanced by integrating\nbackgrounds. Extensive evaluations were conducted to analyze the impact of\nmulti-camera inputs and varying BEV occupancy map sizes on model performance.\nAdditionally, a real-world data collection pipeline was developed to assess the\nmodel's ability to generalize to real-world environments. The sim-to-real\ncapabilities of the model were evaluated using zero-shot and few-shot\nfine-tuning, demonstrating its potential for practical application. This\nresearch aims to advance perception systems in traffic monitoring, contributing\nto improved traffic management, operational efficiency, and road safety.\n","authors":["Arpitsinh Vaghela","Duo Lu","Aayush Atul Verma","Bharatesh Chakravarthi","Hua Wei","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2502.11287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19785v3","updated":"2025-02-16T21:54:38Z","published":"2024-10-14T22:25:06Z","title":"How to Backdoor Consistency Models?","summary":"  Consistency models are a new class of models that generate images by directly\nmapping noise to data, allowing for one-step generation and significantly\naccelerating the sampling process. However, their robustness against\nadversarial attacks has not yet been thoroughly investigated. In this work, we\nconduct the first study on the vulnerability of consistency models to backdoor\nattacks. While previous research has explored backdoor attacks on diffusion\nmodels, those studies have primarily focused on conventional diffusion models,\nemploying a customized backdoor training process and objective, whereas\nconsistency models have distinct training processes and objectives. Our\nproposed framework demonstrates the vulnerability of consistency models to\nbackdoor attacks. During image generation, poisoned consistency models produce\nimages with a Fr\\'echet Inception Distance (FID) comparable to that of a clean\nmodel when sampling from Gaussian noise. However, once the trigger is\nactivated, they generate backdoor target images. We explore various trigger and\ntarget configurations to evaluate the vulnerability of consistency models,\nincluding the use of random noise as a trigger. This novel trigger is visually\ninconspicuous, more challenging to detect, and aligns well with the sampling\nprocess of consistency models. Across all configurations, our framework\nsuccessfully compromises the consistency models while maintaining high utility\nand specificity. We also examine the stealthiness of our proposed attack, which\nis attributed to the unique properties of consistency models and the elusive\nnature of the Gaussian noise trigger. Our code is available at\n\\href{https://github.com/chengenw/backdoorCM}{https://github.com/chengenw/backdoorCM}.\n","authors":["Chengen Wang","Murat Kantarcioglu"],"pdf_url":"https://arxiv.org/pdf/2410.19785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11271v1","updated":"2025-02-16T21:18:47Z","published":"2025-02-16T21:18:47Z","title":"OctoTools: An Agentic Framework with Extensible Tools for Complex\n  Reasoning","summary":"  Solving complex reasoning tasks may involve visual understanding, domain\nknowledge retrieval, numerical calculation, and multi-step reasoning. Existing\nmethods augment large language models (LLMs) with external tools but are\nrestricted to specialized domains, limited tool types, or require additional\ntraining data. In this paper, we introduce OctoTools, a training-free,\nuser-friendly, and easily extensible open-source agentic framework designed to\ntackle complex reasoning across diverse domains. OctoTools introduces\nstandardized tool cards to encapsulate tool functionality, a planner for both\nhigh-level and low-level planning, and an executor to carry out tool usage. We\nvalidate OctoTools' generality across 16 diverse tasks (including MathVista,\nMMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains\nof 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions\nand LangChain by up to 10.6% when given the same set of tools. Through\ncomprehensive analysis and ablations, OctoTools demonstrates advantages in task\nplanning, effective tool usage, and multi-step problem solving.\n","authors":["Pan Lu","Bowen Chen","Sheng Liu","Rahul Thapa","Joseph Boen","James Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11271v1.pdf","comment":"89 pages, 18 figures. Project website: https://octotools.github.io/"},{"id":"http://arxiv.org/abs/2502.11265v1","updated":"2025-02-16T20:43:53Z","published":"2025-02-16T20:43:53Z","title":"Towards Automatic Identification of Missing Tissues using a\n  Geometric-Learning Correspondence Model","summary":"  Missing tissue presents a big challenge for dose mapping, e.g., in the\nreirradiation setting. We propose a pipeline to identify missing tissue on\nintra-patient structure meshes using a previously trained geometric-learning\ncorrespondence model. For our application, we relied on the prediction\ndiscrepancies between forward and backward correspondences of the input meshes,\nquantified using a correspondence-based Inverse Consistency Error (cICE). We\noptimised the threshold applied to cICE to identify missing points in a dataset\nof 35 simulated mandible resections. Our identified threshold, 5.5 mm, produced\na balanced accuracy score of 0.883 in the training data, using an ensemble\napproach. This pipeline produced plausible results for a real case where ~25%\nof the mandible was removed after a surgical intervention. The pipeline,\nhowever, failed on a more extreme case where ~50% of the mandible was removed.\nThis is the first time geometric-learning modelling is proposed to identify\nmissing points in corresponding anatomy.\n","authors":["Eliana M. Vasquez Osorio","Edward Henderson"],"pdf_url":"https://arxiv.org/pdf/2502.11265v1.pdf","comment":"Presented in XXth International Conference on the use of Computers in\n  Radiation therapy. Pages 759-762 in XXth ICCR Proceedings, found in\n  https://udl.hal.science/hal-04720234v1"},{"id":"http://arxiv.org/abs/2502.11259v1","updated":"2025-02-16T20:27:56Z","published":"2025-02-16T20:27:56Z","title":"Exploiting network optimization stability for enhanced PET image\n  denoising using deep image prior","summary":"  PET is affected by statistical noise due to constraints on tracer dose and\nscan duration, impacting both diagnostic performance and quantitative accuracy.\nWhile deep learning (DL)-based PET denoising methods have been used to improve\nimage quality, they may introduce over-smoothing, compromising quantitative\naccuracy. We propose a method for making a DL solution more reliable and apply\nit to the conditional deep image prior (DIP). We introduce the idea of\nstability information in the optimization process of conditional DIP, enabling\nthe identification of unstable regions within the network's optimization\ntrajectory. Our method incorporates a stability map, which is derived from\nmultiple intermediate outputs of moderate network at different optimization\nsteps. The final denoised image is then obtained by computing linear\ncombination of the DIP output and the original reconstructed image, weighted by\nthe stability map. Our method effectively reduces noise while preserving small\nstructure details in brain FDG images. Results demonstrated that our approach\noutperformed existing methods in peak-to-valley ratio and noise suppression\nacross various low-dose levels. Region-of-interest analysis confirmed that the\nproposed method maintains quantitative accuracy without introducing under- or\nover-estimation. We applied our method to full-dose PET data to assess its\nimpact on image quality. The results revealed that the proposed method\nsignificantly reduced background noise while preserving the peak-to-valley\nratio at a level comparable to that of unfiltered full-dose PET images. The\nproposed method introduces a robust approach to DL-based PET denoising,\nenhancing its reliability and preserving quantitative accuracy. This strategy\nhas the potential to advance performance in high-sensitivity PET scanners,\ndemonstrating that DL can extend PET imaging capabilities beyond low-dose\napplications.\n","authors":["Fumio Hashimoto","Kibo Ote","Yuya Onishi","Hideaki Tashima","Go Akamatsu","Yuma Iwao","Miwako Takahashi","Taiga Yamaya"],"pdf_url":"https://arxiv.org/pdf/2502.11259v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.09070v3","updated":"2025-02-16T19:55:25Z","published":"2024-06-13T12:55:10Z","title":"FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of\n  Thought Reasoning with Multimodal Large Language Models","summary":"  In the domain of text-to-image generative models, biases inherent in training\ndatasets often propagate into generated content, posing significant ethical\nchallenges, particularly in socially sensitive contexts. We introduce FairCoT,\na novel framework that enhances fairness in text to image models through Chain\nof Thought (CoT) reasoning within multimodal generative large language models.\nFairCoT employs iterative CoT refinement to systematically mitigate biases, and\ndynamically adjusts textual prompts in real time, ensuring diverse and\nequitable representation in generated images. By integrating iterative\nreasoning processes, FairCoT addresses the limitations of zero shot CoT in\nsensitive scenarios, balancing creativity with ethical responsibility.\nExperimental evaluations across popular text-to-image systems including DALLE\nand various Stable Diffusion variants, demonstrate that FairCoT significantly\nenhances fairness and diversity without sacrificing image quality or semantic\nfidelity. By combining robust reasoning, lightweight deployment, and\nextensibility to multiple models, FairCoT represents a promising step toward\nmore socially responsible and transparent AI driven content generation.\n","authors":["Zahraa Al Sahili","Ioannis Patras","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2406.09070v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11234v1","updated":"2025-02-16T18:59:11Z","published":"2025-02-16T18:59:11Z","title":"MaskFlow: Discrete Flows For Flexible and Efficient Long Video\n  Generation","summary":"  Generating long, high-quality videos remains a challenge due to the complex\ninterplay of spatial and temporal dynamics and hardware limitations. In this\nwork, we introduce \\textbf{MaskFlow}, a unified video generation framework that\ncombines discrete representations with flow-matching to enable efficient\ngeneration of high-quality long videos. By leveraging a frame-level masking\nstrategy during training, MaskFlow conditions on previously generated unmasked\nframes to generate videos with lengths ten times beyond that of the training\nsequences. MaskFlow does so very efficiently by enabling the use of fast Masked\nGenerative Model (MGM)-style sampling and can be deployed in both fully\nautoregressive as well as full-sequence generation modes. We validate the\nquality of our method on the FaceForensics (FFS) and Deepmind Lab (DMLab)\ndatasets and report Fr\\'echet Video Distance (FVD) competitive with\nstate-of-the-art approaches. We also provide a detailed analysis on the\nsampling efficiency of our method and demonstrate that MaskFlow can be applied\nto both timestep-dependent and timestep-independent models in a training-free\nmanner.\n","authors":["Michael Fuest","Vincent Tao Hu","Bj√∂rn Ommer"],"pdf_url":"https://arxiv.org/pdf/2502.11234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01131v3","updated":"2025-02-16T18:44:39Z","published":"2024-07-01T09:53:53Z","title":"M$^2$IST: Multi-Modal Interactive Side-Tuning for Efficient Referring\n  Expression Comprehension","summary":"  Referring expression comprehension (REC) is a vision-language task to locate\na target object in an image based on a language expression. Fully fine-tuning\ngeneral-purpose pre-trained vision-language foundation models for REC yields\nimpressive performance but becomes increasingly costly. Parameter-efficient\ntransfer learning (PETL) methods have shown strong performance with fewer\ntunable parameters. However, directly applying PETL to REC faces two\nchallenges: (1) insufficient multi-modal interaction between pre-trained\nvision-language foundation models, and (2) high GPU memory usage due to\ngradients passing through the heavy vision-language foundation models. To this\nend, we present M$^2$IST: Multi-Modal Interactive Side-Tuning with M$^3$ISAs:\nMixture of Multi-Modal Interactive Side-Adapters. During fine-tuning, we fix\nthe pre-trained uni-modal encoders and update M$^3$ISAs to enable efficient\nvision-language alignment for REC. Empirical results reveal that M$^2$IST\nachieves better performance-efficiency trade-off than full fine-tuning and\nother PETL methods, requiring only 2.11% tunable parameters, 39.61% GPU memory,\nand 63.46% training time while maintaining competitive performance. Our code is\nreleased at https://github.com/xuyang-liu16/M2IST.\n","authors":["Xuyang Liu","Ting Liu","Siteng Huang","Yi Xin","Yue Hu","Quanjun Yin","Donglin Wang","Yuanyuan Wu","Honggang Chen"],"pdf_url":"https://arxiv.org/pdf/2407.01131v3.pdf","comment":"Our code is released at https://github.com/xuyang-liu16/M2IST"},{"id":"http://arxiv.org/abs/2501.05179v3","updated":"2025-02-16T18:33:57Z","published":"2025-01-09T11:57:58Z","title":"Compression with Global Guidance: Towards Training-free High-Resolution\n  MLLMs Acceleration","summary":"  Multimodal large language models (MLLMs) have attracted considerable\nattention due to their exceptional performance in visual content understanding\nand reasoning. However, their inference efficiency has been a notable concern,\nas the increasing length of multimodal contexts leads to quadratic complexity.\nToken compression techniques, which reduce the number of visual tokens, have\ndemonstrated their effectiveness in reducing computational costs. Yet, these\napproaches have struggled to keep pace with the rapid advancements in MLLMs,\nespecially the AnyRes strategy in the context of high-resolution image\nunderstanding. In this paper, we propose a novel token compression method,\nGlobalCom$^2$, tailored for high-resolution MLLMs that receive both the\nthumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the\nthumbnail as the \"commander\" of the entire token compression process, directing\nthe allocation of retention ratios and the specific compression for each crop.\nIn this way, redundant tokens are eliminated while important local details are\nadaptively preserved to the highest extent feasible. Empirical results across\n10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between\nperformance and efficiency, and consistently outperforms state-of-the-art token\ncompression methods with LLaVA-NeXT-7B/13B models. Our code is released at\nhttps://github.com/xuyang-liu16/GlobalCom2.\n","authors":["Xuyang Liu","Ziming Wang","Yuhang Han","Yingyao Wang","Jiale Yuan","Jun Song","Bo Zheng","Linfeng Zhang","Siteng Huang","Honggang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.05179v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15167v2","updated":"2025-02-16T18:02:47Z","published":"2025-01-25T10:32:00Z","title":"Enhancing Intent Understanding for Ambiguous Prompts through\n  Human-Machine Co-Adaptation","summary":"  Today's image generation systems are capable of producing realistic and\nhigh-quality images. However, user prompts often contain ambiguities, making it\ndifficult for these systems to interpret users' actual intentions.\nConsequently, many users must modify their prompts several times to ensure the\ngenerated images meet their expectations. While some methods focus on enhancing\nprompts to make the generated images fit user needs, the model is still hard to\nunderstand users' real needs, especially for non-expert users. In this\nresearch, we aim to enhance the visual parameter-tuning process, making the\nmodel user-friendly for individuals without specialized knowledge and better\nunderstand user needs. We propose a human-machine co-adaption strategy using\nmutual information between the user's prompts and the pictures under\nmodification as the optimizing target to make the system better adapt to user\nneeds. We find that an improved model can reduce the necessity for multiple\nrounds of adjustments. We also collect multi-round dialogue datasets with\nprompts and images pairs and user intent. Various experiments demonstrate the\neffectiveness of the proposed method in our proposed dataset. Our annotation\ntools and several examples of our dataset are available at\nhttps://zenodo.org/records/14876029 for easier review. And we will open source\nour full dataset and code.\n","authors":["Yangfan He","Jianhui Wang","Yijin Wang","Kun Li","Li Sun","Jiayi Su","Jingyuan Lu","Jinhua Song","Haoyuan Li","Sida Li","Tianyu Shi","Miao Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.15167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19378v2","updated":"2025-02-16T17:29:23Z","published":"2024-11-28T21:07:22Z","title":"Libra: Leveraging Temporal Images for Biomedical Radiology Analysis","summary":"  Radiology report generation (RRG) requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. While multimodal\nlarge language models (MLLMs) align with pre-trained vision encoders to enhance\nvisual-language understanding, most existing methods rely on single-image\nanalysis or rule-based heuristics to process multiple images, failing to fully\nleverage temporal information in multi-modal medical datasets. In this paper,\nwe introduce Libra, a temporal-aware MLLM tailored for chest X-ray report\ngeneration. Libra combines a radiology-specific image encoder with a novel\nTemporal Alignment Connector (TAC), designed to accurately capture and\nintegrate temporal differences between paired current and prior images.\nExtensive experiments on the MIMIC-CXR dataset demonstrate that Libra\nestablishes a new state-of-the-art benchmark among similarly scaled MLLMs,\nsetting new standards in both clinical relevance and lexical accuracy.\n","authors":["Xi Zhang","Zaiqiao Meng","Jake Lever","Edmond S. L. Ho"],"pdf_url":"https://arxiv.org/pdf/2411.19378v2.pdf","comment":"30 pages, 5 figures, Adding Appendix"},{"id":"http://arxiv.org/abs/2502.11211v1","updated":"2025-02-16T17:21:05Z","published":"2025-02-16T17:21:05Z","title":"A Survey of LLM-based Agents in Medicine: How far are we from Baymax?","summary":"  Large Language Models (LLMs) are transforming healthcare through the\ndevelopment of LLM-based agents that can understand, reason about, and assist\nwith medical tasks. This survey provides a comprehensive review of LLM-based\nagents in medicine, examining their architectures, applications, and\nchallenges. We analyze the key components of medical agent systems, including\nsystem profiles, clinical planning mechanisms, medical reasoning frameworks,\nand external capacity enhancement. The survey covers major application\nscenarios such as clinical decision support, medical documentation, training\nsimulations, and healthcare service optimization. We discuss evaluation\nframeworks and metrics used to assess these agents' performance in healthcare\nsettings. While LLM-based agents show promise in enhancing healthcare delivery,\nseveral challenges remain, including hallucination management, multimodal\nintegration, implementation barriers, and ethical considerations. The survey\nconcludes by highlighting future research directions, including advances in\nmedical reasoning inspired by recent developments in LLM architectures,\nintegration with physical systems, and improvements in training simulations.\nThis work provides researchers and practitioners with a structured overview of\nthe current state and future prospects of LLM-based agents in medicine.\n","authors":["Wenxuan Wang","Zizhan Ma","Zheng Wang","Chenghan Wu","Wenting Chen","Xiang Li","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.11211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11196v1","updated":"2025-02-16T16:55:43Z","published":"2025-02-16T16:55:43Z","title":"How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on\n  Continual Pre-Training","summary":"  Despite exceptional capabilities in knowledge-intensive tasks, Large Language\nModels (LLMs) face a critical gap in understanding how they internalize new\nknowledge, particularly how to structurally embed acquired knowledge in their\nneural computations. We address this issue through the lens of knowledge\ncircuit evolution, identifying computational subgraphs that facilitate\nknowledge storage and processing. Our systematic analysis of circuit evolution\nthroughout continual pre-training reveals several key findings: (1) the\nacquisition of new knowledge is influenced by its relevance to pre-existing\nknowledge; (2) the evolution of knowledge circuits exhibits a distinct phase\nshift from formation to optimization; (3) the evolution of knowledge circuits\nfollows a deep-to-shallow pattern. These insights not only advance our\ntheoretical understanding of the mechanisms of new knowledge acquisition in\nLLMs, but also provide potential implications for improving continual\npre-training strategies to enhance model performance. Code and data will be\navailable at https://github.com/zjunlp/DynamicKnowledgeCircuits.\n","authors":["Yixin Ou","Yunzhi Yao","Ningyu Zhang","Hui Jin","Jiacheng Sun","Shumin Deng","Zhenguo Li","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11196v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.11195v1","updated":"2025-02-16T16:55:28Z","published":"2025-02-16T16:55:28Z","title":"From Deception to Perception: The Surprising Benefits of Deepfakes for\n  Detecting, Measuring, and Mitigating Bias","summary":"  While deepfake technologies have predominantly been criticized for potential\nmisuse, our study demonstrates their significant potential as tools for\ndetecting, measuring, and mitigating biases in key societal domains. By\nemploying deepfake technology to generate controlled facial images, we extend\nthe scope of traditional correspondence studies beyond mere textual\nmanipulations. This enhancement is crucial in scenarios such as pain\nassessments, where subjective biases triggered by sensitive features in facial\nimages can profoundly affect outcomes. Our results reveal that deepfakes not\nonly maintain the effectiveness of correspondence studies but also introduce\ngroundbreaking advancements in bias measurement and correction techniques. This\nstudy emphasizes the constructive role of deepfake technologies as essential\ntools for advancing societal equity and fairness.\n","authors":["Yizhi Liu","Balaji Padmanabhan","Siva Viswanathan"],"pdf_url":"https://arxiv.org/pdf/2502.11195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07627v2","updated":"2025-02-16T16:41:43Z","published":"2024-11-12T08:17:15Z","title":"Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion","summary":"  Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.\n","authors":["Kaiyu Song","Hanjiang Lai"],"pdf_url":"https://arxiv.org/pdf/2411.07627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01201v5","updated":"2025-02-16T16:38:13Z","published":"2022-11-02T15:23:16Z","title":"Human alignment of neural network representations","summary":"  Today's computer vision models achieve human or near-human level performance\nacross a wide variety of vision tasks. However, their architectures, data, and\nlearning algorithms differ in numerous ways from those that give rise to human\nvision. In this paper, we investigate the factors that affect the alignment\nbetween the representations learned by neural networks and human mental\nrepresentations inferred from behavioral responses. We find that model scale\nand architecture have essentially no effect on the alignment with human\nbehavioral responses, whereas the training dataset and objective function both\nhave a much larger impact. These findings are consistent across three datasets\nof human similarity judgments collected using two different tasks. Linear\ntransformations of neural network representations learned from behavioral\nresponses from one dataset substantially improve alignment with human\nsimilarity judgments on the other two datasets. In addition, we find that some\nhuman concepts such as food and animals are well-represented by neural networks\nwhereas others such as royal or sports-related objects are not. Overall,\nalthough models trained on larger, more diverse datasets achieve better\nalignment with humans than models trained on ImageNet alone, our results\nindicate that scaling alone is unlikely to be sufficient to train neural\nnetworks with conceptual representations that match those used by humans.\n","authors":["Lukas Muttenthaler","Jonas Dippel","Lorenz Linhardt","Robert A. Vandermeulen","Simon Kornblith"],"pdf_url":"https://arxiv.org/pdf/2211.01201v5.pdf","comment":"Accepted for publication at ICLR 2023"},{"id":"http://arxiv.org/abs/2502.11190v1","updated":"2025-02-16T16:31:00Z","published":"2025-02-16T16:31:00Z","title":"ReLearn: Unlearning via Learning for Large Language Models","summary":"  Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.\n","authors":["Haoming Xu","Ningyuan Zhao","Liming Yang","Sendong Zhao","Shumin Deng","Mengru Wang","Bryan Hooi","Nay Oo","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11190v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.11184v1","updated":"2025-02-16T16:12:40Z","published":"2025-02-16T16:12:40Z","title":"Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs","summary":"  Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.\n","authors":["Wenxuan Wang","Xiaoyuan Liu","Kuiyi Gao","Jen-tse Huang","Youliang Yuan","Pinjia He","Shuai Wang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2502.11184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14876v2","updated":"2025-02-16T16:00:00Z","published":"2024-09-23T10:17:13Z","title":"Mammo-Clustering: A Weakly Supervised Multi-view Tri-level Information\n  Fusion Context Clustering Network for Localization and Classification in\n  Mammography","summary":"  Breast cancer is a significant global health issue, and the diagnosis of\nbreast imaging has always been challenging. Mammography images typically have\nextremely high resolution, with lesions occupying only a very small area.\nDown-sampling in neural networks can easily lead to the loss of\nmicrocalcifications or subtle structures, making it difficult for traditional\nneural network architectures to address these issues. To tackle these\nchallenges, we propose a Context Clustering Network with triple information\nfusion. Firstly, compared to CNNs or transformers, we find that Context\nclustering methods (1) are more computationally efficient and (2) can more\neasily associate structural or pathological features, making them suitable for\nthe clinical tasks of mammography. Secondly, we propose a triple information\nfusion mechanism that integrates global information, feature-based local\ninformation, and patch-based local information. The proposed approach is\nrigorously evaluated on two public datasets, Vindr-Mammo and CBIS-DDSM, using\nfive independent splits to ensure statistical robustness. Our method achieves\nan AUC of 0.828 on Vindr-Mammo and 0.805 on CBIS-DDSM, outperforming the next\nbest method by 3.1% and 2.4%, respectively. These improvements are\nstatistically significant (p<0.05), underscoring the benefits of Context\nClustering Network with triple information fusion. Overall, our Context\nClustering framework demonstrates strong potential as a scalable and\ncost-effective solution for large-scale mammography screening, enabling more\nefficient and accurate breast cancer detection. Access to our method is\navailable at https://github.com/Sohyu1/Mammo_Clustering.\n","authors":["Shilong Yang","Chulong Zhang","Qi Zang","Juan Yu","Liang Zeng","Xiao Luo","Yexuan Xing","Xin Pan","Qi Li","Xiaokun Liang","Yaoqin Xie"],"pdf_url":"https://arxiv.org/pdf/2409.14876v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.11179v1","updated":"2025-02-16T15:59:06Z","published":"2025-02-16T15:59:06Z","title":"RT-DEMT: A hybrid real-time acupoint detection model combining mamba and\n  transformer","summary":"  Traditional Chinese acupuncture methods often face controversy in clinical\npractice due to their high subjectivity. Additionally, current\nintelligent-assisted acupuncture systems have two major limitations: slow\nacupoint localization speed and low accuracy. To address these limitations, a\nnew method leverages the excellent inference efficiency of the state-space\nmodel Mamba, while retaining the advantages of the attention mechanism in the\ntraditional DETR architecture, to achieve efficient global information\nintegration and provide high-quality feature information for acupoint\nlocalization tasks. Furthermore, by employing the concept of residual\nlikelihood estimation, it eliminates the need for complex upsampling processes,\nthereby accelerating the acupoint localization task. Our method achieved\nstate-of-the-art (SOTA) accuracy on a private dataset of acupoints on the human\nback, with an average Euclidean distance pixel error (EPE) of 7.792 and an\naverage time consumption of 10.05 milliseconds per localization task. Compared\nto the second-best algorithm, our method improved both accuracy and speed by\napproximately 14\\%. This significant advancement not only enhances the efficacy\nof acupuncture treatment but also demonstrates the commercial potential of\nautomated acupuncture robot systems. Access to our method is available at\nhttps://github.com/Sohyu1/RT-DEMT\n","authors":["Shilong Yang","Qi Zang","Chulong Zhang","Lingfeng Huang","Yaoqin Xie"],"pdf_url":"https://arxiv.org/pdf/2502.11179v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.11178v1","updated":"2025-02-16T15:58:54Z","published":"2025-02-16T15:58:54Z","title":"DAViMNet: SSMs-Based Domain Adaptive Object Detection","summary":"  Unsupervised domain adaptation (UDA) for object detection adapts models\ntrained on labeled source domains to unlabeled target domains, ensuring robust\nperformance across domain shifts. Transformer-based architectures excel at\ncapturing long-range dependencies but face efficiency challenges due to their\nquadratic attention complexity, which limits scalability in UDA tasks. To\naddress these issues, we propose a hybrid domain-adaptive Mamba Transformer\narchitecture that combines Mamba's efficient state-space modeling with\nattention mechanisms to tackle domain-specific spatial and channel-wise\nvariations. Each hybrid block integrates domain-adaptive Mamba blocks and\nattention mechanisms: Domain-Adaptive Mamba employs spatial and channel\nstate-space models to adaptively model domain variations, while attention\nmechanisms leverage self-attention for intra-domain feature enhancement and\ncross-attention for effective source-target alignment. Our approach processes\nboth shallow and deeper features, employing an entropy-based knowledge\ndistillation framework with margin ReLU to emphasize discriminative features\nand suppress noise. Gradient Reversal Layers enable adversarial alignment\nacross network layers, while entropy-driven gating attention with random\nperturbations refines target features and mitigates overfitting. By unifying\nthese components, our architecture achieves state-of-the-art performance in UDA\nobject detection, balancing efficiency with robust generalization.\n","authors":["A. Enes Doruk","Hasan F. Ates"],"pdf_url":"https://arxiv.org/pdf/2502.11178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17191v3","updated":"2025-02-16T15:49:00Z","published":"2024-05-27T14:15:52Z","title":"MCGAN: Enhancing GAN Training with Regression-Based Generator Loss","summary":"  Generative adversarial networks (GANs) have emerged as a powerful tool for\ngenerating high-fidelity data. However, the main bottleneck of existing\napproaches is the lack of supervision on the generator training, which often\nresults in undamped oscillation and unsatisfactory performance. To address this\nissue, we propose an algorithm called Monte Carlo GAN (MCGAN). This approach,\nutilizing an innovative generative loss function, termly the regression loss,\nreformulates the generator training as a regression task and enables the\ngenerator training by minimizing the mean squared error between the\ndiscriminator's output of real data and the expected discriminator of fake\ndata. We demonstrate the desirable analytic properties of the regression loss,\nincluding discriminability and optimality, and show that our method requires a\nweaker condition on the discriminator for effective generator training. These\nproperties justify the strength of this approach to improve the training\nstability while retaining the optimality of GAN by leveraging strong\nsupervision of the regression loss. Extensive experiments on diverse datasets,\nincluding image data (CIFAR-10/100, FFHQ256, ImageNet, and LSUN Bedroom), time\nseries data (VAR and stock data) and video data, are conducted to demonstrate\nthe flexibility and effectiveness of our proposed MCGAN. Numerical results show\nthat the proposed MCGAN is versatile in enhancing a variety of backbone GAN\nmodels and achieves consistent and significant improvement in terms of quality,\naccuracy, training stability, and learned latent space.\n","authors":["Baoren Xiao","Hao Ni","Weixin Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17191v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11168v1","updated":"2025-02-16T15:38:33Z","published":"2025-02-16T15:38:33Z","title":"Knowing Your Target: Target-Aware Transformer Makes Better\n  Spatio-Temporal Video Grounding","summary":"  Transformer has attracted increasing interest in STVG, owing to its\nend-to-end pipeline and promising result. Existing Transformer-based STVG\napproaches often leverage a set of object queries, which are initialized simply\nusing zeros and then gradually learn target position information via iterative\ninteractions with multimodal features, for spatial and temporal localization.\nDespite simplicity, these zero object queries, due to lacking target-specific\ncues, are hard to learn discriminative target information from interactions\nwith multimodal features in complicated scenarios (\\e.g., with distractors or\nocclusion), resulting in degradation. Addressing this, we introduce a novel\nTarget-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate\nobject queries via exploring target-specific cues from the given video-text\npair, for improving STVG. The key lies in two simple yet effective modules,\ncomprising text-guided temporal sampling (TTS) and attribute-aware spatial\nactivation (ASA), working in a cascade. The former focuses on selecting\ntarget-relevant temporal cues from a video utilizing holistic text information,\nwhile the latter aims at further exploiting the fine-grained visual attribute\ninformation of the object from previous target-aware temporal cues, which is\napplied for object query initialization. Compared to existing methods\nleveraging zero-initialized queries, object queries in our TA-STVG, directly\ngenerated from a given video-text pair, naturally carry target-specific cues,\nmaking them adaptive and better interact with multimodal features for learning\nmore discriminative information to improve STVG. In our experiments on three\nbenchmarks, TA-STVG achieves state-of-the-art performance and significantly\noutperforms the baseline, validating its efficacy.\n","authors":["Xin Gu","Yaojie Shen","Chenxi Luo","Tiejian Luo","Yan Huang","Yuewei Lin","Heng Fan","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11163v1","updated":"2025-02-16T15:28:34Z","published":"2025-02-16T15:28:34Z","title":"VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and\n  Privacy Risks","summary":"  Visual-Language Models (VLMs) have shown remarkable performance across\nvarious tasks, particularly in recognizing geographic information from images.\nHowever, significant challenges remain, including biases and privacy concerns.\nTo systematically address these issues in the context of geographic information\nrecognition, we introduce a benchmark dataset consisting of 1,200 images paired\nwith detailed geographic metadata. Evaluating four VLMs, we find that while\nthese models demonstrate the ability to recognize geographic information from\nimages, achieving up to $53.8\\%$ accuracy in city prediction, they exhibit\nsignificant regional biases. Specifically, performance is substantially higher\nfor economically developed and densely populated regions compared to less\ndeveloped ($-12.5\\%$) and sparsely populated ($-17.0\\%$) areas. Moreover, the\nmodels exhibit regional biases, frequently overpredicting certain locations;\nfor instance, they consistently predict Sydney for images taken in Australia.\nThe strong performance of VLMs also raises privacy concerns, particularly for\nusers who share images online without the intent of being identified. Our code\nand dataset are publicly available at\nhttps://github.com/uscnlp-lime/FairLocator.\n","authors":["Jingyuan Huang","Jen-tse Huang","Ziyi Liu","Xiaoyuan Liu","Wenxuan Wang","Jieyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.11163v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.11161v1","updated":"2025-02-16T15:26:21Z","published":"2025-02-16T15:26:21Z","title":"BFA: Best-Feature-Aware Fusion for Multi-View Fine-grained Manipulation","summary":"  In real-world scenarios, multi-view cameras are typically employed for\nfine-grained manipulation tasks. Existing approaches (e.g., ACT) tend to treat\nmulti-view features equally and directly concatenate them for policy learning.\nHowever, it will introduce redundant visual information and bring higher\ncomputational costs, leading to ineffective manipulation. For a fine-grained\nmanipulation task, it tends to involve multiple stages while the most\ncontributed view for different stages is varied over time. In this paper, we\npropose a plug-and-play best-feature-aware (BFA) fusion strategy for multi-view\nmanipulation tasks, which is adaptable to various policies. Built upon the\nvisual backbone of the policy network, we design a lightweight network to\npredict the importance score of each view. Based on the predicted importance\nscores, the reweighted multi-view features are subsequently fused and input\ninto the end-to-end policy network, enabling seamless integration. Notably, our\nmethod demonstrates outstanding performance in fine-grained manipulations.\nExperimental results show that our approach outperforms multiple baselines by\n22-46% success rate on different tasks. Our work provides new insights and\ninspiration for tackling key challenges in fine-grained manipulations.\n","authors":["Zihan Lan","Weixin Mao","Haosheng Li","Le Wang","Tiancai Wang","Haoqiang Fan","Osamu Yoshie"],"pdf_url":"https://arxiv.org/pdf/2502.11161v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.11158v1","updated":"2025-02-16T15:12:40Z","published":"2025-02-16T15:12:40Z","title":"AnyRefill: A Unified, Data-Efficient Framework for Left-Prompt-Guided\n  Vision Tasks","summary":"  In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to\naddress a diverse range of reference-based vision tasks. Inspired by the human\ncreative process, we reformulate these tasks using a left-right stitching\nformulation to construct contextual input. Building upon this foundation, we\npropose AnyRefill, an extension of LeftRefill, that effectively adapts\nText-to-Image (T2I) models to various vision tasks. AnyRefill leverages the\ninpainting priors of advanced T2I model based on the Diffusion Transformer\n(DiT) architecture, and incorporates flexible components to enhance its\ncapabilities. By combining task-specific LoRAs with the stitching input,\nAnyRefill unlocks its potential across diverse tasks, including conditional\ngeneration, visual perception, and image editing, without requiring additional\nvisual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency,\nrequiring minimal task-specific fine-tuning while maintaining high generative\nperformance. Through extensive ablation studies, we demonstrate that AnyRefill\noutperforms other image condition injection methods and achieves competitive\nresults compared to state-of-the-art open-source methods. Notably, AnyRefill\ndelivers results comparable to advanced commercial tools, such as IC-Light and\nSeedEdit, even in challenging scenarios. Comprehensive experiments and ablation\nstudies across versatile tasks validate the strong generation of the proposed\nsimple yet effective LPG formulation, establishing AnyRefill as a unified,\nhighly data-efficient solution for reference-based vision tasks.\n","authors":["Ming Xie","Chenjie Cao","Yunuo Cai","Xiangyang Xue","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.11158v1.pdf","comment":"19 pages, submitted to TPAMI"},{"id":"http://arxiv.org/abs/2408.10599v2","updated":"2025-02-16T14:59:08Z","published":"2024-08-20T07:14:28Z","title":"Vision Calorimeter: Migrating Visual Object Detector to High-energy\n  Particle Images","summary":"  In high-energy physics, accurately estimating the kinematic parameters\n(position and momentum) of anti-neutrons ($\\bar{n}$) is essential for exploring\nthe fundamental governing principles. However, this process is particularly\nchallenging when using an electromagnetic calorimeter (EMC) as the energy\ndetector, due to their limited accuracy and efficiency in interacting with\n$\\bar{n}$. To address this issue, we propose Vision Calorimeter (ViC), a\ndata-driven framework which migrates visual object detection techniques to\nhigh-energy particle images. To accommodate the unique characteristics of\nparticle images, we introduce the heat-conduction operator (HCO) into both the\nbackbone and the head of the conventional object detector and conduct\nsignificant structural improvements. HCO enjoys the advantage of both radial\nprior and global attention, as it is inspired by physical heat conduction which\nnaturally aligns with the pattern of particle incidence. Implemented via the\nDiscrete Cosine Transform (DCT), HCO extracts frequency-domain features,\nbridging the distribution gap between the particle images and the natural\nimages on which visual object detectors are pre-trained. Experimental results\ndemonstrate that ViC significantly outperforms traditional approaches, reducing\nthe incident position prediction error by 46.16% (from 17.31$^{\\circ}$ to\n9.32$^{\\circ}$) and providing the first baseline result with an incident\nmomentum regression error of 21.48%. This study underscores ViC's great\npotential as a general-purpose particle parameter estimator in high-energy\nphysics. Code is available at https://github.com/yuhongtian17/ViC.\n","authors":["Hongtian Yu","Yangu Li","Yunfan Liu","Yunxuan Song","Xiaorui Lyu","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2408.10599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07966v4","updated":"2025-02-16T14:23:08Z","published":"2024-09-12T11:53:05Z","title":"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE","summary":"  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n","authors":["Sichun Wu","Kazi Injamamul Haque","Zerrin Yumak"],"pdf_url":"https://arxiv.org/pdf/2409.07966v4.pdf","comment":"14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024"},{"id":"http://arxiv.org/abs/2502.11142v1","updated":"2025-02-16T14:17:36Z","published":"2025-02-16T14:17:36Z","title":"NavRAG: Generating User Demand Instructions for Embodied Navigation\n  through Retrieval-Augmented LLM","summary":"  Vision-and-Language Navigation (VLN) is an essential skill for embodied\nagents, allowing them to navigate in 3D environments following natural language\ninstructions. High-performance navigation models require a large amount of\ntraining data, the high cost of manually annotating data has seriously hindered\nthis field. Therefore, some previous methods translate trajectory videos into\nstep-by-step instructions for expanding data, but such instructions do not\nmatch well with users' communication styles that briefly describe destinations\nor state specific needs. Moreover, local navigation trajectories overlook\nglobal context and high-level task planning. To address these issues, we\npropose NavRAG, a retrieval-augmented generation (RAG) framework that generates\nuser demand instructions for VLN. NavRAG leverages LLM to build a hierarchical\nscene description tree for 3D scene understanding from global layout to local\ndetails, then simulates various user roles with specific demands to retrieve\nfrom the scene tree, generating diverse instructions with LLM. We annotate over\n2 million navigation instructions across 861 scenes and evaluate the data\nquality and navigation performance of trained models.\n","authors":["Zihan Wang","Yaohui Zhu","Gim Hee Lee","Yachun Fan"],"pdf_url":"https://arxiv.org/pdf/2502.11142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01643v3","updated":"2025-02-16T14:14:16Z","published":"2024-03-03T23:40:35Z","title":"Cost-Effective Attention Mechanisms for Low Resource Settings: Necessity\n  & Sufficiency of Linear Transformations","summary":"  From natural language processing to vision, Scaled Dot Product Attention\n(SDPA) is the backbone of most modern deep learning applications.\nUnfortunately, its memory and computational requirements can be prohibitive in\nlow-resource settings. In this paper, we improve its efficiency without\nsacrificing its versatility. We propose three attention variants where we\nremove consecutive linear transformations or add a novel one, and evaluate them\non a range of standard NLP and vision tasks. Our proposed models are\nsubstantially lighter than standard SDPA (and have 25-50% fewer parameters). We\nshow that the performance cost of these changes is negligible relative to size\nreduction and that in one case (Super Attention) we succeed in outperforming\nSDPA by up to 10% while improving its speed and reducing its parameters by 25%.\n","authors":["Peyman Hosseini","Mehran Hosseini","Ignacio Castro","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2403.01643v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11069v2","updated":"2025-02-16T13:07:16Z","published":"2025-01-19T15:05:15Z","title":"Refinement Module based on Parse Graph of Feature Map for Human Pose\n  Estimation","summary":"  Parse graphs of the human body can be obtained in the human brain to help\nhumans complete the human pose estimation (HPE). It contains a hierarchical\nstructure, like a tree structure, and context relations among nodes. Many\nresearchers predefine the parse graph of body structure to design HPE\nframeworks. However, these frameworks struggle to adapt to instances that\ndeviate from the predefined parse graph and are often parameter-heavy. Unlike\nthem, we view the feature map holistically, much like the human body. It can be\noptimized using parse graphs, where each node's feature is an implicit\nexpression rather than a fixed one. This allows it to adapt to more instances,\nunconstrained by rigid structural features. In this paper, we design the\nRefinement Module based on the Parse Graph of feature map (RMPG), which\nincludes two stages: top-down decomposition and bottom-up combination. In the\nfirst stage, the feature map is decomposed into multiple sub-feature maps along\nthe channel. In the second stage, the context relations of sub-feature maps are\ncalculated to obtain their respective context information and the sub-feature\nmaps with context information are concatenated along channels to obtain the\nrefined feature map. Additionally, we design a hierarchical network with fewer\nparameters using multiple RMPG modules for HPE according to the parse graph of\nbody structure, some of which are supervised to obtain context relations among\nbody parts. Our network achieves excellent results on multiple mainstream human\npose datasets. More importantly, the effectiveness of RMPG is proven on\ndifferent methods. The code of RMPG will be open.\n","authors":["Shibang Liu","Xuemei Xie","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2501.11069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07182v2","updated":"2025-02-16T12:20:40Z","published":"2024-12-10T04:41:10Z","title":"An Enhancement of CNN Algorithm for Rice Leaf Disease Image\n  Classification in Mobile Applications","summary":"  This study focuses on enhancing rice leaf disease image classification\nalgorithms, which have traditionally relied on Convolutional Neural Network\n(CNN) models. We employed transfer learning with MobileViTV2_050 using\nImageNet-1k weights, a lightweight model that integrates CNN's local feature\nextraction with Vision Transformers' global context learning through a\nseparable self-attention mechanism. Our approach resulted in a significant\n15.66% improvement in classification accuracy for MobileViTV2_050-A, our first\nenhanced model trained on the baseline dataset, achieving 93.14%. Furthermore,\nMobileViTV2_050-B, our second enhanced model trained on a broader rice leaf\ndataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy.\nAdditionally, MobileViTV2-A attained an F1-score of 93% across four rice labels\nand a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In\nterms of resource consumption, our enhanced models reduced the total parameters\nof the baseline CNN model by up to 92.50%, from 14 million to 1.1 million.\nThese results indicate that MobileViTV2_050 not only improves computational\nefficiency through its separable self-attention mechanism but also enhances\nglobal context learning. Consequently, it offers a lightweight and robust\nsolution suitable for mobile deployment, advancing the interpretability and\npracticality of models in precision agriculture.\n","authors":["Kayne Uriel K. Rodrigo","Jerriane Hillary Heart S. Marcial","Samuel C. Brillo","Khatalyn E. Mata","Jonathan C. Morano"],"pdf_url":"https://arxiv.org/pdf/2412.07182v2.pdf","comment":"Presented at 46th World Conference on Applied Science, Engineering &\n  Technology (WCASET) from Institute for Educational Research and Publication\n  (IFERP)"},{"id":"http://arxiv.org/abs/2502.11093v1","updated":"2025-02-16T12:13:11Z","published":"2025-02-16T12:13:11Z","title":"Text-promptable Propagation for Referring Medical Image Sequence\n  Segmentation","summary":"  Medical image sequences, generated by both 2D video-based examinations and 3D\nimaging techniques, consist of sequential frames or slices that capture the\nsame anatomical entities (e.g., organs or lesions) from multiple perspectives.\nExisting segmentation studies typically process medical images using either 2D\nor 3D methods in isolation, often overlooking the inherent consistencies among\nthese images. Additionally, interactive segmentation, while highly beneficial\nin clinical scenarios, faces the challenge of integrating text prompts\neffectively across multi-modalities. To address these issues, we introduce an\ninnovative task, Referring Medical Image Sequence Segmentation for the first\ntime, which aims to segment the referred anatomical entities corresponding to\nmedical text prompts. We develop a strong baseline model, Text-Promptable\nPropagation (TPP), designed to exploit the intrinsic relationships among\nsequential images and their associated textual descriptions. TPP supports the\nsegmentation of arbitrary objects of interest based on cross-modal prompt\nfusion. Carefully designed medical prompts are fused and employed as queries to\nguide image sequence segmentation through triple-propagation. We curate a large\nand comprehensive benchmark covering 4 modalities and 20 different organs and\nlesions. Experimental results consistently demonstrate the superior performance\nof our approach compared to previous methods across these datasets.\n","authors":["Runtian Yuan","Jilan Xu","Mohan Chen","Qingqiu Li","Yuejie Zhang","Rui Feng","Tao Zhang","Shang Gao"],"pdf_url":"https://arxiv.org/pdf/2502.11093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19814v2","updated":"2025-02-16T12:12:28Z","published":"2024-11-29T16:20:36Z","title":"Gaussian multi-target filtering with target dynamics driven by a\n  stochastic differential equation","summary":"  This paper proposes multi-target filtering algorithms in which target\ndynamics are given in continuous time and measurements are obtained at discrete\ntime instants. In particular, targets appear according to a Poisson point\nprocess (PPP) in time with a given Gaussian spatial distribution, targets move\naccording to a general time-invariant linear stochastic differential equation,\nand the life span of each target is modelled with an exponential distribution.\nFor this multi-target dynamic model, we derive the distribution of the set of\nnew born targets and calculate closed-form expressions for the best fitting\nmean and covariance of each target at its time of birth by minimising the\nKullback-Leibler divergence via moment matching. This yields a novel Gaussian\ncontinuous-discrete Poisson multi-Bernoulli mixture (PMBM) filter, and its\napproximations based on Poisson multi-Bernoulli and probability hypothesis\ndensity filtering. These continuous-discrete multi-target filters are also\nextended to target dynamics driven by nonlinear stochastic differential\nequations.\n","authors":["√Ångel F. Garc√≠a-Fern√°ndez","Simo S√§rkk√§"],"pdf_url":"https://arxiv.org/pdf/2411.19814v2.pdf","comment":"Matlab code available at https://github.com/Agarciafernandez"},{"id":"http://arxiv.org/abs/2502.11079v1","updated":"2025-02-16T11:02:50Z","published":"2025-02-16T11:02:50Z","title":"Phantom: Subject-consistent video generation via cross-modal alignment","summary":"  The continuous development of foundational models for video generation is\nevolving into various applications, with subject-consistent video generation\nstill in the exploratory stage. We refer to this as Subject-to-Video, which\nextracts subject elements from reference images and generates\nsubject-consistent video through textual instructions. We believe that the\nessence of subject-to-video lies in balancing the dual-modal prompts of text\nand image, thereby deeply and simultaneously aligning both text and visual\ncontent. To this end, we propose Phantom, a unified video generation framework\nfor both single and multi-subject references. Building on existing\ntext-to-video and image-to-video architectures, we redesign the joint\ntext-image injection model and drive it to learn cross-modal alignment via\ntext-image-video triplet data. In particular, we emphasize subject consistency\nin human generation, covering existing ID-preserving video generation while\noffering enhanced advantages. The project homepage is here\nhttps://phantom-video.github.io/Phantom/.\n","authors":["Lijie Liu","Tianxiang Ma","Bingchuan Li","Zhuowei Chen","Jiawei Liu","Qian He","Xinglong Wu"],"pdf_url":"https://arxiv.org/pdf/2502.11079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07590v2","updated":"2025-02-16T10:50:24Z","published":"2025-02-11T14:39:59Z","title":"DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT\n  Training","summary":"  Diffusion Transformers (DiTs) have shown remarkable performance in modeling\nand generating high-quality videos. However, the quadratic computational\ncomplexity of 3D full attention mechanism presents significant challenges in\nscaling video DiT training, especially for high-definition and lengthy videos,\nwhere attention can dominate up to 95% of the end-to-end time and necessitate\nspecialized communication paradigms to handle large input sizes.\n  This paper introduces DSV, a novel framework designed to accelerate and scale\nthe training of video DiTs by leveraging the inherent dynamic attention\nsparsity throughout the training process. DSV employs a two-stage training\nalgorithm that exploits sparsity patterns, focusing on critical elements\nsupported by efficient, tailored kernels. To accommodate the new sparsity\ndimension, we develop a hybrid sparsity-aware context parallelism that\neffectively scales to large inputs by addressing the heterogeneity of sparsity\nacross attention heads and blocks, resulting in optimized sparse computation\nand communication. Extensive evaluations demonstrate that DSV achieves up to\n3.02x gain in training throughput with nearly no quality degradation.\n","authors":["Xin Tan","Yuetao Chen","Yimin Jiang","Xing Chen","Kun Yan","Nan Duan","Yibo Zhu","Daxin Jiang","Hong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07590v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13785v3","updated":"2025-02-16T10:14:02Z","published":"2024-01-24T20:06:59Z","title":"A Spatiotemporal Approach to Tri-Perspective Representation for 3D\n  Semantic Occupancy Prediction","summary":"  Holistic understanding and reasoning in 3D scenes are crucial for the success\nof autonomous driving systems. The evolution of 3D semantic occupancy\nprediction as a pretraining task for autonomous driving and robotic\napplications captures finer 3D details compared to traditional 3D detection\nmethods. Vision-based 3D semantic occupancy prediction is increasingly\noverlooked in favor of LiDAR-based approaches, which have shown superior\nperformance in recent years. However, we present compelling evidence that there\nis still potential for enhancing vision-based methods. Existing approaches\npredominantly focus on spatial cues such as tri-perspective view (TPV)\nembeddings, often overlooking temporal cues. This study introduces S2TPVFormer,\na spatiotemporal transformer architecture designed to predict temporally\ncoherent 3D semantic occupancy. By introducing temporal cues through a novel\nTemporal Cross-View Hybrid Attention mechanism (TCVHA), we generate\nSpatiotemporal TPV (S2TPV) embeddings that enhance the prior process.\nExperimental evaluations on the nuScenes dataset demonstrate a significant\n+4.1% of absolute gain in mean Intersection over Union (mIoU) for 3D semantic\noccupancy compared to baseline TPVFormer, validating the effectiveness of\nS2TPVFormer in advancing 3D scene perception.\n","authors":["Sathira Silva","Savindu Bhashitha Wannigama","Gihan Jayatilaka","Muhammad Haris Khan","Roshan Ragel"],"pdf_url":"https://arxiv.org/pdf/2401.13785v3.pdf","comment":"Accepted to the 2025 Workshop on Machine Learning for Autonomous\n  Driving at AAAI"},{"id":"http://arxiv.org/abs/2501.02593v2","updated":"2025-02-16T10:11:23Z","published":"2025-01-05T16:16:10Z","title":"Evolving Skeletons: Motion Dynamics in Action Recognition","summary":"  Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.\n","authors":["Jushang Qiu","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2501.02593v2.pdf","comment":"Accepted at the Companion Proceedings of the ACM Web Conference (WWW\n  Companion 2025)"},{"id":"http://arxiv.org/abs/2408.03677v6","updated":"2025-02-16T09:39:05Z","published":"2024-08-07T10:36:26Z","title":"L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection","summary":"  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n","authors":["Xun Huang","Ziyu Xu","Hai Wu","Jinlong Wang","Qiming Xia","Yan Xia","Jonathan Li","Kyle Gao","Chenglu Wen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03677v6.pdf","comment":"Accepted by AAAI2025(Oral)"},{"id":"http://arxiv.org/abs/2408.07675v2","updated":"2025-02-16T09:25:18Z","published":"2024-08-14T17:22:41Z","title":"G$^2$V$^2$former: Graph Guided Video Vision Transformer for Face\n  Anti-Spoofing","summary":"  In videos containing spoofed faces, we may uncover the spoofing evidence\nbased on either photometric or dynamic abnormality, even a combination of both.\nPrevailing face anti-spoofing (FAS) approaches generally concentrate on the\nsingle-frame scenario, however, purely photometric-driven methods overlook the\ndynamic spoofing clues that may be exposed over time. This may lead FAS systems\nto conclude incorrect judgments, especially in cases where it is easily\ndistinguishable in terms of dynamics but challenging to discern in terms of\nphotometrics. To this end, we propose the Graph Guided Video Vision Transformer\n(G$^2$V$^2$former), which combines faces with facial landmarks for photometric\nand dynamic feature fusion. We factorize the attention into space and time, and\nfuse them via a spatiotemporal block. Specifically, we design a novel temporal\nattention called Kronecker temporal attention, which has a wider receptive\nfield, and is beneficial for capturing dynamic information. Moreover, we\nleverage the low-semantic motion of facial landmarks to guide the high-semantic\nchange of facial expressions based on the motivation that regions containing\nlandmarks may reveal more dynamic clues. Extensive experiments on nine\nbenchmark datasets demonstrate that our method achieves superior performance\nunder various scenarios. The codes will be released soon.\n","authors":["Jingyi Yang","Zitong Yu","Xiuming Ni","Jia He","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2408.07675v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.11049v1","updated":"2025-02-16T09:23:16Z","published":"2025-02-16T09:23:16Z","title":"Faces of Fairness: Examining Bias in Facial Expression Recognition\n  Datasets and Models","summary":"  Building AI systems, including Facial Expression Recognition (FER), involves\ntwo critical aspects: data and model design. Both components significantly\ninfluence bias and fairness in FER tasks. Issues related to bias and fairness\nin FER datasets and models remain underexplored. This study investigates bias\nsources in FER datasets and models. Four common FER datasets--AffectNet, ExpW,\nFer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and\nExpW exhibit high generalizability despite data imbalances. Additionally, this\nresearch evaluates the bias and fairness of six deep models, including three\nstate-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet,\nXceptionNet, as well as three transformer-based models: ViT, CLIP, and\nGPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve\nthe highest accuracy scores, they also display the highest levels of bias.\nThese findings underscore the urgent need for developing new methodologies to\nmitigate bias and ensure fairness in datasets and models, particularly in\naffective computing applications. See our implementation details at\nhttps://github.com/MMHosseini/bias_in_FER.\n","authors":["Mohammad Mehdi Hosseini","Ali Pourramezan Fard","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2502.11049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19179v2","updated":"2025-02-16T09:13:25Z","published":"2024-12-26T11:35:57Z","title":"Mask Approximation Net: A Novel Diffusion Model Approach for Remote\n  Sensing Change Captioning","summary":"  Remote sensing image change description represents an innovative multimodal\ntask within the realm of remote sensing processing. This task not only\nfacilitates the detection of alterations in surface conditions, but also\nprovides comprehensive descriptions of these changes, thereby improving human\ninterpretability and interactivity.Generally, existing deep-learning-based\nmethods predominantly utilized a three-stage framework that successively\nperform feature extraction, feature fusion, and localization from bitemporal\nimages before text generation. However, this reliance often leads to an\nexcessive focus on the design of specific network architectures and restricts\nthe feature distributions to the dataset at hand, which in turn results in\nlimited generalizability and robustness during application.To address these\nlimitations, this paper proposes a novel approach for remote sensing image\nchange detection and description that incorporates diffusion models, aiming to\ntransition the emphasis of modeling paradigms from conventional feature\nlearning to data distribution learning. The proposed method primarily includes\na simple multi-scale change detection module, whose output features are\nsubsequently refined by an well-designed diffusion model. Furthermore, we\nintroduce a frequency-guided complex filter module to boost the model\nperformance by managing high-frequency noise throughout the diffusion process.\nWe validate the effectiveness of our proposed method across several datasets\nfor remote sensing change detection and description, showcasing its superior\nperformance compared to existing techniques. The code will be available at\n\\href{https://github.com/sundongwei}{MaskApproxNet} after a possible\npublication.\n","authors":["Dongwei Sun","Jing Yao","Changsheng Zhou","Xiangyong Cao","Pedram Ghamisi"],"pdf_url":"https://arxiv.org/pdf/2412.19179v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.05803v2","updated":"2025-02-16T22:05:21Z","published":"2025-02-09T08:14:11Z","title":"FlashCheck: Exploration of Efficient Evidence Retrieval for Fast\n  Fact-Checking","summary":"  The advances in digital tools have led to the rampant spread of\nmisinformation. While fact-checking aims to combat this, manual fact-checking\nis cumbersome and not scalable. It is essential for automated fact-checking to\nbe efficient for aiding in combating misinformation in real-time and at the\nsource. Fact-checking pipelines primarily comprise a knowledge retrieval\ncomponent which extracts relevant knowledge to fact-check a claim from large\nknowledge sources like Wikipedia and a verification component. The existing\nworks primarily focus on the fact-verification part rather than evidence\nretrieval from large data collections, which often face scalability issues for\npractical applications such as live fact-checking. In this study, we address\nthis gap by exploring various methods for indexing a succinct set of factual\nstatements from large collections like Wikipedia to enhance the retrieval phase\nof the fact-checking pipeline. We also explore the impact of vector\nquantization to further improve the efficiency of pipelines that employ dense\nretrieval approaches for first-stage retrieval. We study the efficiency and\neffectiveness of the approaches on fact-checking datasets such as HoVer and\nWiCE, leveraging Wikipedia as the knowledge source. We also evaluate the\nreal-world utility of the efficient retrieval approaches by fact-checking 2024\npresidential debate and also open source the collection of claims with\ncorresponding labels identified in the debate. Through a combination of indexed\nfacts together with Dense retrieval and Index compression, we achieve up to a\n10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the\nclassical fact-checking pipelines over large collections.\n","authors":["Kevin Nanekhan","Venktesh V","Erik Martin","Henrik Vatndal","Vinay Setty","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2502.05803v2.pdf","comment":"Accepted to ECIR 2025, 15 pages"},{"id":"http://arxiv.org/abs/2502.11246v1","updated":"2025-02-16T19:46:24Z","published":"2025-02-16T19:46:24Z","title":"MemeSense: An Adaptive In-Context Framework for Social Commonsense\n  Driven Meme Moderation","summary":"  Memes present unique moderation challenges due to their subtle, multimodal\ninterplay of images, text, and social context. Standard systems relying\npredominantly on explicit textual cues often overlook harmful content\ncamouflaged by irony, symbolism, or cultural references. To address this gap,\nwe introduce MemeSense, an adaptive in-context learning framework that fuses\nsocial commonsense reasoning with visually and semantically related reference\nexamples. By encoding crucial task information into a learnable cognitive shift\nvector, MemeSense effectively balances lexical, visual, and ethical\nconsiderations, enabling precise yet context-aware meme intervention. Extensive\nevaluations on a curated set of implicitly harmful memes demonstrate that\nMemeSense substantially outperforms strong baselines, paving the way for safer\nonline communities. Code and data available at:\nhttps://github.com/sayantan11995/MemeSense\n","authors":["Sayantan Adak","Somnath Banerjee","Rajarshi Mandal","Avik Halder","Sayan Layek","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2502.11246v1.pdf","comment":"Code and data available at:\n  https://github.com/sayantan11995/MemeSense"},{"id":"http://arxiv.org/abs/2502.11197v1","updated":"2025-02-16T16:56:15Z","published":"2025-02-16T16:56:15Z","title":"CSP: A Simulator For Multi-Agent Ranking Competitions","summary":"  In ranking competitions, document authors compete for the highest rankings by\nmodifying their content in response to past rankings. Previous studies focused\non human participants, primarily students, in controlled settings. The rise of\ngenerative AI, particularly Large Language Models (LLMs), introduces a new\nparadigm: using LLMs as document authors. This approach addresses scalability\nconstraints in human-based competitions and reflects the growing role of\nLLM-generated content on the web-a prime example of ranking competition. We\nintroduce a highly configurable ranking competition simulator that leverages\nLLMs as document authors. It includes analytical tools to examine the resulting\ndatasets. We demonstrate its capabilities by generating multiple datasets and\nconducting an extensive analysis. Our code and datasets are publicly available\nfor research.\n","authors":["Tommy Mordo","Tomer Kordonsky","Haya Nachimovsky","Moshe Tennenholtz","Oren Kurland"],"pdf_url":"https://arxiv.org/pdf/2502.11197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11181v1","updated":"2025-02-16T15:59:50Z","published":"2025-02-16T15:59:50Z","title":"Improving Scientific Document Retrieval with Concept Coverage-based\n  Query Set Generation","summary":"  In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.\n","authors":["SeongKu Kang","Bowen Jin","Wonbin Kweon","Yu Zhang","Dongha Lee","Jiawei Han","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2502.11181v1.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2502.08557v2","updated":"2025-02-16T14:24:44Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.11116v1","updated":"2025-02-16T13:23:39Z","published":"2025-02-16T13:23:39Z","title":"Gumbel Reranking: Differentiable End-to-End Reranker Optimization","summary":"  RAG systems rely on rerankers to identify relevant documents. However,\nfine-tuning these models remains challenging due to the scarcity of annotated\nquery-document pairs. Existing distillation-based approaches suffer from\ntraining-inference misalignment and fail to capture interdependencies among\ncandidate documents. To overcome these limitations, we reframe the reranking\nprocess as an attention-mask problem and propose Gumbel Reranking, an\nend-to-end training framework for rerankers aimed at minimizing the\ntraining-inference gap. In our approach, reranker optimization is reformulated\nas learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel\nTrick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end\noptimization by minimizing the overall language loss. Experiments across\nvarious settings consistently demonstrate performance gains, including a 10.4\\%\nimprovement in recall on HotpotQA for distinguishing indirectly relevant\ndocuments.\n","authors":["Siyuan Huang","Zhiyuan Ma","Jintao Du","Changhua Meng","Weiqiang Wang","Jingwen Leng","Minyi Guo","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.11116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11105v1","updated":"2025-02-16T12:46:34Z","published":"2025-02-16T12:46:34Z","title":"Graceful forgetting: Memory as a process","summary":"  A rational theory of memory is proposed to explain how we can accommodate\nunbounded sensory input within bounded storage space. Memory is stored as\nstatistics, organized into complex structures that are constantly summarized\nand compressed to make room for new input. This process, driven by space\nconstraints, is guided by heuristics that optimize the memory for future needs.\nSensory input is rapidly encoded as simple statistics that are more slowly\nelaborated into more abstract constructs. This theory differs from previous\naccounts of memory by (a) its reliance on statistics, (b) its use of heuristics\nto guide the choice of statistics, and (c) the emphasis on memory as a process\nthat is intensive, complex, and expensive. The theory is intended as an aid to\nmake sense of our extensive knowledge of memory, and bring us closer to an\nunderstanding of memory in functional and mechanistic terms.\n","authors":["Alain de Cheveign√©"],"pdf_url":"https://arxiv.org/pdf/2502.11105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22353v3","updated":"2025-02-16T11:50:09Z","published":"2024-10-15T14:51:45Z","title":"RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models\n  for Question Answering","summary":"  Retrieval-augmented generation (RAG) has shown promising potential in\nknowledge intensive question answering (QA). However, existing approaches only\nconsider the query itself, neither specifying the retrieval preferences for the\nretrievers nor informing the generators of how to refer to the retrieved\ndocuments for the answers, which poses a significant challenge to the QA\nperformance. To address these issues, we propose Rule-guided\nRetrieval-Augmented Generation with LMs, which explicitly introduces rules for\nin-context learning (RuleRAG-ICL) to guide retrievers to recall related\ndocuments in the directions of rules and uniformly guide generators to reason\nattributed by the same rules. Moreover, most existing RAG datasets were\nconstructed without considering rules and Knowledge Graphs (KGs) are recognized\nas providing high-quality rules. Therefore, we construct five rule-aware RAG\nbenchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval\nand reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL\nimproves the retrieval quality of +89.2% in Recall@10 and answer accuracy of\n+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,\nexperiments on four existing RAG datasets show RuleRAG is also effective by\noffering rules in RuleQA to them, further proving the generalization of rule\nguidance in RuleRAG.\n","authors":["Zhongwu Chen","Chengjin Xu","Dingmin Wang","Zhen Huang","Yong Dou","Xuhui Jiang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2410.22353v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13694v3","updated":"2025-02-16T11:07:13Z","published":"2024-09-03T03:31:37Z","title":"Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study","summary":"  Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach to mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of external knowledge source. In contrast,\nmost real-world applications involve diverse knowledge from various sources, a\nscenario that has been relatively underexplored. The main dilemma is the lack\nof a suitable dataset incorporating multiple knowledge sources and\npre-exploration of the associated issues. To address these challenges, we\nstandardize a benchmark dataset that combines structured and unstructured\nknowledge across diverse and complementary domains. Building upon the dataset,\nwe identify the limitations of existing methods under such conditions.\nTherefore, we develop PruningRAG, a plug-and-play RAG framework that uses\nmulti-granularity pruning strategies to more effectively incorporate relevant\ncontext and mitigate the negative impact of misleading information. Extensive\nexperimental results demonstrate superior performance of PruningRAG and our\ninsightful findings are also reported. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}.\n","authors":["Shuo Yu","Mingyue Cheng","Jiqian Yang","Jie Ouyang","Yucong Luo","Chenyi Lei","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.13694v3.pdf","comment":"12 pages, 9 figures;"},{"id":"http://arxiv.org/abs/2410.12228v2","updated":"2025-02-16T09:41:32Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Kai Zhao","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08109v2","updated":"2025-02-16T08:19:12Z","published":"2024-02-12T22:56:18Z","title":"From Data to Decisions: The Transformational Power of Machine Learning\n  in Business Recommendations","summary":"  This research aims to explore the impact of Machine Learning (ML) on the\nevolution and efficacy of Recommendation Systems (RS), particularly in the\ncontext of their growing significance in commercial business environments.\nMethodologically, the study delves into the role of ML in crafting and refining\nthese systems, focusing on aspects such as data sourcing, feature engineering,\nand the importance of evaluation metrics, thereby highlighting the iterative\nnature of enhancing recommendation algorithms. The deployment of Recommendation\nEngines (RE), driven by advanced algorithms and data analytics, is explored\nacross various domains, showcasing their significant impact on user experience\nand decision-making processes. These engines not only streamline information\ndiscovery and enhance collaboration but also accelerate knowledge acquisition,\nproving vital in navigating the digital landscape for businesses. They\ncontribute significantly to sales, revenue, and the competitive edge of\nenterprises by offering improved recommendations that align with individual\ncustomer needs. The research identifies the increasing expectation of users for\na seamless, intuitive online experience, where content is personalized and\ndynamically adapted to changing preferences. Future research directions include\nexploring advancements in deep learning models, ethical considerations in the\ndeployment of RS, and addressing scalability challenges. This study emphasizes\nthe indispensability of comprehending and leveraging ML in RS for researchers\nand practitioners, to tap into the full potential of personalized\nrecommendation in commercial business prospects.\n","authors":["Kapilya Gangadharan","K. Malathi","Anoop Purandaran","Barathi Subramanian","Rathinaraja Jeyaraj","Soon Ki Jung"],"pdf_url":"https://arxiv.org/pdf/2402.08109v2.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.12032v2","updated":"2025-02-16T06:51:05Z","published":"2024-06-17T18:59:43Z","title":"Balancing Embedding Spectrum for Recommendation","summary":"  Modern recommender systems heavily rely on high-quality representations\nlearned from high-dimensional sparse data. While significant efforts have been\ninvested in designing powerful algorithms for extracting user preferences, the\nfactors contributing to good representations have remained relatively\nunexplored. In this work, we shed light on an issue in the existing pair-wise\nlearning paradigm (i.e., the embedding collapse problem), that the\nrepresentations tend to span a subspace of the whole embedding space, leading\nto a suboptimal solution and reducing the model capacity. Specifically,\noptimization on observed interactions is equivalent to a low pass filter\ncausing users/items to have the same representations and resulting in a\ncomplete collapse. While negative sampling acts as a high pass filter to\nalleviate the collapse by balancing the embedding spectrum, its effectiveness\nis only limited to certain losses, which still leads to an incomplete collapse.\nTo tackle this issue, we propose a novel method called DirectSpec, acting as a\nreliable all pass filter to balance the spectrum distribution of the embeddings\nduring training, ensuring that users/items effectively span the entire\nembedding space. Additionally, we provide a thorough analysis of DirectSpec\nfrom a decorrelation perspective and propose an enhanced variant, DirectSpec+,\nwhich employs self-paced gradients to optimize irrelevant samples more\neffectively. Moreover, we establish a close connection between DirectSpec+ and\nuniformity, demonstrating that contrastive learning (CL) can alleviate the\ncollapse issue by indirectly balancing the spectrum. Finally, we implement\nDirectSpec and DirectSpec+ on two popular recommender models: MF and LightGCN.\nOur experimental results demonstrate its effectiveness and efficiency over\ncompetitive baselines.\n","authors":["Shaowen Peng","Kazunari Sugiyama","Xin Liu","Tsunenori Mine"],"pdf_url":"https://arxiv.org/pdf/2406.12032v2.pdf","comment":"ACM Trans on Recommender Systems"},{"id":"http://arxiv.org/abs/2502.10990v1","updated":"2025-02-16T04:23:52Z","published":"2025-02-16T04:23:52Z","title":"FinMTEB: Finance Massive Text Embedding Benchmark","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, FinPersona-E5, using a persona-based\ndata synthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including FinPersona-E5,\nwe show three key findings: (1) performance on general-purpose benchmarks shows\nlimited correlation with financial domain tasks; (2) domain-adapted models\nconsistently outperform their general-purpose counterparts; and (3)\nsurprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated\ndense embeddings in financial Semantic Textual Similarity (STS) tasks,\nunderscoring current limitations in dense embedding techniques. Our work\nestablishes a robust evaluation framework for financial NLP applications and\nprovides crucial insights for developing domain-specific embedding models.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10990v1.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2502.10976v1","updated":"2025-02-16T03:37:13Z","published":"2025-02-16T03:37:13Z","title":"QuOTE: Question-Oriented Text Embeddings","summary":"  We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to\nretrieval-augmented generation (RAG) systems, aimed at improving document\nrepresentation for accurate and nuanced retrieval. Unlike traditional RAG\npipelines, which rely on embedding raw text chunks, QuOTE augments chunks with\nhypothetical questions that the chunk can potentially answer, enriching the\nrepresentation space. This better aligns document embeddings with user query\nsemantics, and helps address issues such as ambiguity and context-dependent\nrelevance. Through extensive experiments across diverse benchmarks, we\ndemonstrate that QuOTE significantly enhances retrieval accuracy, including in\nmulti-hop question-answering tasks. Our findings highlight the versatility of\nquestion generation as a fundamental indexing strategy, opening new avenues for\nintegrating question generation into retrieval-based AI pipelines.\n","authors":["Andrew Neeser","Kaylen Latimer","Aadyant Khatri","Chris Latimer","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.10976v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.11184v1","updated":"2025-02-16T16:12:40Z","published":"2025-02-16T16:12:40Z","title":"Can't See the Forest for the Trees: Benchmarking Multimodal Safety\n  Awareness for Multimodal LLMs","summary":"  Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.\n","authors":["Wenxuan Wang","Xiaoyuan Liu","Kuiyi Gao","Jen-tse Huang","Youliang Yuan","Pinjia He","Shuai Wang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2502.11184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06490v2","updated":"2025-02-16T08:11:09Z","published":"2025-02-10T14:08:25Z","title":"Recent Advances in Discrete Speech Tokens: A Review","summary":"  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n","authors":["Yiwei Guo","Zhihan Li","Hankun Wang","Bohan Li","Chongtian Shao","Hanglei Zhang","Chenpeng Du","Xie Chen","Shujie Liu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06490v2.pdf","comment":"23 pages, 8 figures, 3 tables. Work in progress"},{"id":"http://arxiv.org/abs/2502.10999v1","updated":"2025-02-16T05:30:18Z","published":"2025-02-16T05:30:18Z","title":"ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering\n  without Font Annotations","summary":"  This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations. Visual text rendering remains a significant challenge. While\nrecent methods condition diffusion on glyphs, it is impossible to retrieve\nexact font annotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering.\n","authors":["Bowen Jiang","Yuan Yuan","Xinyi Bai","Zhuoqun Hao","Alyson Yin","Yaojie Hu","Wenyu Liao","Lyle Ungar","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.10999v1.pdf","comment":"This is preliminary work and code will be released at\n  github.com/bowen-upenn/ControlText"},{"id":"http://arxiv.org/abs/2405.15757v3","updated":"2025-02-16T03:01:55Z","published":"2024-05-24T17:53:06Z","title":"Looking Backward: Streaming Video-to-Video Translation with Feature\n  Banks","summary":"  This paper introduces StreamV2V, a diffusion model that achieves real-time\nstreaming video-to-video (V2V) translation with user prompts. Unlike prior V2V\nmethods using batches to process limited frames, we opt to process frames in a\nstreaming fashion, to support unlimited frames. At the heart of StreamV2V lies\na backward-looking principle that relates the present to the past. This is\nrealized by maintaining a feature bank, which archives information from past\nframes. For incoming frames, StreamV2V extends self-attention to include banked\nkeys and values and directly fuses similar past features into the output. The\nfeature bank is continually updated by merging stored and new features, making\nit compact but informative. StreamV2V stands out for its adaptability and\nefficiency, seamlessly integrating with image diffusion models without\nfine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x\nfaster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative\nmetrics and user studies confirm StreamV2V's exceptional ability to maintain\ntemporal consistency.\n","authors":["Feng Liang","Akio Kodaira","Chenfeng Xu","Masayoshi Tomizuka","Kurt Keutzer","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2405.15757v3.pdf","comment":"ICLR 2025. Project page:\n  https://jeff-liangf.github.io/projects/streamv2v"}]},"2025-02-15T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.10916v1","updated":"2025-02-15T22:08:53Z","published":"2025-02-15T22:08:53Z","title":"Enhancing Conversational Agents from Open-Source Large Language Models\n  with Illocutionary Force and Document-Based Knowledge Retrieval","summary":"  In this paper, we first present a novel way of computationally analysing and\nextracting illocutionary forces from dialogue using Bert-based Large Language\nModels, and demonstrate how these features impact the response of a\nconversational agent guided by a document-based knowledge bank demonstrated by\na bespoke web conversational chat agent system developed. Our proposed\nillocutionary force extraction and classification technique is the first of its\nkind using the Argument Interchange Format (AIF) Dataset, showing an improved\nperformance compared to two methods for carrying out similar tasks with a macro\nF1 of approximately 45%. When we evaluated the system based on 2 knowledge\nfiles, with 2 user queries each, across 5 open-source large language models\n(LLMs) using 10 standard metrics we found out that larger open-source models,\nsuch as Llama2:13b and Llama3-chatqa-latest, demonstrated an improved alignment\nwhen the user illocutionary force was included with their query, achieving\nhigher QA and linguistic similarity scores. The smaller models on the other\nhand like Tinyllama:latest showed an increased perplexity and mixed\nperformance, which explicitly indicated struggles in processing queries that\nexplicitly included illocutionary forces. The results from the analysis\nhighlight the potential of illocutionary force to enhance conversational depth\nwhile underscoring the need for model-specific optimizations to address\nincreased computational costs and response times.\n","authors":["Godfrey Inyama"],"pdf_url":"https://arxiv.org/pdf/2502.10916v1.pdf","comment":"23 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2502.10875v1","updated":"2025-02-15T18:18:00Z","published":"2025-02-15T18:18:00Z","title":"A Geometric Approach to Personalized Recommendation with Set-Theoretic\n  Constraints Using Box Embeddings","summary":"  Personalized item recommendation typically suffers from data sparsity, which\nis most often addressed by learning vector representations of users and items\nvia low-rank matrix factorization. While this effectively densifies the matrix\nby assuming users and movies can be represented by linearly dependent latent\nfeatures, it does not capture more complicated interactions. For example,\nvector representations struggle with set-theoretic relationships, such as\nnegation and intersection, e.g. recommending a movie that is \"comedy and\naction, but not romance\". In this work, we formulate the problem of\npersonalized item recommendation as matrix completion where rows are\nset-theoretically dependent. To capture this set-theoretic dependence we\nrepresent each user and attribute by a hyper-rectangle or box (i.e. a Cartesian\nproduct of intervals). Box embeddings can intuitively be understood as\ntrainable Venn diagrams, and thus not only inherently represent similarity (via\nthe Jaccard index), but also naturally and faithfully support arbitrary\nset-theoretic relationships. Queries involving set-theoretic constraints can be\nefficiently computed directly on the embedding space by performing geometric\noperations on the representations. We empirically demonstrate the superiority\nof box embeddings over vector-based neural methods on both simple and complex\nitem recommendation queries by up to 30 \\% overall.\n","authors":["Shib Dasgupta","Michael Boratko","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2502.10875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10833v1","updated":"2025-02-15T15:25:38Z","published":"2025-02-15T15:25:38Z","title":"Order-agnostic Identifier for Large Language Model-based Generative\n  Recommendation","summary":"  Leveraging Large Language Models (LLMs) for generative recommendation has\nattracted significant research interest, where item tokenization is a critical\nstep. It involves assigning item identifiers for LLMs to encode user history\nand generate the next item. Existing approaches leverage either token-sequence\nidentifiers, representing items as discrete token sequences, or single-token\nidentifiers, using ID or semantic embeddings. Token-sequence identifiers face\nissues such as the local optima problem in beam search and low generation\nefficiency due to step-by-step generation. In contrast, single-token\nidentifiers fail to capture rich semantics or encode Collaborative Filtering\n(CF) information, resulting in suboptimal performance.\n  To address these issues, we propose two fundamental principles for item\nidentifier design: 1) integrating both CF and semantic information to fully\ncapture multi-dimensional item information, and 2) designing order-agnostic\nidentifiers without token dependency, mitigating the local optima issue and\nachieving simultaneous generation for generation efficiency. Accordingly, we\nintroduce a novel set identifier paradigm for LLM-based generative\nrecommendation, representing each item as a set of order-agnostic tokens. To\nimplement this paradigm, we propose SETRec, which leverages CF and semantic\ntokenizers to obtain order-agnostic multi-dimensional tokens. To eliminate\ntoken dependency, SETRec uses a sparse attention mask for user history encoding\nand a query-guided generation mechanism for simultaneous token generation. We\ninstantiate SETRec on T5 and Qwen (from 1.5B to 7B). Extensive experiments\ndemonstrate its effectiveness under various scenarios (e.g., full ranking,\nwarm- and cold-start ranking, and various item popularity groups). Moreover,\nresults validate SETRec's superior efficiency and show promising scalability on\ncold-start items as model sizes increase.\n","authors":["Xinyu Lin","Haihan Shi","Wenjie Wang","Fuli Feng","Qifan Wang","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.10833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02400v2","updated":"2025-02-15T12:55:01Z","published":"2024-10-17T11:45:59Z","title":"Decomposition Dilemmas: Does Claim Decomposition Boost or Burden\n  Fact-Checking Performance?","summary":"  Fact-checking pipelines increasingly adopt the Decompose-Then-Verify\nparadigm, where texts are broken down into smaller claims for individual\nverification and subsequently combined for a veracity decision. While\ndecomposition is widely-adopted in such pipelines, its effects on final\nfact-checking performance remain underexplored. Some studies have reported\nimprovements from decompostition, while others have observed performance\ndeclines, indicating its inconsistent impact. To date, no comprehensive\nanalysis has been conducted to understand this variability. To address this\ngap, we present an in-depth analysis that explicitly examines the impact of\ndecomposition on downstream verification performance. Through error case\ninspection and experiments, we introduce a categorization of decomposition\nerrors and reveal a trade-off between accuracy gains and the noise introduced\nthrough decomposition. Our analysis provides new insights into understanding\ncurrent system's instability and offers guidance for future studies toward\nimproving claim decomposition in fact-checking pipelines.\n","authors":["Qisheng Hu","Quanyu Long","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2411.02400v2.pdf","comment":"NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.10768v1","updated":"2025-02-15T11:17:37Z","published":"2025-02-15T11:17:37Z","title":"Evaluating improvements on using Large Language Models (LLMs) for\n  property extraction in the Open Research Knowledge Graph (ORKG)","summary":"  Current research highlights the great potential of Large Language Models\n(LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly\ncomplex step in this process is relation extraction, aimed at identifying\nsuitable properties to describe the content of research. This study builds\ndirectly on previous research of three Open Research Knowledge Graph (ORKG)\nteam members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and\nMistral for property extraction in scientific literature. Given the moderate\nperformance observed, the previous work concluded that fine-tuning is needed to\nimprove these models' alignment with scientific tasks and their emulation of\nhuman expertise. Expanding on this prior experiment, this study evaluates the\nimpact of advanced prompt engineering techniques and demonstrates that these\ntechniques can highly significantly enhance the results. Additionally, this\nstudy extends the property extraction process to include property matching to\nexisting ORKG properties, which are retrieved via the API. The evaluation\nreveals that results generated through advanced prompt engineering achieve a\nhigher proportion of matches with ORKG properties, further emphasizing the\nenhanced alignment achieved. Moreover, this lays the groundwork for addressing\nchallenges such as the inconsistency of ORKG properties, an issue highlighted\nin prior studies. By assigning unique URIs and using standardized terminology,\nthis work increases the consistency of the properties, fulfilling a crucial\naspect of Linked Data and FAIR principles - core commitments of ORKG. This, in\nturn, significantly enhances the applicability of ORKG content for subsequent\ntasks such as comparisons of research publications. Finally, the study\nconcludes with recommendations for future improvements in the overall property\nextraction process.\n","authors":["Sandra Schaftner"],"pdf_url":"https://arxiv.org/pdf/2502.10768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10674v3","updated":"2025-02-15T08:31:04Z","published":"2024-12-14T04:22:09Z","title":"USM: Unbiased Survey Modeling for Limiting Negative User Experiences in\n  Recommendation Systems","summary":"  Reducing negative user experiences is essential for the success of\nrecommendation platforms. Exposing users to inappropriate content could not\nonly adversely affect users' psychological well-beings, but also potentially\ndrive users away from the platform, sabotaging the platform's long-term\nsuccess. However, recommendation algorithms tend to weigh more heavily on\npositive feedback signals due to the scarcity of negative ones, which may\nresult in the neglect of valuable negative user feedback. In this paper, we\npropose an approach aimed at limiting negative user experiences. Our method\nprimarily relies on distributing in-feed surveys to the users, modeling the\nusers' feedback collected from the survey, and integrating the model\npredictions into the recommendation system. We further enhance the baseline\nsurvey model by integrating the Learning Hidden Unit Contributions module and\nthe Squeeze-and-Excitation module. In addition, we strive to resolve the\nproblem of response Bias by applying a survey-submit model; The A/B testing\nresults indicate a reduction in survey sexual rate and survey inappropriate\nrate, ranging from -1.44\\% to -3.9\\%. Additionally, we compared our methods\nagainst an online baseline that does not incorporate our approach. The results\nindicate that our approach significantly reduces the report rate and dislike\nrate by 1\\% to 2.27\\% compared to the baseline, confirming the effectiveness of\nour methods in enhancing user experience. After we launched the survey model\nbased our approach on our platform, the model is able to bring reductions of\n1.75\\%, 2.57\\%, 2.06\\% on reports, dislikes, survey inappropriate rate,\nrespectively.\n","authors":["Chenghui Yu","Peiyi Li","Haoze Wu","Yiri Wen","Bingfeng Deng","Hongyu Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.10674v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.17799v3","updated":"2025-02-15T06:31:08Z","published":"2025-01-29T17:38:39Z","title":"Leveraging Multimodal LLM for Inspirational User Interface Search","summary":"  Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies.\n","authors":["Seokhyeon Park","Yumin Song","Soohyun Lee","Jaeyoung Kim","Jinwook Seo"],"pdf_url":"https://arxiv.org/pdf/2501.17799v3.pdf","comment":"In Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems (CHI '25)"},{"id":"http://arxiv.org/abs/2502.10639v1","updated":"2025-02-15T02:33:01Z","published":"2025-02-15T02:33:01Z","title":"LSTM-based Selective Dense Text Retrieval Guided by Sparse Lexical\n  Retrieval","summary":"  This paper studies fast fusion of dense retrieval and sparse lexical\nretrieval, and proposes a cluster-based selective dense retrieval method called\nCluSD guided by sparse lexical retrieval. CluSD takes a lightweight\ncluster-based approach and exploits the overlap of sparse retrieval results and\nembedding clusters in a two-stage selection process with an LSTM model to\nquickly identify relevant clusters while incurring limited extra memory space\noverhead. CluSD triggers partial dense retrieval and performs cluster-based\nblock disk I/O if needed. This paper evaluates CluSD and compares it with\nseveral baselines for searching in-memory and on-disk MS MARCO and BEIR\ndatasets.\n","authors":["Yingrui Yang","Parker Carlson","Yifan Qiao","Wentai Xie","Shanxiu He","Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10639v1.pdf","comment":"This paper is accepted by ECIR'25"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.10663v1","updated":"2025-02-15T04:00:43Z","published":"2025-02-15T04:00:43Z","title":"REAL: Realism Evaluation of Text-to-Image Generation Models for\n  Effective Data Augmentation","summary":"  Recent advancements in text-to-image (T2I) generation models have transformed\nthe field. However, challenges persist in generating images that reflect\ndemanding textual descriptions, especially for fine-grained details and unusual\nrelationships. Existing evaluation metrics focus on text-image alignment but\noverlook the realism of the generated image, which can be crucial for\ndownstream applications like data augmentation in machine learning. To address\nthis gap, we propose REAL, an automatic evaluation framework that assesses\nrealism of T2I outputs along three dimensions: fine-grained visual attributes,\nunusual visual relationships, and visual styles. REAL achieves a Spearman's rho\nscore of up to 0.62 in alignment with human judgement and demonstrates utility\nin ranking and filtering augmented data for tasks like im- age captioning,\nclassification, and visual relationship detection. Empirical results show that\nhigh-scoring images evaluated by our metrics improve F1 scores of image\nclassification by up to 11.3%, while low-scoring ones degrade that by up to\n4.95%. We benchmark four major T2I models across the realism dimensions,\nproviding insights for future improvements in T2I output realism.\n","authors":["Ran Li","Xiaomeng Jin","Heng ji"],"pdf_url":"https://arxiv.org/pdf/2502.10663v1.pdf","comment":null}]}}