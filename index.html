<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-17T00:00:00Z">2025-02-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">200</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Idiosyncrasies in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we unveil and study idiosyncrasies in Large Language Models
(LLMs) -- unique patterns in their outputs that can be used to distinguish the
models. To do so, we consider a simple classification task: given a particular
text output, the objective is to predict the source LLM that generates the
text. We evaluate this synthetic task across various groups of LLMs and find
that simply fine-tuning existing text embedding models on LLM-generated texts
yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on
held-out validation data in the five-way classification problem involving
ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals
that these idiosyncrasies are rooted in word-level distributions. These
patterns persist even when the texts are rewritten, translated, or summarized
by an external LLM, suggesting that they are also encoded in the semantic
content. Additionally, we leverage LLM as judges to generate detailed,
open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the
broader implications of our findings, particularly for training on synthetic
data and inferring model similarity. Code is available at
https://github.com/locuslab/llm-idiosyncrasies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at
  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HARBOR: Exploring Persona Dynamics in Multi-Agent Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenan Jiang, Li Xiong, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate factors contributing to LLM agents' success in competitive
multi-agent environments, using auctions as a testbed where agents bid to
maximize profit. The agents are equipped with bidding domain knowledge,
distinct personas that reflect item preferences, and a memory of auction
history. Our work extends the classic auction scenario by creating a realistic
environment where multiple agents bid on houses, weighing aspects such as size,
location, and budget to secure the most desirable homes at the lowest prices.
Particularly, we investigate three key questions: (a) How does a persona
influence an agent's behavior in a competitive setting? (b) Can an agent
effectively profile its competitors' behavior during auctions? (c) How can
persona profiling be leveraged to create an advantage using strategies such as
theory of mind? Through a series of experiments, we analyze the behaviors of
LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a
valuable platform for deepening our understanding of multi-agent workflows in
competitive environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to
  Enhance Wikipedia Tail Bio<span class="highlight-title">graph</span>ies through Personal Narratives <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Pauras Mangesh Meher, Paramita Das, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wikipedia is an invaluable resource for factual information about a wide
range of entities. However, the quality of articles on less-known entities
often lags behind that of the well-known ones. This study proposes a novel
approach to enhancing Wikipedia's B and C category biography articles by
leveraging personal narratives such as autobiographies and biographies. By
utilizing a multi-staged retrieval-augmented generation technique -- REVerSum
-- we aim to enrich the informational content of these lesser-known articles.
Our study reveals that personal narratives can significantly improve the
quality of Wikipedia articles, providing a rich source of reliable information
that has been underutilized in previous studies. Based on crowd-based
evaluation, REVerSum generated content outperforms the best performing baseline
by 17% in terms of integrability to the original Wikipedia article and 28.5\%
in terms of informativeness. Code and Data are available at:
https://github.com/sayantan11995/wikipedia_enrichment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to
solve complex reasoning tasks by generating intermediate reasoning steps.
However, most existing approaches focus on hard token decoding, which
constrains reasoning within the discrete vocabulary space and may not always be
optimal. While recent efforts explore continuous-space reasoning, they often
suffer from catastrophic forgetting, limiting their applicability to
state-of-the-art LLMs that already perform well in zero-shot settings with a
proper instruction. To address this challenge, we propose a novel approach for
continuous-space reasoning that does not require modifying the underlying LLM.
Specifically, we employ a lightweight assistant model to generate
instance-specific soft thought tokens speculatively as the initial chain of
thoughts, which are then mapped into the LLM's representation space via a
projection module. Experimental results on five reasoning benchmarks
demonstrate that our method enhances LLM reasoning performance through
supervised, parameter-efficient fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for
  Inspirational Quote Extraction from Long Documents <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspirational quotes from famous individuals are often used to convey
thoughts in news articles, essays, and everyday conversations. In this paper,
we propose a novel context-based quote extraction system that aims to extract
the most relevant quote from a long text. We formulate this quote extraction as
an open domain question answering problem first by employing a vector-store
based retriever and then applying a multi-task reader. We curate three
context-based quote extraction datasets and introduce a novel multi-task
framework RA-MTR that improves the state-of-the-art performance, achieving a
maximum improvement of 5.08% in BoW F1-score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING2025-MAIN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Query Complexity of Verifier-Assisted Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan T. Ash, Cyril Zhang, Andrej Risteski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a plethora of works have proposed inference-time algorithms (e.g.
best-of-n), which incorporate verifiers to assist the generation process. Their
quality-efficiency trade-offs have been empirically benchmarked on a variety of
constrained generation tasks, but the algorithmic design landscape is still
largely poorly understood. In this paper, we develop a mathematical framework
for reasoning about constrained generation using a pre-trained language model
generator oracle and a process verifier--which can decide whether a prefix can
be extended to a string which satisfies the constraints of choice. We show that
even in very simple settings, access to a verifier can render an intractable
problem (information-theoretically or computationally) to a tractable one. In
fact, we show even simple algorithms, like tokenwise rejection sampling, can
enjoy significant benefits from access to a verifier. Empirically, we show that
a natural modification of tokenwise rejection sampling, in which the sampler is
allowed to "backtrack" (i.e., erase the final few generated tokens) has robust
and substantive benefits over natural baselines (e.g. (blockwise) rejection
sampling, nucleus sampling)--both in terms of computational efficiency,
accuracy and diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws guide the development of large language models (LLMs) by
offering estimates for the optimal balance of model size, tokens, and compute.
More recently, loss-to-loss scaling laws that relate losses across pretraining
datasets and downstream tasks have emerged as a powerful tool for understanding
and improving LLM performance. In this work, we investigate which factors most
strongly influence loss-to-loss scaling. Our experiments reveal that the
pretraining data and tokenizer determine the scaling trend. In contrast, model
size, optimization hyperparameters, and even significant architectural
differences, such as between transformer-based models like Llama and
state-space models like Mamba, have limited impact. Consequently, practitioners
should carefully curate suitable pretraining datasets for optimal downstream
performance, while architectures and other settings can be freely optimized for
training efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRISM: Self-Pruning Intrinsic Selection Method for Training-Free
  <span class="highlight-title">Multimodal</span> Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning refines pre-trained Multimodal Large Language
Models (MLLMs) to enhance their real-world task performance. However, the rapid
expansion of visual instruction datasets introduces significant data
redundancy, leading to excessive computational costs. Existing data selection
methods predominantly rely on proxy models or loss-based metrics, both of which
impose substantial computational overheads due to the necessity of model
inference and backpropagation. To address this challenge, we propose PRISM, a
novel training-free approach for efficient multimodal data selection. Unlike
existing methods, PRISM eliminates the reliance on proxy models, warm-up
pretraining, and gradient-based optimization. Instead, it leverages Pearson
correlation analysis to quantify the intrinsic visual encoding properties of
MLLMs, computing a task-specific correlation score to identify high-value
instances. This not only enbles data-efficient selection,but maintains the
original performance. Empirical evaluations across multiple MLLMs demonstrate
that PRISM reduces the overall time required for visual instruction tuning and
data selection to just 30% of conventional methods, while surpassing fully
fine-tuned models across eight multimodal and three language understanding
benchmarks, achieving a 101.7% relative improvement in final performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Test-Time Compute Without Verification or RL is Suboptimal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial advances in scaling test-time compute, an ongoing debate
in the community is how it should be scaled up to enable continued and
efficient improvements with scaling. There are largely two approaches: first,
distilling successful search or thinking traces; and second, using verification
(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement
learning (RL) and search algorithms. In this paper, we prove that finetuning
LLMs with verifier-based (VB) methods based on RL or search is far superior to
verifier-free (VF) approaches based on distilling or cloning search traces,
given a fixed amount of compute/data budget. Further, we show that as we scale
test-time compute (measured as the output token length) and training data,
suboptimality of VF methods scales poorly compared to VB when the base
pre-trained LLM presents a heterogeneous distribution over correct solution
traces (e.g., different lengths, styles, etc.) and admits a non-sharp
distribution over rewards on traces sampled from it. We formalize this
condition using anti-concentration [Erd\H{o}s, 1945]. This implies a stronger
result that VB methods scale better asymptotically, with the performance gap
between VB and VF methods widening as test-time budget grows. We corroborate
our theory empirically on both didactic and math reasoning problems with
3/8/32B-sized pre-trained LLMs, where we find verification is crucial for
scaling test-time compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A-MEM: Agentic Memory for LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language model (LLM) agents can effectively use external tools
for complex real-world tasks, they require memory systems to leverage
historical experiences. Current memory systems enable basic storage and
retrieval but lack sophisticated memory organization, despite recent attempts
to incorporate graph databases. Moreover, these systems' fixed operations and
structures limit their adaptability across diverse tasks. To address this
limitation, this paper proposes a novel agentic memory system for LLM agents
that can dynamically organize memories in an agentic way. Following the basic
principles of the Zettelkasten method, we designed our memory system to create
interconnected knowledge networks through dynamic indexing and linking. When a
new memory is added, we generate a comprehensive note containing multiple
structured attributes, including contextual descriptions, keywords, and tags.
The system then analyzes historical memories to identify relevant connections,
establishing links where meaningful similarities exist. Additionally, this
process enables memory evolution - as new memories are integrated, they can
trigger updates to the contextual representations and attributes of existing
historical memories, allowing the memory network to continuously refine its
understanding. Our approach combines the structured organization principles of
Zettelkasten with the flexibility of agent-driven decision making, allowing for
more adaptive and context-aware memory management. Empirical experiments on six
foundation models show superior improvement against existing SOTA baselines.
The source code is available at https://github.com/WujiangXu/AgenticMemory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personality Structured Interview for Large Language Model Simulation in
  Personality Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although psychometrics researchers have recently explored the use of large
language models (LLMs) as proxies for human participants, LLMs often fail to
generate heterogeneous data with human-like diversity, which diminishes their
value in advancing social science research. To address these challenges, we
explored the potential of the theory-informed Personality Structured Interview
(PSI) as a tool for simulating human responses in personality research. In this
approach, the simulation is grounded in nuanced real-human interview
transcripts that target the personality construct of interest. We have provided
a growing set of 357 structured interview transcripts from a representative
sample, each containing an individual's response to 32 open-ended questions
carefully designed to gather theory-based personality evidence. Additionally,
grounded in psychometric research, we have summarized an evaluation framework
to systematically validate LLM-generated psychometric data. Results from three
experiments demonstrate that well-designed structured interviews could improve
human-like heterogeneity in LLM-simulated personality data and predict
personality-related behavioral outcomes (i.e., organizational citizenship
behaviors and counterproductive work behavior). We further discuss the role of
theory-informed structured interviews in LLM-based simulation and outline a
general framework for designing structured interviews to simulate human-like
data for psychometric research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 Pages, 30 Tables, 5 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on Leveraging Search and Self-Feedback for Agent Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthikeyan K, Michelle Yuan, Elman Mansimov, Katerina Margatina, Anurag Pratik, Daniele Bonadiman, Monica Sunkara, Yi Zhang, Yassine Benajiba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have demonstrated that incorporating search during inference can
significantly improve reasoning capabilities of language agents. Some
approaches may make use of the ground truth or rely on model's own generated
feedback. The search algorithm uses this feedback to then produce values that
will update its criterion for exploring and exploiting various reasoning paths.
In this study, we investigate how search and model's self-feedback can be
leveraged for reasoning tasks. First, we explore differences in ground-truth
feedback and self-feedback during search for math reasoning. Second, we observe
limitations in applying search techniques to more complex tasks like
tool-calling and design domain-specific approaches to address these gaps. Our
experiments reveal challenges related to generalization when solely relying on
self-feedback during search. For search to work effectively, either access to
the ground-truth is needed or feedback mechanisms need to be carefully designed
for the specific task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APB: Accelerating Distributed Long-Context Inference by Passing
  Compressed Context Blocks across GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Hao Zhou, Jie Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While long-context inference is crucial for advancing large language model
(LLM) applications, its prefill speed remains a significant bottleneck. Current
approaches, including sequence parallelism strategies and compute reduction
through approximate attention mechanisms, still fall short of delivering
optimal inference efficiency. This hinders scaling the inputs to longer
sequences and processing long-context queries in a timely manner. To address
this, we introduce APB, an efficient long-context inference framework that
leverages multi-host approximate attention to enhance prefill speed by reducing
compute and enhancing parallelism simultaneously. APB introduces a
communication mechanism for essential key-value pairs within a sequence
parallelism framework, enabling a faster inference speed while maintaining task
performance. We implement APB by incorporating a tailored FlashAttn kernel
alongside optimized distribution strategies, supporting diverse models and
parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x
compared with FlashAttn, RingAttn, and StarAttn, respectively, without any
observable task performance degradation. We provide the implementation and
experiment code of APB in https://github.com/thunlp/APB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit
  Matching Visual Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R.,  Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually linking matching cues is a crucial ability in daily life, such as
identifying the same person in multiple photos based on their cues, even
without knowing who they are. Despite the extensive knowledge that
vision-language models (VLMs) possess, it remains largely unexplored whether
they are capable of performing this fundamental task. To address this, we
introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can
Visually Link Matching cues, with 9 subtasks and over 3,000 test cases.
Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with
further analysis of various language-side and vision-side prompting methods,
leads to a total of eight key findings. We identify critical challenges in
models' ability to link visual cues, highlighting a significant performance gap
where even GPT-4o lags 34.80% behind humans. Based on these insights, we
advocate for (i) enhancing core visual capabilities to improve adaptability and
reduce reliance on prior knowledge, (ii) establishing clearer principles for
integrating language-based reasoning in vision-centric tasks to prevent
unnecessary biases, and (iii) shifting vision-text training paradigms toward
fostering models' ability to independently structure and infer relationships
among visual cues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vlm2-bench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaSplash: Adaptive Sparse Flash Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuno Gonçalves, Marcos Treviso, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computational cost of softmax-based attention in transformers limits
their applicability to long-context tasks. Adaptive sparsity, of which
$\alpha$-entmax attention is an example, offers a flexible data-dependent
alternative, but existing implementations are inefficient and do not leverage
the sparsity to obtain runtime and memory gains. In this work, we propose
AdaSplash, which combines the efficiency of GPU-optimized algorithms with the
sparsity benefits of $\alpha$-entmax. We first introduce a hybrid
Halley-bisection algorithm, resulting in a 7-fold reduction in the number of
iterations needed to compute the $\alpha$-entmax transformation. Then, we
implement custom Triton kernels to efficiently handle adaptive sparsity.
Experiments with RoBERTa and ModernBERT for text classification and
single-vector retrieval, along with GPT-2 for language modeling, show that our
method achieves substantial improvements in runtime and memory efficiency
compared to existing $\alpha$-entmax implementations. It approaches -- and in
some cases surpasses -- the efficiency of highly optimized softmax
implementations like FlashAttention-2, enabling long-context training while
maintaining strong task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unhackable Temporal Rewarding for Scalable Video MLLMs <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of superior video-processing MLLMs, we have encountered a
perplexing paradox: the "anti-scaling law", where more data and larger models
lead to worse performance. This study unmasks the culprit: "temporal hacking",
a phenomenon where models shortcut by fixating on select frames, missing the
full video narrative. In this work, we systematically establish a comprehensive
theory of temporal hacking, defining it from a reinforcement learning
perspective, introducing the Temporal Perplexity (TPL) score to assess this
misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework
to mitigate the temporal hacking. Both theoretically and empirically, TPL
proves to be a reliable indicator of temporal modeling quality, correlating
strongly with frame activation patterns. Extensive experiments reveal that UTR
not only counters temporal hacking but significantly elevates video
comprehension capabilities. This work not only advances video-AI systems but
also illuminates the critical importance of aligning proxy rewards with true
objectives in MLLM development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Simulate Social Media Engagement? A Study on Action-Guided
  Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media enables dynamic user engagement with trending topics, and recent
research has explored the potential of large language models (LLMs) for
response generation. While some studies investigate LLMs as agents for
simulating user behavior on social media, their focus remains on practical
viability and scalability rather than a deeper understanding of how well LLM
aligns with human behavior. This paper analyzes LLMs' ability to simulate
social media engagement through action guided response generation, where a
model first predicts a user's most likely engagement action-retweet, quote, or
rewrite-towards a trending post before generating a personalized response
conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and
DeepSeek-R1 in social media engagement simulation regarding a major societal
event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT
in action prediction, while few-shot prompting initially degrades the
prediction accuracy of LLMs with limited examples. However, in response
generation, few-shot LLMs achieve stronger semantic alignment with ground truth
posts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TokenSkip: Controllable Chain-of-Thought Compression in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning
capabilities of large language models (LLMs). Recent advancements, such as
OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT
sequences during inference could further boost LLM reasoning performance.
However, due to the autoregressive nature of LLM decoding, longer CoT outputs
lead to a linear increase in inference latency, adversely affecting user
experience, particularly when the CoT exceeds 10,000 tokens. To address this
limitation, we analyze the semantic importance of tokens within CoT outputs and
reveal that their contributions to reasoning vary. Building on this insight, we
propose TokenSkip, a simple yet effective approach that enables LLMs to
selectively skip less important tokens, allowing for controllable CoT
compression. Extensive experiments across various models and tasks demonstrate
the effectiveness of TokenSkip in reducing CoT token usage while preserving
strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,
TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less
than a 0.4% performance drop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formalizing Complex Mathematical Statements with LLMs: A Study on
  Mathematical Definitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Zhang, Marco Valentino, Andre Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge
the gap between informal mathematics and formal languages through
autoformalization. However, it is still unclear how well LLMs generalize to
sophisticated and naturally occurring mathematical statements. To address this
gap, we investigate the task of autoformalizing real-world mathematical
definitions -- a critical component of mathematical discourse. Specifically, we
introduce two novel resources for autoformalisation, collecting definitions
from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically
evaluate a range of LLMs, analyzing their ability to formalize definitions into
Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'
performance including refinement through external feedback from Proof
Assistants, and formal definition grounding, where we guide LLMs through
relevant contextual elements from formal mathematical libraries. Our findings
reveal that definitions present a greater challenge compared to existing
benchmarks, such as miniF2F. In particular, we found that LLMs still struggle
with self-correction, and aligning with relevant mathematical libraries. At the
same time, structured refinement methods and definition grounding strategies
yield notable improvements of up to 16% on self-correction capabilities and 43%
on the reduction of undefined errors, highlighting promising directions for
enhancing LLM-based autoformalization in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-generated Text Detection with a GLTR-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucía Yan Wu, Isabel Segura-Bedmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of LLMs (Large Language Models) has contributed to the improved
performance and development of cutting-edge NLP applications. However, these
can also pose risks when used maliciously, such as spreading fake news, harmful
content, impersonating individuals, or facilitating school plagiarism, among
others. This is because LLMs can generate high-quality texts, which are
challenging to differentiate from those written by humans. GLTR, which stands
for Giant Language Model Test Room and was developed jointly by the MIT-IBM
Watson AI Lab and HarvardNLP, is a visual tool designed to help detect
machine-generated texts based on GPT-2, that highlights the words in text
depending on the probability that they were machine-generated. One limitation
of GLTR is that the results it returns can sometimes be ambiguous and lead to
confusion. This study aims to explore various ways to improve GLTR's
effectiveness for detecting AI-generated texts within the context of the
IberLef-AuTexTification 2023 shared task, in both English and Spanish
languages. Experiment results show that our GLTR-based GPT-2 model overcomes
the state-of-the-art models on the English dataset with a macro F1-score of
80.19%, except for the first ranking model (80.91%). However, for the Spanish
dataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%
compared to the top-performing model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Culture is Not Trivia: Sociocultural Theory for Cultural NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naitian Zhou, David Bamman, Isaac L. Bleaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of cultural NLP has recently experienced rapid growth, driven by a
pressing need to ensure that language technologies are effective and safe
across a pluralistic user base. This work has largely progressed without a
shared conception of culture, instead choosing to rely on a wide array of
cultural proxies. However, this leads to a number of recurring limitations:
coarse national boundaries fail to capture nuanced differences that lay within
them, limited coverage restricts datasets to only a subset of usually
highly-represented cultures, and a lack of dynamicity results in static
cultural benchmarks that do not change as culture evolves. In this position
paper, we argue that these methodological limitations are symptomatic of a
theoretical gap. We draw on a well-developed theory of culture from
sociocultural linguistics to fill this gap by 1) demonstrating in a case study
how it can clarify methodological constraints and affordances, 2) offering
theoretically-motivated paths forward to achieving cultural competence, and 3)
arguing that localization is a more useful framing for the goals of much
current work in cultural NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Role Vectors to Improve LLM Inference Behaviour 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Potertì, Andrea Seveso, Fabio Mercorio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of personas on Large Language Models (LLMs) has been widely
studied, yet their direct impact on performance remains uncertain. This work
explores a novel approach to guiding LLM behaviour through role vectors, an
alternative to persona-based prompting. We construct 29 role vectors derived
from model activations and evaluate their impact on benchmark performance
across multiple domains. Our analysis investigates whether these vectors can
effectively steer models toward domain-specific expertise. We measure two key
interventions: (i) activation addition, which reinforces role-specific
directions, and (ii) directional ablation, which removes them. Results on
well-established benchmarks indicate that role vectors do, in fact, influence
model behaviour, improving task performance in relevant domains while
marginally affecting unrelated tasks. This, in turn, suggests that manipulating
internal model representations has a greater impact on outcomes than
persona-based prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ARR 2025 February cycle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dual-Perspective NLG Meta-Evaluation Framework with Automatic
  Benchmark and Better Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Hu, Mingqi Gao, Li Lin, Zhenghan Yu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In NLG meta-evaluation, evaluation metrics are typically assessed based on
their consistency with humans. However, we identify some limitations in
traditional NLG meta-evaluation approaches, such as issues in handling human
ratings and ambiguous selections of correlation measures, which undermine the
effectiveness of meta-evaluation. In this work, we propose a dual-perspective
NLG meta-evaluation framework that focuses on different evaluation
capabilities, thereby providing better interpretability. In addition, we
introduce a method of automatically constructing the corresponding benchmarks
without requiring new human annotations. Furthermore, we conduct experiments
with 16 representative LLMs as the evaluators based on our proposed framework,
comprehensively analyzing their evaluation performance from different
perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Upscale Neural Networks with Scaling Law? A <span class="highlight-title">Survey</span> and Practical
  Guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Sengupta, Yash Goel, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural scaling laws have revolutionized the design and optimization of
large-scale AI models by revealing predictable relationships between model
size, dataset volume, and computational resources. Early research established
power-law relationships in model performance, leading to compute-optimal
scaling strategies. However, recent studies highlighted their limitations
across architectures, modalities, and deployment contexts. Sparse models,
mixture-of-experts, retrieval-augmented learning, and multimodal models often
deviate from traditional scaling patterns. Moreover, scaling behaviors vary
across domains such as vision, reinforcement learning, and fine-tuning,
underscoring the need for more nuanced approaches. In this survey, we
synthesize insights from over 50 studies, examining the theoretical
foundations, empirical findings, and practical implications of scaling laws. We
also explore key challenges, including data efficiency, inference scaling, and
architecture-specific constraints, advocating for adaptive scaling strategies
tailored to real-world applications. We suggest that while scaling laws provide
a useful guide, they do not always generalize across all architectures and
training strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechT: Findings of the First Mentorship in Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmin Moslem, Juan Julián Cea Morán, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents the details and findings of the first mentorship in speech
translation (SpeechT), which took place in December 2024 and January 2025. To
fulfil the requirements of the mentorship, the participants engaged in key
activities, including data preparation, modelling, and advanced research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeChain: Safety of Language Models with Long Chain-of-Thought
  Reasoning Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage
long chain-of-thought (CoT) reasoning to generate structured intermediate
steps, enhancing their reasoning capabilities. However, long CoT does not
inherently guarantee safe outputs, potentially leading to harmful consequences
such as the introduction of security vulnerabilities in code or the spread of
misinformation. Current research on large language model (LLM) safety usually
focuses on short-answer responses, overlooking the long CoT style outputs of
LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,
we investigate safety evaluators calibrated against human annotations. Using
our newly developed metrics, we thoroughly assess the safety of 12
state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results
show that LRMs are not safe compared to their reasoning advance. Further, we
perform a fine-grained analysis of the reasoning trace and final answer. We
find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can
improve model safety without additional training. However, these strategies
either use constrained reasoning traces or incur high inference costs. To
better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind
safety training dataset in CoT style. We fine-tune two LRMs with SafeChain,
showing that it not only enhances model safety but also preserves performance
across 6 reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching LLMs According to Their Aptitude: Adaptive Reasoning for
  Mathematical Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to mathematical reasoning with large language models
(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated
Reasoning (TIR) for precise computation. While efforts have been made to
combine these methods, they primarily rely on post-selection or predefined
strategies, leaving an open question: whether LLMs can autonomously adapt their
reasoning strategy based on their inherent capabilities. In this work, we
propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework
that enables LLMs to personalize their reasoning strategy spontaneously,
aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware
data selection during supervised fine-tuning (SFT) to tailor training data to
the model's unique abilities. This approach equips LLMs to autonomously
determine and apply the appropriate reasoning strategy at test time. We
evaluate TATA through extensive experiments on six mathematical reasoning
benchmarks, using both general-purpose and math-specialized LLMs. Empirical
results demonstrate that TATA effectively combines the complementary strengths
of CoT and TIR, achieving superior or comparable performance with improved
inference efficiency compared to TIR alone. Further analysis underscores the
critical role of aptitude-aware data selection in enabling LLMs to make
effective and adaptive reasoning decisions and align reasoning strategies with
model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atom of Thoughts for Markov LLM Test-Time Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) achieve superior performance through
training-time scaling, and test-time scaling further enhances their
capabilities by conducting effective reasoning during inference. However, as
the scale of reasoning increases, existing test-time scaling methods suffer
from accumulated historical information, which not only wastes computational
resources but also interferes with effective reasoning. To address this issue,
we observe that complex reasoning progress is often achieved by solving a
sequence of independent subquestions, each being self-contained and verifiable.
These subquestions are essentially atomic questions, relying primarily on their
current state rather than accumulated history, similar to the memoryless
transitions in a Markov process. Based on this observation, we propose Atom of
Thoughts (AoT), where each state transition in the reasoning process consists
of decomposing the current question into a dependency-based directed acyclic
graph and contracting its subquestions, forming a new atomic question state.
This iterative decomposition-contraction process continues until reaching
directly solvable atomic questions, naturally realizing Markov transitions
between question states. Furthermore, these atomic questions can be seamlessly
integrated into existing test-time scaling methods, enabling AoT to serve as a
plug-in enhancement for improving reasoning capabilities. Experiments across
six benchmarks demonstrate the effectiveness of AoT both as a standalone
framework and a plug-in enhancement. Notably, on HotpotQA, when applied to
gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and
DeepSeek-R1 by 10.6%. The code will be available at
https://github.com/qixucen/atom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demo<span class="highlight-title">graph</span>ic Attributes Prediction from Speech Using WavLM Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Yang, Thomas Thebaud, Najim Dehak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a general classifier based on WavLM features, to infer
demographic characteristics, such as age, gender, native language, education,
and country, from speech. Demographic feature prediction plays a crucial role
in applications like language learning, accessibility, and digital forensics,
enabling more personalized and inclusive technologies. Leveraging pretrained
models for embedding extraction, the proposed framework identifies key acoustic
and linguistic fea-tures associated with demographic attributes, achieving a
Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy
for gender classification across various datasets. Our system improves upon
existing models by up to relative 30% in MAE and up to relative 10% in accuracy
and F1 scores across tasks, leveraging a diverse range of datasets and large
pretrained models to ensure robustness and generalizability. This study offers
new insights into speaker diversity and provides a strong foundation for future
research in speech-based demographic profiling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, accepted by The Conference on Information Sciences and
  Systems (CISS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging Language and Domain Specific Models: The Impact on Technical
  Vocabulary Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the integration of technical vocabulary in merged
language models. We explore the knowledge transfer mechanisms involved when
combining a general-purpose language-specific model with a domain-specific
model, focusing on the resulting model's comprehension of technical jargon. Our
experiments analyze the impact of this merging process on the target model's
proficiency in handling specialized terminology. We present a quantitative
evaluation of the performance of the merged model, comparing it with that of
the individual constituent models. The findings offer insights into the
effectiveness of different model merging methods for enhancing domain-specific
knowledge and highlight potential challenges and future directions in
leveraging these methods for cross-lingual knowledge transfer in Natural
Language Processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 263rd IPSJ-NL Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Presumed Cultural Identity: How Names Shape LLM Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhesh Pawar, Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Names are deeply tied to human identity. They can serve as markers of
individuality, cultural heritage, and personal history. However, using names as
a core indicator of identity can lead to over-simplification of complex
identities. When interacting with LLMs, user names are an important point of
information for personalisation. Names can enter chatbot conversations through
direct user input (requested by chatbots), as part of task contexts such as CV
reviews, or as built-in memory features that store user information for
personalisation. We study biases associated with names by measuring cultural
presumptions in the responses generated by LLMs when presented with common
suggestion-seeking queries, which might involve making assumptions about the
user. Our analyses demonstrate strong assumptions about cultural identity
associated with names present in LLM generations across multiple cultures. Our
work has implications for designing more nuanced personalisation systems that
avoid reinforcing stereotypes while maintaining meaningful customisation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 Pages, 13 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Text from Uniform Meaning Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Markle, Reihaneh Iranmanesh, Shira Wein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uniform Meaning Representation (UMR) is a recently developed graph-based
semantic representation, which expands on Abstract Meaning Representation (AMR)
in a number of ways, in particular through the inclusion of document-level
information and multilingual flexibility. In order to effectively adopt and
leverage UMR for downstream tasks, efforts must be placed toward developing a
UMR technological ecosystem. Though still limited amounts of UMR annotations
have been produced to date, in this work, we investigate the first approaches
to producing text from multilingual UMR graphs: (1) a pipeline conversion of
UMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large
language models with UMR data, and (3) fine-tuning existing AMR-to-text
generation models with UMR data. Our best performing model achieves a
multilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared
to the reference, which is a promising indication of the effectiveness of
fine-tuning approaches for UMR-to-text generation with even limited amounts of
UMR data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware
  Instruction Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language
Models (LLMs), but it may lower their truthfulness. This trade-off arises
because IFT steers LLMs to generate responses with long-tail knowledge that is
not well covered during pre-training, leading to more informative but less
truthful answers when generalizing to unseen tasks. In this paper, we
empirically demonstrate this helpfulness-truthfulness trade-off in IFT and
propose $\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs
to recognize their uncertainty and explicitly reflect it at the end of their
responses. Experimental results show that UNIT-tuned models maintain their
helpfulness while distinguishing between certain and uncertain claims, thereby
reducing hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Your Uncertainty Scores Detect Hallucinated Entity? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mitigate the impact of hallucination nature of LLMs, many studies propose
detecting hallucinated generation through uncertainty estimation. However,
these approaches predominantly operate at the sentence or paragraph level,
failing to pinpoint specific spans or entities responsible for hallucinated
content. This lack of granularity is especially problematic for long-form
outputs that mix accurate and fabricated information. To address this
limitation, we explore entity-level hallucination detection. We propose a new
data set, HalluEntity, which annotates hallucination at the entity level. Based
on the dataset, we comprehensively evaluate uncertainty-based hallucination
detection approaches across 17 modern LLMs. Our experimental results show that
uncertainty estimation approaches focusing on individual token probabilities
tend to over-predict hallucinations, while context-aware methods show better
but still suboptimal performance. Through an in-depth qualitative study, we
identify relationships between hallucination tendencies and linguistic
properties and highlight important directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step-Audio: Unified Understanding and Generation in Intelligent Speech
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuting Yan, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time speech interaction, serving as a fundamental interface for
human-machine collaboration, holds immense potential. However, current
open-source models face limitations such as high costs in voice data
collection, weakness in dynamic control, and limited intelligence. To address
these challenges, this paper introduces Step-Audio, the first production-ready
open-source solution. Key contributions include: 1) a 130B-parameter unified
speech-text multi-modal model that achieves unified understanding and
generation, with the Step-Audio-Chat version open-sourced; 2) a generative
speech data engine that establishes an affordable voice cloning framework and
produces the open-sourced lightweight Step-Audio-TTS-3B model through
distillation; 3) an instruction-driven fine control system enabling dynamic
adjustments across dialects, emotions, singing, and RAP; 4) an enhanced
cognitive architecture augmented with tool calling and role-playing abilities
to manage complex tasks effectively. Based on our new StepEval-Audio-360
evaluation benchmark, Step-Audio achieves state-of-the-art performance in human
evaluations, especially in terms of instruction following. On open-source
benchmarks like LLaMA Question, shows 9.3% average performance improvement,
demonstrating our commitment to advancing the development of open-source
multi-modal language technologies. Our code and models are available at
https://github.com/stepfun-ai/Step-Audio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Representational Dissociation of Language and Arithmetic in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riku Kisako, Tatsuki Kuribayashi, Ryohei Sasano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The association between language and (non-linguistic) thinking ability in
humans has long been debated, and recently, neuroscientific evidence of brain
activity patterns has been considered. Such a scientific context naturally
raises an interdisciplinary question -- what about such a language-thought
dissociation in large language models (LLMs)? In this paper, as an initial
foray, we explore this question by focusing on simple arithmetic skills (e.g.,
$1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in
LLMs' representation space. Our experiments with linear classifiers and cluster
separability tests demonstrate that simple arithmetic equations and general
language input are encoded in completely separated regions in LLMs' internal
representation space across all the layers, which is also supported with more
controlled stimuli (e.g., spelled-out equations). These tentatively suggest
that arithmetic reasoning is mapped into a distinct region from general
language input, which is in line with the neuroscientific observations of human
brain activations, while we also point out their somewhat cognitively
implausible geometric properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion
  Recognition <span class="highlight-title">Dataset</span>s for 28 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Andrew Piper, Alexander Panchenko, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People worldwide use language in subtle and complex ways to express emotions.
While emotion recognition -- an umbrella term for several NLP tasks --
significantly impacts different applications in NLP and other fields, most work
in the area is focused on high-resource languages. Therefore, this has led to
major disparities in research and proposed solutions, especially for
low-resource languages that suffer from the lack of high-quality datasets. In
this paper, we present BRIGHTER-- a collection of multilabeled
emotion-annotated datasets in 28 different languages. BRIGHTER covers
predominantly low-resource languages from Africa, Asia, Eastern Europe, and
Latin America, with instances from various domains annotated by fluent
speakers. We describe the data collection and annotation processes and the
challenges of building these datasets. Then, we report different experimental
results for monolingual and crosslingual multi-label emotion identification, as
well as intensity-level emotion recognition. We investigate results with and
without using LLMs and analyse the large variability in performance across
languages and text domains. We show that BRIGHTER datasets are a step towards
bridging the gap in text-based emotion recognition and discuss their impact and
utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Text to Trust: Empowering AI-assisted Decision Making with Adaptive
  LLM-powered Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ziang Xiao, Ming Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-assisted decision making becomes increasingly prevalent, yet individuals
often fail to utilize AI-based decision aids appropriately especially when the
AI explanations are absent, potentially as they do not %understand reflect on
AI's decision recommendations critically. Large language models (LLMs), with
their exceptional conversational and analytical capabilities, present great
opportunities to enhance AI-assisted decision making in the absence of AI
explanations by providing natural-language-based analysis of AI's decision
recommendation, e.g., how each feature of a decision making task might
contribute to the AI recommendation. In this paper, via a randomized
experiment, we first show that presenting LLM-powered analysis of each task
feature, either sequentially or concurrently, does not significantly improve
people's AI-assisted decision performance. To enable decision makers to better
leverage LLM-powered analysis, we then propose an algorithmic framework to
characterize the effects of LLM-powered analysis on human decisions and
dynamically decide which analysis to present. Our evaluation with human
subjects shows that this approach effectively improves decision makers'
appropriate reliance on AI in AI-assisted decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay
  Scoring Capabilities of <span class="highlight-title">Multimodal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Essay Scoring (AES) plays a crucial role in educational assessment
by providing scalable and consistent evaluations of writing tasks. However,
traditional AES systems face three major challenges: (1) reliance on
handcrafted features that limit generalizability, (2) difficulty in capturing
fine-grained traits like coherence and argumentation, and (3) inability to
handle multimodal contexts. In the era of Multimodal Large Language Models
(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES
capabilities across lexical-, sentence-, and discourse-level traits. By
leveraging MLLMs' strengths in trait-specific scoring and multimodal context
understanding, EssayJudge aims to offer precise, context-rich evaluations
without manual feature engineering, addressing longstanding AES limitations.
Our experiments with 18 representative MLLMs reveal gaps in AES performance
compared to human evaluation, particularly in discourse-level traits,
highlighting the need for further advancements in MLLM-based AES research. Our
dataset and code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JS and YY are co-first authors. XH is the corresponding author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRC: A Large-Scale Benchmark for Understanding <span class="highlight-title">Multimodal</span> Large
  Language Model in Real-World Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal large language models (MLLMs) have demonstrated significant
potential in open-ended conversation, generating more accurate and personalized
responses. However, their abilities to memorize, recall, and reason in
sustained interactions within real-world scenarios remain underexplored. This
paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for
evaluating six core open-ended abilities of MLLMs: information extraction,
multi-turn reasoning, information update, image management, memory recall, and
answer refusal. With data collected from real-world scenarios, MMRC comprises
5,120 conversations and 28,720 corresponding manually labeled questions, posing
a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC
indicate an accuracy drop during open-ended interactions. We identify four
common failure patterns: long-term memory degradation, inadequacies in updating
factual knowledge, accumulated assumption of error propagation, and reluctance
to say no. To mitigate these issues, we propose a simple yet effective
NOTE-TAKING strategy, which can record key information from the conversation
and remind the model during its responses, enhancing conversational
capabilities. Experiments across six MLLMs demonstrate significant performance
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o
  Under Data Scarsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Tianran Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing LMs struggle with proof-oriented programming due to data scarcity,
which manifest in two key ways: (1) a lack of sufficient corpora for
proof-oriented programming languages such as F*, and (2) the absence of
large-scale, project-level proof-oriented implementations that can teach the
model the intricate reasoning process when performing proof-oriented
programming. We present the first on synthetic data augmentation for project
level proof oriented programming for both generation and repair. Our method
addresses data scarcity by synthesizing basic proof-oriented programming
problems for proficiency in that language; incorporating diverse coding data
for reasoning capability elicitation and creating new proofs and repair data
within existing repositories. This approach enables language models to both
synthesize and repair proofs for function- and repository-level code. We show
that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of
the models that outperforms GPT-4o in project-level proof-oriented programming
by 64% relative margin, and can improve GPT-4o's performance by 54% by
repairing its outputs over GPT-4o's self-repair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Classification Taxonomy for Grammatical Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deqing Zou, Jingheng Ye, Yulu Liu, Yu Wu, Zishan Xu, Yinghui Li, Hai-Tao Zheng, Bingxu An, Zhao Wei, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammatical error classification plays a crucial role in language learning
systems, but existing classification taxonomies often lack rigorous validation,
leading to inconsistencies and unreliable feedback. In this paper, we revisit
previous classification taxonomies for grammatical errors by introducing a
systematic and qualitative evaluation framework. Our approach examines four
aspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability.
Then, we construct a high-quality grammatical error classification dataset
annotated with multiple classification taxonomies and evaluate them grounding
on our proposed evaluation framework. Our experiments reveal the drawbacks of
existing taxonomies. Our contributions aim to improve the precision and
effectiveness of error analysis, providing more understandable and actionable
feedback for language learners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 4 figures and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIMR: Less is More for RL Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefeng Li, Haoyang Zou, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we ask: what truly determines the effectiveness of RL training
data for enhancing language models' reasoning capabilities? While recent
advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack
of transparency about training data requirements has hindered systematic
progress. Starting directly from base models without distillation, we challenge
the assumption that scaling up RL training data inherently improves
performance. we demonstrate that a strategically selected subset of just 1,389
samples can outperform the full 8,523-sample dataset. We introduce Learning
Impact Measurement (LIM), an automated method to evaluate and prioritize
training samples based on their alignment with model learning trajectories,
enabling efficient resource utilization and scalable implementation. Our method
achieves comparable or even superior performance using only 1,389 samples
versus the full 8,523 samples dataset. Notably, while recent data-efficient
approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it
significantly underperforms at 7B-scale through supervised fine-tuning (SFT).
In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and
outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results
fundamentally reshape our understanding of RL scaling in LLMs, demonstrating
that precise sample selection, rather than data scale, may be the key to
unlocking enhanced reasoning capabilities. For reproducible research and future
innovation, we are open-sourcing LIMR, including implementation of LIM,
training and evaluation code, curated datasets, and trained models at
https://github.com/GAIR-NLP/LIMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. To the best of our
knowledge, DPT-Agent is the first language agent framework that achieves
successful real-time simultaneous human-AI collaboration autonomously. Code of
DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing LLM reasoning methods have shown impressive capabilities across
various tasks, such as solving math and coding problems. However, applying
these methods to scenarios without ground-truth answers or rule-based
verification methods - such as tracking the mental states of an agent - remains
challenging. Inspired by the sequential Monte Carlo algorithm, we introduce
thought-tracing, an inference-time reasoning algorithm designed to trace the
mental states of specific agents by generating hypotheses and weighting them
based on observations without relying on ground-truth solutions to questions in
datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,
using LLMs to approximate probabilistic inference over agents' evolving mental
states based on their perceptions and actions. We evaluate thought-tracing on
diverse theory-of-mind benchmarks, demonstrating significant performance
improvements compared to baseline LLMs. Our experiments also reveal interesting
behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,
highlighting the difference of social reasoning compared to other domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bitnet.cpp: Efficient Edge Inference for Ternary LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has
spurred interest in ternary LLMs. Despite this, research and practical
applications focusing on efficient edge inference for ternary LLMs remain
scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system
optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix
multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,
Bitnet.cpp incorporates a novel mpGEMM library to facilitate
sub-2-bits-per-weight, efficient and lossless inference. The library features
two core solutions: Ternary Lookup Table (TL), which addresses spatial
inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),
which ensures lossless edge inference, both enabling high-speed inference. Our
experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over
full-precision baselines and up to 2.32x over low-bit baselines, setting new
benchmarks in the field. Additionally, we expand TL to element-wise lookup
table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and
empirical evidence of its considerable potential. Bitnet.cpp is publicly
available at https://github.com/microsoft/BitNet/tree/paper , offering a
sophisticated solution for the efficient and practical deployment of edge LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAQUUM: Are Vague Quantifiers Grounded in Visual Data? <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugh Mee Wong, Rick Nouwen, Albert Gatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vague quantifiers such as "a few" and "many" are influenced by many
contextual factors, including how many objects are present in a given context.
In this work, we evaluate the extent to which vision-and-language models (VLMs)
are compatible with humans when producing or judging the appropriateness of
vague quantifiers in visual contexts. We release a novel dataset, VAQUUM,
containing 20300 human ratings on quantified statements across a total of 1089
images. Using this dataset, we compare human judgments and VLM predictions
using three different evaluation methods. Our findings show that VLMs, like
humans, are influenced by object counts in vague quantifier use. However, we
find significant inconsistencies across models in different evaluation
settings, suggesting that judging and producing vague quantifiers rely on two
different processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ARR ACL 2025, 12 pages for main paper (5 figures), 15
  pages including appendix (2 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Southern Newswire Corpus: A Large-Scale <span class="highlight-title">Dataset</span> of Mid-Century Wire
  Articles Beyond the Front Page 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael McRae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I introduce a new large-scale dataset of historical wire articles from U.S.
Southern newspapers, spanning 1960-1975 and covering multiple wire services:
The Associated Press, United Press International, Newspaper Enterprise
Association. Unlike prior work focusing on front-page content, this dataset
captures articles across the entire newspaper, offering broader insight into
mid-century Southern coverage. The dataset includes a version that has
undergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its
suitability for quantitative text analysis. Additionally, duplicate versions of
articles are retained to enable analysis of editorial differences in language
and framing across newspapers. Each article is tagged by wire service,
facilitating comparative studies of editorial patterns across agencies. This
resource opens new avenues for research in computational social science,
digital humanities, and historical linguistics, providing a detailed
perspective on how Southern newspapers relayed national and international news
during a transformative period in American history. The dataset will be made
available upon publication or request for research purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding In-Context Machine Translation for Low-Resource Languages:
  A Case Study on Manchu 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhao Pei, Yihong Liu, Peiqin Lin, François Yvon, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context machine translation (MT) with large language models (LLMs) is a
promising approach for low-resource MT, as it can readily take advantage of
linguistic resources such as grammar books and dictionaries. Such resources are
usually selectively integrated into the prompt so that LLMs can directly
perform translation without any specific training, via their in-context
learning capability (ICL). However, the relative importance of each type of
resource e.g., dictionary, grammar book, and retrieved parallel examples, is
not entirely clear. To address this gap, this study systematically investigates
how each resource and its quality affects the translation performance, with the
Manchu language as our case study. To remove any prior knowledge of Manchu
encoded in the LLM parameters and single out the effect of ICL, we also
experiment with an encrypted version of Manchu texts. Our results indicate that
high-quality dictionaries and good parallel examples are very helpful, while
grammars hardly help. In a follow-up study, we showcase a promising application
of in-context MT: parallel data augmentation as a way to bootstrap the
conventional MT model. When monolingual data abound, generating synthetic
parallel data through in-context MT offers a pathway to mitigate data scarcity
and build effective and efficient low-resource neural MT systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Models in Healthcare: Insights into Corpora
  Sources, Customization Strategies, and Evaluation Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqi Yang, Mingrui Jing, Shuai Wang, Jiaxin Kou, Manfei Shi, Weijie Xing, Yan Hu, Zheng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study reviewed the use of Large Language Models (LLMs) in healthcare,
focusing on their training corpora, customization techniques, and evaluation
metrics. A systematic search of studies from 2021 to 2024 identified 61
articles. Four types of corpora were used: clinical resources, literature,
open-source datasets, and web-crawled data. Common construction techniques
included pre-training, prompt engineering, and retrieval-augmented generation,
with 44 studies combining multiple methods. Evaluation metrics were categorized
into process, usability, and outcome metrics, with outcome metrics divided into
model-based and expert-assessed outcomes. The study identified critical gaps in
corpus fairness, which contributed to biases from geographic, cultural, and
socio-economic factors. The reliance on unverified or unstructured data
highlighted the need for better integration of evidence-based clinical
guidelines. Future research should focus on developing a tiered corpus
architecture with vetted sources and dynamic weighting, while ensuring model
transparency. Additionally, the lack of standardized evaluation frameworks for
domain-specific models called for comprehensive validation of LLMs in
real-world healthcare settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining and Evaluating Visual Language Models' Basic Spatial Abilities:
  A Perspective from Psychometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Theory of Multiple Intelligences underscores the hierarchical nature of
cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer
a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual
Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial
Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13
mainstream VLMs through nine validated psychometric experiments reveals
significant gaps versus humans (average score 24.95 vs. 68.38), with three key
findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,
weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller
models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading
(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought
(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from
architectural constraints. Identified barriers include weak geometry encoding
and missing dynamic simulation. By linking psychometric BSAs to VLM
capabilities, we provide a diagnostic toolkit for spatial intelligence
evaluation, methodological foundations for embodied AI development, and a
cognitive science-informed roadmap for achieving human-like spatial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as a synthesis between symbolic and continuous approaches to
  language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gemma Boleda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the middle of the 20th century, a fierce battle is being fought between
symbolic and continuous approaches to language and cognition. The success of
deep learning models, and LLMs in particular, has been alternatively taken as
showing that the continuous camp has won, or dismissed as an irrelevant
engineering development. However, in this position paper I argue that deep
learning models for language actually represent a synthesis between the two
traditions. This is because 1) deep learning architectures allow for both
continuous/distributed and symbolic/discrete-like representations and
computations; 2) models trained on language make use this flexibility. In
particular, I review recent research in mechanistic interpretability that
showcases how a substantial part of morphosyntactic knowledge is encoded in a
near-discrete fashion in LLMs. This line of research suggests that different
behaviors arise in an emergent fashion, and models flexibly alternate between
the two modes (and everything in between) as needed. This is possibly one of
the main reasons for their wild success; and it is also what makes them
particularly interesting for the study of language and cognition. Is it time
for peace?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLM Agents Maintain a Persona in Discourse? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Bhandari, Nicolas Fay, Michael Wise, Amitava Datta, Stephanie Meek, Usman Naseem, Mehwish Nasim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Classification in the LLM Era - Where do we stand? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowmya Vajjala, Shwetali Shimangaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models revolutionized NLP and showed dramatic performance
improvements across several tasks. In this paper, we investigated the role of
such language models in text classification and how they compare with other
approaches relying on smaller pre-trained language models. Considering 32
datasets spanning 8 languages, we compared zero-shot classification, few-shot
fine-tuning and synthetic data based classifiers with classifiers built using
the complete human labeled dataset. Our results show that zero-shot approaches
do well for sentiment classification, but are outperformed by other approaches
for the rest of the tasks, and synthetic data sourced from multiple LLMs can
build better classifiers than zero-shot open LLMs. We also see wide performance
disparities across languages in all the classification scenarios. We expect
that these findings would guide practitioners working on developing text
classification systems across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code-Vision: Evaluating <span class="highlight-title">Multimodal</span> LLMs Logic Understanding and Code
  Generation Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Code-Vision, a benchmark designed to evaluate the
logical understanding and code generation capabilities of Multimodal Large
Language Models (MLLMs). It challenges MLLMs to generate a correct program that
fulfills specific functionality requirements based on a given flowchart, which
visually represents the desired algorithm or process. Code-Vision comprises
three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding
abilities across basic programming, algorithmic, and mathematical
problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.
Experimental results demonstrate that there is a large performance difference
between proprietary and open-source models. On Hard problems, GPT-4o can
achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further
experiments reveal that Code-Vision can pose unique challenges compared to
other multimodal reasoning benchmarks MMCode and MathVista. We also explore the
reason for the poor performance of the open-source models. All data and codes
are available at https://github.com/wanghanbinpanda/CodeVision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M-ABSA: A Multilingual <span class="highlight-title">Dataset</span> for Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Barbara Plank, Yun Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ABSA) is a crucial task in information
extraction and sentiment analysis, aiming to identify aspects with associated
sentiment elements in text. However, existing ABSA datasets are predominantly
English-centric, limiting the scope for multilingual evaluation and research.
To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7
domains and 21 languages, making it the most extensive multilingual parallel
dataset for ABSA to date. Our primary focus is on triplet extraction, which
involves identifying aspect terms, aspect categories, and sentiment polarities.
The dataset is constructed through an automatic translation process with human
review to ensure quality. We perform extensive experiments using various
baselines to assess performance and compatibility on M-ABSA. Our empirical
findings highlight that the dataset enables diverse evaluation tasks, such as
multilingual and multi-domain transfer learning, and large language model
evaluation, underscoring its inclusivity and its potential to drive
advancements in multilingual ABSA research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning significantly improves the performance of Large Language Models
(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims
to provide an in-depth interpretation of the fine-tuning process through
circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike
previous studies
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
that focus on tasks where pre-trained models already perform well, we develop a
set of mathematical tasks where fine-tuning yields substantial performance
gains, which are closer to the practical setting. In our experiments, we
identify circuits at various checkpoints during fine-tuning and examine the
interplay between circuit analysis, fine-tuning methods, and task complexities.
First, we find that while circuits maintain high node similarity before and
after fine-tuning, their edges undergo significant changes, which is in
contrast to the previous work
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
that show circuits only add some additional components after fine-tuning. Based
on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)
method, which assigns ranks to layers based on edge changes in the circuits.
Experimental results demonstrate that our circuit-based LoRA algorithm achieves
an average performance improvement of 2.46\% over standard LoRA with similar
parameter sizes. Furthermore, we explore how combining circuits from subtasks
can enhance fine-tuning in compositional tasks, providing new insights into the
design of such tasks and deepening the understanding of circuit dynamics and
fine-tuning mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineFilter: A Fine-grained Noise Filtering Mechanism for
  Retrieval-Augmented Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieved documents containing noise will hinder Retrieval-Augmented
Generation (RAG) from detecting answer clues, necessitating noise filtering
mechanisms to enhance accuracy.Existing methods use re-ranking or summarization
to identify the most relevant sentences, but directly and accurately locating
answer clues from these large-scale and complex documents remains challenging.
Unlike these document-level operations, we treat noise filtering as a
sentence-level MinMax optimization problem: first identifying the potential
clues from multiple documents using contextual information, then ranking them
by relevance, and finally retaining the least clues through truncation. In this
paper, we propose FineFilter, a novel fine-grained noise filtering mechanism
for RAG consisting of a clue extractor, a re-ranker, and a truncator. We
optimize each module to tackle complex reasoning challenges: (1) Clue extractor
firstly uses sentences containing the answer and similar ones as fine-tuned
targets, aiming at extracting sufficient potential clues; (2) Re-ranker is
trained to prioritize effective clues based on the real feedback from
generation module, with clues capable of generating correct answer as positive
samples and others as negative; (3) Truncator takes the minimum clues needed to
answer the question (truncation point) as fine-tuned targets, and performs
truncation on the re-ranked clues to achieve fine-grained noise filtering.
Experiments on three QA datasets demonstrate that FineFilter significantly
outperforms baselines in terms of performance and inference cost. Further
analysis on each module shows the effectiveness of our optimizations for
complex reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Translation Mechanism of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Zhang, Kehai Chen, Xuefeng Bai, Xiucheng Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have succeeded remarkably in multilingual
translation tasks. However, the inherent translation mechanisms of LLMs remain
poorly understood, largely due to sophisticated architectures and vast
parameter scales. In response to this issue, this study explores the
translation mechanism of LLM from the perspective of computational components
(e.g., attention heads and MLPs). Path patching is utilized to explore causal
relationships between components, detecting those crucial for translation tasks
and subsequently analyzing their behavioral patterns in human-interpretable
terms. Comprehensive analysis reveals that translation is predominantly
facilitated by a sparse subset of specialized attention heads (less than 5\%),
which extract source language, indicator, and positional features. MLPs
subsequently integrate and process these features by transiting towards
English-centric latent representations. Notably, building on the above
findings, targeted fine-tuning of only 64 heads achieves translation
improvement comparable to full-parameter tuning while preserving general
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Table-Critic: A Multi-Agent Framework for Collaborative Criticism and
  Refinement in Table Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiying Yu, Guoxin Chen, Jingjing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable capabilities of large language models (LLMs) in
various reasoning tasks, they still struggle with table reasoning tasks,
particularly in maintaining consistency throughout multi-step reasoning
processes. While existing approaches have explored various decomposition
strategies, they often lack effective mechanisms to identify and correct errors
in intermediate reasoning steps, leading to cascading error propagation. To
address these issues, we propose Table-Critic, a novel multi-agent framework
that facilitates collaborative criticism and iterative refinement of the
reasoning process until convergence to correct solutions. Our framework
consists of four specialized agents: a Judge for error identification, a Critic
for comprehensive critiques, a Refiner for process improvement, and a Curator
for pattern distillation. To effectively deal with diverse and unpredictable
error types, we introduce a self-evolving template tree that systematically
accumulates critique knowledge through experience-driven learning and guides
future reflections. Extensive experiments have demonstrated that Table-Critic
achieves substantial improvements over existing methods, achieving superior
accuracy and error correction rates while maintaining computational efficiency
and lower solution degradation rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personality Editing for Language Models through Relevant Knowledge
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seojin Hwang, Yumin Kim, Byeongjeong Kim, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) play a vital role in applications like
conversational agents and content creation, where controlling a model's
personality is crucial for maintaining tone, consistency, and engagement.
However, traditional prompt-based techniques for controlling personality often
fall short, as they do not effectively mitigate the model's inherent biases. In
this paper, we introduce a novel method PALETTE that enhances personality
control through knowledge editing. By generating adjustment queries inspired by
psychological assessments, our approach systematically adjusts responses to
personality-related queries similar to modifying factual knowledge, thereby
achieving controlled shifts in personality traits. Experimental results from
both automatic and human evaluations demonstrate that our method enables more
stable and well-balanced personality control in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Response Generation Method Selection for Fine-Tuning Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Ren, Qi Chen, Lingqiao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training data for fine-tuning large language models (LLMs) is typically
structured as input-output pairs. However, for many tasks, there can be
multiple equally valid output variations for the same input. Recent studies
have observed that the choice of output variation used in training can affect
the model's performance. This raises an important question: how can we generate
the most effective output from the many possible response generation strategy
options? Rather than relying on the traditional but resource-intensive
train-and-evaluate approach, this paper proposes a scalable, approximate method
for estimating the quality of a small subset of generated training data derived
from the same input. We then evaluate how well this small subset of generated
output fits the target model we are trying to train. We present a large-scale
benchmark covering diverse reasoning-based datasets to support our study.
  The central idea is that a good output should closely resemble the output
generated by the target LLM. We formalize this 'closeness' as the expected
alignment score between a candidate output and the output sampled from the
target LLM. We connect this measurement to the perplexity metric used in
previous literature and demonstrate that leveraging an alignment-based metric
can provide better predictions of model performance. Using this strategy, we
can evaluate a small subset of the generated output from each response
generation strategy option, then select the most effective strategy. We show
that an LLM trained on data generated by the selected strategy could lead to a
significant performance gain in many cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Validation Gap: A Mechanistic Analysis of How Language Models
  Compute Arithmetic but Fail to Validate It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Bertolazzi, Philipp Mondorf, Barbara Plank, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of large language models (LLMs) to validate their output and
identify potential errors is crucial for ensuring robustness and reliability.
However, current research indicates that LLMs struggle with self-correction,
encountering significant challenges in detecting errors. While studies have
explored methods to enhance self-correction in LLMs, relatively little
attention has been given to understanding the models' internal mechanisms
underlying error detection. In this paper, we present a mechanistic analysis of
error detection in LLMs, focusing on simple arithmetic problems. Through
circuit analysis, we identify the computational subgraphs responsible for
detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal
that all models heavily rely on $\textit{consistency heads}$--attention heads
that assess surface-level alignment of numerical values in arithmetic
solutions. Moreover, we observe that the models' internal arithmetic
computation primarily occurs in higher layers, whereas validation takes place
in middle layers, before the final arithmetic results are fully encoded. This
structural dissociation between arithmetic computation and validation seems to
explain why current LLMs struggle to detect even simple arithmetic errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 31 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Selection to Generation: A <span class="highlight-title">Survey</span> of LLM-based Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning (AL) has been a powerful paradigm for improving model
efficiency and performance by selecting the most informative data points for
labeling and training. In recent active learning frameworks, Large Language
Models (LLMs) have been employed not only for selection but also for generating
entirely new data instances and providing more cost-effective annotations.
Motivated by the increasing importance of high-quality data and efficient model
training in the era of LLMs, we present a comprehensive survey on LLM-based
Active Learning. We introduce an intuitive taxonomy that categorizes these
techniques and discuss the transformative roles LLMs can play in the active
learning loop. We further examine the impact of AL on LLM learning paradigms
and its applications across various domains. Finally, we identify open
challenges and propose future research directions. This survey aims to serve as
an up-to-date resource for researchers and practitioners seeking to gain an
intuitive understanding of LLM-based AL techniques and deploy them to new
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Warmup-Distill: Bridge the Distribution Mismatch between Teacher and
  Student before Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengkui Sun, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread deployment of Large Language Models (LLMs) is hindered by the
high computational demands, making knowledge distillation (KD) crucial for
developing compact smaller ones. However, the conventional KD methods endure
the distribution mismatch issue between the teacher and student models, leading
to the poor performance of distillation. For instance, the widely-used KL-based
methods suffer the mode-averaging and mode-collapsing problems, since the
mismatched probabitliy distribution between both models. Previous studies
mainly optimize this issue via different distance calculations towards the
distribution of both models. Unfortunately, the distribution mismatch issue
still exists in the early stage of the distillation. Hence, to reduce the
impact of distribution mismatch, we propose a simple yet efficient method,
named Warmup-Distill, which aligns the distillation of the student to that of
the teacher in advance of distillation. Specifically, we first detect the
distribution of the student model in practical scenarios with its internal
knowledge, and then modify the knowledge with low probability via the teacher
as the checker. Consequently, Warmup-Distill aligns the internal student's
knowledge to that of the teacher, which expands the distribution of the student
with the teacher's, and assists the student model to learn better in the
subsequent distillation. Experiments on the seven benchmarks demonstrate that
Warmup-Distill could provide a warmup student more suitable for distillation,
which outperforms the vanilla student by as least +0.4 averaged score among all
benchmarks. Noteably, with the assistance of Warmup-Distill, the distillation
on the math task could yield a further improvement, at most +1.9% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, 4 figures, Code at https://github.com/Acerkoo/WarmupDistill</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span>Eval: An Evaluation Framework for AI-Generated <span class="highlight-title">Review</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chavvi Kirtani, Madhav Krishan Garg, Tejash Prasad, Tanmay Singhal, Murari Mandal, Dhruv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating volume of academic research, coupled with a shortage of
qualified reviewers, necessitates innovative approaches to peer review. While
large language model (LLMs) offer potential for automating this process, their
current limitations include superficial critiques, hallucinations, and a lack
of actionable insights. This research addresses these challenges by introducing
a comprehensive evaluation framework for AI-generated reviews, that measures
alignment with human evaluations, verifies factual accuracy, assesses
analytical depth, and identifies actionable insights. We also propose a novel
alignment mechanism that tailors LLM-generated reviews to the unique evaluation
priorities of individual conferences and journals. To enhance the quality of
these reviews, we introduce a self-refinement loop that iteratively optimizes
the LLM's review prompts. Our framework establishes standardized metrics for
evaluating AI-based review systems, thereby bolstering the reliability of
AI-generated reviews in academic research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review: 8 pages, 2 figures, 2 tables, 3 pages for appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MT-RAIG: Novel Benchmark and Evaluation Framework for
  Retrieval-Augmented Insight Generation over Multiple Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwangwook Seo, Donguk Kwon, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in table-based reasoning have expanded beyond
factoid-level QA to address insight-level tasks, where systems should
synthesize implicit knowledge in the table to provide explainable analyses.
Although effective, existing studies remain confined to scenarios where a
single gold table is given alongside the user query, failing to address cases
where users seek comprehensive insights from multiple unknown tables. To bridge
these gaps, we propose MT-RAIG Bench, design to evaluate systems on
Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to
tackle the suboptimality of existing automatic evaluation methods in the table
domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval,
which achieves better alignment with human quality judgments on the generated
insights. We conduct extensive experiments and reveal that even frontier LLMs
still struggle with complex multi-table reasoning, establishing our MT-RAIG
Bench as a challenging testbed for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking
  Practical Reasoning and Situation Modelling in a Text-Simulated Situated
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Jordan, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have risen to prominence as 'chatbots' for users
to interact via natural language. However, their abilities to capture
common-sense knowledge make them seem promising as language-based planners of
situated or embodied action as well. We have implemented a simple text-based
environment -- similar to others that have before been used for
reinforcement-learning of agents -- that simulates, very abstractly, a
household setting. We use this environment and the detailed error-tracking
capabilities we implemented for targeted benchmarking of LLMs on the problem of
practical reasoning: Going from goals and observations to actions. Our findings
show that environmental complexity and game restrictions hamper performance,
and concise action planning is demanding for current LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "See the World, Discover Knowledge": A Chinese Factuality Evaluation for
  Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of factual accuracy in large vision language models (LVLMs)
has lagged behind their rapid development, making it challenging to fully
reflect these models' knowledge capacity and reliability. In this paper, we
introduce the first factuality-based visual question-answering benchmark in
Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of
LVLMs across 8 major topics and 56 subtopics. The key features of this
benchmark include a focus on the Chinese language, diverse knowledge types, a
multi-hop question construction, high-quality data, static consistency, and
easy-to-evaluate through short answers. Moreover, we contribute a rigorous data
construction pipeline and decouple the visual factuality into two parts: seeing
the world (i.e., object recognition) and discovering knowledge. This decoupling
allows us to analyze the capability boundaries and execution mechanisms of
LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source
models, revealing critical performance gaps within this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherzod Hakimov, Lara Pfennigschmidt, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study utilizes the game Codenames as a benchmarking tool to evaluate
large language models (LLMs) with respect to specific linguistic and cognitive
skills. LLMs play each side of the game, where one side generates a clue word
covering several target words and the other guesses those target words. We
designed various experiments by controlling the choice of words (abstract vs.
concrete words, ambiguous vs. monosemic) or the opponent (programmed to be
faster or slower in revealing words). Recent commercial and open-weight models
were compared side-by-side to find out factors affecting their performance. The
evaluation reveals details about their strategies, challenging cases, and
limitations of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Agents Making Agent Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool use has turned large language models (LLMs) into powerful agents that
can perform complex multi-step tasks by dynamically utilising external software
components. However, these tools must be implemented in advance by human
developers, hindering the applicability of LLM agents in domains which demand
large numbers of highly specialised tools, like in life sciences and medicine.
Motivated by the growing trend of scientific studies accompanied by public code
repositories, we propose ToolMaker, a novel agentic framework that autonomously
transforms papers with code into LLM-compatible tools. Given a short task
description and a repository URL, ToolMaker autonomously installs required
dependencies and generates code to perform the task, using a closed-loop
self-correction mechanism to iteratively diagnose and rectify errors. To
evaluate our approach, we introduce a benchmark comprising 15 diverse and
complex computational tasks spanning both medical and non-medical domains with
over 100 unit tests to objectively assess tool correctness and robustness.
ToolMaker correctly implements 80% of the tasks, substantially outperforming
current state-of-the-art software engineering agents. ToolMaker therefore is a
step towards fully autonomous agent-based scientific workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models
  in Medical Quality Control Indicator Calculation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical quality control indicators are essential to assess the qualifications
of healthcare institutions for medical services. With the impressive
performance of large language models (LLMs) like GPT-4 in the medical field,
leveraging these technologies for the Medical Quality Control Indicator
Calculation (MQCIC) presents a promising approach. In this work, (1) we
introduce a real-world task MQCIC and propose an open-source Chinese electronic
medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances
and 76 indicators. (2) We propose a semi-automatic method to enhance the rule
representation. Then we propose the Clinical Facts-based Inferential Rule
(CF-IR) method that disentangles the clinical fact verification and inferential
rule reasoning actions. (3) We conduct comprehensive experiments on 20
representative LLMs, covering general and medical models. Our findings reveal
that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct
an error analysis and investigate the capabilities of clinical fact
verification and inferential rule reasoning, providing insights to improve
performance in the MQCIC further. The dataset and code is available in this
repo https://anonymous.4open.science/r/C-MQCIC-1151.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve LLM-as-a-Judge Ability as a General Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge leverages the generative and reasoning capabilities of large
language models (LLMs) to evaluate LLM responses across diverse scenarios,
providing accurate preference signals. This approach plays a vital role in
aligning LLMs with human values, ensuring ethical and reliable AI outputs that
align with societal norms. Recent studies have raised many methods to train LLM
as generative judges, but most of them are data consuming or lack accuracy, and
only focus on LLM's judge ability. In this work, we regard judge ability as a
general ability of LLM and implement a two-stage training approach, comprising
supervised fine-tuning (SFT) warm-up and direct preference optimization (DPO)
enhancement, to achieve judge style adaptation and improve judgment accuracy.
Additionally, we introduce an efficient data synthesis method to generate
judgmental content. Experimental results demonstrate that our approach,
utilizing only about 2% to 40% of the data required by other methods, achieves
SOTA performance on RewardBench. Furthermore, our training method enhances the
general capabilities of the model by constructing complicated judge task, and
the judge signals provided by our model have significantly enhanced the
downstream DPO training performance of our internal models in our test to
optimize policy model with Judge Model. We also open-source our model weights
and training data to facilitate further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Isolates to Families: Using Neural Networks for Automated Language
  Affiliation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Blum, Steffen Herbold, Johann-Mattis List
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In historical linguistics, the affiliation of languages to a common language
family is traditionally carried out using a complex workflow that relies on
manually comparing individual languages. Large-scale standardized collections
of multilingual wordlists and grammatical language structures might help to
improve this and open new avenues for developing automated language affiliation
workflows. Here, we present neural network models that use lexical and
grammatical data from a worldwide sample of more than 1,000 languages with
known affiliations to classify individual languages into families. In line with
the traditional assumption of most linguists, our results show that models
trained on lexical data alone outperform models solely based on grammatical
data, whereas combining both types of data yields even better performance. In
additional experiments, we show how our models can identify long-ranging
relations between entire subgroups, how they can be employed to investigate
potential relatives of linguistic isolates, and how they can help us to obtain
first hints on the affiliation of so far unaffiliated languages. We conclude
that models for automated language affiliation trained on lexical and
grammatical data provide comparative linguists with a valuable tool for
evaluating hypotheses about deep and unknown language relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 63rd Annual Meeting of the Association for
  Computational Linguistics, Vienna, Austria</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps
  through Fill-in-the-Middle Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning represents a critical frontier in advancing large
language models (LLMs). While step-by-step approaches have emerged as the
dominant paradigm for mathematical problem-solving in LLMs, the quality of
reasoning steps in training data fundamentally constrains the performance of
the models. Recent studies has demonstrated that more detailed intermediate
steps can enhance model performance, yet existing methods for step expansion
either require more powerful external models or incur substantial computational
costs. In this paper, we introduce MathFimer, a novel framework for
mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task
from code completion. By decomposing solution chains into prefix-suffix pairs
and training models to reconstruct missing intermediate steps, we develop a
specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM
dataset. We then apply these models to enhance existing mathematical reasoning
datasets by inserting detailed intermediate steps into their solution chains,
creating MathFimer-expanded versions. Through comprehensive experiments on
multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA
and etc., we demonstrate that models trained on MathFimer-expanded data
consistently outperform their counterparts trained on original data across
various benchmarks such as GSM8K and MATH. Our approach offers a practical,
scalable solution for enhancing mathematical reasoning capabilities in LLMs
without relying on powerful external models or expensive inference procedures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RIDE: Enhancing Large Language Model Alignment through Restyled
  In-Context Learning Demonstration Exemplars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment tuning is crucial for ensuring large language models (LLMs) behave
ethically and helpfully. Current alignment approaches require high-quality
annotations and significant training resources. This paper proposes a low-cost,
tuning-free method using in-context learning (ICL) to enhance LLM alignment.
Through an analysis of high-quality ICL demos, we identified style as a key
factor influencing LLM alignment capabilities and explicitly restyled ICL
exemplars based on this stylistic framework. Additionally, we combined the
restyled demos to achieve a balance between the two conflicting aspects of LLM
alignment--factuality and safety. We packaged the restyled examples as prompts
to trigger few-shot learning, improving LLM alignment. Compared to the best
baseline approach, with an average score of 5.00 as the maximum, our method
achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22
enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum
improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the
code and data at https://github.com/AnonymousCode-ComputerScience/RIDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 1 figure, 20 tables; The paper is under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLM-based Student Simulation for Metacognitive Cultivation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Li, Jifan Yu, Xin Cong, Yang Dang, Yisi Zhan, Huiqin Liu, Zhiyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metacognitive education plays a crucial role in cultivating students'
self-regulation and reflective thinking, providing essential support for those
with learning difficulties through academic advising. Simulating students with
insufficient learning capabilities using large language models offers a
promising approach to refining pedagogical methods without ethical concerns.
However, existing simulations often fail to authentically represent students'
learning struggles and face challenges in evaluation due to the lack of
reliable metrics and ethical constraints in data collection. To address these
issues, we propose a pipeline for automatically generating and filtering
high-quality simulated student agents. Our approach leverages a two-round
automated scoring system validated by human experts and employs a score
propagation module to obtain more consistent scores across the student graph.
Experimental results demonstrate that our pipeline efficiently identifies
high-quality student agents, and we discuss the traits that influence the
simulation's effectiveness. By simulating students with varying degrees of
learning difficulties, our work paves the way for broader applications in
personalized learning and educational assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fully Exploiting LLM Internal States to Enhance Knowledge
  Boundary Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit impressive performance across diverse
tasks but often struggle to accurately gauge their knowledge boundaries,
leading to confident yet incorrect responses. This paper explores leveraging
LLMs' internal states to enhance their perception of knowledge boundaries from
efficiency and risk perspectives. We investigate whether LLMs can estimate
their confidence using internal states before response generation, potentially
saving computational resources. Our experiments on datasets like Natural
Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant
pre-generation perception, which is further refined post-generation, with
perception gaps remaining stable across varying conditions. To mitigate risks
in critical domains, we introduce Consistency-based Confidence Calibration
($C^3$), which assesses confidence consistency through question reformulation.
$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,
enhancing the unknown perception rate by 5.6\% on NQ and 4.9\% on HotpotQA. Our
findings suggest that pre-generation confidence estimation can optimize
efficiency, while $C^3$ effectively controls output risks, advancing the
reliability of LLMs in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-Oriented Data Augmentation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is an essential technique in natural language processing
(NLP) for enriching training datasets by generating diverse samples. This
process is crucial for improving the robustness and generalization capabilities
of NLP models. However, a significant challenge remains: \textit{Insufficient
Attention to Sample Distribution Diversity}. Most existing methods focus on
increasing the sample numbers while neglecting the sample distribution
diversity, which can lead to model overfitting. In response, we explore data
augmentation's impact on dataset diversity and propose a
\textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data
\textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). %
\(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning
approach to train an LLM as a diverse paraphraser, which is capable of
augmenting textual datasets by generating diversified paraphrases. Then, we
apply the LLM paraphraser to a selected coreset of highly informative samples
and integrate the paraphrases with the original data to create a more diverse
augmented dataset. Finally, we conduct extensive experiments on 12 real-world
textual datasets. The results show that our fine-tuned LLM augmenter improves
diversity while preserving label consistency, thereby enhancing the robustness
and performance of downstream tasks. Specifically, it achieves an average
performance gain of \(10.52\%\), surpassing the runner-up baseline with more
than three percentage points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the Impact of Chain-of-Thought Reasoning for Direct
  Preference Optimization: Lessons from Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbing Liu, Haoyang Li, Xiaokang Zhang, Ruotong Chen, Haiyong Xu, Tian Tian, Qi Qi, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has proven effective in complex
reasoning tasks like math word problems and code generation. However, when
applied to Text-to-SQL datasets, it often fails to improve performance and can
even degrade it. Our investigation reveals the root cause: unlike math and code
tasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO,
Text-to-SQL datasets typically include only final answers (gold SQL queries)
without detailed CoT solutions. By augmenting Text-to-SQL datasets with
synthetic CoT solutions, we achieve, for the first time, consistent and
significant performance improvements using DPO. Our analysis shows that CoT
reasoning is crucial for unlocking DPO's potential, as it mitigates reward
hacking, strengthens discriminative capabilities, and improves scalability.
These findings offer valuable insights for building more robust Text-to-SQL
models. To support further research, we publicly release the code and
CoT-enhanced datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deviation Ratings: A General, Clone-Invariant Rating Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Marris, Siqi Liu, Ian Gemp, Georgios Piliouras, Marc Lanctot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world multi-agent or multi-task evaluation scenarios can be
naturally modelled as normal-form games due to inherent strategic (adversarial,
cooperative, and mixed motive) interactions. These strategic interactions may
be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or
complementary (e.g. niche finding and specialization). In such a formulation,
it is the strategies (actions, policies, agents, models, tasks, prompts, etc.)
that are rated. However, the rating problem is complicated by redundancy and
complexity of N-player strategic interactions. Repeated or similar strategies
can distort ratings for those that counter or complement them. Previous work
proposed ``clone invariant'' ratings to handle such redundancies, but this was
limited to two-player zero-sum (i.e. strictly competitive) interactions. This
work introduces the first N-player general-sum clone invariant rating, called
deviation ratings, based on coarse correlated equilibria. The rating is
explored on several domains including LLMs evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and
  Training Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyan Wu, Peijian Zeng, Weixiong Zheng, Lianxi Wang, Nankai Lin, Shengyi Jiang, Aimin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal text-molecule retrieval task bridges molecule structures and
natural language descriptions. Existing methods predominantly focus on aligning
text modality and molecule modality, yet they overlook adaptively adjusting the
learning states at different training stages and enhancing training efficiency.
To tackle these challenges, this paper proposes a Curriculum Learning-bAsed
croSS-modal text-molecule training framework (CLASS), which can be integrated
with any backbone to yield promising performance improvement. Specifically, we
quantify the sample difficulty considering both text modality and molecule
modality, and design a sample scheduler to introduce training samples via an
easy-to-difficult paradigm as the training advances, remarkably reducing the
scale of training samples at the early stage of training and improving training
efficiency. Moreover, we introduce adaptive intensity learning to increase the
training intensity as the training progresses, which adaptively controls the
learning intensity across all curriculum stages. Experimental results on the
ChEBI-20 dataset demonstrate that our proposed method gains superior
performance, simultaneously achieving prominent time savings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Human-Like Text Liked by Humans? Multilingual Human Detection and
  Preference Against AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, Ekaterina Artemova, Vladislav Mikhailov, Akim Tsvigun, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior studies have shown that distinguishing text generated by large language
models (LLMs) from human-written one is highly challenging, and often no better
than random guessing. To verify the generalizability of this finding across
languages and domains, we perform an extensive case study to identify the upper
bound of human detection accuracy. Across 16 datasets covering 9 languages and
9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus
challenging previous conclusions. We find that major gaps between human and
machine text lie in concreteness, cultural nuances, and diversity. Prompting by
explicitly explaining the distinctions in the prompts can partially bridge the
gaps in over 50% of the cases. However, we also find that humans do not always
prefer human-written text, particularly when they cannot clearly identify its
source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Gender Stereotypes and Biases in Automated Translation from
  English to Italian using Similarity Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Mohammadi, Marta Annamaria Tamborini, Paolo Ceravolo, Costanza Nardocci, Samira Maghool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is a collaborative effort between Linguistics, Law, and Computer
Science to evaluate stereotypes and biases in automated translation systems. We
advocate gender-neutral translation as a means to promote gender inclusion and
improve the objectivity of machine translation. Our approach focuses on
identifying gender bias in English-to-Italian translations. First, we define
gender bias following human rights law and linguistics literature. Then we
proceed by identifying gender-specific terms such as she/lei and he/lui as key
elements. We then evaluate the cosine similarity between these target terms and
others in the dataset to reveal the model's perception of semantic relations.
Using numerical features, we effectively evaluate the intensity and direction
of the bias. Our findings provide tangible insights for developing and training
gender-neutral translation algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware
  Prompting with Demonstration and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Qiu, Yue Xu, Meikang Qiu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit strong natural language processing
capabilities but also inherit and amplify societal biases, including gender
bias, raising fairness concerns. Existing debiasing methods face significant
limitations: parameter tuning requires access to model weights, prompt-based
approaches often degrade model utility, and optimization-based techniques lack
generalizability. To address these challenges, we propose DR.GAP (Demonstration
and Reasoning for Gender-Aware Prompting), an automated and model-agnostic
approach that mitigates gender bias while preserving model performance. DR.GAP
selects bias-revealing examples and generates structured reasoning to guide
models toward more impartial responses. Extensive experiments on coreference
resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and
Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and
robustness. DR.GAP can generalize to vision-language models (VLMs), achieving
significant bias reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The radioactive nature of Large Language Model (LLM) watermarking enables the
detection of watermarks inherited by student models when trained on the outputs
of watermarked teacher models, making it a promising tool for preventing
unauthorized knowledge distillation. However, the robustness of watermark
radioactivity against adversarial actors remains largely unexplored. In this
paper, we investigate whether student models can acquire the capabilities of
teacher models through knowledge distillation while avoiding watermark
inheritance. We propose two categories of watermark removal approaches:
pre-distillation removal through untargeted and targeted training data
paraphrasing (UP and TP), and post-distillation removal through inference-time
watermark neutralization (WN). Extensive experiments across multiple model
pairs, watermarking schemes and hyper-parameter settings demonstrate that both
TP and WN thoroughly eliminate inherited watermarks, with WN achieving this
while maintaining knowledge transfer efficiency and low computational overhead.
Given the ongoing deployment of watermarking techniques in production LLMs,
these findings emphasize the urgent need for more robust defense strategies.
Our code is available at
https://github.com/THU-BPM/Watermark-Radioactivity-Attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 12 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Complexity Measurement as a Noisy Zero-Shot Proxy for
  Evaluating LLM Performance <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Birger Moell, Johan Boye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant strides in natural
language generation but often face challenges in tasks requiring precise
calculations and structural analysis. This paper investigates the performance
of state-of-the-art LLMs on language complexity measurement tasks, through the
computation of the LIX readability metric and Average Dependency Distance
(ADD). Using Swedish high school and university-level essays, we evaluate the
models' abilities to compute LIX scores and perform dependency parsing,
comparing their results to established ground truths. Our findings reveal that
while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini
performs most consistently, achieving the highest accuracy in both LIX
computation and dependency parsing. Additionally, we observe a strong
significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in
computing LIX and their overall performance on the Massive Multitask Language
Understanding (MMLU) benchmark. These results suggest that language complexity
measurement abilities can serve as a noisy zero-shot proxies for assessing the
general capabilities of LLMs, providing a practical method for model evaluation
without the need for extensive benchmarking datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiR : Crafting Effective Small Language Models and <span class="highlight-title">Multimodal</span> Small
  Language Models in Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have made significant advancements in reasoning capabilities. However, they
still face challenges such as high computational demands and privacy concerns.
This paper focuses on developing efficient Small Language Models (SLMs) and
Multimodal Small Language Models (MSLMs) that retain competitive reasoning
abilities. We introduce a novel training pipeline that enhances reasoning
capabilities and facilitates deployment on edge devices, achieving
state-of-the-art performance while minimizing development costs. \InfR~ aims to
advance AI systems by improving reasoning, reducing adoption barriers, and
addressing privacy concerns through smaller model sizes. Resources are
available at https://github. com/Reallm-Labs/InfiR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaMTEB: Massive Text Embedding Benchmark in Persian Language <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)
text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our
benchmark includes 63 datasets spanning seven different tasks: classification,
clustering, pair classification, reranking, retrieval, summary retrieval, and
semantic textual similarity. The datasets are formed as a combination of
existing, translated, and newly generated data, offering a diverse evaluation
framework for Persian language models. Given the increasing use of text
embedding models in chatbots, evaluation datasets are becoming inseparable
ingredients in chatbot challenges and Retrieval-Augmented Generation systems.
As a contribution, we include chatbot evaluation datasets in the MTEB benchmark
for the first time. In addition, in this paper, we introduce the new task of
summary retrieval which is not part of the tasks included in standard MTEB.
Another contribution of this paper is the introduction of a substantial number
of new Persian language NLP datasets suitable for training and evaluation, some
of which have no previous counterparts in Persian. We evaluate the performance
of several Persian and multilingual embedding models in a range of tasks. This
work introduces an open-source benchmark with datasets, code and a public
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reasoning Ability of Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Srivastava, Shuxiang Cao, Xuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning has long been viewed as an emergent property of large language
models (LLMs), appearing at or above a certain scale ($\sim$100B parameters).
However, recent studies challenge this assumption, showing that small language
models (SLMs) can also achieve competitive reasoning performance. SLMs are
increasingly favored for their efficiency and deployability. However, there is
a lack of systematic study on the reasoning abilities of diverse SLMs,
including those trained from scratch or derived from LLMs through quantization,
pruning, and distillation. This raises a critical question: Can SLMs achieve
reasoning abilities comparable to LLMs? In this work, we systematically survey,
benchmark, and analyze 72 SLMs from six model families across 14 reasoning
benchmarks. For reliable evaluation, we examine four evaluation methods and
compare four LLM judges against human evaluations on 800 data points. We repeat
all experiments three times to ensure a robust performance assessment.
Additionally, we analyze the impact of different prompting strategies in small
models. Beyond accuracy, we also evaluate model robustness under adversarial
conditions and intermediate reasoning steps. Our findings challenge the
assumption that scaling is the only way to achieve strong reasoning. Instead,
we foresee a future where SLMs with strong reasoning capabilities can be
developed through structured training or post-training compression. They can
serve as efficient alternatives to LLMs for reasoning-intensive tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Li, Zheng Liu, Jianlyv Chen, Defu Lian, Yingxia Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While retrieval techniques are widely used in practice, they still face
significant challenges in cross-domain scenarios. Recently,
generation-augmented methods have emerged as a promising solution to this
problem. These methods enhance raw queries by incorporating additional
information from an LLM-based generator, facilitating more direct retrieval of
relevant documents. However, existing methods struggle with highly specialized
situations that require extensive domain expertise. To address this problem, we
present \textbf{Reinforced-IR}, a novel approach that jointly adapts a
pre-trained retriever and generator for precise cross-domain retrieval. A key
innovation of Reinforced-IR is its \textbf{Self-Boosting} framework, which
enables retriever and generator to learn from each other's feedback.
Specifically, the generator is reinforced to generate query augmentations that
enhance the retriever's performance, while the retriever is trained to better
discriminate the relevant documents identified by the generator. This iterative
process allows the end-to-end retrieval performance to be progressively
optimized using an unlabeled corpus from the target domain. In our experiment,
Reinforced-IR outperforms existing domain adaptation methods by a large margin,
leading to substantial improvements in retrieval quality across a wide range of
application scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto-Search and Refinement: An Automated Framework for Gender Bias
  Mitigation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large language models (LLMs) on vast text corpora enhances
natural language processing capabilities but risks encoding social biases,
particularly gender bias. While parameter-modification methods like fine-tuning
mitigate bias, they are resource-intensive, unsuitable for closed-source
models, and lack adaptability to evolving societal norms. Instruction-based
approaches offer flexibility but often compromise task performance. To address
these limitations, we propose $\textit{FaIRMaker}$, an automated and
model-independent framework that employs an $\textbf{auto-search and
refinement}$ paradigm to adaptively generate Fairwords, which act as
instructions integrated into input queries to reduce gender bias and enhance
response quality. Extensive experiments demonstrate that $\textit{FaIRMaker}$
automatically searches for and dynamically refines Fairwords, effectively
mitigating gender bias while preserving task integrity and ensuring
compatibility with both API-based and open-source LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Metaphor-Fluid Conversation Design for Voice User Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Smit Desai, Jessie Chin, Dakuo Wang, Benjamin Cowan, Michael Twidale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors play a critical role in shaping user experiences with Voice User
Interfaces (VUIs), yet existing designs often rely on static, human-centric
metaphors that fail to adapt to diverse contexts and user needs. This paper
introduces Metaphor-Fluid Design, a novel approach that dynamically adjusts
metaphorical representations based on conversational use-contexts. We compare
this approach to a Default VUI, which characterizes the present implementation
of commercial VUIs commonly designed around the persona of an assistant,
offering a uniform interaction style across contexts. In Study 1 (N=130),
metaphors were mapped to four key use-contexts-commands, information seeking,
sociality, and error recovery-along the dimensions of formality and hierarchy,
revealing distinct preferences for task-specific metaphorical designs. Study 2
(N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the
Metaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and
likability by aligning better with user expectations for different contexts.
However, individual differences in metaphor preferences highlight the need for
personalization. These findings challenge the one-size-fits-all paradigm of VUI
design and demonstrate the potential of Metaphor-Fluid Design to create more
adaptive and engaging human-AI interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCAD-2000: A Multilingual <span class="highlight-title">Dataset</span> across 2000+ Languages with Data
  Cleaning as Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingli Shen, Wen Lai, Shuo Wang, Xueren Zhang, Kangyang Luo, Alexander Fraser, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multilingual large language models (LLMs) highlights
the need for high-quality, diverse, and clean multilingual datasets. In this
paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a
large-scale multilingual corpus built using newly extracted Common Crawl data
and existing multilingual datasets. DCAD-2000 includes over 2,282 languages,
46.72TB of data, and 8.63 billion documents, spanning 155 high- and
medium-resource languages and 159 writing scripts. To overcome the limitations
of current data cleaning methods, which rely on manual heuristic thresholds, we
propose reframing data cleaning as an anomaly detection task. This dynamic
filtering approach significantly enhances data quality by identifying and
removing noisy or anomalous content. We evaluate the quality of DCAD-2000 on
the FineTask benchmark, demonstrating substantial improvements in multilingual
dataset quality and task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through
  Comprehensive Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, Min zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The o1-Like LLMs are transforming AI by simulating human cognitive processes,
but their performance in multilingual machine translation (MMT) remains
underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks
and (2) what factors influence their translation quality. We evaluate multiple
o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o.
Results show that o1-Like LLMs establish new multilingual translation
benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They
demonstrate strengths in historical and cultural translation but exhibit a
tendency for rambling issues in Chinese-centric outputs. Further analysis
reveals three key insights: (1) High inference costs and slower processing
speeds make complex translation tasks more resource-intensive. (2) Translation
quality improves with model size, enhancing commonsense reasoning and cultural
translation. (3) The temperature parameter significantly impacts output
quality-lower temperatures yield more stable and accurate translations, while
higher temperatures reduce coherence and precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuSC: Improving Complex Instruction Following with Multi-granularity
  Self-Contrastive Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex instruction-following with elaborate constraints is imperative for
Large Language Models (LLMs). While existing methods have constructed data for
complex instruction alignment, they all rely on a more advanced model,
especially GPT-4, limiting their application. In this paper, we propose a
Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the
complex instruction alignment without relying on a stronger model. Our method
is conducted on both coarse and fine granularity. On coarse-granularity, we
construct constraint-aware preference data based on instruction decomposition
and recombination. On fine-granularity, we perform token-aware preference
optimization with dynamic token-level supervision. Our method is evaluated on
open-sourced models, and experiment results show our method achieves
significant improvement on both complex and general instruction-following
benchmarks, surpassing previous self-alignment methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of
  Stealing Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Guo, Yi Shi, Wenlong Meng, Chen Gong, Chengkun Wei, Wenzhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is a widespread technology in large language models (LLMs) that
integrates multiple task-specific LLMs into a unified one, enabling the merged
model to inherit the specialized capabilities of these LLMs. Most task-specific
LLMs are sourced from open-source communities and have not undergone rigorous
auditing, potentially imposing risks in model merging. This paper highlights an
overlooked privacy risk: \textit{an unsafe model could compromise the privacy
of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a
privacy attack approach that trains a phishing model capable of stealing
privacy using a crafted privacy phishing instruction dataset. Furthermore, we
introduce a novel model cloaking method that mimics a specialized capability to
conceal attack intent, luring users into merging the phishing model. Once
victims merge the phishing model, the attacker can extract personally
identifiable information (PII) or infer membership information (MI) by querying
the merged model with the phishing instruction. Experimental results show that
merging a phishing model increases the risk of privacy breaches. Compared to
the results before merging, PII leakage increased by 3.9\% and MI leakage
increased by 17.4\% on average. We release the code of PhiMM through a link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Large Language Models to be Better Rule Followers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Shijia Kang, Haotong Yang, Haotian Xu, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive performance across a wide
range of tasks. However, they often exhibit unexpected failures in seemingly
straightforward tasks, suggesting a reliance on case-based reasoning rather
than rule-based reasoning. While the vast training corpus of LLMs contains
numerous textual "rules", current training methods fail to leverage these rules
effectively. Crucially, the relationships between these "rules" and their
corresponding "instances" are not explicitly modeled. As a result, while LLMs
can often recall rules with ease, they fail to apply these rules strictly and
consistently in relevant reasoning scenarios. In this paper, we investigate the
rule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning
(Meta-RFFT) to enhance the cross-task transferability of rule-following
abilities. We first construct a dataset of 88 tasks requiring following rules,
encompassing diverse reasoning domains. We demonstrate through extensive
experiments that models trained on large-scale rule-following tasks are better
rule followers, outperforming the baselines in both downstream fine-tuning and
few-shot prompting scenarios. This highlights the cross-task transferability of
models with the aid of Meta-RFFT. Furthermore, we examine the influence of
factors such as dataset size, rule formulation, and in-context learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AURORA:Automated Training Framework of Universal Process Reward Models
  via Ensemble Prompting and Reverse Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning capabilities of advanced large language models (LLMs) like o1
have revolutionized artificial intelligence applications. Nevertheless,
evaluating and optimizing complex reasoning processes remain significant
challenges due to diverse policy distributions and the inherent limitations of
human effort and accuracy. In this paper, we present AURORA, a novel automated
framework for training universal process reward models (PRMs) using ensemble
prompting and reverse verification. The framework employs a two-phase approach:
First, it uses diverse prompting strategies and ensemble methods to perform
automated annotation and evaluation of processes, ensuring robust assessments
for reward learning. Second, it leverages practical reference answers for
reverse verification, enhancing the model's ability to validate outputs and
improving training accuracy. To assess the framework's performance, we extend
beyond the existing ProcessBench benchmark by introducing UniversalBench, which
evaluates reward predictions across full trajectories under diverse policy
distribtion with long Chain-of-Thought (CoT) outputs. Experimental results
demonstrate that AURORA enhances process evaluation accuracy, improves PRMs'
accuracy for diverse policy distributions and long-CoT responses. The project
will be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is
available at https://huggingface.co/infly/Universal-PRM-7B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Keep a Promise: Scaling Language Model Decoding Parallelism
  with Learned Asynchronous Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding with autoregressive large language models (LLMs) traditionally
occurs sequentially, generating one token after another. An emerging line of
work explored parallel decoding by identifying and simultaneously generating
semantically independent chunks of LLM responses. However, these techniques
rely on hand-crafted heuristics tied to syntactic structures like lists and
paragraphs, making them rigid and imprecise. We present PASTA, a learning-based
system that teaches LLMs to identify semantic independence and express parallel
decoding opportunities in their own responses. At its core are PASTA-LANG and
its interpreter: PASTA-LANG is an annotation language that enables LLMs to
express semantic independence in their own responses; the language interpreter
acts on these annotations to orchestrate parallel decoding on-the-fly at
inference time. Through a two-stage finetuning process, we train LLMs to
generate PASTA-LANG annotations that optimize both response quality and
decoding speed. Evaluation on AlpacaEval, an instruction following benchmark,
shows that our approach Pareto-dominates existing methods in terms of decoding
speed and response quality; our results demonstrate geometric mean speedups
ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to
-7.1%, measured by length-controlled win rates against sequential decoding
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Inference-time Scaling for Chain of <span class="highlight-title">Multi-modal</span> Thought: A
  Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Lin, Ante Wang, Moye Chen, Jingyao Liu, Hao Liu, Jinsong Su, Xinyan Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, inference-time scaling of chain-of-thought (CoT) has been
demonstrated as a promising approach for addressing multi-modal reasoning
tasks. While existing studies have predominantly centered on text-based
thinking, the integration of both visual and textual modalities within the
reasoning process remains unexplored. In this study, we pioneer the exploration
of inference-time scaling with multi-modal thought, aiming to bridge this gap.
To provide a comprehensive analysis, we systematically investigate popular
sampling-based and tree search-based inference-time scaling methods on 10
challenging tasks spanning various domains. Besides, we uniformly adopt a
consistency-enhanced verifier to ensure effective guidance for both methods
across different thought paradigms. Results show that multi-modal thought
promotes better performance against conventional text-only thought, and
blending the two types of thought fosters more diverse thinking. Despite these
advantages, multi-modal thoughts necessitate higher token consumption for
processing richer visual inputs, which raises concerns in practical
applications. We hope that our findings on the merits and drawbacks of this
research line will inspire future works in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Spelling Correction: A Comprehensive <span class="highlight-title">Survey</span> of Progress,
  Challenges, and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changchun Liu, Kai Zhang, Junzhe Jiang, Zixiao Kong, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Spelling Correction (CSC) is a critical task in natural language
processing, aimed at detecting and correcting spelling errors in Chinese text.
This survey provides a comprehensive overview of CSC, tracing its evolution
from pre-trained language models to large language models, and critically
analyzing their respective strengths and weaknesses in this domain. Moreover,
we further present a detailed examination of existing benchmark datasets,
highlighting their inherent challenges and limitations. Finally, we propose
promising future research directions, particularly focusing on leveraging the
potential of LLMs and their reasoning capabilities for improved CSC
performance. To the best of our knowledge, this is the first comprehensive
survey dedicated to the field of CSC. We believe this work will serve as a
valuable resource for researchers, fostering a deeper understanding of the
field and inspiring future advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Pruning in <span class="highlight-title">Multimodal</span> Large Language Models: Are We Solving the
  Right Problem? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown remarkable performance
for cross-modal understanding and generation, yet still suffer from severe
inference costs. Recently, abundant works have been proposed to solve this
problem with token pruning, which identifies the redundant tokens in MLLMs and
then prunes them to reduce the computation and KV storage costs, leading to
significant acceleration without training. While these methods claim efficiency
gains, critical questions about their fundamental design and evaluation remain
unanswered: Why do many existing approaches underperform even compared to naive
random token selection? Are attention-based scoring sufficient for reliably
identifying redundant tokens? Is language information really helpful during
token pruning? What makes a good trade-off between token importance and
duplication? Are current evaluation protocols comprehensive and unbiased? The
ignorance of previous research on these problems hinders the long-term
development of token pruning. In this paper, we answer these questions one by
one, providing insights into the design of future token pruning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balanced Multi-Factor In-Context Learning for Multilingual Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kaneko, Alham Fikri Aji, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (MLLMs) are able to leverage in-context
learning (ICL) to achieve high performance by leveraging cross-lingual
knowledge transfer without parameter updates. However, their effectiveness is
highly sensitive to example selection, particularly in multilingual settings.
Based on the findings of existing work, three key factors influence
multilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3)
language-specific performance. However, existing approaches address these
factors independently, without explicitly disentangling their combined impact,
leaving optimal example selection underexplored. To address this gap, we
propose balanced multi-factor ICL (\textbf{BMF-ICL}), a method that quantifies
and optimally balances these factors for improved example selection.
Experiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL
outperforms existing methods. Further analysis highlights the importance of
incorporating all three factors and the importance of selecting examples from
multiple languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stop Looking for Important Tokens in <span class="highlight-title">Multimodal</span> Language Models:
  Duplication Matters More 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision tokens in multimodal large language models often dominate huge
computational overhead due to their excessive length compared to linguistic
modality. Abundant recent methods aim to solve this problem with token pruning,
which first defines an importance criterion for tokens and then prunes the
unimportant vision tokens during inference. However, in this paper, we show
that the importance is not an ideal indicator to decide whether a token should
be pruned. Surprisingly, it usually results in inferior performance than random
token pruning and leading to incompatibility to efficient attention computation
operators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),
which prunes tokens based on its duplication with other tokens, leading to
significant and training-free acceleration. Concretely, DART selects a small
subset of pivot tokens and then retains the tokens with low duplication to the
pivots, ensuring minimal information loss during token pruning. Experiments
demonstrate that DART can prune 88.9% vision tokens while maintaining
comparable performance, leading to a 1.99$\times$ and 2.99$\times$ speed-up in
total time and prefilling stage, respectively, with good compatibility to
efficient attention operators. Our codes are available at
https://github.com/ZichenWen1/DART.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft
  Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoshen Chen, Yangning Li, Zishan Xu, Yinghui Li, Xin Su, Zifei Shan, Hai-tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face computational inefficiencies and redundant
processing when handling long context inputs, prompting a focus on compression
techniques. While existing semantic vector-based compression methods achieve
promising performance, these methods fail to account for the intrinsic
information density variations between context chunks, instead allocating soft
tokens uniformly across context chunks. This uniform distribution inevitably
diminishes allocation to information-critical regions. To address this, we
propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method
that leverages the LLM's intrinsic understanding of contextual relevance to
guide compression. DAST combines perplexity-based local information with
attention-driven global information to dynamically allocate soft tokens to the
informative-rich chunks, enabling effective, context-aware compression.
Experimental results across multiple benchmarks demonstrate that DAST surpasses
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Vision Language Models Struggle with Visual Arithmetic? Towards
  Enhanced Chart and Geometry Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have achieved remarkable progress in multimodal
tasks, yet they often struggle with visual arithmetic, seemingly simple
capabilities like object counting or length comparison, which are essential for
relevant complex tasks like chart understanding and geometric reasoning. In
this work, we first investigate the root causes of this deficiency through a
suite of probing tasks focusing on basic visual arithmetic. Our analysis
reveals that while pre-trained vision encoders typically capture sufficient
information, the text decoder often fails to decode it correctly for arithmetic
reasoning. To address this, we propose CogAlign, a novel post-training strategy
inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to
recognize invariant properties under visual transformations. We demonstrate
that this approach significantly improves the performance of three diverse VLMs
on our proposed probing tasks. Furthermore, CogAlign enhances performance by an
average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching
supervised fine-tuning methods while requiring only 60% less training data.
These results highlight the effectiveness and generalizability of CogAlign in
improving fundamental visual arithmetic capabilities and their transfer to
downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on
  Knowledge <span class="highlight-title">Graph</span> Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in natural
language processing. However, in knowledge graph question answering tasks
(KGQA), there remains the issue of answering questions that require multi-hop
reasoning. Existing methods rely on entity vector matching, but the purpose of
the question is abstract and difficult to match with specific entities. As a
result, it is difficult to establish reasoning paths to the purpose, which
leads to information loss and redundancy. To address this issue, inspired by
human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a
novel framework that constructs reasoning paths from purposes back to
conditions. ORT operates in three key phases: (1) using LLM to extract purpose
labels and condition labels, (2) constructing label reasoning paths based on
the KG ontology, and (3) using the label reasoning paths to guide knowledge
retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves
state-of-the-art performance and significantly enhances the capability of LLMs
for KGQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanxuan Liao, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) is essential for Large Language Models (LLMs) to
adapt to evolving real-world demands, yet they are susceptible to catastrophic
forgetting (CF). While traditional CF solutions rely on expensive data
rehearsal, recent rehearsal-free methods employ model-based and
regularization-based strategies to address this issue. However, these
approaches often neglect the model's plasticity, which is crucial to achieving
optimal performance on newly learned tasks. Consequently, a key challenge in CL
is striking a balance between preserving plasticity and mitigating CF. To
tackle this challenge, we propose the $\textbf{D}$ecomposed
$\textbf{A}$ttention-based $\textbf{T}$ask $\textbf{A}$daptation (DATA), which
explicitly decouples and learns both task-specific and task-shared knowledge
using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA
dynamically adjusts the weights of adapters of different ranks based on their
relevance and distinction from previous tasks, allowing the model to acquire
new task-specific skills while effectively retaining previously learned
knowledge. Specifically, we implement a decomposed component weighting strategy
comprising learnable components that collectively generate attention-based
weights, allowing the model to integrate and utilize diverse knowledge from
each DATA. Extensive experiments on three widely used benchmarks demonstrate
that our proposed method achieves state-of-the-art performance. Notably, our
approach significantly enhances model plasticity and mitigates CF by extending
learnable components and employing stochastic restoration during training
iterations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastMCTS: A Simple Sampling Strategy for Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, Qipeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic high-quality multi-step reasoning data can significantly enhance
the performance of large language models on various tasks. However, most
existing methods rely on rejection sampling, which generates trajectories
independently and suffers from inefficiency and imbalanced sampling across
problems of varying difficulty. In this work, we introduce FastMCTS, an
innovative data synthesis strategy inspired by Monte Carlo Tree Search.
FastMCTS provides a more efficient sampling method for multi-step reasoning
data, offering step-level evaluation signals and promoting balanced sampling
across problems of different difficulty levels. Experiments on both English and
Chinese reasoning datasets demonstrate that FastMCTS generates over 30\% more
correct reasoning paths compared to rejection sampling as the number of
generated tokens scales up. Furthermore, under comparable synthetic data
budgets, models trained on FastMCTS-generated data outperform those trained on
rejection sampling data by 3.9\% across multiple benchmarks. As a lightweight
sampling strategy, FastMCTS offers a practical and efficient alternative for
synthesizing high-quality reasoning data. Our code will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLTW: Joint Improved <span class="highlight-title">Graph</span> <span class="highlight-title">Transformer</span> and LLM via Three-Word Language
  for Knowledge <span class="highlight-title">Graph</span> Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If Attention Serves as a Cognitive Model of Human Memory Retrieval, What
  is the Plausible Memory Representation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Yoshida, Shinnosuke Isono, Kohei Kajikawa, Taiga Someya, Yushi Sugimito, Yohei Oseki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in computational psycholinguistics has revealed intriguing
parallels between attention mechanisms and human memory retrieval, focusing
primarily on Transformer architectures that operate on token-level
representations. However, computational psycholinguistic research has also
established that syntactic structures provide compelling explanations for human
sentence processing that word-level factors alone cannot fully account for. In
this study, we investigate whether the attention mechanism of Transformer
Grammar (TG), which uniquely operates on syntactic structures as
representational units, can serve as a cognitive model of human memory
retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis
between model behavior and human processing difficulty. Our experiments
demonstrate that TG's attention achieves superior predictive power for
self-paced reading times compared to vanilla Transformer's, with further
analyses revealing independent contributions from both models. These findings
suggest that human sentence processing involves dual memory representations --
one based on syntactic structures and another on token sequences -- with
attention serving as the general retrieval algorithm, while highlighting the
importance of incorporating syntactic structures as representational units.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GiFT: Gibbs Fine-Tuning for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Wanjin Feng, Xin Zhou, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) with synthetic data is a prevalent
practice in code generation. A key approach is self-training, where LLMs are
iteratively trained on self-generated correct code snippets. In this case, the
self-generated codes are drawn from a conditional distribution, conditioned on
a specific seed description. However, the seed description is not the only
valid representation that aligns with its intended meaning. With all valid
descriptions and codes forming a joint space, codes drawn from the conditional
distribution would lead to an underrepresentation of the full description-code
space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training
method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn
from the marginal distribution of the joint space, thereby mitigating the
biases inherent in conditional sampling. We provide a theoretical analysis
demonstrating the potential benefits of fine-tuning LLMs with code derived from
the marginal distribution. Furthermore, we propose a perplexity-based code
selection method to mitigate the imbalanced long-tail distribution of the
self-generated codes. Empirical evaluation of two LLMs across four datasets
demonstrates that GiFT achieves superior performance, particularly on more
challenging benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Ma, Yunfan Shao, Peiji Li, Demin Song, Qipeng Guo, Linyang Li, Xipeng Qiu, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet code generation remains a major challenge. Current
approaches for obtaining high-quality code data primarily focus on (i)
collecting large-scale pre-training data and (ii) synthesizing instruction data
through prompt engineering with powerful models. While pre-training data faces
quality consistency issues, instruction-based synthesis suffers from limited
instruction diversity and inherent biases of LLMs. To address this gap, we
introduce UnitCoder, a systematic pipeline leveraging model-generated unit
tests to both guide and validate the code generation process. Combined with
large-scale package-based retrieval from pre-training corpus, we generate a
dataset of 500K+ verifiable programs containing diverse API calls. Evaluations
on multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that
models fine-tuned on our synthetic data exhibit consistent performance
improvements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\% and
28\% to 40\% and 39\% success rates on BigCodeBench, respectively. Our work
presents a scalable approach that leverages model-generated unit tests to guide
the synthesis of high-quality code data from pre-training corpora,
demonstrating the potential for producing diverse and high-quality
post-training data at scale. All code and data will be released
(https://github.com).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Sentence Simplification with ESL Learner's Proficiency for
  Language Acquisition <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanlin Li, Yuki Arase, Noel Crespi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text simplification is crucial for improving accessibility and comprehension
for English as a Second Language (ESL) learners. This study goes a step further
and aims to facilitate ESL learners' language acquisition by simplification.
Specifically, we propose simplifying complex sentences to appropriate levels
for learners while also increasing vocabulary coverage of the target level in
the simplifications. We achieve this without a parallel corpus by conducting
reinforcement learning on a large language model. Our method employs
token-level and sentence-level rewards, and iteratively trains the model on its
self-generated outputs to guide the model to search for simplification
hypotheses that satisfy the target attributes. Experiment results on CEFR-SP
and TurkCorpus datasets show that the proposed method can effectively increase
the frequency and diversity of vocabulary of the target level by more than
$20\%$ compared to baseline models, while maintaining high simplification
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL2025 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with
  Unified Multi-Objective Optimization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preference plays a significant role in measuring large language models
and guiding them to align with human values. Unfortunately, current
comparing-based evaluation (CBE) methods typically focus on a single
optimization objective, failing to effectively utilize scarce yet valuable
preference signals. To address this, we delve into key factors that can enhance
the accuracy, convergence, and scalability of CBE: suppressing sampling bias,
balancing descending process of uncertainty, and mitigating updating
uncertainty. Following the derived guidelines, we propose UniCBE, a unified
uniformity-driven CBE framework which simultaneously optimize these core
objectives by constructing and integrating three decoupled sampling probability
matrices, each designed to ensure uniformity in specific aspects. We further
ablate the optimal tuple sampling and preference aggregation strategies to
achieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of
evaluation budgets while achieving a Pearson correlation with ground truth
exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios
where new models are continuously introduced, UniCBE can even save over 50% of
evaluation costs, highlighting its improved scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Personas to Talks: Revisiting the Impact of Personas on
  LLM-Synthesized Emotional Support Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghan Wu, Yang Deng, Yimo Zhu, Wynne Hsu, Mong Li Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has revolutionized the
generation of emotional support conversations (ESC), offering scalable
solutions with reduced costs and enhanced data privacy. This paper explores the
role of personas in the creation of ESC by LLMs. Our research utilizes
established psychological frameworks to measure and infuse persona traits into
LLMs, which then generate dialogues in the emotional support scenario. We
conduct extensive evaluations to understand the stability of persona traits in
dialogues, examining shifts in traits post-generation and their impact on
dialogue quality and strategy distribution. Experimental results reveal several
notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in
emotionality and extraversion occur, influencing the dialogue dynamics, and 3)
the application of persona traits modifies the distribution of emotional
support strategies, enhancing the relevance and empathetic quality of the
responses. These findings highlight the potential of persona-driven LLMs in
crafting more personalized, empathetic, and effective emotional support
dialogues, which has significant implications for the future design of
AI-driven emotional support systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical forms complement probability in understanding language model
  (and human) performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Freda Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing interest in using large language models (LLMs) for
planning in natural language, understanding their behaviors becomes an
important research question. This work conducts a systematic investigation of
LLMs' ability to perform logical reasoning in natural language. We introduce a
controlled dataset of hypothetical and disjunctive syllogisms in propositional
and modal logic and use it as the testbed for understanding LLM performance.
Our results lead to novel insights in predicting LLM behaviors: in addition to
the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical
forms should be considered as important factors. In addition, we show
similarities and discrepancies between the logical reasoning performances of
humans and LLMs by collecting and comparing behavioral data from both.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement --
  LLaMA3.1 and GPT-4o for Complete Abstract Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07381v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07381v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report is the system description of the MaLei team (Manchester and
Leiden) for the shared task Plain Language Adaptation of Biomedical Abstracts
(PLABA) 2024 (we had an earlier name BeeManc following last year), affiliated
with TREC2024 (33rd Text REtrieval Conference
https://ir.nist.gov/evalbase/conf/trec-2024). This report contains two sections
corresponding to the two sub-tasks in PLABA-2024. In task one (term
replacement), we applied fine-tuned ReBERTa-Base models to identify and
classify the difficult terms, jargon, and acronyms in the biomedical abstracts
and reported the F1 score (Task 1A and 1B). In task two (complete abstract
adaptation), we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot
prompts to complete the abstract adaptation and reported the scores in BLEU,
SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024
on Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd
and 2nd respectively on the two sub-tasks, and the 1st on averaged F1 scores
across the two tasks from 9 evaluated systems. Our LLaMA-3.1-70B-instructed
model achieved the highest Completeness score for Task 2. We share our source
codes, fine-tuned models, and related resources at
https://github.com/HECTA-UoM/PLABA2024
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work - system report for PLABA2024 with TREC-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in
  Financial Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuting Wang, Jiejun Tan, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a typical and practical application of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) techniques have gained extensive
attention, particularly in vertical domains where LLMs may lack domain-specific
knowledge. In this paper, we introduce an omnidirectional and automatic RAG
benchmark, OmniEval, in the financial domain. Our benchmark is characterized by
its multi-dimensional evaluation framework, including (1) a matrix-based RAG
scenario evaluation system that categorizes queries into five task classes and
16 financial topics, leading to a structured assessment of diverse query
scenarios; (2) a multi-dimensional evaluation data generation approach, which
combines GPT-4-based automatic generation and human annotation, achieving an
87.47\% acceptance ratio in human evaluations on generated instances; (3) a
multi-stage evaluation system that evaluates both retrieval and generation
performance, result in a comprehensive evaluation on the RAG pipeline; and (4)
robust evaluation metrics derived from rule-based and LLM-based ones, enhancing
the reliability of assessments through manual annotations and supervised
fine-tuning of an LLM evaluator. Our experiments demonstrate the
comprehensiveness of OmniEval, which includes extensive test datasets and
highlights the performance variations of RAG systems across diverse topics and
tasks, revealing significant opportunities for RAG models to improve their
capabilities in vertical domains. We open source the code of our benchmark in
\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-LLM Coevolution: Evidence from Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a statistical analysis of arXiv paper abstracts, we report a marked drop
in the frequency of several words previously identified as overused by ChatGPT,
such as "delve", starting soon after they were pointed out in early 2024. The
frequency of certain other words favored by ChatGPT, such as "significant", has
instead kept increasing. These phenomena suggest that some authors of academic
papers have adapted their use of large language models (LLMs), for example, by
selecting outputs or applying modifications to the LLM-generated content. Such
coevolution and cooperation of humans and LLMs thus introduce additional
challenges to the detection of machine-generated text in real-world scenarios.
Estimating the impact of LLMs on academic writing by examining word frequency
remains feasible, and more attention should be paid to words that were already
frequently employed, including those that have decreased in frequency due to
LLMs' disfavor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Point of View of a Sentiment: Towards Clinician Bias Detection in
  Psychiatric Notes <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alissa A. Valentine, Lauren A. Lepow, Lili Chan, Alexander W. Charney, Isotta Landi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative patient descriptions and stigmatizing language can contribute to
generating healthcare disparities in two ways: (1) read by patients, they can
harm their trust and engagement with the medical center; (2) read by
physicians, they may negatively influence their perspective of a future
patient. In psychiatry, the patient-clinician therapeutic alliance is a major
determinant of clinical outcomes. Therefore, language usage in psychiatric
clinical notes may not only create healthcare disparities, but also perpetuate
them. Recent advances in NLP systems have facilitated the efforts to detect
discriminatory language in healthcare. However, such attempts have only focused
on the perspectives of the medical center and its physicians. Considering both
physicians and non-physicians' point of view is a more translatable approach to
identifying potentially harmful language in clinical notes. By leveraging
pre-trained and large language models (PLMs and LLMs), this work aims to
characterize potentially harmful language usage in psychiatric notes by
identifying the sentiment expressed in sentences describing patients based on
the reader's point of view. Extracting 39 sentences from the Mount Sinai Health
System containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa,
GatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and
few-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to
classify the sentiment of the sentences according to the physician or
non-physician point of view. Results showed that GPT-3.5 aligned best to
physician point of view and Mistral aligned best to non-physician point of
view. These results underline the importance of recognizing the reader's point
of view, not only for improving the note writing process, but also for the
quantification, identification, and reduction of bias in computational systems
for downstream analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral presentation at NAACL 2024 Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Acoustic Side-Channel Attacks on Keyboards Using <span class="highlight-title">Transformer</span>s
  and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of microphones in everyday devices and the growing
reliance on online services have amplified the risk of acoustic side-channel
attacks (ASCAs) targeting keyboards. This study explores deep learning
techniques, specifically vision transformers (VTs) and large language models
(LLMs), to enhance the effectiveness and applicability of such attacks. We
present substantial improvements over prior research, with the CoAtNet model
achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement
for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via
Zoom compared to previous benchmarks. We also evaluate transformer
architectures and language models, with the best VT model matching CoAtNet's
performance. A key advancement is the introduction of a noise mitigation method
for real-world scenarios. By using LLMs for contextual understanding, we detect
and correct erroneous keystrokes in noisy environments, enhancing ASCA
performance. Additionally, fine-tuned lightweight language models with Low-Rank
Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X
more parameters. This integration of VTs and LLMs improves the practical
applicability of ASCA mitigation, marking the first use of these technologies
to address ASCAs and error correction in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We will reflect comments from the reviewers and re-submit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELL your Model: Contrastive Explanations for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronny Luss, Erik Miehling, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of black-box deep neural network classification models has sparked
the need to explain their decisions. However, in the case of generative AI,
such as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing a contrastive explanation
method requiring simply black-box/query access. Our explanations suggest that
an LLM outputs a reply to a given prompt because if the prompt was slightly
modified, the LLM would have given a different response that is either less
preferable or contradicts the original response. The key insight is that
contrastive explanations simply require a scoring function that has meaning to
the user and not necessarily a specific real valued quantity (viz. class
label). To this end, we offer a novel budgeted algorithm, our main algorithmic
contribution, which intelligently creates contrasts based on such a scoring
function while adhering to a query budget, necessary for longer contexts. We
show the efficacy of our method on important natural language tasks such as
open-text generation and chatbot conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Both Text and Images Leaked! A Systematic Analysis of <span class="highlight-title">Multimodal</span> LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology
  <span class="highlight-title">Dataset</span> (GIST) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18367v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18367v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Liu, Iman Ouzzani, Wenkai Li, Lechen Zhang, Tianyue Ou, Houda Bouamor, Zhijing Jin, Mona Diab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of machine translation has achieved significant advancements, yet
domain-specific terminology translation, particularly in AI, remains
challenging. We introduce GIST, a large-scale multilingual AI terminology
dataset containing 5K terms extracted from top AI conference papers spanning
2000 to 2023. The terms are translated into Arabic, Chinese, French, Japanese,
and Russian using a hybrid framework that combines LLMs for extraction with
human expertise for translation. The dataset's quality is benchmarked against
existing resources, demonstrating superior translation accuracy through
crowdsourced evaluation. GIST is integrated into translation workflows using
post-translation refinement methods that require no retraining, where LLM
prompting consistently improves BLEU and COMET scores. A web demonstration on
the ACL Anthology platform highlights its practical application, showcasing
improved accessibility for non-English speakers. This work aims to address
critical gaps in AI terminology resources and fosters global inclusivity and
collaboration in AI research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization
  Degradation for Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Li, Yupeng Su, Runming Yang, Congkai Xie, Zheng Wang, Zhongwei Xie, Ngai Wong, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have achieved significant advancements in complex
mathematical reasoning benchmarks, such as MATH. However, their substantial
computational requirements present challenges for practical deployment. Model
quantization has emerged as an effective strategy to reduce memory usage and
computational costs by employing lower precision and bit-width representations.
In this study, we systematically evaluate the impact of quantization on
mathematical reasoning tasks. Our results demonstrate that aggressive
quantization methods like AWQ and GPTQ introduce up to 32.39% accuracy
degradation (average 11.31%) on Llama-3 models, particularly in numerical
computation and reasoning planning. To address this, we introduce a
multidimensional evaluation framework combining qualitative capability analysis
and quantitative error assessment. We further develop targeted recovery
strategies, showing that fine-tuning quantized models on only 545 task-specific
examples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to
near full-precision levels. Additionally, our error assessment pipeline
achieves 98.9% accuracy in diagnosing and localizing errors across 3,366
failure cases, providing actionable insights for mitigating
quantization-induced degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLEAR: Character Unlearning in Textual and Visual Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) is critical for removing private or hazardous
information from deep learning models. While MU has advanced significantly in
unimodal (text or vision) settings, multimodal unlearning (MMU) remains
underexplored due to the lack of open benchmarks for evaluating cross-modal
data removal. To address this gap, we introduce CLEAR, the first open-source
benchmark designed specifically for MMU. CLEAR contains 200 fictitious
individuals and 3,700 images linked with corresponding question-answer pairs,
enabling a thorough evaluation across modalities. We conduct a comprehensive
analysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four
evaluation sets, demonstrating that jointly unlearning both modalities
outperforms single-modality approaches. The dataset is available at
https://huggingface.co/datasets/therem/CLEAR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenge of embedding realistic human
personality traits into LLMs. Previous approaches have primarily focused on
prompt-based methods that describe the behavior associated with the desired
personality traits, suffering from realism and validity issues. To address
these limitations, we introduce BIG5-CHAT, a large-scale dataset containing
100,000 dialogues designed to ground models in how humans express their
personality in language. Leveraging this dataset, we explore Supervised
Fine-Tuning and Direct Preference Optimization as training-based methods to
align LLMs more naturally with human personality patterns. Our methods
outperform prompting on personality assessments such as BFI and IPIP-NEO, with
trait correlations more closely matching human data. Furthermore, our
experiments reveal that models trained to exhibit higher conscientiousness,
higher agreeableness, lower extraversion, and lower neuroticism display better
performance on reasoning tasks, aligning with psychological findings on how
these traits impact human cognitive performance. To our knowledge, this work is
the first comprehensive study to demonstrate how training-based methods can
shape LLM personalities through learning from real human behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversation Routines: A Prompt Engineering Framework for Task-Oriented
  Dialog Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11613v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11613v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Robino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces Conversation Routines (CR), a structured prompt
engineering framework for developing task-oriented dialog systems using Large
Language Models (LLMs). While LLMs demonstrate remarkable natural language
understanding capabilities, engineering them to reliably execute complex
business workflows remains challenging. The proposed CR framework enables the
development of Conversation Agentic Systems (CAS) through natural language
specifications, embedding task-oriented logic within LLM prompts. This approach
provides a systematic methodology for designing and implementing complex
conversational workflows while maintaining behavioral consistency. We
demonstrate the framework's effectiveness through two proof-of-concept
implementations: a Train Ticket Booking System and an Interactive
Troubleshooting Copilot. These case studies validate CR's capability to encode
sophisticated behavioral patterns and decision logic while preserving natural
conversational flexibility. Results show that CR enables domain experts to
design conversational workflows in natural language while leveraging custom
functions (tools) developed by software engineers, creating an efficient
division of responsibilities where developers focus on core API implementation
and domain experts handle conversation design. While the framework shows
promise in accessibility and adaptability, we identify key challenges including
computational overhead, non-deterministic behavior, and domain-specific logic
optimization. Future research directions include CR evaluation methods based on
prompt engineering frameworks driven by goal-oriented grading criteria,
improving scalability for complex multi-agent interactions, and enhancing
system robustness to address the identified limitations across diverse business
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added Experimental Results sections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiTTo-TTS: Diffusion <span class="highlight-title">Transformer</span>s for Scalable Text-to-Speech without
  Domain-Specific Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon Lee, Dong Won Kim, Jaehyeon Kim, Seungjun Chung, Jaewoong Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale latent diffusion models (LDMs) excel in content generation across
various modalities, but their reliance on phonemes and durations in
text-to-speech (TTS) limits scalability and access from other fields. While
recent studies show potential in removing these domain-specific factors,
performance remains suboptimal. In this work, we introduce DiTTo-TTS, a
Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based
TTS can achieve state-of-the-art performance without domain-specific factors.
Through rigorous analysis and empirical exploration, we find that (1) DiT with
minimal modifications outperforms U-Net, (2) variable-length modeling with a
speech length predictor significantly improves results over fixed-length
approaches, and (3) conditions like semantic alignment in speech latent
representations are key to further enhancement. By scaling our training data to
82K hours and the model size to 790M parameters, we achieve superior or
comparable zero-shot performance to state-of-the-art TTS models in naturalness,
intelligibility, and speaker similarity, all without relying on domain-specific
factors. Speech samples are available at https://ditto-tts.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Figurative Meaning through Explainable Visual Entailment <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in
tasks requiring a fine-grained understanding of literal meaning in images and
text, such as visual question-answering or visual entailment. However, there
has been little exploration of the capabilities of these models when presented
with images and captions containing figurative meaning, such as metaphors or
humor. To close this gap, we propose a new task framing the figurative meaning
understanding problem as an explainable visual entailment task, where the model
has to predict whether the image (premise) entails a caption (hypothesis) and
justify the predicted label with a textual explanation. The figurative
phenomena can be present in the image, in the caption, or both. Using a
human-AI collaboration approach, we build the accompanying expert-verified
dataset V-FLUTE, containing 6,027 {image, caption, label, explanation}
instances spanning five diverse figurative phenomena: metaphors, similes,
idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs
struggle to generalize from literal to figurative meaning, particularly when it
is present in images. Further, we identify common types of errors in VLM
reasoning (hallucination and incomplete or unsound reasoning) across classes of
models via human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a novel language model architecture that is capable of scaling
test-time computation by implicitly reasoning in latent space. Our model works
by iterating a recurrent block, thereby unrolling to arbitrary depth at
test-time. This stands in contrast to mainstream reasoning models that scale up
compute by producing more tokens. Unlike approaches based on chain-of-thought,
our approach does not require any specialized training data, can work with
small context windows, and can capture types of reasoning that are not easily
represented in words. We scale a proof-of-concept model to 3.5 billion
parameters and 800 billion tokens. We show that the resulting model can improve
its performance on reasoning benchmarks, sometimes dramatically, up to a
computation load equivalent to 50 billion parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The model is available at
  https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can
  be found at https://github.com/seal-rg/recurrent-pretraining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, Jianhua Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of specialized Large Language Models (LLMs) in domains
such as mathematical reasoning and coding has led to growing interest in
methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE)
model, with the goal of enhancing performance in each domain while retaining
effectiveness on general tasks. However, the effective merging of expert models
remains an open challenge, especially for models with highly divergent weight
parameters or different architectures. State-of-the-art MoE merging methods
only work with homogeneous model architectures and rely on simple unweighted
averaging to merge expert layers, which does not address parameter interference
and requires extensive fine-tuning of the merged MoE to restore performance. To
address these limitations, this paper introduces new MoE merging techniques,
including strategies to mitigate parameter interference, routing heuristics to
reduce the need for MoE fine-tuning, and a novel method for merging experts
with different architectures. Extensive experiments across multiple domains
demonstrate the effectiveness of our proposed methods, reducing fine-tuning
costs, improving performance over state-of-the-art methods, and expanding the
applicability of MoE merging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Valuation using Neural Networks for Efficient Instruction
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Dilek Hakkani-Tür
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide crucial insights into model training, but
existing methods suffer from large computational costs and limited
generalization. Particularly, recent works have proposed various metrics and
algorithms to calculate the influence of data using language models, which do
not scale well with large models and datasets. This is because of the expensive
forward and backward passes required for computation, substantial memory
requirements to store large models, and poor generalization of influence
estimates to new data. In this paper, we explore the use of small neural
networks -- which we refer to as the InfluenceNetwork -- to estimate influence
values, achieving up to 99% cost reduction. Our evaluation demonstrates that
influence values can be estimated with models just 0.0027% the size of full
language models (we use 7B and 8B versions). We apply our algorithm of
estimating influence values (called NN-CIFT: Neural Networks for effiCient
Instruction Fine-Tuning) to the downstream task of subset selection for general
instruction fine-tuning. In our study, we include four state-of-the-art
influence functions and show no compromise in performance, despite large
speedups, between NN-CIFT and the original influence functions. We provide an
in-depth hyperparameter analyses of NN-CIFT. The code for our method can be
found here: https://github.com/agarwalishika/NN-CIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roland Daynauth, Christopher Clarke, Krisztian Flautner, Lingjia Tang, Jason Mars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deciding which large language model (LLM) to use is a complex challenge.
Pairwise ranking has emerged as a new method for evaluating human preferences
for LLMs. This approach entails humans evaluating pairs of model outputs based
on a predefined criterion. By collecting these comparisons, a ranking can be
constructed using methods such as Elo. However, applying these algorithms as
constructed in the context of LLM evaluation introduces several challenges. In
this paper, we explore the effectiveness of ranking systems for head-to-head
comparisons of LLMs. We formally define a set of fundamental principles for
effective ranking and conduct a series of extensive evaluations on the
robustness of several ranking algorithms in the context of LLMs. Our analysis
uncovers key insights into the factors that affect ranking accuracy and
efficiency, offering guidelines for selecting the most appropriate methods
based on specific evaluation contexts and resource constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large <span class="highlight-title">Multimodal</span> Models Solve Caption Generation for Scientific
  Figures? Lessons Learned from SCICAP Challenge 2023 <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the SCICAP datasets launch in 2021, the research community has made
significant progress in generating captions for scientific figures in scholarly
articles. In 2023, the first SCICAP Challenge took place, inviting global teams
to use an expanded SCICAP dataset to develop models for captioning diverse
figure types across various academic fields. At the same time, text generation
models advanced quickly, with many powerful pre-trained large multimodal models
(LMMs) emerging that showed impressive capabilities in various
vision-and-language tasks. This paper presents an overview of the first SCICAP
Challenge and details the performance of various models on its data, capturing
a snapshot of the fields state. We found that professional editors
overwhelmingly preferred figure captions generated by GPT-4V over those from
all other models and even the original captions written by authors. Following
this key finding, we conducted detailed analyses to answer this question: Have
advanced LMMs solved the task of generating captions for scientific figures?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Better: Avoiding Pitfalls in Developing Language Resources when
  Data is Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12691v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12691v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-Budget-Aware LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18547v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18547v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is critical for large language models (LLMs) to excel in a wide
range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM
performance by decomposing problems into intermediate steps, they also incur
significant overhead in token usage, leading to increased costs. We find that
the reasoning process of current LLMs is unnecessarily lengthy and it can be
compressed by including a reasonable token budget in the prompt, but the choice
of token budget plays a crucial role in the actual compression effectiveness.
We then propose a token-budget-aware LLM reasoning framework, which dynamically
estimates token budgets for different problems based on reasoning complexity
and uses the estimated token budgets to guide the reasoning process.
Experiments show that our method effectively reduces token costs in CoT
reasoning with only a slight performance reduction, offering a practical
solution to balance efficiency and accuracy in LLM reasoning. Code:
https://github.com/GeniusHTX/TALE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORI: O Routing Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Shadid, Rahul Kumar, Mohit Mayank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single large language models (LLMs) often fall short when faced with the
ever-growing range of tasks, making a single-model approach insufficient. We
address this challenge by proposing ORI (O Routing Intelligence), a dynamic
framework that leverages a set of LLMs. By intelligently routing incoming
queries to the most suitable model, ORI not only improves task-specific
accuracy, but also maintains efficiency. Comprehensive evaluations across
diverse benchmarks demonstrate consistent accuracy gains while controlling
computational overhead. By intelligently routing queries, ORI outperforms the
strongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,
ties the top performance on ARC, and on BBH. These results underscore the
benefits of a multi-model strategy and demonstrate how ORI's adaptive
architecture can more effectively handle diverse tasks, offering a scalable,
high-performance solution for a system of multiple large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Multilingual Image Captioning: How far can we get with
  CLIP models? <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Gomes, Chrysoula Zerva, Bruno Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of image captions, looking at both linguistic fluency and
semantic correspondence to visual contents, has witnessed a significant effort.
Still, despite advancements such as the CLIPScore metric, multilingual
captioning evaluation has remained relatively unexplored. This work presents
several strategies, and extensive experiments, related to evaluating CLIPScore
variants in multilingual settings. To address the lack of multilingual test
data, we consider two different strategies: (1) using quality aware
machine-translated datasets with human judgements, and (2) re-purposing
multilingual datasets that target semantic inference and reasoning. Our results
highlight the potential of finetuned multilingual models to generalize across
languages and to handle complex linguistic challenges. Tests with
machine-translated data show that multilingual CLIPScore models can maintain a
high correlation with human judgements across different languages, and
additional tests with natively multilingual and multicultural data further
attest to the high-quality assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Findings of NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt Stability Scoring for Text Annotation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Barrie, Elli Palaiologou, Petter Törnberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers are increasingly using language models (LMs) for text annotation.
These approaches rely only on a prompt telling the model to return a given
output according to a set of instructions. The reproducibility of LM outputs
may nonetheless be vulnerable to small changes in the prompt design. This calls
into question the replicability of classification routines. To tackle this
problem, researchers have typically tested a variety of semantically similar
prompts to determine what we call ``prompt stability." These approaches remain
ad-hoc and task specific. In this article, we propose a general framework for
diagnosing prompt stability by adapting traditional approaches to intra- and
inter-coder reliability scoring. We call the resulting metric the Prompt
Stability Score (PSS) and provide a Python package \texttt{promptstability} for
its estimation. Using six different datasets and twelve outcomes, we classify
$\sim$3.1m rows of data and $\sim$300m input tokens to: a) diagnose when prompt
stability is low; and b) demonstrate the functionality of the package. We
conclude by providing best practice recommendations for applied researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Argumentative Large Language Models for Explainable and Contestable
  Decision-Making <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Freedman, Adam Dejl, Deniz Gorur, Xiang Yin, Antonio Rago, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The profusion of knowledge encoded in large language models (LLMs) and their
ability to apply this knowledge zero-shot in a range of settings makes them
promising candidates for use in decision-making. However, they are currently
limited by their inability to provide outputs which can be faithfully explained
and effectively contested to correct mistakes. In this paper, we attempt to
reconcile these strengths and weaknesses by introducing \emph{argumentative
LLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.
Concretely, ArgLLMs construct argumentation frameworks, which then serve as the
basis for formal reasoning in support of decision-making. The interpretable
nature of these argumentation frameworks and formal reasoning means that any
decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'
performance experimentally in comparison with state-of-the-art techniques, in
the context of the decision-making task of claim verification. We also define
novel properties to characterise contestability and assess ArgLLMs formally in
terms of these properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 18 figures, Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient KV cache management in LLMs is crucial for long-context tasks like
RAG and summarization. Existing KV cache compression methods enforce a fixed
pattern, neglecting task-specific characteristics and reducing the retention of
essential information. However, we observe distinct activation patterns across
layers in various tasks, highlighting the need for adaptive strategies tailored
to each task's unique demands. Based on this insight, we propose DynamicKV, a
method that dynamically optimizes token retention by adjusting the number of
tokens retained at each layer to adapt to the specific task. DynamicKV
establishes global and per-layer maximum KV cache budgets, temporarily
retaining the maximum budget for the current layer, and periodically updating
the KV cache sizes of all preceding layers during inference. Our method retains
only 1.7% of the KV cache size while achieving ~85% of the Full KV cache
performance on LongBench. Notably, even under extreme compression (0.9%),
DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the
Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Systematic Knowledge Injection into Large Language Models via Diverse
  Augmentation for Domain-Specific RAG <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a prominent method for
incorporating domain knowledge into Large Language Models (LLMs). While RAG
enhances response relevance by incorporating retrieved domain knowledge in the
context, retrieval errors can still lead to hallucinations and incorrect
answers. To recover from retriever failures, domain knowledge is injected by
fine-tuning the model to generate the correct response, even in the case of
retrieval errors. However, we observe that without systematic knowledge
augmentation, fine-tuned LLMs may memorize new information but still fail to
extract relevant domain knowledge, leading to poor performance. In this work,
we present a novel framework that significantly enhances the fine-tuning
process by augmenting the training data in two ways -- context augmentation and
knowledge paraphrasing. In context augmentation, we create multiple training
samples for a given QA pair by varying the relevance of the retrieved
information, teaching the model when to ignore and when to rely on retrieved
content. In knowledge paraphrasing, we fine-tune with multiple answers to the
same question, enabling LLMs to better internalize specialized knowledge. To
mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific
identifier to a question and also utilize a replay buffer containing general QA
pairs. Experimental results demonstrate the efficacy of our method over
existing techniques, achieving up to 10\% relative gain in token-level recall
while preserving the LLM's generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 tables, to be published in NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BitStack: Any-Size Compression of Large Language Models in Variable
  Memory Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks
  For LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialun Cao, Yuk-Kit Chan, Zixuan Ling, Wenxuan Wang, Shuqing Li, Mingwei Liu, Ruixi Qiao, Yuting Han, Chaozheng Wang, Boxi Yu, Pinjia He, Shuai Wang, Zibin Zheng, Michael R. Lyu, Shing-Chi Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various benchmarks have been proposed to assess the performance of large
language models (LLMs) in different coding scenarios. We refer to them as
code-related benchmarks. However, there are no systematic guidelines by which
such a benchmark should be developed to ensure its quality, reliability, and
reproducibility. We propose How2Bench, which is comprised of a 55-criteria
checklist as a set of guidelines to govern the development of code-related
benchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks
released within the past decade and found concerning issues. Nearly 70% of the
benchmarks did not take measures for data quality assurance; over 10% did not
even open source or only partially open source. Many highly cited benchmarks
have loopholes, including duplicated samples, incorrect reference
codes/tests/prompts, and unremoved sensitive/confidential information. Finally,
we conducted a human study involving 49 participants, which revealed
significant gaps in awareness of the importance of data quality,
reproducibility, and transparency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office
  Usage Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19318v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19318v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokang Zhang, Sijia Luo, Bohan Zhang, Zeyao Ma, Jing Zhang, Yang Li, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TableLLM, a robust large language model (LLM) with 8 billion
parameters, purpose-built for proficiently handling tabular data manipulation
tasks, whether they are embedded within documents or spreadsheets, catering to
real-world office scenarios. We propose a distant supervision method for
training, which comprises a reasoning process extension strategy, aiding in
training LLMs to understand reasoning patterns more effectively as well as a
cross-way validation strategy, ensuring the quality of the automatically
generated data. To evaluate the performance of TableLLM, we have crafted
benchmarks tailored to address both document and spreadsheet formats as well as
constructed a well-organized evaluation pipeline capable of handling both
scenarios. Thorough evaluations underscore the advantages of TableLLM when
compared to various existing general-purpose and tabular data-focused LLMs. We
have publicly released the model checkpoint, source code, benchmarks, and a web
application for user interaction. Our codes and data are publicly available at
https://github.com/TableLLM/TableLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://tablellm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Circuit Compositions: Exploring Modular Structures in <span class="highlight-title">Transformer</span>-Based
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Mondorf, Sondre Wold, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in interpretability research is to what extent neural
networks, particularly language models, implement reusable functions through
subnetworks that can be composed to perform more complex tasks. Recent advances
in mechanistic interpretability have made progress in identifying
$\textit{circuits}$, which represent the minimal computational subgraphs
responsible for a model's behavior on specific tasks. However, most studies
focus on identifying circuits for individual tasks without investigating how
functionally similar circuits $\textit{relate}$ to each other. To address this
gap, we study the modularity of neural networks by analyzing circuits for
highly compositional subtasks within a transformer-based language model.
Specifically, given a probabilistic context-free grammar, we identify and
compare circuits responsible for ten modular string-edit operations. Our
results indicate that functionally similar circuits exhibit both notable node
overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits
identified can be reused and combined through set operations to represent more
complex functional model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time Verification and Refinement of Language Model Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonho Ko, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance across a wide
range of natural language tasks. However, a critical challenge remains in that
they sometimes generate factually incorrect answers. To address this, while
many previous work has focused on identifying errors in their generation and
further refining them, they are slow in deployment since they are designed to
verify the response from LLMs only after their entire generation (from the
first to last tokens) is done. Further, we observe that once LLMs generate
incorrect tokens early on, there is a higher likelihood that subsequent tokens
will also be factually incorrect. To this end, in this work, we propose
Streaming-VR (Streaming Verification and Refinement), a novel approach designed
to enhance the efficiency of verification and refinement of LLM outputs.
Specifically, the proposed Streaming-VR enables on-the-fly verification and
correction of tokens as they are being generated, similar to a streaming
process, ensuring that each subset of tokens is checked and refined in
real-time by another LLM as the LLM constructs its response. Through
comprehensive evaluations on multiple datasets, we demonstrate that our
approach not only enhances the factual accuracy of LLMs, but also offers a more
efficient solution compared to prior refinement methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Language Models Exhibit Higher Visual Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well do text-only Large Language Models (LLMs) naturally align with the
visual world? We provide the first direct analysis by utilizing frozen text
representations in a discriminative vision-language model framework and
measuring zero-shot generalization on unseen classes. We find decoder-based
LLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs
reliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs
leads to strong gains in cross-lingual settings, where our approach surpasses
CLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves
both robustness and generalization and also significantly reduces the need for
paired data and compute, making vision-language models more accessible and
adaptable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models Struggle to Achieve a Consistent Temporal Representation
  of Facts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) have shown substantial improvements in handling factual
knowledge, yet their capability to consistently represent temporal facts, which
are valid only within specific timeframes, remains underexplored. To
investigate this, we introduce TimeStress, a novel dataset comprising 521K
statements on 2003 of the most popular temporal facts in Wikidata. Each
statement contextualizes a fact with correct and incorrect dates across three
precisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to
discern between correct and incorrect temporal statements based on their
probability of being generated. We assess 18 LMs across various architectures
using two metrics: the win rate, indicating how often correct dates outperform
incorrect ones, and robustness, reflecting consistent performance across all
dates. Our findings reveal that while some LMs achieve a win rate exceeding
80\%, robustness remains low, with the best model achieving only 6\%.
Furthermore, robust knowledge at one date precision does not reliably transfer
to others, highlighting a significant generalization gap. These results
underscore the struggle of LMs to maintain a consistent temporal
representation, supporting their limitations as reliable sources of temporal
knowledge. We provide all data and code for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for
  Low-Resource Language Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited impressive multilingual reasoning
capabilities, driven by extensive multilingual pre-training corpora and
instruction fine-tuning data. However, a performance gap exists between high-
and low-resource language reasoning tasks due to the language imbalance in the
pre-training corpus, which is exacerbated by evaluation bias in existing
reasoning benchmarks lacking low-resource language coverage. To alleviate this
issue, we propose LinguaLIFT, a two-stage instruction tuning framework for
advancing low-resource language reasoning. LinguaLIFT employs a language
alignment layer to capture multilingual alignment in a code-switched tuning way
without requiring multilingual instruction or parallel data, thereby
transferring the cross-lingual reasoning capabilities to low-resource languages
through English-only instruction tuning data. To comprehensively evaluate the
multilingual reasoning capabilities, we introduce the Multilingual Math World
Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and
10 high-resource languages. Experimental results show that LinguaLIFT
outperforms several competitive baselines across MMWP and four widely used
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Language-Specific LLMs to a Reasoning Model in One Day via
  Model Merging - An Open Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates data selection and model merging methodologies aimed
at incorporating advanced reasoning capabilities such as those of DeepSeek R1
into language-specific large language models (LLMs), with a particular focus on
the Thai LLM. Our goal is to enhance the reasoning capabilities of
language-specific LLMs while maintaining their target language abilities.
DeepSeek R1 excels in reasoning but primarily benefits high-resource languages
such as English and Chinese. However, low-resource languages remain underserved
due to the dominance of English-centric training data and model optimizations,
which limit performance in these languages. This limitation results in
unreliable code-switching and diminished effectiveness on tasks in low-resource
languages. Meanwhile, local and regional LLM initiatives have attempted to
bridge this gap by developing language-specific LLMs that focus on improving
local linguistic fidelity. We demonstrate that, with only publicly available
datasets and a computational budget of $120, it is possible to enhance the
reasoning capabilities of language-specific LLMs to match the level of DeepSeek
R1, without compromising their performance on target language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Alleviate Catastrophic Forgetting in LLMs Finetuning?
  Hierarchical Layer-Wise and Element-Wise Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit strong general language capabilities.
However, fine-tuning these models on domain-specific tasks often leads to
catastrophic forgetting, where the model overwrites or loses essential
knowledge acquired during pretraining. This phenomenon significantly limits the
broader applicability of LLMs. To address this challenge, we propose a novel
approach to compute the element-wise importance of model parameters crucial for
preserving general knowledge during fine-tuning. Our method utilizes a
dual-objective optimization strategy: (1) regularization loss based on
element-wise parameter importance, which constrains the updates to parameters
crucial for general knowledge; (2) cross-entropy loss to adapt to
domain-specific tasks. Additionally, we introduce layer-wise coefficients to
account for the varying contributions of different layers, dynamically
balancing the dual-objective optimization. Extensive experiments on scientific,
medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our
approach mitigates catastrophic forgetting while enhancing model adaptability.
Compared to previous methods, our solution is approximately 20 times faster and
requires only 10-15% of the storage, highlighting the practical efficiency. The
code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Informal to Formal -- Incorporating and Evaluating LLMs on Natural
  Language Requirements to Verifiable Formal Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialun Cao, Yaojie Lu, Meiziniu Li, Haoyang Ma, Haokun Li, Mengda He, Cheng Wen, Le Sun, Hongyu Zhang, Shengchao Qin, Shing-Chi Cheung, Cong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research in AI-based formal mathematical reasoning has shown an unstop-
pable growth trend. These studies have excelled in mathematical competitions
like IMO and have made significant progress. This paper focuses on formal
verification, an immediate application scenario of formal reasoning, and breaks
it down into sub-tasks. We constructed 18k high-quality instruction-response
pairs across five formal specification languages (Coq, Lean4, Dafny, ACSL, and
TLA+) by distilling gpt-4o and evaluated against ten open-sourced LLMs,
including recent popular DeepSeek-R1. We also fine-tuned several 7~8B small
models to achieve comparable performance with Deepseek-R1-671B. Interestingly,
we observed that fine-tuning with formal data also enhances mathematics,
reasoning, and coding capabilities. Fine-tuned models are released at https:
//huggingface.co/fm-universe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Promoting the Responsible Development of Speech <span class="highlight-title">Dataset</span>s for Mental
  Health and Neurological Disorders Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Mancini, Ana Tanevska, Andrea Galassi, Alessio Galatolo, Federico Ruggeri, Paolo Torroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research in machine learning and artificial intelligence is largely
centered on modeling and performance evaluation, less so on data collection.
However, recent research demonstrated that limitations and biases in data may
negatively impact trustworthiness and reliability. These aspects are
particularly impactful on sensitive domains such as mental health and
neurological disorders, where speech data are used to develop AI applications
for patients and healthcare providers. In this paper, we chart the landscape of
available speech datasets for this domain, to highlight possible pitfalls and
opportunities for improvement and promote fairness and diversity. We present a
comprehensive list of desiderata for building speech datasets for mental health
and neurological disorders and distill it into an actionable checklist focused
on ethical concerns to foster more responsible research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional [MASK] Discrete Diffusion Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although auto-regressive models excel in natural language processing, they
often struggle to generate diverse text and provide limited controllability.
Non-auto-regressive methods could be an alternative but often produce
degenerate outputs and exhibit shortcomings in conditional generation. To
address these challenges, we propose Diffusion-EAGS, a novel framework that
integrates conditional masked language models into diffusion language models
through the theoretical lens of a conditional Markov Random Field. In doing so,
we propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling
to counterbalance each model's shortcomings. Experimental results show that
Diffusion-EAGS outperforms baselines and achieves the best quality-diversity
tradeoff, demonstrating its effectiveness in non-autoregressive text
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms:
  Exploring Tuning Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshko Koloski, Blaž Škrlj, Marko Robnik-Šikonja, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cross-lingual transfer is a promising technique to solve tasks in
less-resourced languages. In this empirical study, we compare two fine-tuning
approaches combined with zero-shot and full-shot learning approaches for large
language models in a cross-lingual setting. As fine-tuning strategies, we
compare parameter-efficient adapter methods with fine-tuning of all parameters.
As cross-lingual transfer strategies, we compare the intermediate-training
(\textit{IT}) that uses each language sequentially and cross-lingual validation
(\textit{CLV}) that uses a target language already in the validation phase of
fine-tuning. We assess the success of transfer and the extent of catastrophic
forgetting in a source language due to cross-lingual transfer, i.e., how much
previously acquired knowledge is lost when we learn new information in a
different language. The results on two different classification problems, hate
speech detection and product reviews, each containing datasets in several
languages, show that the \textit{IT} cross-lingual strategy outperforms
\textit{CLV} for the target language. Our findings indicate that, in the
majority of cases, the \textit{CLV} strategy demonstrates superior retention of
knowledge in the base language (English) compared to the \textit{IT} strategy,
when evaluating catastrophic forgetting in multiple cross-lingual transfers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonhee Kim, Marco Valentino, André Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on logical reasoning in Language Models (LMs) have sparked a
debate on whether they can learn systematic reasoning principles during
pre-training or merely exploit superficial patterns in the training data. This
paper presents a mechanistic interpretation of syllogistic reasoning in LMs to
advance the understanding of internal dynamics. Specifically, we present a
methodology for circuit discovery aimed at interpreting content-independent
reasoning mechanisms. Through two distinct intervention methods, we uncover a
sufficient and necessary circuit involving middle-term suppression that
elucidates how LMs transfer information to derive valid conclusions from
premises. Furthermore, we investigate how belief biases manifest in syllogistic
reasoning, finding evidence of partial contamination from additional attention
heads responsible for encoding commonsense and contextualized knowledge.
Finally, we explore the generalization of the discovered mechanisms across
various syllogistic schemes, model sizes and architectures, finding that the
identified circuit is sufficient and necessary for the schemes on which the
models achieve high downstream accuracy (> 60%), and that the activation
patterns apply to models of different families. Overall, our findings suggest
that LMs indeed learn transferable content-independent reasoning mechanisms,
but that, at the same time, such mechanisms do not involve generalizable and
abstract logical primitives, being susceptible to contamination by the same
world knowledge acquired during pre-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Vision Language Model Training via High Quality Data Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, Jiao Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning
via High QuaLity Data Curation), an open-source vision language model (VLM)
series achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.
The following three key improvements contribute to SAIL-VL's leading
performance: (1) Scalable high-quality visual understanding data construction:
We implement a data construction pipeline to enable hundred-million-scale
high-quality recaption data annotation, and the resulted dataset SAIL-Caption
is validated to be of the highest data quality compared with opensource
alternatives. (2) Scalable Pretraining with High-Quality Visual Understanding
Data: We scale SAIL-VL's pretraining budget up to 655B tokens and show that
even a 2B VLM benefits from scaled up training data sizes, exhibiting expected
data size scaling laws in visual understanding and instruction following
performance. (3) Scalable SFT via data quantity and complexity scaling: We
curate a high-quality SFT dataset collection which outperforms opensource
alternatives in data quantity scaling effectiveness. We also demonstrate that
training with progressively higher-complexity data surpasses baseline one-stage
training by a large margin. SAIL-VL series models achieve the highest average
score in 18 widely used VLM benchmarks in our evaluation, with the 2B model
takes the top position over VLMs of comparable sizes on OpenCompass 2024
(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust
visual comprehension abilities. SAIL-VL series models are released at
HuggingFace (https://huggingface.co/BytedanceDouyinContent).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety
  Re-Alignment for Fine-Tuned Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wu, Xin Lu, Yanyan Zhao, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) achieve effective safety alignment at
the time of release, they still face various safety challenges. A key issue is
that fine-tuning often compromises the safety alignment of LLMs. To address
this issue, we propose a method named IRR (Identify, Remove, and Recalibrate
for Safety Realignment) that performs safety realignment for LLMs. The core of
IRR is to identify and remove unsafe delta parameters from the fine-tuned
models, while recalibrating the retained ones. We evaluate the effectiveness of
IRR across various datasets, including both full fine-tuning and LoRA methods.
Our results demonstrate that IRR significantly enhances the safety performance
of fine-tuned models on safety benchmarks, such as harmful queries and
jailbreak attacks, while maintaining their performance on downstream tasks. The
source code is available at: https://anonymous.4open.science/r/IRR-BD4F.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Is That Talk About? A Video-to-Text Summarization <span class="highlight-title">Dataset</span> for
  Scientific Presentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming recorded videos into concise and accurate textual summaries is a
growing challenge in multimodal learning. This paper introduces VISTA, a
dataset specifically designed for video-to-text summarization in scientific
domains. VISTA contains 18,599 recorded AI conference presentations paired with
their corresponding paper abstracts. We benchmark the performance of
state-of-the-art large models and apply a plan-based framework to better
capture the structured nature of abstracts. Both human and automated
evaluations confirm that explicit planning enhances summary quality and factual
consistency. However, a considerable gap remains between models and human
performance, highlighting the challenges of scientific video summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogicPro: Improving Complex Logical Reasoning via Program-Guided
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, Zhi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new data synthesis method called
\textbf{LogicPro}, which leverages LeetCode-style algorithm
\underline{Pro}blems and their corresponding \underline{Pro}gram solutions to
synthesize Complex \underline{Logic}al Reasoning data in text format. First, we
synthesize complex reasoning problems through source algorithm problems and
test cases. Then, standard answers and intermediate variable outputs are
obtained for each problem based on standard python solutions and test cases.
Finally, with the guidance of code intermediate variables, we synthesize the
text reasoning process for each reasoning problems. Through this method, we can
synthesize data that is difficult, scalable, effective, and comes with golden
standard answers and high-quality reasoning processes. As a result, with our
540K synthesized dataset constructed solely from 2,360 algorithm problems, our
approach
  Code and data are publicly available at
https://github.com/jiangjin1999/LogicPro achieves significant improvements in
multiple models for the datasets \textit{BBH$^{27}$}, \textit{LogicBench},
\textit{DROP}, \textit{AR-LSAT}, and \textit{GSM8K}, etc. outperforming a wide
range of existing reasoning datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Curriculum: LLMs Should Always Be Pretrained on Their
  Preferred Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemiao Zhang, Liangyu Xu, Feiyu Duan, Yongwei Zhou, Sirui Wang, Rongxiang Weng, Jingang Wang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) generally utilize a consistent data distribution
throughout the pretraining process. However, as the model's capability
improves, it is intuitive that its data preferences dynamically change,
indicating the need for pretraining with different data at various training
stages. To achieve it, we propose the Perplexity Difference (PD) based
Preference Curriculum learning (PDPC) framework, which always perceives and
uses the data preferred by LLMs to train and boost them. First, we introduce
the PD metric to quantify the difference in how challenging a sample is for
weak versus strong models. Samples with high PD are more challenging for weak
models to learn and are more suitable to be arranged in the later stage of
pretraining. Second, we propose the preference function to approximate and
predict the data preference of the LLM at any training step, so as to complete
the arrangement of the dataset offline and ensure continuous training without
interruption. Experimental results on 1.3B and 3B models demonstrate that PDPC
significantly surpasses baselines. Notably, the 3B model trained on 1T tokens
achieves an increased average accuracy of over 8.1% across MMLU and CMMLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi3Hate: <span class="highlight-title">Multimodal</span>, Multilingual, and Multicultural Hate Speech
  Detection with Vision-Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Duc Bui, Katharina von der Wense, Anne Lauscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this paper contains content that may be offensive or upsetting
  Hate speech moderation on global platforms poses unique challenges due to the
multimodal and multilingual nature of content, along with the varying cultural
perceptions. How well do current vision-language models (VLMs) navigate these
nuances? To investigate this, we create the first multimodal and multilingual
parallel hate speech dataset, annotated by a multicultural set of annotators,
called Multi3Hate. It contains 300 parallel meme samples across 5 languages:
English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural
background significantly affects multimodal hate speech annotation in our
dataset. The average pairwise agreement among countries is just 74%,
significantly lower than that of randomly selected annotator groups. Our
qualitative analysis indicates that the lowest pairwise label agreement-only
67% between the USA and India-can be attributed to cultural factors. We then
conduct experiments with 5 large VLMs in a zero-shot setting, finding that
these models align more closely with annotations from the US than with those
from other cultures, even when the memes and prompts are presented in the
dominant language of the other culture. Code and dataset are available at
https://github.com/MinhDucBui/Multi3Hate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main (Camera-Ready Version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party
  Dialogue Understanding of Conversation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13144v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13144v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced the capabilities of conversation systems, making them applicable to
various fields (e.g., education). Despite their progress, the evaluation of the
systems often overlooks the complexities of real-world conversations, such as
real-time interactions, multi-party dialogues, and extended contextual
dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue
simulator. In this simulator, a conversation system is assigned the role of a
character from popular TV shows, requiring it to respond to spontaneous
questions using past dialogue information and to distinguish between known and
unknown information. Key features of DialSim include assessing the system's
ability to respond within a reasonable time limit, handling long-term
multi-party dialogues, and evaluating performance under randomized questioning
with LongDialQA, a novel, high-quality question-answering dataset. Our
experiments using DialSim reveal the strengths and weaknesses of the latest
conversation systems, offering valuable insights for future advancements in
conversational AI. DialSim is available at https://dialsim.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Autoregression: Discrete Diffusion for Complex Reasoning and
  Planning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models, despite their impressive capabilities,
struggle with complex reasoning and long-term planning tasks. We introduce
discrete diffusion models as a novel solution to these challenges. Through the
lens of subgoal imbalance, we demonstrate how diffusion models effectively
learn difficult subgoals that elude autoregressive approaches. We propose
Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on
difficulty during learning. On complex tasks like Countdown, Sudoku, and
Boolean Satisfiability Problems, MDM significantly outperforms autoregressive
models without using search techniques. For instance, MDM achieves 91.5\% and
100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and
20.7\% for autoregressive models. Our work highlights the potential of
diffusion-based approaches in advancing AI capabilities for sophisticated
language understanding and problem-solving tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stevan Harnad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Apart from what (little) OpenAI may be concealing from us, we all know
(roughly) how ChatGPT works (its huge text database, its statistics, its vector
representations, and their huge number of parameters, its next-word training,
and so on). But none of us can say (hand on heart) that we are not surprised by
what ChatGPT has proved to be able to do with these resources. This has even
driven some of us to conclude that ChatGPT actually understands. It is not true
that it understands. But it is also not true that we understand how it can do
what it can do. I will suggest some hunches about benign biases: convergent
constraints that emerge at LLM scale that may be helping ChatGPT do so much
better than we would have expected. These biases are inherent in the nature of
language itself, at LLM scale, and they are closely linked to what it is that
ChatGPT lacks, which is direct sensorimotor grounding to connect its words to
their referents and its propositions to their meanings. These convergent biases
are related to (1) the parasitism of indirect verbal grounding on direct
sensorimotor grounding, (2) the circularity of verbal definition, (3) the
mirroring of language production and comprehension, (4) iconicity in
propositions at LLM scale, (5) computational counterparts of human categorical
perception in category learning by neural nets, and perhaps also (6) a
conjecture by Chomsky about the laws of thought. The exposition will be in the
form of a dialogue with ChatGPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 29 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Language Prompting to Ease False Positives in Medical
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou, Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F. Chen, Ai Ti Aw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have significantly
enhanced natural language processing capabilities, facilitating the development
of AudioLLMs that process and understand speech and audio inputs alongside
text. Existing AudioLLMs typically combine a pre-trained audio encoder with a
pre-trained LLM, which are subsequently finetuned on specific audio tasks.
However, the pre-trained audio encoder has constrained capacity to capture
features for new tasks and datasets. To address this, we propose to incorporate
mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE
supplements a base encoder with a pool of relatively light weight encoders,
selectively activated based on the audio input to enhance feature extraction
without significantly increasing model size. Our empirical results demonstrate
that MoWE effectively improves multi-task performance, broadening the
applicability of AudioLLMs to more diverse audio tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Deductive and Inductive Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkun Cai, Xu Zhao, Haoliang Liu, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Serge Belongie, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning tasks, yet their reliance on static prompt structures and limited
adaptability to complex scenarios remains a significant challenge. In this
paper, we propose the Deductive and InDuctive(DID) method, a novel framework
that enhances LLM reasoning by dynamically integrating both deductive and
inductive reasoning approaches. Drawing from cognitive science principles, DID
implements a dual-metric complexity evaluation system that combines Littlestone
dimension and information entropy to precisely assess task difficulty and guide
decomposition strategies. DID enables the model to progressively adapt its
reasoning pathways based on problem complexity, mirroring human cognitive
processes. We evaluate DID's effectiveness across multiple benchmarks,
including the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset
for temporal reasoning. Our results demonstrate significant improvements in
reasoning quality and solution accuracy - achieving 70.3% accuracy on AIW
(compared to 62.2% for Tree of Thought) while maintaining lower computational
costs. The success of DID in improving LLM performance while preserving
computational efficiency suggests promising directions for developing more
cognitively aligned and capable language models. Our work contributes a
theoretically grounded, input-centric approach to enhancing LLM reasoning
capabilities, offering an efficient alternative to traditional
output-exploration methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Active Critics in NLG Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuying Xu, Junjie Hu, Ming Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional paradigm of using large language models (LLMs) for natural
language generation (NLG) evaluation relies on pre-defined task definitions and
evaluation criteria, positioning LLMs as "passive critics" that strictly follow
developer-provided guidelines. However, human evaluators often apply implicit
criteria, and their expectations in practice can vary widely based on specific
end-user needs. Consequently, these rigid evaluation methods struggle to adapt
to diverse scenarios without extensive prompt customization. To address this,
we introduce Active-Critic, a novel LLM-based evaluator that transforms LLMs
into "active critics'' capable of adapting to diverse NLG tasks using limited
example data. Active-Critic consists of two stages: (1) self-inferring the
target NLG task and relevant evaluation criteria, and (2) dynamically
optimizing prompts to produce human-aligned scores along with detailed
justifications. Our experiments show that Active-Critic can generate nuanced,
context-aware evaluation criteria, enabling it to achieve superior alignment
with human judgments across multiple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span>Eval36K: Benchmarking Coding and Reasoning Capabilities of Large
  Language Models on <span class="highlight-title">Graph</span> <span class="highlight-title">Dataset</span>s <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success in natural
language processing (NLP), demonstrating significant capabilities in processing
and understanding text data. However, recent studies have identified
limitations in LLMs' ability to manipulate, program, and reason about
structured data, especially graphs. We introduce GraphEval36K, the first
comprehensive graph dataset, comprising 40 graph coding problems and 36,900
test cases to evaluate the ability of LLMs on graph problem-solving. Our
dataset is categorized into eight primary and four sub-categories to ensure a
thorough evaluation across different types of graphs. We benchmark ten LLMs,
finding that private models outperform open-source ones, though the gap is
narrowing. We also analyze the performance of LLMs across directed vs
undirected graphs, different kinds of graph concepts, and network models.
Furthermore, to improve the usability of our evaluation framework, we propose
Structured Symbolic Decomposition (SSD), an instruction-based method designed
to enhance LLM performance on complex graph tasks. Results show that SSD
improves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and
Claude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. This paper
  has been accepted by NAACL 2025. GraphEval36K is available at
  https://grapheval36k.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation
  in Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Xu, Feng Jiang, Anningzhe Gao, Luis Fernando D'Haro, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dialogue systems, discourse plays a crucial role in managing
conversational focus and coordinating interactions. It consists of two key
structures: rhetorical structure and topic structure. The former captures the
logical flow of conversations, while the latter detects transitions between
topics. Together, they improve the ability of a dialogue system to track
conversation dynamics and generate contextually relevant high-quality
responses. These structures are typically identified through discourse parsing
and topic segmentation, respectively. However, existing supervised methods rely
on costly manual annotations, while unsupervised methods often focus on a
single task, overlooking the deep linguistic interplay between rhetorical and
topic structures. To address these issues, we first introduce a unified
representation that integrates rhetorical and topic structures, ensuring
semantic consistency between them. Under the unified representation, we further
propose two linguistically grounded hypotheses based on discourse theories: (1)
Local Discourse Coupling, where rhetorical cues dynamically enhance topic-aware
information flow, and (2) Global Topology Constraint, where topic structure
patterns probabilistically constrain rhetorical relation distributions.
Building on the unified representation and two hypotheses, we propose an
unsupervised mutual learning framework (UMLF) that jointly models rhetorical
and topic structures, allowing them to mutually reinforce each other without
requiring additional annotations. We evaluate our approach on two rhetorical
datasets and three topic segmentation datasets. Experimental results
demonstrate that our method surpasses all strong baselines built on pre-trained
language models. Furthermore, when applied to LLMs, our framework achieves
notable improvements, demonstrating its effectiveness in improving discourse
structure modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16205v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16205v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Lin, Hongming Yang, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Large Language Models (LLMs) has brought significant
advancements across various tasks. However, despite these achievements, LLMs
still exhibit inherent safety vulnerabilities, especially when confronted with
jailbreak attacks. Existing jailbreak methods suffer from two main limitations:
reliance on complicated prompt engineering and iterative optimization, which
lead to low attack success rate (ASR) and attack efficiency (AE). In this work,
we propose an efficient jailbreak attack method, Analyzing-based Jailbreak
(ABJ), which leverages the advanced reasoning capability of LLMs to
autonomously generate harmful content, revealing their underlying safety
vulnerabilities during complex reasoning process. We conduct comprehensive
experiments on ABJ across various open-source and closed-source LLMs. In
particular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional
AE among all target LLMs, showcasing its remarkable attack effectiveness,
transferability, and efficiency. Our findings underscore the urgent need to
prioritize and improve the safety of LLMs to mitigate the risks of misuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciPIP: An LLM-based Scientific Paper Idea Proposer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has opened new
possibilities for automating the proposal of innovative scientific ideas. This
process involves two key phases: literature retrieval and idea generation.
However, existing approaches often fall short due to their reliance on
keyword-based search tools during the retrieval phase, which neglects crucial
semantic information and frequently results in incomplete retrieval outcomes.
Similarly, in the idea generation phase, current methodologies tend to depend
solely on the internal knowledge of LLMs or metadata from retrieved papers,
thereby overlooking significant valuable insights contained within the full
texts. To address these limitations, we introduce SciPIP, an innovative
framework designed to enhance the LLM-based proposal of scientific ideas
through improvements in both literature retrieval and idea generation. Our
approach begins with the construction of a comprehensive literature database
that supports advanced retrieval based not only on keywords but also on
semantics and citation relationships. This is complemented by the introduction
of a multi-granularity retrieval algorithm aimed at ensuring more thorough and
exhaustive retrieval results. For the idea generation phase, we propose a
dual-path framework that effectively integrates both the content of retrieved
papers and the extensive internal knowledge of LLMs. This integration
significantly boosts the novelty, feasibility, and practical value of proposed
ideas. Our experiments, conducted across various domains such as natural
language processing and computer vision, demonstrate SciPIP's capability to
generate a multitude of innovative and useful ideas. These findings underscore
SciPIP's potential as a valuable tool for researchers seeking to advance their
fields with groundbreaking concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 12 tables. The code has been availabel:
  https://github.com/cheerss/SciPIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-Video-T2V Technical Report: The Practice, Challenges, and Future of
  Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile
  Device Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Jiamu Zhou, Muning Wen, Xiaoyun Mo, Haoyu Zhang, Qiqiang Lin, Cheng Jin, Xihuai Wang, Weinan Zhang, Qiuying Peng, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the performance of LLMs in multi-turn human-agent interactions
presents significant challenges, particularly due to the complexity and
variability of user behavior. In this paper, we introduce HammerBench, a novel
benchmark framework for assessing LLMs' function-calling capabilities in
real-world, multi-turn dialogues. HammerBench simulates diverse mobile
assistant use cases, incorporating imperfect instructions, dynamic
question-answer trajectories, intent and argument shifts, and the indirect use
of external information through pronouns. To construct this benchmark, we
curate a comprehensive dataset derived from popular mobile app functionalities
and anonymized user logs, complemented by a cost-effective data generation
pipeline leveraging open-source models. HammerBench is further augmented with
fine-grained interaction snapshots and metrics, enabling detailed evaluation of
function-calling performance across individual conversational turns. We
demonstrate the effectiveness of HammerBench by evaluating several leading LLMs
and uncovering key performance trends. Our experiments reveal that different
types of parameter name errors are a significant source of failure across
different interaction scenarios, highlighting critical areas for further
improvement in LLM robustness for mobile assistant applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assistive Large Language Model Agents for Socially-Aware Negotiation
  Dialogues <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Hua, Lizhen Qu, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop assistive agents based on Large Language Models (LLMs) that aid
interlocutors in business negotiations. Specifically, we simulate business
negotiations by letting two LLM-based agents engage in role play. A third LLM
acts as a remediator agent to rewrite utterances violating norms for improving
negotiation outcomes. We introduce a simple tuning-free and label-free
In-Context Learning (ICL) method to identify high-quality ICL exemplars for the
remediator, where we propose a novel select criteria, called value impact, to
measure the quality of the negotiation outcomes. We provide rich empirical
evidence to demonstrate its effectiveness in negotiations across three
different negotiation topics. We have released our source code and the
generated dataset at: https://github.com/tk1363704/SADAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 3 figures, 14 tables; The paper has been published in the
  Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Discovery Inspired Unsupervised Domain Adaptation for
  Emotion-Cause Pair Extraction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Hua, Yujin Huang, Shuo Huang, Tao Feng, Lizhen Qu, Chris Bain, Richard Bassed, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the task of emotion-cause pair extraction in the
unsupervised domain adaptation setting. The problem is challenging as the
distributions of the events causing emotions in target domains are dramatically
different than those in source domains, despite the distributions of emotional
expressions between domains are overlapped. Inspired by causal discovery, we
propose a novel deep latent model in the variational autoencoder (VAE)
framework, which not only captures the underlying latent structures of data but
also utilizes the easily transferable knowledge of emotions as the bridge to
link the distributions of events in different domains. To facilitate knowledge
transfer across domains, we also propose a novel variational posterior
regularization technique to disentangle the latent representations of emotions
from those of events in order to mitigate the damage caused by the spurious
correlations related to the events in source domains. Through extensive
experiments, we demonstrate that our model outperforms the strongest baseline
by approximately 11.05\% on a Chinese benchmark and 2.45\% on a English
benchmark in terms of weighted-average F1 score. We have released our source
code and the generated dataset publicly at:
https://github.com/tk1363704/CAREL-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 5 tables. The paper has been published in the
  Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCoE: A Compact and Efficient LLM Framework with Multi-Expert
  Collaboration for Resource-Limited Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11686v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11686v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaomang Huang, Jianfeng Pan, Min Peng, Hanzhong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved exceptional performance across
diverse domains through training on massive datasets. However, scaling LLMs to
support multiple downstream domain applications remains a significant
challenge, especially under resource constraints. Existing approaches often
struggle to balance performance across multiple domains with resource
efficiency, limiting their broader applicability. To address this, we introduce
the CCoE architecture, a modular framework that seamlessly integrates
domain-specific experts into a unified LLM. By leveraging independently trained
expert subnetworks on a shared backbone partition, CCoE achieves
state-of-the-art performance while significantly reducing the resource
requirements for multi-expert deployments. Furthermore, rule-based gating and
expert planning in CCoE enable flexible task allocation, promoting expert
collaboration to handle complex reasoning tasks. CCoE not only reduces
inference costs but also provides a flexible and scalable solution for
integrating domain expertise across diverse applications. Experiments on five
domains demonstrate that CCoE achieves comparable performance to current
domain-specific LLMs. Moreover, compared to existing multi-domain model
ensemble methods, CCoE reduces memory usage by 61.3%, while improving inference
efficiency by 0.76x over parameter-efficient multi-expert integration
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large Language Models via Random Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijin Hong, Hao Wu, Su Dong, Junnan Dong, Yilin Xiao, Yujing Zhang, Zhu Wang, Feiran Huang, Linyi Li, Hongxia Yang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have raised concerns about the reliability of current
mathematical benchmarks, highlighting issues such as simplistic design and
potential data contamination. Therefore, creating a reliable benchmark that
effectively evaluates the genuine capabilities of large language models (LLMs)
in mathematical reasoning remains a significant challenge. To address this, we
propose RV-Bench, a framework for Benchmarking LLMs via Random Variables in
mathematical reasoning. Specifically, the background content of a random
variable question (RV question) mirrors the original problem in existing
benchmarks, but the variable combinations are randomized, making it "unseen" by
the LLMs. Models must completely understand the question pattern of the
original problem to correctly answer RV questions with various variable values.
As a result, the LLM's genuine capability in mathematical reasoning is
reflected by its accuracy and robustness on RV-Bench. We conducted extensive
experiments on over 30 representative LLMs across more than 1000 RV questions.
Our findings suggest that LLMs exhibit an imbalance in proficiency between
encountered and "unseen" data domains. Proficiency generalization across
similar mathematical reasoning tasks is verified to be limited by accuracy and
robustness, but it can still be enhanced through test-time scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot generation of synthetic neurosurgical data with large language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin A. Barr, Eddie Guo, Emre Sezgin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical data is fundamental to advance neurosurgical research, but access is
often constrained by data availability, small sample sizes, privacy
regulations, and resource-intensive preprocessing and de-identification
procedures. Synthetic data offers a potential solution to challenges associated
with accessing and using real-world data (RWD). This study aims to evaluate the
capability of zero-shot generation of synthetic neurosurgical data with a large
language model (LLM), GPT-4o, by benchmarking with the conditional tabular
generative adversarial network (CTGAN). Synthetic datasets were compared to
real-world neurosurgical data to assess fidelity (means, proportions,
distributions, and bivariate correlations), utility (ML classifier performance
on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated
datasets matched or exceeded CTGAN performance, despite no fine-tuning or
access to RWD for pre-training. Datasets demonstrated high univariate and
bivariate fidelity to RWD without directly exposing any real patient records,
even at amplified sample size. Training an ML classifier on GPT-4o-generated
data and testing on RWD for a binary prediction task showed an F1 score (0.706)
with comparable performance to training on the CTGAN data (0.705) for
predicting postoperative functional status deterioration. GPT-4o demonstrated a
promising ability to generate high-fidelity synthetic neurosurgical data. These
findings also indicate that data synthesized with GPT-4o can effectively
augment clinical data with small sample sizes, and train ML models for
prediction of neurosurgical outcomes. Further investigation is necessary to
improve the preservation of distributional characteristics and boost classifier
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 4 tables (updated version, fixed typos and
  formatting)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScholaWrite: A <span class="highlight-title">Dataset</span> of End-to-End Scholarly Writing Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linghe Wang, Minhwa Lee, Ross Volkov, Luan Tuyen Chau, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing is a cognitively demanding task involving continuous decision-making,
heavy use of working memory, and frequent switching between multiple
activities. Scholarly writing is particularly complex as it requires authors to
coordinate many pieces of multiform knowledge. To fully understand writers'
cognitive thought process, one should fully decode the end-to-end writing data
(from individual ideas to final manuscript) and understand their complex
cognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset, a
first-of-its-kind keystroke corpus of an end-to-end scholarly writing process
for complete manuscripts, with thorough annotations of cognitive writing
intentions behind each keystroke. Our dataset includes LaTeX-based keystroke
data from five preprints with nearly 62K total text changes and annotations
across 4 months of paper writing. ScholaWrite shows promising usability and
applications (e.g., iterative self-writing), demonstrating the importance of
collection of end-to-end writing data, rather than the final manuscript, for
the development of future writing assistants to support the cognitive thinking
process of scientists. Our de-identified data examples and code are available
on our project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Linghe Wang, Minhwa Lee | project page:
  https://minnesotanlp.github.io/scholawrite/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Boundaries: Investigating the Effects of Model Editing on
  Cross-linguistic Performance <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11139v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11139v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Banerjee, Avik Halder, Rajarshi Mandal, Sayan Layek, Ian Soboroff, Rima Hazra, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of pretrained language models (PLMs) like BERT and GPT has
revolutionized NLP, particularly for English, but it has also created
linguistic imbalances. This paper strategically identifies the need for
linguistic equity by examining several knowledge editing techniques in
multilingual contexts. We evaluate the performance of models such as Mistral,
TowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including
English, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our
research identifies significant discrepancies in normal and merged models
concerning cross-lingual consistency. We employ strategies like 'each language
for itself' (ELFI) and 'each language for others' (ELFO) to stress-test these
models. Our findings demonstrate the potential for LLMs to overcome linguistic
barriers, laying the groundwork for future research in achieving linguistic
inclusivity in AI technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 (Industry track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KcMF: A Knowledge-compliant Framework for Schema and Entity Matching
  with Fine-tuning-free LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqin Xu, Huan Li, Ke Chen, Lidan Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema matching (SM) and entity matching (EM) tasks are crucial for data
integration. While large language models (LLMs) have shown promising results in
these tasks, they suffer from hallucinations and confusion about task
instructions. This study presents the Knowledge-Compliant Matching Framework
(KcMF), an LLM-based approach that addresses these issues without the need for
domain-specific fine-tuning. KcMF employs a once-and-for-all pseudo-code-based
task decomposition strategy to adopt natural language statements that guide LLM
reasoning and reduce confusion across various task types. We also propose two
mechanisms, Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build
domain knowledge sets when unstructured domain knowledge is lacking. Moreover,
we introduce a result-ensemble strategy to leverage multiple knowledge sources
and suppress badly formatted outputs. Extensive evaluations confirm that KcMF
clearly enhances five LLM backbones in both SM and EM tasks while outperforming
the non-LLM competitors by an average F1-score of 17.93%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under reveiw; new results and analysis added, typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GINopic: Topic Modeling with <span class="highlight-title">Graph</span> Isomorphism Network <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Adhya, Debarshi Kumar Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is a widely used approach for analyzing and exploring large
document collections. Recent research efforts have incorporated pre-trained
contextualized language models, such as BERT embeddings, into topic modeling.
However, they often neglect the intrinsic informational value conveyed by
mutual dependencies between words. In this study, we introduce GINopic, a topic
modeling framework based on graph isomorphism networks to capture the
correlation between words. By conducting intrinsic (quantitative as well as
qualitative) and extrinsic evaluations on diverse benchmark datasets, we
demonstrate the effectiveness of GINopic compared to existing topic models and
highlight its potential for advancing topic modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper for NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image
  Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Ma, Xiayang Xiao, Sihao Dong, Peidong Wang, HaiPeng Wang, Qingyun Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a powerful all-weather Earth observation tool, synthetic aperture radar
(SAR) remote sensing enables critical military reconnaissance, maritime
surveillance, and infrastructure monitoring. Although Vision language models
(VLMs) have made remarkable progress in natural language processing and image
understanding, their applications remain limited in professional domains due to
insufficient domain expertise. This paper innovatively proposes the first
large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which
contains approximately 2 million high-quality image-text pairs, encompasses
diverse scenarios with detailed target annotations. This dataset not only
supports several key tasks such as visual understanding and object detection
tasks, but also has unique innovative aspects: this study develop a
visual-language dataset and benchmark for the SAR domain, enabling and
evaluating VLMs' capabilities in SAR image interpretation, which provides a
paradigmatic framework for constructing multimodal datasets across various
remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the
effectiveness of the dataset has been fully verified. The project will be
released at https://github.com/JimmyMa99/SARChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Input Attributions Interpret the Inductive Reasoning Process
  Elicited in In-Context Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpreting the internal process of neural models has long been a challenge.
This challenge remains relevant in the era of large language models (LLMs) and
in-context learning (ICL); for example, ICL poses a new issue of interpreting
which example in the few-shot examples contributed to identifying/solving the
task. To this end, in this paper, we design synthetic diagnostic tasks of
inductive reasoning, inspired by the generalization tests in linguistics; here,
most in-context examples are ambiguous w.r.t. their underlying rule, and one
critical example disambiguates the task demonstrated. The question is whether
conventional input attribution (IA) methods can track such a reasoning process,
i.e., identify the influential example, in ICL. Our experiments provide several
practical findings; for example, a certain simple IA method works the best, and
the larger the model, the generally harder it is to interpret the ICL with
gradient-based IA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-Based Decoding for Retrieval-Augmented Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting Large Language Models (LLMs) with retrieved external knowledge has
proven effective for improving the factual accuracy of generated responses.
Despite their success, retrieval-augmented LLMs still face the distractibility
issue, where the generated responses are negatively influenced by noise from
both external and internal knowledge sources. In this paper, we introduce a
novel, training-free decoding method guided by entropy considerations to
mitigate this issue. Our approach utilizes entropy-based document-parallel
ensemble decoding to prioritize low-entropy distributions from retrieved
documents, thereby enhancing the extraction of relevant information of context.
Additionally, it incorporates a contrastive decoding mechanism that contrasts
the obtained low-entropy ensemble distribution with the high-entropy
distribution derived from the model's internal knowledge across layers, which
ensures a greater emphasis on reliable external information. Extensive
experiments on open-domain question answering datasets demonstrate the
superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities
  of Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyue Jiang, Pengan Chen, Liheng Chen, Sheng Wang, Qinghang Bao, Lingpeng Kong, Yu Li, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of large language models (LLMs) has transformed the
competitive landscape in natural language processing (NLP), particularly for
English and other data-rich languages. However, underrepresented languages like
Cantonese, spoken by over 85 million people, face significant development gaps,
which is particularly concerning given the economic significance of the
Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial
Cantonese-speaking populations in places like Singapore and North America.
Despite its wide use, Cantonese has scant representation in NLP research,
especially compared to other languages from similarly developed regions. To
bridge these gaps, we outline current Cantonese NLP methods and introduce new
benchmarks designed to evaluate LLM performance in factual generation,
mathematical logic, complex reasoning, and general knowledge in Cantonese,
which aim to advance open-source Cantonese LLM technology. We also propose
future research directions and recommended models to enhance Cantonese LLM
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models are Contrastive Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08211v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08211v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting methods play a crucial role in enhancing the capabilities of
pre-trained large language models (LLMs). We explore how contrastive prompting
(CP) significantly improves the ability of large language models to perform
complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by
simply adding "Let's give a correct and a wrong answer." before LLMs provide
answers. Experiments on various large language models show that zero-shot
contrastive prompting improves the performance of standard zero-shot prompting
on a range of arithmetic, commonsense, and symbolic reasoning tasks without any
hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from
35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4
model. Our method not only surpasses zero-shot CoT and few-shot CoT in most
arithmetic and commonsense reasoning tasks but also can seamlessly integrate
with existing prompting methods, resulting in improved or comparable results
when compared to state-of-the-art methods. Our code is available at
https://github.com/yao8839836/cp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step Guided Reasoning: Improving Mathematical Reasoning using Guidance
  Generation and Step Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Cao, Chao Peng, Renhong Chen, Wu Ning, Yingtian Zou, Yitong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning has been challenging for large language models (LLMs).
However, the introduction of step-by-step Chain-of-Thought (CoT) inference has
significantly advanced the mathematical capabilities of LLMs. Despite this
progress, current approaches either necessitate extensive inference datasets
for training or depend on few-shot methods that frequently compromise
computational accuracy. To address these bottlenecks in mathematical reasoning,
we propose a novel method called Step Guidied Reasoning, which is more stable
and generalizable than few-shot methods and does not involve further
fine-tuning of the model. In this approach, LLMs reflect on small reasoning
steps, similar to how humans deliberate and focus attention on what to do next.
By incorporating this reflective process into the inference stage, LLMs can
effectively guide their reasoning from one step to the next. Through extensive
experiments, we demonstrate the significant effect of Step Guidied Reasoning in
augmenting mathematical performance in state-of-the-art language models.
Qwen2-72B-Instruct outperforms its math-specific counterpart,
Qwen2.5-72B-Math-Instruct, on MMLU- STEM with a score of 90.9%, compared to
87.3%. The average scores of Qwen2-7B-Instruct and Qwen2-72B-Instruct increase
from 27.1% to 36.3% and from 36.5% to 47.4% on the mathematics domain,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoostStep: Boosting mathematical capability of Large Language Models via
  improved single-step reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive ability in solving
complex mathematical problems with multi-step reasoning and can be further
enhanced with well-designed in-context learning (ICL) examples. However, this
potential is often constrained by two major challenges in ICL: granularity
mismatch and irrelevant information. We observe that while LLMs excel at
decomposing mathematical problems, they often struggle with reasoning errors in
fine-grained steps. Moreover, ICL examples retrieved at the question level may
omit critical steps or even mislead the model with irrelevant details. To
address this issue, we propose BoostStep, a method that enhances reasoning
accuracy through step-aligned ICL, a novel mechanism that carefully aligns
retrieved reference steps with the corresponding reasoning steps. Additionally,
BoostStep incorporates an effective "first-try" strategy to deliver exemplars
highly relevant to the current state of reasoning. BoostStep is a flexible and
powerful method that integrates seamlessly with chain-of-thought (CoT) and tree
search algorithms, refining both candidate selection and decision-making.
Empirical results show that BoostStep improves GPT-4o's CoT performance by 4.6%
across mathematical benchmarks, significantly surpassing traditional few-shot
learning's 1.2%. Moreover, it can achieve an additional 7.5\% gain combined
with tree search. Surprisingly, it enhances state-of-the-art LLMs to solve
challenging math problems using simpler examples. It improves
DeepSeek-R1-671B's performance on AIME by 2.2%, leveraging simple examples only
from the MATH dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and Data are available at
  https://github.com/beichenzbc/BoostStep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent Skill Acquisition for Large Language Models via CycleQD <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14735v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14735v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        So Kuroki, Taishi Nakamura, Takuya Akiba, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models to acquire specific skills remains a
challenging endeavor. Conventional training approaches often struggle with data
distribution imbalances and inadequacies in objective functions that do not
align well with task-specific performance. To address these challenges, we
introduce CycleQD, a novel approach that leverages the Quality Diversity
framework through a cyclic adaptation of the algorithm, along with a model
merging based crossover and an SVD-based mutation. In CycleQD, each task's
performance metric is alternated as the quality measure while the others serve
as the behavioral characteristics. This cyclic focus on individual tasks allows
for concentrated effort on one task at a time, eliminating the need for data
ratio tuning and simplifying the design of the objective function. Empirical
results from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT
based models not only enables them to surpass traditional fine-tuning methods
in coding, operating systems, and database tasks, but also achieves performance
on par with GPT-3.5-TURBO, which potentially contains much more parameters,
across these domains. Crucially, this enhanced performance is achieved while
retaining robust language capabilities, as evidenced by its performance on
widely adopted language benchmark tasks. We highlight the key design choices in
CycleQD, detailing how these contribute to its effectiveness. Furthermore, our
method is general and can be applied to image segmentation models, highlighting
its applicability across different domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 13th International Conference on Learning
  Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Jauhar, Sihao Chen, Shan Xia, Hongfei Zhang, Jieyu Zhao, Xiaofeng Xu, Xia Song, Jennifer Neville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, aligning these models
with human preferences has emerged as a critical challenge. Traditional
alignment methods, relying on human or LLM annotated datasets, are limited by
their resource-intensive nature, inherent subjectivity, misalignment with
real-world user preferences, and the risk of feedback loops that amplify model
biases. To overcome these limitations, we introduce WildFeedback, a novel
framework that leverages in-situ user feedback during conversations with LLMs
to create preference datasets automatically. Given a corpus of multi-turn
user-LLM conversation, WildFeedback identifies and classifies user feedback to
LLM responses between conversation turns. The user feedback is then used to
create examples of preferred and dispreferred responses according to users'
preference. Our experiments demonstrate that LLMs fine-tuned on WildFeedback
dataset exhibit significantly improved alignment with user preferences, as
evidenced by both traditional benchmarks and our proposed checklist-guided
evaluation. By incorporating in-situ feedback from actual users, WildFeedback
addresses the scalability, subjectivity, and bias challenges that plague
existing approaches, marking a significant step toward developing LLMs that are
more responsive to the diverse and evolving needs of their users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CuriousLLM: Elevating Multi-Document Question Answering with
  LLM-Enhanced Knowledge <span class="highlight-title">Graph</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zukang Yang, Zixuan Zhu, Xuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success in open-domain
question answering. However, they continue to face challenges such as
hallucinations and knowledge cutoffs. These issues can be mitigated through
in-context learning by providing LLMs with relevant context before generating
answers. Recent literature proposes Knowledge Graph Prompting (KGP) which
integrates knowledge graphs with an LLM-based traversal agent to substantially
enhance document retrieval quality. However, KGP requires costly fine-tuning
with large datasets and remains prone to hallucination. In this paper, we
propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning
mechanism into an LLM agent. This mechanism enables the agent to generate
relevant follow-up questions, thereby guiding the information retrieval process
more efficiently. Central to our approach is the development of the new
Follow-upQA dataset, which includes questions and supporting evidence as input,
with follow-up questions serving as ground truths. These follow-up questions
either inquire about what is still missing to fully answer the user's query or
use special tokens to signify that the retrieved evidence is sufficient. Our
experiments show that CuriousLLM significantly boosts LLM performance in
multi-document question answering (MD-QA), circumventing the substantial
computational costs and latency from the original KGP framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Hong, Yuan Gong, Vidhyasaharan Sethu, Ting Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have demonstrated great
success in many Natural Language Processing (NLP) tasks. In addition to their
cognitive intelligence, exploring their capabilities in emotional intelligence
is also crucial, as it enables more natural and empathetic conversational AI.
Recent studies have shown LLMs' capability in recognizing emotions, but they
often focus on single emotion labels and overlook the complex and ambiguous
nature of human emotions. This study is the first to address this gap by
exploring the potential of LLMs in recognizing ambiguous emotions, leveraging
their strong generalization capabilities and in-context learning. We design
zero-shot and few-shot prompting and incorporate past dialogue as context
information for ambiguous emotion recognition. Experiments conducted using
three datasets indicate significant potential for LLMs in recognizing ambiguous
emotions, and highlight the substantial benefits of including context
information. Furthermore, our findings indicate that LLMs demonstrate a high
degree of effectiveness in recognizing less ambiguous emotions and exhibit
potential for identifying more ambiguous emotions, paralleling human perceptual
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detect, Investigate, Judge and Determine: A Knowledge-guided Framework
  for Few-shot Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08952v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08952v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Liu, Jiajun Zhu, Xukai Liu, Haoyu Tang, Yanghai Zhang, Kai Zhang, Xiaofang Zhou, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news
from real ones in extremely low-resource scenarios. This task has garnered
increased attention due to the widespread dissemination and harmful impact of
fake news on social media. Large Language Models (LLMs) have demonstrated
competitive performance with the help of their rich prior knowledge and
excellent in-context learning abilities. However, existing methods face
significant limitations, such as the Understanding Ambiguity and Information
Scarcity, which significantly undermine the potential of LLMs. To address these
shortcomings, we propose a Dual-perspective Knowledge-guided Fake News
Detection (DKFND) model, designed to enhance LLMs from both inside and outside
perspectives. Specifically, DKFND first identifies the knowledge concepts of
each news article through a Detection Module. Subsequently, DKFND creatively
designs an Investigation Module to retrieve inside and outside valuable
information concerning to the current news, followed by another Judge Module to
evaluate the relevance and confidence of them. Finally, a Determination Module
further derives two respective predictions and obtain the final result.
Extensive experiments on two public datasets show the efficacy of our proposed
method, particularly in low-resource settings.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">151</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models without Classifier-free Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Jianmin Bao, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Model-guidance (MG), a novel objective for training
diffusion model that addresses and removes of the commonly used Classifier-free
guidance (CFG). Our innovative approach transcends the standard modeling of
solely data distribution to incorporating the posterior probability of
conditions. The proposed technique originates from the idea of CFG and is easy
yet effective, making it a plug-and-play module for existing models. Our method
significantly accelerates the training process, doubles the inference speed,
and achieve exceptional quality that parallel and even surpass concurrent
diffusion models with CFG. Extensive experiments demonstrate the effectiveness,
efficiency, scalability on different models and datasets. Finally, we establish
state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.
Our code is available at https://github.com/tzco/Diffusion-wo-CFG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoLUT: Efficient Volumetric streaming enhanced by LUT-based
  super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendong Wang, Anlan Zhang, Yifan Yang, Lili Qiu, Yuqing Yang, Xinyang Jiang, Feng Qian, Suman Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D volumetric video provides immersive experience and is gaining traction in
digital media. Despite its rising popularity, the streaming of volumetric video
content poses significant challenges due to the high data bandwidth
requirement. A natural approach to mitigate the bandwidth issue is to reduce
the volumetric video's data rate by downsampling the content prior to
transmission. The video can then be upsampled at the receiver's end using a
super-resolution (SR) algorithm to reconstruct the high-resolution details.
While super-resolution techniques have been extensively explored and advanced
for 2D video content, there is limited work on SR algorithms tailored for
volumetric videos.
  To address this gap and the growing need for efficient volumetric video
streaming, we have developed VoLUT with a new SR algorithm specifically
designed for volumetric content. Our algorithm uniquely harnesses the power of
lookup tables (LUTs) to facilitate the efficient and accurate upscaling of
low-resolution volumetric data. The use of LUTs enables our algorithm to
quickly reference precomputed high-resolution values, thereby significantly
reducing the computational complexity and time required for upscaling. We
further apply adaptive video bit rate algorithm (ABR) to dynamically determine
the downsampling rate according to the network condition and stream the
selected video rate to the receiver. Compared to related work, VoLUT is the
first to enable high-quality 3D SR on commodity mobile devices at line-rate.
Our evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by
36.7% for volumetric video streaming and achieve
  3D SR speed-up with no quality compromise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HermesFlow: Seamlessly Closing the Gap in <span class="highlight-title">Multimodal</span> Understanding and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Xinchen Zhang, Ye Tian, Chenming Shang, Minghao Xu, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of the autoregressive paradigm has made significant
advancement in Multimodal Large Language Models (MLLMs), with powerful models
like Show-o, Transfusion and Emu3 achieving notable progress in unified image
understanding and generation. For the first time, we uncover a common
phenomenon: the understanding capabilities of MLLMs are typically stronger than
their generative capabilities, with a significant gap between the two. Building
on this insight, we propose HermesFlow, a simple yet general framework designed
to seamlessly bridge the gap between understanding and generation in MLLMs.
Specifically, we take the homologous data as input to curate homologous
preference data of both understanding and generation. Through Pair-DPO and
self-play iterative optimization, HermesFlow effectively aligns multimodal
understanding and generation using homologous preference data. Extensive
experiments demonstrate the significant superiority of our approach over prior
methods, particularly in narrowing the gap between multimodal understanding and
generation. These findings highlight the potential of HermesFlow as a general
alignment framework for next-generation multimodal foundation models. Code:
https://github.com/Gen-Verse/HermesFlow
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Gen-Verse/HermesFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising
  Trajectory Sharpening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tian, Ling Yang, Xinchen Zhang, Yunhai Tong, Mengdi Wang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diffusion-Sharpening, a fine-tuning approach that enhances
downstream alignment by optimizing sampling trajectories. Existing RL-based
fine-tuning methods focus on single training timesteps and neglect
trajectory-level alignment, while recent sampling trajectory optimization
methods incur significant inference NFE costs. Diffusion-Sharpening overcomes
this by using a path integral framework to select optimal trajectories during
training, leveraging reward feedback, and amortizing inference costs. Our
method demonstrates superior training efficiency with faster convergence, and
best inference efficiency without requiring additional NFEs. Extensive
experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning
methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods
(e.g., Inference Scaling) across diverse metrics including text alignment,
compositional capabilities, and human preferences, offering a scalable and
efficient solution for future diffusion model fine-tuning. Code:
https://github.com/Gen-Verse/Diffusion-Sharpening
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Gen-Verse/Diffusion-Sharpening</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLARE: Feed-forward Geometry, Appearance and Camera Estimation from
  Uncalibrated Sparse Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLARE, a feed-forward model designed to infer high-quality camera
poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8
inputs), which is a challenging yet practical setting in real-world
applications. Our solution features a cascaded learning paradigm with camera
pose serving as the critical bridge, recognizing its essential role in mapping
3D structures onto 2D image planes. Concretely, FLARE starts with camera pose
estimation, whose results condition the subsequent learning of geometric
structure and appearance, optimized through the objectives of geometry
reconstruction and novel-view synthesis. Utilizing large-scale public datasets
for training, our method delivers state-of-the-art performance in the tasks of
pose estimation, geometry reconstruction, and novel view synthesis, while
maintaining the inference efficiency (i.e., less than 0.5 seconds). The project
page and code can be found at: https://zhanghe3z.github.io/FLARE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Website: https://zhanghe3z.github.io/FLARE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicArticulate: Make Your 3D Models Articulation-Ready 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of 3D content creation, there is an increasing
demand for automatically converting static 3D models into articulation-ready
versions that support realistic animation. Traditional approaches rely heavily
on manual annotation, which is both time-consuming and labor-intensive.
Moreover, the lack of large-scale benchmarks has hindered the development of
learning-based solutions. In this work, we present MagicArticulate, an
effective framework that automatically transforms static 3D models into
articulation-ready assets. Our key contributions are threefold. First, we
introduce Articulation-XL, a large-scale benchmark containing over 33k 3D
models with high-quality articulation annotations, carefully curated from
Objaverse-XL. Second, we propose a novel skeleton generation method that
formulates the task as a sequence modeling problem, leveraging an
auto-regressive transformer to naturally handle varying numbers of bones or
joints within skeletons and their inherent dependencies across different 3D
models. Third, we predict skinning weights using a functional diffusion process
that incorporates volumetric geodesic distance priors between vertices and
joints. Extensive experiments demonstrate that MagicArticulate significantly
outperforms existing methods across diverse object categories, achieving
high-quality articulation that enables realistic animation. Project page:
https://chaoyuesong.github.io/MagicArticulate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRISM: Self-Pruning Intrinsic Selection Method for Training-Free
  <span class="highlight-title">Multimodal</span> Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning refines pre-trained Multimodal Large Language
Models (MLLMs) to enhance their real-world task performance. However, the rapid
expansion of visual instruction datasets introduces significant data
redundancy, leading to excessive computational costs. Existing data selection
methods predominantly rely on proxy models or loss-based metrics, both of which
impose substantial computational overheads due to the necessity of model
inference and backpropagation. To address this challenge, we propose PRISM, a
novel training-free approach for efficient multimodal data selection. Unlike
existing methods, PRISM eliminates the reliance on proxy models, warm-up
pretraining, and gradient-based optimization. Instead, it leverages Pearson
correlation analysis to quantify the intrinsic visual encoding properties of
MLLMs, computing a task-specific correlation score to identify high-value
instances. This not only enbles data-efficient selection,but maintains the
original performance. Empirical evaluations across multiple MLLMs demonstrate
that PRISM reduces the overall time required for visual instruction tuning and
data selection to just 30% of conventional methods, while surpassing fully
fine-tuned models across eight multimodal and three language understanding
benchmarks, achieving a 101.7% relative improvement in final performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Monocular Event-Camera Motion Capture System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion capture systems are a widespread tool in research to record
ground-truth poses of objects. Commercial systems use reflective markers
attached to the object and then triangulate pose of the object from multiple
camera views. Consequently, the object must be visible to multiple cameras
which makes such multi-view motion capture systems unsuited for deployments in
narrow, confined spaces (e.g. ballast tanks of ships). In this technical report
we describe a monocular event-camera motion capture system which overcomes this
limitation and is ideally suited for narrow spaces. Instead of passive markers
it relies on active, blinking LED markers such that each marker can be uniquely
identified from the blinking frequency. The markers are placed at known
locations on the tracking object. We then solve the PnP (perspective-n-points)
problem to obtain the position and orientation of the object. The developed
system has millimeter accuracy, millisecond latency and we demonstrate that its
state estimate can be used to fly a small, agile quadrotor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Communications: A Unified Framework for Cross-modal Context-aware
  Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce token communications (TokCom), a unified
framework to leverage cross-modal context information in generative semantic
communications (GenSC). TokCom is a new paradigm, motivated by the recent
success of generative foundation models and multimodal large language models
(GFM/MLLMs), where the communication units are tokens, enabling efficient
transformer-based token processing at the transmitter and receiver. In this
paper, we introduce the potential opportunities and challenges of leveraging
context in GenSC, explore how to integrate GFM/MLLMs-based token processing
into semantic communication systems to leverage cross-modal context
effectively, present the key principles for efficient TokCom at various layers
in future wireless networks. We demonstrate the corresponding TokCom benefits
in a GenSC setup for image, leveraging cross-modal context information, which
increases the bandwidth efficiency by 70.8% with negligible loss of
semantic/perceptual quality. Finally, the potential research directions are
identified to facilitate adoption of TokCom in future wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Descriminative-Generative Custom Tokens for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pramuditha Perera, Matthew Trager, Luca Zancato, Alessandro Achille, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the possibility of learning custom tokens for
representing new concepts in Vision-Language Models (VLMs). Our aim is to learn
tokens that can be effective for both discriminative and generative tasks while
composing well with words to form new input queries. The targeted concept is
specified in terms of a small set of images and a parent concept described
using text. We operate on CLIP text features and propose to use a combination
of a textual inversion loss and a classification loss to ensure that text
features of the learned token are aligned with image features of the concept in
the CLIP embedding space. We restrict the learned token to a low-dimensional
subspace spanned by tokens for attributes that are appropriate for the given
super-class. These modifications improve the quality of compositions of the
learned token with natural language for generating new scenes. Further, we show
that learned custom tokens can be used to form queries for text-to-image
retrieval task, and also have the important benefit that composite queries can
be visualized to ensure that the desired concept is faithfully encoded. Based
on this, we introduce the method of Generation Aided Image Retrieval, where the
query is modified at inference time to better suit the search intent. On the
DeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over
relevant baselines by 7%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unhackable Temporal Rewarding for Scalable Video MLLMs <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of superior video-processing MLLMs, we have encountered a
perplexing paradox: the "anti-scaling law", where more data and larger models
lead to worse performance. This study unmasks the culprit: "temporal hacking",
a phenomenon where models shortcut by fixating on select frames, missing the
full video narrative. In this work, we systematically establish a comprehensive
theory of temporal hacking, defining it from a reinforcement learning
perspective, introducing the Temporal Perplexity (TPL) score to assess this
misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework
to mitigate the temporal hacking. Both theoretically and empirically, TPL
proves to be a reliable indicator of temporal modeling quality, correlating
strongly with frame activation patterns. Extensive experiments reveal that UTR
not only counters temporal hacking but significantly elevates video
comprehension capabilities. This work not only advances video-AI systems but
also illuminates the critical importance of aligning proxy rewards with true
objectives in MLLM development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanGif: Single-View Human Diffusion with Generative Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While previous single-view-based 3D human reconstruction methods made
significant progress in novel view synthesis, it remains a challenge to
synthesize both view-consistent and pose-consistent results for animatable
human avatars from a single image input. Motivated by the success of 2D
character animation, we propose <strong>HumanGif</strong>, a single-view human
diffusion model with generative prior. Specifically, we formulate the
single-view-based 3D human novel view and pose synthesis as a
single-view-conditioned human diffusion process, utilizing generative priors
from foundational diffusion models. To ensure fine-grained and consistent novel
view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn
spatially aligned features from the input image, implicitly capturing the
relative camera and human pose transformation. Furthermore, we introduce an
image-level loss during optimization to bridge the gap between latent and image
spaces in diffusion models. Extensive experiments on RenderPeople and
DNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual
performance, with better generalizability for novel view and pose synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://skhu101.github.io/HumanGif/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Transparent Object Pose Estimation: A Fusion of GDR-Net and
  Edge Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tessa Pulli, Peter Hönig, Stefan Thalhammer, Matthias Hirschmanner, Markus Vincze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object pose estimation of transparent objects remains a challenging task in
the field of robot vision due to the immense influence of lighting, background,
and reflections. However, the edges of clear objects have the highest contrast,
which leads to stable and prominent features. We propose a novel approach by
incorporating edge detection in a pre-processing step for the tasks of object
detection and object pose estimation. We conducted experiments to investigate
the effect of edge detectors on transparent objects. We examine the performance
of the state-of-the-art 6D object pose estimation pipeline GDR-Net and the
object detector YOLOX when applying different edge detectors as pre-processing
steps (i.e., Canny edge detection with and without color information, and
holistically-nested edges (HED)). We evaluate the physically-based rendered
dataset Trans6D-32 K of transparent objects with parameters proposed by the BOP
Challenge. Our results indicate that applying edge detection as a
pre-processing enhances performance for certain objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at First Austrian Symposium on AI, Robotics, and Vision
  (AIROV 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Next-Day Wildfire Spread with Time Series and Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Lahrichi, Jesse Johnson, Jordan Malof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has demonstrated the potential of deep neural networks (DNNs)
to accurately predict next-day wildfire spread, based upon the current extent
of a fire and geospatial rasters of influential environmental covariates e.g.,
vegetation, topography, climate, and weather. In this work, we investigate a
recent transformer-based model, termed the SwinUnet, for next-day wildfire
prediction. We benchmark Swin-based models against several current
state-of-the-art models on WildfireSpreadTS (WFTS), a large public benchmark
dataset of historical wildfire events. We consider two next-day fire prediction
scenarios: when the model is given input of (i) a single previous day of data,
or (ii) five previous days of data. We find that, with the proper
modifications, SwinUnet achieves state-of-the-art accuracy on next-day
prediction for both the single-day and multi-day scenarios. SwinUnet's success
depends heavily upon utilizing pre-trained weights from ImageNet. Consistent
with prior work, we also found that models with multi-day-input always
outperformed models with single-day input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis
  with Differential Digital Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liang, Fangkun Liu, Andong Li, Xiaodong Li, Chengshi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in visual speech recognition (VSR) have promoted progress
in lip-to-speech synthesis, where pre-trained VSR models enhance the
intelligibility of synthesized speech by providing valuable semantic
information. The success achieved by cascade frameworks, which combine
pseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the
transcribed text, highlights the benefits of leveraging VSR models. However,
these methods typically rely on mel-spectrograms as an intermediate
representation, which may introduce a key bottleneck: the domain gap between
synthetic mel-spectrograms, generated from inherently error-prone lip-to-speech
mappings, and real mel-spectrograms used to train vocoders. This mismatch
inevitably degrades synthesis quality. To bridge this gap, we propose Natural
Lip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic
inductive biases with differentiable speech generation components.
Specifically, we introduce a fundamental frequency (F0) predictor to capture
prosodic variations in synthesized speech. The predicted F0 then drives a
Differentiable Digital Signal Processing (DDSP) synthesizer to generate a
coarse signal which serves as prior information for subsequent speech
synthesis. Additionally, instead of relying on a reference speaker embedding as
an auxiliary input, our approach achieves satisfactory performance on speaker
similarity without explicitly modelling speaker characteristics. Both objective
and subjective evaluation results demonstrate that NaturalL2S can effectively
enhance the quality of the synthesized speech when compared to state-of-the-art
methods. Our demonstration page is accessible at
https://yifan-liang.github.io/NaturalL2S/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFlow: A unified deep learning framework for multi-vessel
  classification, segmentation and clustering of phase-contrast <span class="highlight-title">MRI</span> validated
  on a multi-site single ventricle patient cohort 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Yao, Nicole St. Clair, Gabriel F. Miller, FORCE Investigators, Jennifer A. Steeden, Rahul H. Rathod, Vivek Muthurangu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a unified deep learning (DL) framework, MultiFlowSeg, for
classification and segmentation of velocity-encoded phase-contrast magnetic
resonance imaging data, and MultiFlowDTC for temporal clustering of flow
phenotypes. Applied to the FORCE registry of Fontan procedure patients,
MultiFlowSeg achieved 100% classification accuracy for the aorta, SVC, and IVC,
and 94% for the LPA and RPA. It demonstrated robust segmentation with a median
Dice score of 0.91 (IQR: 0.86-0.93). The automated pipeline processed registry
data, achieving high segmentation success despite challenges like poor image
quality and dextrocardia. Temporal clustering identified five distinct patient
subgroups, with significant differences in clinical outcomes, including
ejection fraction, exercise tolerance, liver disease, and mortality. These
results demonstrate the potential of combining DL and time-varying flow data
for improved CHD prognosis and personalized care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Logic Elements Associated with Round-Off Errors and Gaussian Blur
  in Image Registration: A Simple Case of Commingling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serap A. Savari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete image registration can be a strategy to reconstruct signals from
samples corrupted by blur and noise. We examine superresolution and discrete
image registration for one-dimensional spatially-limited piecewise constant
functions which are subject to blur which is Gaussian or a mixture of Gaussians
as well as to round-off errors. Previous approaches address the signal recovery
problem as an optimization problem. We focus on a regime with low blur and
suggest that the operations of blur, sampling, and quantization are not unlike
the operation of a computer program and have an abstraction that can be studied
with a type of logic. When the minimum distance between discontinuity points is
between $1.5$ and 2 times the sampling interval, we can encounter the simplest
form of a type of interference between discontinuity points that we call
``commingling.'' We describe a way to reason about two sets of samples of the
same signal that will often result in the correct recovery of signal
amplitudes. We also discuss ways to estimate bounds on the distances between
discontinuity points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing Photorealism and Artifacts in Diffusion Model-Generated
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Kamali, Karyn Nakamura, Aakriti Kumar, Angelos Chatzimparmpas, Jessica Hullman, Matthew Groh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model-generated images can appear indistinguishable from authentic
photographs, but these images often contain artifacts and implausibilities that
reveal their AI-generated provenance. Given the challenge to public trust in
media posed by photorealistic AI-generated images, we conducted a large-scale
experiment measuring human detection accuracy on 450 diffusion-model generated
images and 149 real images. Based on collecting 749,828 observations and 34,675
comments from 50,444 participants, we find that scene complexity of an image,
artifact types within an image, display time of an image, and human curation of
AI-generated images all play significant roles in how accurately people
distinguish real from AI-generated images. Additionally, we propose a taxonomy
characterizing artifacts often appearing in images generated by diffusion
models. Our empirical observations and taxonomy offer nuanced insights into the
capabilities and limitations of diffusion models to generate photorealistic
images in 2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 24 Figures, Accepted by ACM CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Inversion: A <span class="highlight-title">Survey</span> from GANs to Diffusion and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Chen, Jiangning Zhang, Yali Bi, Xiaobin Hu, Teng Hu, Zhucun Xue, Ran Yi, Yong Liu, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inversion is a fundamental task in generative models, aiming to map
images back to their latent representations to enable downstream applications
such as editing, restoration, and style transfer. This paper provides a
comprehensive review of the latest advancements in image inversion techniques,
focusing on two main paradigms: Generative Adversarial Network (GAN) inversion
and diffusion model inversion. We categorize these techniques based on their
optimization methods. For GAN inversion, we systematically classify existing
methods into encoder-based approaches, latent optimization approaches, and
hybrid approaches, analyzing their theoretical foundations, technical
innovations, and practical trade-offs. For diffusion model inversion, we
explore training-free strategies, fine-tuning methods, and the design of
additional trainable modules, highlighting their unique advantages and
limitations. Additionally, we discuss several popular downstream applications
and emerging applications beyond image tasks, identifying current challenges
and future research directions. By synthesizing the latest developments, this
paper aims to provide researchers and practitioners with a valuable reference
resource, promoting further advancements in the field of image inversion. We
keep track of the latest works at https://github.com/RyanChenYN/ImageInversion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust 6DoF Pose Tracking Considering Contour and Interior
  Correspondence Uncertainty for AR Assembly Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented reality assembly guidance is essential for intelligent
manufacturing and medical applications, requiring continuous measurement of the
6DoF poses of manipulated objects. Although current tracking methods have made
significant advancements in accuracy and efficiency, they still face challenges
in robustness when dealing with cluttered backgrounds, rotationally symmetric
objects, and noisy sequences. In this paper, we first propose a robust
contour-based pose tracking method that addresses error-prone contour
correspondences and improves noise tolerance. It utilizes a fan-shaped search
strategy to refine correspondences and models local contour shape and noise
uncertainty as mixed probability distribution, resulting in a highly robust
contour energy function. Secondly, we introduce a CPU-only strategy to better
track rotationally symmetric objects and assist the contour-based method in
overcoming local minima by exploring sparse interior correspondences. This is
achieved by pre-sampling interior points from sparse viewpoint templates
offline and using the DIS optical flow algorithm to compute their
correspondences during tracking. Finally, we formulate a unified energy
function to fuse contour and interior information, which is solvable using a
re-weighted least squares algorithm. Experiments on public datasets and real
scenarios demonstrate that our method significantly outperforms
state-of-the-art monocular tracking methods and can achieve more than 100 FPS
using only a CPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Instrumentation and Measurement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Generalizable Prompt for CLIP with Class Similarity Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehun Jung, Hyang-won Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vision-language models (VLMs), prompt tuning has shown its effectiveness
in adapting models to downstream tasks. However, learned prompts struggle to
generalize to unseen classes, as they tend to overfit to the classes that are
targeted during prompt tuning. Examining failure cases, we observed that
learned prompts disrupt the semantics of unseen classes, generating text
embeddings with incorrect semantic relationships among classes. To address
this, we propose Similarity Alignment Regularization (SAR), which regularizes
learnable prompts to preserve the semantic relationships among classes captured
by hand-crafted prompts. Specifically, we first obtain novel classes related to
base classes using ChatGPT-4o and utilize them as potential unseen classes
during prompt tuning. Then, by targeting both base and novel classes, SAR
aligns the similarity relationships among text embeddings generated by
learnable prompts with the similarity relationships from hand-crafted prompts.
Extensive experiments applying SAR to existing prompt tuning methods
demonstrate its effectiveness in improving generalization to unseen classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Freda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  pySLAM is an open-source Python framework for Visual SLAM, supporting
monocular, stereo, and RGB-D cameras. It provides a flexible interface for
integrating both classical and modern local features, making it adaptable to
various SLAM tasks. The framework includes different loop closure methods, a
volumetric reconstruction pipeline, and support for depth prediction models.
Additionally, it offers a suite of tools for visual odometry and SLAM
applications. Designed for both beginners and experienced researchers, pySLAM
encourages community contributions, fostering collaborative development in the
field of Visual SLAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GRAPH</span>GPT-O: Synergistic <span class="highlight-title">Multimodal</span> Comprehension and Generation on
  <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Multimodal Large Language Models (MLLMs) has enabled
the integration of multiple modalities, including texts and images, within the
large language model (LLM) framework. However, texts and images are usually
interconnected, forming a multimodal attributed graph (MMAG). It is
underexplored how MLLMs can incorporate the relational information
(\textit{i.e.}, graph structure) and semantic information (\textit{i.e.,} texts
and images) on such graphs for multimodal comprehension and generation. In this
paper, we propose GraphGPT-o, which supports omni-multimodal understanding and
creation on MMAGs. We first comprehensively study linearization variants to
transform semantic and structural information as input for MLLMs. Then, we
propose a hierarchical aligner that enables deep graph encoding, bridging the
gap between MMAGs and MLLMs. Finally, we explore the inference choices,
adapting MLLM to interleaved text and image generation in graph scenarios.
Extensive experiments on three datasets from different domains demonstrate the
effectiveness of our proposed method. Datasets and codes will be open-sourced
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Yuan, Siyuan Wang, Rui Xie, Hanling Zhang, Tongcheng Fang, Yuzhang Shang, Shengen Yan, Guohao Dai, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a
training-free paradigm that can make use of adaptive temporal compression in
latent space. While existing video generative models apply fixed compression
rates via pretrained VAE, we observe that real-world video content exhibits
substantial temporal non-uniformity, with high-motion segments containing more
information than static scenes. Based on this insight, DLFR-VAE dynamically
adjusts the latent frame rate according to the content complexity.
Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent
Frame Rate Scheduler that partitions videos into temporal chunks and adaptively
determines optimal frame rates based on information-theoretic content
complexity, and (2) A training-free adaptation mechanism that transforms
pretrained VAE architectures into a dynamic VAE that can process features with
variable frame rates. Our simple but effective DLFR-VAE can function as a
plug-and-play module, seamlessly integrating with existing video generation
models and accelerating the video generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Open-Vocabulary to Vocabulary-Free Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klara Reichard, Giulia Rizzoli, Stefano Gasperini, Lukas Hoyer, Pietro Zanuttigh, Nassir Navab, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation enables models to identify novel object
categories beyond their training data. While this flexibility represents a
significant advancement, current approaches still rely on manually specified
class names as input, creating an inherent bottleneck in real-world
applications. This work proposes a Vocabulary-Free Semantic Segmentation
pipeline, eliminating the need for predefined class vocabularies. Specifically,
we address the chicken-and-egg problem where users need knowledge of all
potential objects within a scene to identify them, yet the purpose of
segmentation is often to discover these objects. The proposed approach
leverages Vision-Language Models to automatically recognize objects and
generate appropriate class names, aiming to solve the challenge of class
specification and naming quality. Through extensive experiments on several
public datasets, we highlight the crucial role of the text encoder in model
performance, particularly when the image text classes are paired with generated
descriptions. Despite the challenges introduced by the sensitivity of the
segmentation text encoder to false negatives within the class tagging process,
which adds complexity to the task, we demonstrate that our fully automated
pipeline significantly enhances vocabulary-free segmentation accuracy across
diverse real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to: Pattern Recognition Letters, Klara Reichard and Giulia
  Rizzoli equally contributed to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Knowledge About Perceptual Uncertainty Help an Agent in Automated
  Driving? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Grabowsky, Annika Mütze, Joshua Wendland, Nils Jansen, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents in real-world scenarios like automated driving deal with uncertainty
in their environment, in particular due to perceptual uncertainty. Although,
reinforcement learning is dedicated to autonomous decision-making under
uncertainty these algorithms are typically not informed about the uncertainty
currently contained in their environment. On the other hand, uncertainty
estimation for perception itself is typically directly evaluated in the
perception domain, e.g., in terms of false positive detection rates or
calibration errors based on camera images. Its use for deciding on
goal-oriented actions remains largely unstudied. In this paper, we investigate
how an agent's behavior is influenced by an uncertain perception and how this
behavior changes if information about this uncertainty is available. Therefore,
we consider a proxy task, where the agent is rewarded for driving a route as
fast as possible without colliding with other road users. For controlled
experiments, we introduce uncertainty in the observation space by perturbing
the perception of the given agent while informing the latter. Our experiments
show that an unreliable observation space modeled by a perturbed perception
leads to a defensive driving behavior of the agent. Furthermore, when adding
the information about the current uncertainty directly to the observation
space, the agent adapts to the specific situation and in general accomplishes
its task faster while, at the same time, accounting for risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining and Evaluating Visual Language Models' Basic Spatial Abilities:
  A Perspective from Psychometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Theory of Multiple Intelligences underscores the hierarchical nature of
cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer
a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual
Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial
Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13
mainstream VLMs through nine validated psychometric experiments reveals
significant gaps versus humans (average score 24.95 vs. 68.38), with three key
findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,
weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller
models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading
(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought
(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from
architectural constraints. Identified barriers include weak geometry encoding
and missing dynamic simulation. By linking psychometric BSAs to VLM
capabilities, we provide a diagnostic toolkit for spatial intelligence
evaluation, methodological foundations for embodied AI development, and a
cognitive science-informed roadmap for achieving human-like spatial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Audio-Visual Adversarial Vulnerability from Temporal and
  Modality Perspectives <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zhang, Susan Liang, Daiki Shimada, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While audio-visual learning equips models with a richer understanding of the
real world by leveraging multiple sensory modalities, this integration also
introduces new vulnerabilities to adversarial attacks.
  In this paper, we present a comprehensive study of the adversarial robustness
of audio-visual models, considering both temporal and modality-specific
vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal
invariance attack that exploits the inherent temporal redundancy across
consecutive time segments and 2) a modality misalignment attack that introduces
incongruence between the audio and visual modalities. These attacks are
designed to thoroughly assess the robustness of audio-visual models against
diverse threats. Furthermore, to defend against such attacks, we introduce a
novel audio-visual adversarial training framework. This framework addresses key
challenges in vanilla adversarial training by incorporating efficient
adversarial perturbation crafting tailored to multi-modal data and an
adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds
dataset demonstrate that our proposed temporal and modality-based attacks in
degrading model performance can achieve state-of-the-art performance, while our
adversarial training defense largely improves the adversarial robustness as
well as the adversarial training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aras Yurtman, Daan Van Wesenbeeck, Wannes Meert, Hendrik Blockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Motif Discovery (TSMD) identifies repeating patterns in time
series data, but its unsupervised nature might result in motifs that are not
interesting to the user. To address this, we propose a framework that allows
the user to impose constraints on the motifs to be discovered, where
constraints can easily be defined according to the properties of the desired
motifs in the application domain. We also propose an efficient implementation
of the framework, the LoCoMotif-DoK algorithm. We demonstrate that
LoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic
data, outperforming other TSMD techniques which only support a limited form of
domain knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio
  Chord Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Intuitive physics understanding emerges from self-supervised pretraining
  on natural videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, <span class="highlight-author">Yann LeCun</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the emergence of intuitive physics understanding in
general-purpose deep neural network models trained to predict masked regions in
natural videos. Leveraging the violation-of-expectation framework, we find that
video prediction models trained to predict outcomes in a learned representation
space demonstrate an understanding of various intuitive physics properties,
such as object permanence and shape consistency. In contrast, video prediction
in pixel space and multimodal large language models, which reason through text,
achieve performance closer to chance. Our comparisons of these architectures
reveal that jointly learning an abstract representation space while predicting
missing parts of sensory input, akin to predictive coding, is sufficient to
acquire an understanding of intuitive physics, and that even models trained on
one week of unique video achieve above chance performance. This challenges the
idea that core knowledge -- a set of innate systems to help understand the
world -- needs to be hardwired to develop an understanding of intuitive
physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages,14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Bias Formation in Deep Neural Networks Through the Geometric
  Mechanisms of Human Visual Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ma, Bowei Liu, Wei Dai, Jiayi Chen, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often exhibit biases toward certain categories
during object recognition, even under balanced training data conditions. The
intrinsic mechanisms underlying these biases remain unclear. Inspired by the
human visual system, which decouples object manifolds through hierarchical
processing to achieve object recognition, we propose a geometric analysis
framework linking the geometric complexity of class-specific perceptual
manifolds in DNNs to model bias. Our findings reveal that differences in
geometric complexity can lead to varying recognition capabilities across
categories, introducing biases. To support this analysis, we present the
Perceptual-Manifold-Geometry library, designed for calculating the geometric
properties of perceptual manifolds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When performing 3D inpainting using novel-view rendering methods like Neural
Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture
and geometry consistency across camera views has been a challenge. In this
paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided
Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided
by the rendered depth information from each training view, our 3DGIC exploits
background pixels visible across different views for updating the inpainting
mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive
experiments on benchmark datasets, we confirm that our 3DGIC outperforms
current state-of-the-art 3D inpainting methods quantitatively and
qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Neural Networks for Accurate Depth Estimation with Latent Space
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddiqui Muhammad Yasir, Hyunsik Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation plays a pivotal role in advancing human-robot interactions,
especially in indoor environments where accurate 3D scene reconstruction is
essential for tasks like navigation and object handling. Monocular depth
estimation, which relies on a single RGB camera, offers a more affordable
solution compared to traditional methods that use stereo cameras or LiDAR.
However, despite recent progress, many monocular approaches struggle with
accurately defining depth boundaries, leading to less precise reconstructions.
In response to these challenges, this study introduces a novel depth estimation
framework that leverages latent space features within a deep convolutional
neural network to enhance the precision of monocular depth maps. The proposed
model features dual encoder-decoder architecture, enabling both color-to-depth
and depth-to-depth transformations. This structure allows for refined depth
estimation through latent space encoding. To further improve the accuracy of
depth boundaries and local features, a new loss function is introduced. This
function combines latent loss with gradient loss, helping the model maintain
the integrity of depth boundaries. The framework is thoroughly tested using the
NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in
complex indoor scenarios. The results clearly show that this approach
effectively reduces depth ambiguities and blurring, making it a promising
solution for applications in human-robot interaction and 3D scene
reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Sun, Yudong Yang, Jimin Zhuang, Changli Tang, Yixuan Li, Wei Li, Zejun MA, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in reasoning optimization have significantly
enhanced the capabilities of large language models (LLMs), existing efforts to
improve reasoning have been limited to solving mathematical problems and
focusing on visual graphical inputs, neglecting broader applications in general
video understanding.This paper proposes video-SALMONN-o1, the first open-source
reasoning-enhanced audio-visual LLM designed for general video understanding
tasks. To enhance its reasoning abilities, we develop a reasoning-intensive
dataset featuring challenging audio-visual questions with step-by-step
solutions. We also propose process direct preference optimization (pDPO), which
leverages contrastive step selection to achieve efficient step-level reward
modelling tailored for multimodal inputs. Additionally, we introduce RivaBench,
the first reasoning-intensive video understanding benchmark, featuring over
4,000 high-quality, expert-curated question-answer pairs across scenarios such
as standup comedy, academic presentations, and synthetic video detection.
video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision
baseline across different video reasoning benchmarks. Besides, pDPO achieves
6-8% improvements compared to the supervised fine-tuning model on RivaBench.
Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Deepfake Detection Based on Multi-Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddiqui Muhammad Yasir, Hyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake technology utilizes deep learning based face manipulation techniques
to seamlessly replace faces in videos creating highly realistic but
artificially generated content. Although this technology has beneficial
applications in media and entertainment misuse of its capabilities may lead to
serious risks including identity theft cyberbullying and false information. The
integration of DL with visual cognition has resulted in important technological
improvements particularly in addressing privacy risks caused by artificially
generated deepfake images on digital media platforms. In this study we propose
an efficient and lightweight method for detecting deepfake images and videos
making it suitable for devices with limited computational resources. In order
to reduce the computational burden usually associated with DL models our method
integrates machine learning classifiers in combination with keyframing
approaches and texture analysis. Moreover the features extracted with a
histogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands
were integrated to evaluate using random forest extreme gradient boosting extra
trees and support vector classifier algorithms. Our findings show a
feature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and
96% on FaceForensics++ and Celeb-DFv2 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Computation of the Fisher Information in Continual Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most popular methods for continual learning with deep neural
networks is Elastic Weight Consolidation (EWC), which involves computing the
Fisher Information. The exact way in which the Fisher Information is computed
is however rarely described, and multiple different implementations for it can
be found online. This blog post discusses and empirically compares several
often-used implementations, which highlights that many currently reported
results for EWC could likely be improved by changing the way the Fisher
Information is computed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the blogpost track at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Can See Better: Visual Contrastive Decoding For LLM
  <span class="highlight-title">Multimodal</span> Reasoning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) excel in reasoning and generation for
language tasks, they are not specifically designed for multimodal challenges.
Training Multimodal Large Language Models (MLLMs), however, is
resource-intensive and constrained by various training limitations. In this
paper, we propose the Modular-based Visual Contrastive Decoding (MVCD)
framework to move this obstacle. Our framework leverages LLMs' In-Context
Learning (ICL) capability and the proposed visual contrastive-example decoding
(CED), specifically tailored for this framework, without requiring any
additional training. By converting visual signals into text and focusing on
contrastive output distributions during decoding, we can highlight the new
information introduced by contextual examples, explore their connections, and
avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual
perception to make it see and reason over the input visuals. To demonstrate
MVCD's effectiveness, we conduct experiments with four LLMs across five
question answering datasets. Our results not only show consistent improvement
in model accuracy but well explain the effective components inside our decoding
strategy. Our code will be available at https://github.com/Pbhgit/MVCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling
  Network for Accelerating Dynamic <span class="highlight-title">MRI</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhang, Haiyan Gui, Ningdi Yang, Yue Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint low-rank and sparse unrolling networks have shown superior performance
in dynamic MRI reconstruction. However, existing works mainly utilized matrix
low-rank priors, neglecting the tensor characteristics of dynamic MRI images,
and only a global threshold is applied for the sparse constraint to the
multi-channel data, limiting the flexibility of the network. Additionally, most
of them have inherently complex network structure, with intricate interactions
among variables. In this paper, we propose a novel deep unrolling network,
JotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank
and attention-based sparse priors. Specifically, we utilize tensor low-rank
prior to exploit the structural correlations in high-dimensional data.
Convolutional neural networks are used to adaptively learn the low-rank and
sparse transform domains. A novel attention-based soft thresholding operator is
proposed to assign a unique learnable threshold to each channel of the data in
the CNN-learned sparse domain. The network is unrolled from the elaborately
designed composite splitting algorithm and thus features a simple yet efficient
parallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)
demonstrate the superior performance of JotlasNet in dynamic MRI
reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, accepted by Magnetic Resonance Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILIAS: Instance-Level Image retrieval At Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgos Kordopatis-Zilos, Vladan Stojnić, Anna Manko, Pavel Šuma, Nikolaos-Antonios Ypsilantis, Nikos Efthymiadis, Zakaria Laskar, Jiří Matas, Ondřej Chum, Giorgos Tolias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces ILIAS, a new test dataset for Instance-Level Image
retrieval At Scale. It is designed to evaluate the ability of current and
future foundation models and retrieval techniques to recognize particular
objects. The key benefits over existing datasets include large scale, domain
diversity, accurate ground truth, and a performance that is far from saturated.
ILIAS includes query and positive images for 1,000 object instances, manually
collected to capture challenging conditions and diverse domains. Large-scale
retrieval is conducted against 100 million distractor images from YFCC100M. To
avoid false negatives without extra annotation effort, we include only query
objects confirmed to have emerged after 2014, i.e. the compilation date of
YFCC100M. An extensive benchmarking is performed with the following
observations: i) models fine-tuned on specific domains, such as landmarks or
products, excel in that domain but fail on ILIAS ii) learning a linear
adaptation layer using multi-domain class supervision results in performance
improvements, especially for vision-language models iii) local descriptors in
retrieval re-ranking are still a key ingredient, especially in the presence of
severe background clutter iv) the text-to-image performance of the
vision-language foundation models is surprisingly close to the corresponding
image-to-image case. website: https://vrg.fel.cvut.cz/ilias/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FUNCTO: Function-Centric One-Shot Imitation Learning for Tool
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning tool use from a single human demonstration video offers a highly
intuitive and efficient approach to robot teaching. While humans can
effortlessly generalize a demonstrated tool manipulation skill to diverse tools
that support the same function (e.g., pouring with a mug versus a teapot),
current one-shot imitation learning (OSIL) methods struggle to achieve this. A
key challenge lies in establishing functional correspondences between
demonstration and test tools, considering significant geometric variations
among tools with the same function (i.e., intra-function variations). To
address this challenge, we propose FUNCTO (Function-Centric OSIL for Tool
Manipulation), an OSIL method that establishes function-centric correspondences
with a 3D functional keypoint representation, enabling robots to generalize
tool manipulation skills from a single human demonstration video to novel tools
with the same function despite significant intra-function variations. With this
formulation, we factorize FUNCTO into three stages: (1) functional keypoint
extraction, (2) function-centric correspondence establishment, and (3)
functional keypoint-based action planning. We evaluate FUNCTO against exiting
modular OSIL methods and end-to-end behavioral cloning methods through
real-robot experiments on diverse tool manipulation tasks. The results
demonstrate the superiority of FUNCTO when generalizing to novel tools with
intra-function geometric variations. More details are available at
https://sites.google.com/view/functo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Peng, Fan Lu, Bin Li, Yuan Huang, Sanqing Qu, Guang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-point cloud cross-modal Visual Place Recognition (VPR) is a
challenging task where the query is an RGB image, and the database samples are
LiDAR point clouds. Compared to single-modal VPR, this approach benefits from
the widespread availability of RGB cameras and the robustness of point clouds
in providing accurate spatial geometry and distance information. However,
current methods rely on intermediate modalities that capture either the
vertical or horizontal field of view, limiting their ability to fully exploit
the complementary information from both sensors. In this work, we propose an
innovative initial retrieval + re-rank method that effectively combines
information from range (or RGB) images and Bird's Eye View (BEV) images. Our
approach relies solely on a computationally efficient global descriptor
similarity search process to achieve re-ranking. Additionally, we introduce a
novel similarity label supervision technique to maximize the utility of limited
training data. Specifically, we employ points average distance to approximate
appearance similarity and incorporate an adaptive margin, based on similarity
differences, into the vanilla triplet loss. Experimental results on the KITTI
dataset demonstrate that our method significantly outperforms state-of-the-art
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submmitted to IEEE IV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via
  Modality-decoupled Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wu, Yuxin Xiong, Xintong Li, Yu Xia, Ruoyu Wang, Yu Wang, Tong Yu, Sungchul Kim, Ryan A. Rossi, Lina Yao, Jingbo Shang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent MLLMs have shown emerging visual understanding and reasoning abilities
after being pre-trained on large-scale multimodal datasets. Unlike
pre-training, where MLLMs receive rich visual-text alignment,
instruction-tuning is often text-driven with weaker visual supervision, leading
to the degradation of pre-trained visual understanding and causing visual
forgetting. Existing approaches, such as direct fine-tuning and continual
learning methods, fail to explicitly address this issue, often compressing
visual representations and prioritizing task alignment over visual retention,
which further worsens visual forgetting. To overcome this limitation, we
introduce a novel perspective leveraging effective rank to quantify the
degradation of visual representation richness, interpreting this degradation
through the information bottleneck principle as excessive compression that
leads to the degradation of crucial pre-trained visual knowledge. Building on
this view, we propose a modality-decoupled gradient descent (MDGD) method that
regulates gradient updates to maintain the effective rank of visual
representations while mitigating the over-compression effects described by the
information bottleneck. By explicitly disentangling the optimization of visual
understanding from task-specific alignment, MDGD preserves pre-trained visual
knowledge while enabling efficient task adaptation. To enable lightweight
instruction-tuning, we further develop a memory-efficient fine-tuning approach
using gradient masking, which selectively updates a subset of model parameters
to enable parameter-efficient fine-tuning (PEFT), reducing computational
overhead while preserving rich visual representations. Extensive experiments
across various downstream tasks and backbone MLLMs demonstrate that MDGD
effectively mitigates visual forgetting from pre-trained tasks while enabling
strong adaptation to new tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Graph</span>Morph: Tubular Structure Extraction by Morphing Predicted <span class="highlight-title">Graph</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Zhang, Ziwei Zhao, Dong Wang, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately restoring topology is both challenging and crucial in tubular
structure extraction tasks, such as blood vessel segmentation and road network
extraction. Diverging from traditional approaches based on pixel-level
classification, our proposed method, named GraphMorph, focuses on branch-level
features of tubular structures to achieve more topologically accurate
predictions. GraphMorph comprises two main components: a Graph Decoder and a
Morph Module. Utilizing multi-scale features extracted from an image patch by
the segmentation network, the Graph Decoder facilitates the learning of
branch-level features and generates a graph that accurately represents the
tubular structure in this patch. The Morph Module processes two primary inputs:
the graph and the centerline probability map, provided by the Graph Decoder and
the segmentation network, respectively. Employing a novel SkeletonDijkstra
algorithm, the Morph Module produces a centerline mask that aligns with the
predicted graph. Furthermore, we observe that employing centerline masks
predicted by GraphMorph significantly reduces false positives in the
segmentation task, which is achieved by a simple yet effective post-processing
strategy. The efficacy of our method in the centerline extraction and
segmentation tasks has been substantiated through experimental evaluations
across various datasets. Source code will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No-reference geometry quality assessment for colorless point clouds via
  list-wise rank learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Li, Bingxu Xie, Chao Chu, Weiqing Li, Zhiyong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry quality assessment (GQA) of colorless point clouds is crucial for
evaluating the performance of emerging point cloud-based solutions (e.g.,
watermarking, compression, and 3-Dimensional (3D) reconstruction).
Unfortunately, existing objective GQA approaches are traditional full-reference
metrics, whereas state-of-the-art learning-based point cloud quality assessment
(PCQA) methods target both color and geometry distortions, neither of which are
qualified for the no-reference GQA task. In addition, the lack of large-scale
GQA datasets with subjective scores, which are always imprecise, biased, and
inconsistent, also hinders the development of learning-based GQA metrics.
Driven by these limitations, this paper proposes a no-reference geometry-only
quality assessment approach based on list-wise rank learning, termed LRL-GQA,
which comprises of a geometry quality assessment network (GQANet) and a
list-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the
no-reference GQA as a list-wise rank problem, with the objective of directly
optimizing the entire quality ordering. Specifically, a large dataset
containing a variety of geometry-only distortions is constructed first, named
LRL dataset, in which each sample is label-free but coupled with quality
ranking information. Then, the GQANet is designed to capture intrinsic
multi-scale patch-wise geometric features in order to predict a quality index
for each point cloud. After that, the LRLNet leverages the LRL dataset and a
likelihood loss to train the GQANet and ranks the input list of degraded point
clouds according to their distortion levels. In addition, the pre-trained
GQANet can be fine-tuned further to obtain absolute quality scores.
Experimental results demonstrate the superior performance of the proposed
no-reference LRL-GQA method compared with existing full-reference GQA metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Croce, Christian Schlarmann, Naman Deep Singh, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring perceptual similarity is a key tool in computer vision. In recent
years perceptual metrics based on features extracted from neural networks with
large and diverse training sets, e.g. CLIP, have become popular. At the same
time, the metrics extracted from features of neural networks are not
adversarially robust. In this paper we show that adversarially robust CLIP
models, called R-CLIP$_\textrm{F}$, obtained by unsupervised adversarial
fine-tuning induce a better and adversarially robust perceptual metric that
outperforms existing metrics in a zero-shot setting, and further matches the
performance of state-of-the-art metrics while being robust after fine-tuning.
Moreover, our perceptual metric achieves strong performance on related tasks
such as robust image-to-image retrieval, which becomes especially relevant when
applied to "Not Safe for Work" (NSFW) content detection and dataset filtering.
While standard perceptual metrics can be easily attacked by a small
perturbation completely degrading NSFW detection, our robust perceptual metric
maintains high accuracy under an attack while having similar performance for
unperturbed images. Finally, perceptual metrics induced by robust CLIP models
have higher interpretability: feature inversion can show which images are
considered similar, while text inversion can find what images are associated to
a given prompt. This also allows us to visualize the very rich visual concepts
learned by a CLIP model, including memorized persons, paintings and complex
queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication in the IEEE Conference on
  Secure and Trustworthy Machine Learning (SaTML). The final version will be
  available on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incomplete Modality Disentangled Representation for Ophthalmic Disease
  Grading and Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhi Liu, Zile Huang, Zhe Chen, Feilong Tang, Yu Tian, Zhongxing Xu, Zihong Luo, Yalin Zheng, Yanda Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ophthalmologists typically require multimodal data sources to improve
diagnostic accuracy in clinical decisions. However, due to medical device
shortages, low-quality data and data privacy concerns, missing data modalities
are common in real-world scenarios. Existing deep learning methods tend to
address it by learning an implicit latent subspace representation for different
modality combinations. We identify two significant limitations of these
methods: (1) implicit representation constraints that hinder the model's
ability to capture modality-specific information and (2) modality
heterogeneity, causing distribution gaps and redundancy in feature
representations. To address these, we propose an Incomplete Modality
Disentangled Representation (IMDR) strategy, which disentangles features into
explicit independent modal-common and modal-specific features by guidance of
mutual information, distilling informative knowledge and enabling it to
reconstruct valuable missing semantics and produce robust multimodal
representations. Furthermore, we introduce a joint proxy learning module that
assists IMDR in eliminating intra-modality redundancy by exploiting the
extracted proxies from each class. Experiments on four ophthalmology multimodal
datasets demonstrate that the proposed IMDR outperforms the state-of-the-art
methods significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "See the World, Discover Knowledge": A Chinese Factuality Evaluation for
  Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of factual accuracy in large vision language models (LVLMs)
has lagged behind their rapid development, making it challenging to fully
reflect these models' knowledge capacity and reliability. In this paper, we
introduce the first factuality-based visual question-answering benchmark in
Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of
LVLMs across 8 major topics and 56 subtopics. The key features of this
benchmark include a focus on the Chinese language, diverse knowledge types, a
multi-hop question construction, high-quality data, static consistency, and
easy-to-evaluate through short answers. Moreover, we contribute a rigorous data
construction pipeline and decouple the visual factuality into two parts: seeing
the world (i.e., object recognition) and discovering knowledge. This decoupling
allows us to analyze the capability boundaries and execution mechanisms of
LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source
models, revealing critical performance gaps within this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Component-aware Unsupervised Logical Anomaly Generation for Industrial
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Tong, Yang Chang, Qing Zhao, Jiawen Yu, Boyang Wang, Junxiong Lin, Yuxuan Lin, Xinji Mai, Haoran Wang, Zeng Tao, Yan Wang, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is critical in industrial manufacturing for ensuring
product quality and improving efficiency in automated processes. The scarcity
of anomalous samples limits traditional detection methods, making anomaly
generation essential for expanding the data repository. However, recent
generative models often produce unrealistic anomalies increasing false
positives, or require real-world anomaly samples for training. In this work, we
treat anomaly generation as a compositional problem and propose ComGEN, a
component-aware and unsupervised framework that addresses the gap in logical
anomaly generation. Our method comprises a multi-component learning strategy to
disentangle visual components, followed by subsequent generation editing
procedures. Disentangled text-to-component pairs, revealing intrinsic logical
constraints, conduct attention-guided residual mapping and model training with
iteratively matched references across multiple scales. Experiments on the
MVTecLOCO dataset confirm the efficacy of ComGEN, achieving the best AUROC
score of 91.2%. Additional experiments on the real-world scenario of Diesel
Engine and widely-used MVTecAD dataset demonstrate significant performance
improvements when integrating simulated anomalies generated by ComGEN into
automated production workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Worse The Better: Content-Aware Viewpoint Generation Network for
  Projection-related Point Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyong Su, Bingxu Xie, Zheng Li, Jincan Wu, Weiqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through experimental studies, however, we observed the instability of final
predicted quality scores, which change significantly over different viewpoint
settings. Inspired by the "wooden barrel theory", given the default
content-independent viewpoints of existing projection-related PCQA approaches,
this paper presents a novel content-aware viewpoint generation network (CAVGN)
to learn better viewpoints by taking the distribution of geometric and
attribute features of degraded point clouds into consideration. Firstly, the
proposed CAVGN extracts multi-scale geometric and texture features of the
entire input point cloud, respectively. Then, for each default
content-independent viewpoint, the extracted geometric and texture features are
refined to focus on its corresponding visible part of the input point cloud.
Finally, the refined geometric and texture features are concatenated to
generate an optimized viewpoint. To train the proposed CAVGN, we present a
self-supervised viewpoint ranking network (SSVRN) to select the viewpoint with
the worst quality projected image to construct a default-optimized viewpoint
dataset, which consists of thousands of paired default viewpoints and
corresponding optimized viewpoints. Experimental results show that the
projection-related PCQA methods can achieve higher performance using the
viewpoints generated by the proposed CAVGN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE Transactions on Circuits and Systems for
  Video Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVTokenFlow: High-quality 4D Content Generation using Multiview Token
  Flow <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhuo Huang, Yuan Liu, Ge Zheng, Jiepeng Wang, Zhiyang Dou, Sibei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present MVTokenFlow for high-quality 4D content creation
from monocular videos. Recent advancements in generative models such as video
diffusion models and multiview diffusion models enable us to create videos or
3D models. However, extending these generative models for dynamic 4D content
creation is still a challenging task that requires the generated content to be
consistent spatially and temporally. To address this challenge, MVTokenFlow
utilizes the multiview diffusion model to generate multiview images on
different timesteps, which attains spatial consistency across different
viewpoints and allows us to reconstruct a reasonable coarse 4D field. Then,
MVTokenFlow further regenerates all the multiview images using the rendered 2D
flows as guidance. The 2D flows effectively associate pixels from different
timesteps and improve the temporal consistency by reusing tokens in the
regeneration process. Finally, the regenerated images are spatiotemporally
consistent and utilized to refine the coarse 4D field to get a high-quality 4D
field. Experiments demonstrate the effectiveness of our design and show
significantly improved quality than baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://soolab.github.io/MVTokenFlow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskGWM: A Generalizable Driving World Model with Video Mask
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models that forecast environmental changes from actions are vital for
autonomous driving models with strong generalization. The prevailing driving
world model mainly build on video prediction model. Although these models can
produce high-fidelity video sequences with advanced diffusion-based generator,
they are constrained by their predictive duration and overall generalization
capabilities. In this paper, we explore to solve this problem by combining
generation loss with MAE-style feature-level context learning. In particular,
we instantiate this target with three key design: (1) A more scalable Diffusion
Transformer (DiT) structure trained with extra mask construction task. (2) we
devise diffusion-related mask tokens to deal with the fuzzy relations between
mask reconstruction and generative diffusion process. (3) we extend mask
construction task to spatial-temporal domain by utilizing row-wise mask for
shifted self-attention rather than masked self-attention in MAE. Then, we adopt
a row-wise cross-view module to align with this mask design. Based on above
improvement, we propose MaskGWM: a Generalizable driving World Model embodied
with Video Mask reconstruction. Our model contains two variants: MaskGWM-long,
focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view
generation. Comprehensive experiments on standard benchmarks validate the
effectiveness of the proposed method, which contain normal validation of
Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot
validation of Waymo dataset. Quantitative metrics on these datasets show our
method notably improving state-of-the-art driving world model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Image to Video Generation with Language Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Villar-Corrales, Gjergj Plepi, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and flexible world models are crucial for autonomous systems to
understand their environment and predict future events. Object-centric models,
with structured latent spaces, have shown promise in modeling object dynamics
and interactions, but often face challenges in scaling to complex datasets and
incorporating external guidance, limiting their applicability in robotics. To
address these limitations, we propose TextOCVP, an object-centric model for
image-to-video generation guided by textual descriptions. TextOCVP parses an
observed scene into object representations, called slots, and utilizes a
text-conditioned transformer predictor to forecast future object states and
video frames. Our approach jointly models object dynamics and interactions
while incorporating textual guidance, thus leading to accurate and controllable
predictions. Our method's structured latent space offers enhanced control over
the prediction process, outperforming several image-to-video generative
baselines. Additionally, we demonstrate that structured object-centric
representations provide superior controllability and interpretability,
facilitating the modeling of object dynamics and enabling more precise and
understandable predictions. Videos and code are available at
https://play-slot.github.io/TextOCVP/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMXU: A <span class="highlight-title">Multi-Modal</span> and Multi-X-ray Understanding <span class="highlight-title">Dataset</span> for Disease
  Progression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Mu, Zhongzhen Huang, Shengqian Qin, Yakun Zhu, Shaoting Zhang, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have shown great promise in medical
applications, particularly in visual question answering (MedVQA) and diagnosis
from medical images. However, existing datasets and models often fail to
consider critical aspects of medical diagnostics, such as the integration of
historical records and the analysis of disease progression over time. In this
paper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel
dataset for MedVQA that focuses on identifying changes in specific regions
between two patient visits. Unlike previous datasets that primarily address
single-image questions, MMXU enables multi-image questions, incorporating both
current and historical patient data. We demonstrate the limitations of current
LVLMs in identifying disease progression on MMXU-\textit{test}, even those that
perform well on traditional benchmarks. To address this, we propose a
MedRecord-Augmented Generation (MAG) approach, incorporating both global and
regional historical records. Our experiments show that integrating historical
records significantly enhances diagnostic accuracy by at least 20\%, bridging
the gap between current LVLMs and human expert performance. Additionally, we
fine-tune models with MAG on MMXU-\textit{dev}, which demonstrates notable
improvements. We hope this work could illuminate the avenue of advancing the
use of LVLMs in medical diagnostics by emphasizing the importance of historical
context in interpreting medical images. Our dataset is released at
\href{https://github.com/linjiemu/MMXU}{https://github.com/linjiemu/MMXU}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with
  Pose Guidance from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyumin Shim, Sangmin Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce GaussianMotion, a novel human rendering model
that generates fully animatable scenes aligned with textual descriptions using
Gaussian Splatting. Although existing methods achieve reasonable text-to-3D
generation of human bodies using various 3D representations, they often face
limitations in fidelity and efficiency, or primarily focus on static models
with limited pose control. In contrast, our method generates fully animatable
3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score
distillation, achieving high fidelity and efficient rendering for arbitrary
poses. By densely generating diverse random poses during optimization, our
deformable 3D human model learns to capture a wide range of natural motions
distilled from a pose-conditioned diffusion model in an end-to-end manner.
Furthermore, we propose Adaptive Score Distillation that effectively balances
realistic detail and smoothness to achieve optimal 3D results. Experimental
results demonstrate that our approach outperforms existing baselines by
producing high-quality textures in both static and animated results, and by
generating diverse 3D human models from various textual inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Out-of-Distribution Detection in Medical Imaging with
  Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dariush Lotfi, Mohammad-Ali Nikouei Mahani, Mohamad Koohi-Moghadam, Kyongtae Ty Bae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging
to ensure reliability and safety by identifying inputs outside a model's
training distribution. Existing methods often require retraining or
modifications to pre-trained models, which is impractical for clinical
applications. This study introduces a post-hoc normalizing flow-based approach
that seamlessly integrates with pre-trained models. By leveraging normalizing
flows, it estimates the likelihood of feature vectors extracted from
pre-trained models, capturing semantically meaningful representations without
relying on pixel-level statistics. The method was evaluated using the MedMNIST
benchmark and a newly curated MedOOD dataset simulating clinically relevant
distributional shifts. Performance was measured using standard OOD detection
metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses
comparing it against ten baseline methods. On MedMNIST, the proposed model
achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD,
it achieved an AUROC of 84.61%, demonstrating superior performance against
other methods. Its post-hoc nature ensures compatibility with existing clinical
workflows, addressing the limitations of previous approaches. The model and
code to build OOD datasets are available at
https://github.com/dlotfi/MedOODFlow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Attacks for Face Images Against Fine-Tuned Latent
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauritz Christian Holme, Anton Mosquera Storgaard, Siavash Arjomand Bigdeli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of generative image models leads to privacy concerns when it comes
to the huge datasets used to train such models. This paper investigates the
possibility of inferring if a set of face images was used for fine-tuning a
Latent Diffusion Model (LDM). A Membership Inference Attack (MIA) method is
presented for this task. Using generated auxiliary data for the training of the
attack model leads to significantly better performance, and so does the use of
watermarks. The guidance scale used for inference was found to have a
significant influence. If a LDM is fine-tuned for long enough, the text prompt
used for inference has no significant influence. The proposed MIA is found to
be viable in a realistic black-box setup against LDMs fine-tuned on
face-images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 20th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP
  2025) - Volume 2: VISAPP, pages 439-446</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Neural Rendering of LiDAR Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joni Vanherck, Brent Zoomers, Tom Mertens, Lode Jorissen, Nick Michiels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Static LiDAR scanners produce accurate, dense, colored point clouds, but
often contain obtrusive artifacts which makes them ill-suited for direct
display. We propose an efficient method to render photorealistic images of such
scans without any expensive preprocessing or training of a scene-specific
model. A naive projection of the point cloud to the output view using 1x1
pixels is fast and retains the available detail, but also results in
unintelligible renderings as background points leak in between the foreground
pixels. The key insight is that these projections can be transformed into a
realistic result using a deep convolutional model in the form of a U-Net, and a
depth-based heuristic that prefilters the data. The U-Net also handles
LiDAR-specific problems such as missing parts due to occlusion, color
inconsistencies and varying point densities. We also describe a method to
generate synthetic training data to deal with imperfectly-aligned ground truth
images. Our method achieves real-time rendering rates using an off-the-shelf
GPU and outperforms the state-of-the-art in both speed and quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iMOVE: Instance-Motion-Aware Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaze Li, Yaya Shi, Zongyang Ma, Haoran Xu, Feng Cheng, Huihui Xiao, Ruiwen Kang, Fan Yang, Tingting Gao, Di Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the fine-grained instance spatiotemporal motion perception
capabilities of Video Large Language Models is crucial for improving their
temporal and general video understanding. However, current models struggle to
perceive detailed and complex instance motions. To address these challenges, we
have made improvements from both data and model perspectives. In terms of data,
we have meticulously curated iMOVE-IT, the first large-scale
instance-motion-aware video instruction-tuning dataset. This dataset is
enriched with comprehensive instance motion annotations and spatiotemporal
mutual-supervision tasks, providing extensive training for the model's
instance-motion-awareness. Building on this foundation, we introduce iMOVE, an
instance-motion-aware video foundation model that utilizes Event-aware
Spatiotemporal Efficient Modeling to retain informative instance spatiotemporal
motion details while maintaining computational efficiency. It also incorporates
Relative Spatiotemporal Position Tokens to ensure awareness of instance
spatiotemporal positions. Evaluations indicate that iMOVE excels not only in
video temporal understanding and general video understanding but also
demonstrates significant advantages in long-term video understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis
  from Japanese Haiku <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunan Yu, Yidong Han, Chaotao Ding, Ying Zang, Lanyun Zhu, Xinhao Chen, Zejian Li, Renjun Xu, Tianrun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of the metaverse, where immersive technologies redefine human
experiences, translating abstract literary concepts into navigable 3D
environments presents a fundamental challenge in preserving semantic and
emotional fidelity. This research introduces HaikuVerse, a novel framework for
transforming poetic abstraction into spatial representation, with Japanese
Haiku serving as an ideal test case due to its sophisticated encapsulation of
profound emotions and imagery within minimal text. While existing text-to-3D
methods struggle with nuanced interpretations, we present a literary-guided
approach that synergizes traditional poetry analysis with advanced generative
technologies. Our framework centers on two key innovations: (1) Hierarchical
Literary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both
explicit imagery and implicit emotional resonance through structured semantic
decomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage
pipeline that systematically transforms poetic elements into coherent 3D scenes
through sequential diffusion processes, geometric optimization, and real-time
enhancement. Extensive experiments demonstrate that HaikuVerse significantly
outperforms conventional text-to-3D approaches in both literary fidelity and
visual quality, establishing a new paradigm for preserving cultural heritage in
immersive digital spaces. Project website at:
https://syllables-to-scenes.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, submitted to IJCAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Trustworthy Anomaly Detection for Critical Applications
  through Approximated Partial AUC Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Bougaham, Benoît Frénay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly Detection is a crucial step for critical applications such in the
industrial, medical or cybersecurity domains. These sectors share the same
requirement of handling differently the different types of classification
errors. Indeed, even if false positives are acceptable, false negatives are
not, because it would reflect a missed detection of a quality issue, a disease
or a cyber threat. To fulfill this requirement, we propose a method that
dynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A
binary classifier is trained to optimize the specific range of the AUC ROC
curve that prevents the True Positive Rate (TPR) to reach 100% while minimizing
the False Positive Rate (FPR). The optimal threshold that does not trigger any
false negative is then kept and used at the test step. The results show a TPR
of 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR
improvement of 4.3% for a FPR cost of 12.2% against other state-of-the-art
methods. The code is available at https://github.com/ArnaudBougaham/tapAUC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurgPose: a <span class="highlight-title">Dataset</span> for Articulated Robotic Surgical Tool Pose
  Estimation and Tracking <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Wu, Adam Schmidt, Randy Moore, Haoying Zhou, Alexandre Banks, Peter Kazanzides, Septimiu E. Salcudean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient surgical robotic tool pose estimation is of
fundamental significance to downstream applications such as augmented reality
(AR) in surgical training and learning-based autonomous manipulation. While
significant advancements have been made in pose estimation for humans and
animals, it is still a challenge in surgical robotics due to the scarcity of
published data. The relatively large absolute error of the da Vinci end
effector kinematics and arduous calibration procedure make calibrated
kinematics data collection expensive. Driven by this limitation, we collected a
dataset, dubbed SurgPose, providing instance-aware semantic keypoints and
skeletons for visual surgical tool pose estimation and tracking. By marking
keypoints using ultraviolet (UV) reactive paint, which is invisible under white
light and fluorescent under UV light, we execute the same trajectory under
different lighting conditions to collect raw videos and keypoint annotations,
respectively. The SurgPose dataset consists of approximately 120k surgical
instrument instances (80k for training and 40k for validation) of 6 categories.
Each instrument instance is labeled with 7 semantic keypoints. Since the videos
are collected in stereo pairs, the 2D pose can be lifted to 3D based on
stereo-matching depth. In addition to releasing the dataset, we test a few
baseline approaches to surgical instrument tracking to demonstrate the utility
of SurgPose. More details can be found at surgpose.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-CLIP: Decoupling Category and Style Guidance in CLIP for
  Specific-Domain Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexi Jia, Chuanwei Huang, Hongyan Fei, Yeshuang Zhu, Zhiqiang Yuan, Jinchao Zhang, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have shown remarkable capabilities of
generating high-quality images closely aligned with textual inputs. However,
the effectiveness of text guidance heavily relies on the CLIP text encoder,
which is trained to pay more attention to general content but struggles to
capture semantics in specific domains like styles. As a result, generation
models tend to fail on prompts like "a photo of a cat in Pokemon style" in
terms of simply producing images depicting "a photo of a cat". To fill this
gap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that
enables the CLIP model to learn the meaning of category and style in a
complement manner. With specially designed fine-tuning tasks on minimal data
and a modified cross-attention mechanism, Control-CLIP can precisely guide the
diffusion model to a specific domain. Moreover, the parameters of the diffusion
model remain unchanged at all, preserving the original generation performance
and diversity. Experiments across multiple domains confirm the effectiveness of
our approach, particularly highlighting its robust plug-and-play capability in
generating content with various specific styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SayAnything: Audio-Driven Lip Synchronization with Conditional Video
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxian Ma, Shiwen Wang, Jian Yang, Junyi Hu, Jian Liang, Guosheng Lin, Jingbo chen, Kai Li, Yu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have led to significant progress in
audio-driven lip synchronization. However, existing methods typically rely on
constrained audio-visual alignment priors or multi-stage learning of
intermediate representations to force lip motion synthesis. This leads to
complex training pipelines and limited motion naturalness. In this paper, we
present SayAnything, a conditional video diffusion framework that directly
synthesizes lip movements from audio input while preserving speaker identity.
Specifically, we propose three specialized modules including identity
preservation module, audio guidance module, and editing control module. Our
novel design effectively balances different condition signals in the latent
space, enabling precise control over appearance, motion, and region-specific
generation without requiring additional supervision signals or intermediate
representations. Extensive experiments demonstrate that SayAnything generates
highly realistic videos with improved lip-teeth coherence, enabling unseen
characters to say anything, while effectively generalizing to animated
characters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Pruning in <span class="highlight-title">Multimodal</span> Large Language Models: Are We Solving the
  Right Problem? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown remarkable performance
for cross-modal understanding and generation, yet still suffer from severe
inference costs. Recently, abundant works have been proposed to solve this
problem with token pruning, which identifies the redundant tokens in MLLMs and
then prunes them to reduce the computation and KV storage costs, leading to
significant acceleration without training. While these methods claim efficiency
gains, critical questions about their fundamental design and evaluation remain
unanswered: Why do many existing approaches underperform even compared to naive
random token selection? Are attention-based scoring sufficient for reliably
identifying redundant tokens? Is language information really helpful during
token pruning? What makes a good trade-off between token importance and
duplication? Are current evaluation protocols comprehensive and unbiased? The
ignorance of previous research on these problems hinders the long-term
development of token pruning. In this paper, we answer these questions one by
one, providing insights into the design of future token pruning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stop Looking for Important Tokens in <span class="highlight-title">Multimodal</span> Language Models:
  Duplication Matters More 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision tokens in multimodal large language models often dominate huge
computational overhead due to their excessive length compared to linguistic
modality. Abundant recent methods aim to solve this problem with token pruning,
which first defines an importance criterion for tokens and then prunes the
unimportant vision tokens during inference. However, in this paper, we show
that the importance is not an ideal indicator to decide whether a token should
be pruned. Surprisingly, it usually results in inferior performance than random
token pruning and leading to incompatibility to efficient attention computation
operators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),
which prunes tokens based on its duplication with other tokens, leading to
significant and training-free acceleration. Concretely, DART selects a small
subset of pivot tokens and then retains the tokens with low duplication to the
pivots, ensuring minimal information loss during token pruning. Experiments
demonstrate that DART can prune 88.9% vision tokens while maintaining
comparable performance, leading to a 1.99$\times$ and 2.99$\times$ speed-up in
total time and prefilling stage, respectively, with good compatibility to
efficient attention operators. Our codes are available at
https://github.com/ZichenWen1/DART.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Vision Language Models Struggle with Visual Arithmetic? Towards
  Enhanced Chart and Geometry Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have achieved remarkable progress in multimodal
tasks, yet they often struggle with visual arithmetic, seemingly simple
capabilities like object counting or length comparison, which are essential for
relevant complex tasks like chart understanding and geometric reasoning. In
this work, we first investigate the root causes of this deficiency through a
suite of probing tasks focusing on basic visual arithmetic. Our analysis
reveals that while pre-trained vision encoders typically capture sufficient
information, the text decoder often fails to decode it correctly for arithmetic
reasoning. To address this, we propose CogAlign, a novel post-training strategy
inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to
recognize invariant properties under visual transformations. We demonstrate
that this approach significantly improves the performance of three diverse VLMs
on our proposed probing tasks. Furthermore, CogAlign enhances performance by an
average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching
supervised fine-tuning methods while requiring only 60% less training data.
These results highlight the effectiveness and generalizability of CogAlign in
improving fundamental visual arithmetic capabilities and their transfer to
downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxiang Cui, Zhongyu Li, Xiayue Fan, Peng Huang, Ying Wang, Meng Yang, Shi Chang, Jihua Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intersection of medical imaging and artificial intelligence has become an
important research direction in intelligent medical treatment, particularly in
the analysis of medical images using deep learning for clinical diagnosis.
Despite the advances, existing keyframe classification methods lack extraction
of time series features, while ultrasonic video classification based on
three-dimensional convolution requires uniform frame numbers across patients,
resulting in poor feature extraction efficiency and model classification
performance. This study proposes a novel video classification method based on
CNN and LSTM, introducing NLP's long and short sentence processing scheme into
video classification for the first time. The method reduces CNN-extracted image
features to 1x512 dimension, followed by sorting and compressing feature
vectors for LSTM training. Specifically, feature vectors are sorted by patient
video frame numbers and populated with padding value 0 to form variable
batches, with invalid padding values compressed before LSTM training to
conserve computing resources. Experimental results demonstrate that our
variable-frame CNNLSTM method outperforms other approaches across all metrics,
showing improvements of 3-6% in F1 score and 1.5% in specificity compared to
keyframe methods. The variable-frame CNNLSTM also achieves better accuracy and
precision than equal-frame CNNLSTM. These findings validate the effectiveness
of our approach in classifying variable-frame ultrasound videos and suggest
potential applications in other medical imaging modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Sample Effective and Diverse Prompts for Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, Ling Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image diffusion models have achieved impressive
image generation capabilities. However, it remains challenging to control the
generation process with desired properties (e.g., aesthetic quality, user
intention), which can be expressed as black-box reward functions. In this
paper, we focus on prompt adaptation, which refines the original prompt into
model-preferred prompts to generate desired images. While prior work uses
reinforcement learning (RL) to optimize prompts, we observe that applying RL
often results in generating similar postfixes and deterministic behaviors. To
this end, we introduce \textbf{P}rompt \textbf{A}daptation with
\textbf{G}FlowNets (\textbf{PAG}), a novel approach that frames prompt
adaptation as a probabilistic inference problem. Our key insight is that
leveraging Generative Flow Networks (GFlowNets) allows us to shift from reward
maximization to sampling from an unnormalized density function, enabling both
high-quality and diverse prompt generation. However, we identify that a naive
application of GFlowNets suffers from mode collapse and uncovers a previously
overlooked phenomenon: the progressive loss of neural plasticity in the model,
which is compounded by inefficient credit assignment in sequential prompt
generation. To address this critical challenge, we develop a systematic
approach in PAG with flow reactivation, reward-prioritized sampling, and reward
decomposition for prompt adaptation. Extensive experiments validate that PAG
successfully learns to sample effective and diverse prompts for text-to-image
generation. We also show that PAG exhibits strong robustness across various
reward functions and transferability to different text-to-image models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 14 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically Robust Unsupervised Image Translation for Paired Remote
  Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Fang, Kaiyu Li, Zhe Li, Jianli Zhao, Xingli Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image translation for change detection or classification in bi-temporal
remote sensing images is unique. Although it can acquire paired images, it is
still unsupervised. Moreover, strict semantic preservation in translation is
always needed instead of multimodal outputs. In response to these problems,
this paper proposes a new method, SRUIT (Semantically Robust Unsupervised
Image-to-image Translation), which ensures semantically robust translation and
produces deterministic output. Inspired by previous works, the method explores
the underlying characteristics of bi-temporal Remote Sensing images and designs
the corresponding networks. Firstly, we assume that bi-temporal Remote Sensing
images share the same latent space, for they are always acquired from the same
land location. So SRUIT makes the generators share their high-level layers, and
this constraint will compel two domain mapping to fall into the same latent
space. Secondly, considering land covers of bi-temporal images could evolve
into each other, SRUIT exploits the cross-cycle-consistent adversarial networks
to translate from one to the other and recover them. Experimental results show
that constraints of sharing weights and cross-cycle consistency enable
translated images with both good perceptual image quality and semantic
preservation for significant differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning
  Network for Semi-supervised 3D Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyan Wang, Kechen Song, Yuyuan Liu, Shuai Ma, Yunhui Yan, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised 3D medical image segmentation aims to achieve accurate
segmentation using few labelled data and numerous unlabelled data. The main
challenge in the design of semi-supervised learning methods consists in the
effective use of the unlabelled data for training. A promising solution
consists of ensuring consistent predictions across different views of the data,
where the efficacy of this strategy depends on the accuracy of the
pseudo-labels generated by the model for this consistency learning strategy. In
this paper, we introduce a new methodology to produce high-quality
pseudo-labels for a consistency learning strategy to address semi-supervised 3D
medical image segmentation. The methodology has three important contributions.
The first contribution is the Cooperative Rectification Learning Network (CRLN)
that learns multiple prototypes per class to be used as external knowledge
priors to adaptively rectify pseudo-labels at the voxel level. The second
contribution consists of the Dynamic Interaction Module (DIM) to facilitate
pairwise and cross-class interactions between prototypes and multi-resolution
image features, enabling the production of accurate voxel-level clues for
pseudo-label rectification. The third contribution is the Cooperative Positive
Supervision (CPS), which optimises uncertain representations to align with
unassertive representations of their class distributions, improving the model's
accuracy in classifying uncertain regions. Extensive experiments on three
public 3D medical segmentation datasets demonstrate the effectiveness and
superiority of our semi-supervised learning method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Registration Meets Vision Foundation Model: Prototype
  Learning and Contour Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xu, Tengfei Xue, Jianan Fan, Dongnan Liu, Yuqian Chen, Fan Zhang, Carl-Fredrik Westin, Ron Kikinis, Lauren J. O'Donnell, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image registration is a fundamental task in medical image analysis,
aiming to establish spatial correspondences between paired images. However,
existing unsupervised deformable registration methods rely solely on
intensity-based similarity metrics, lacking explicit anatomical knowledge,
which limits their accuracy and robustness. Vision foundation models, such as
the Segment Anything Model (SAM), can generate high-quality segmentation masks
that provide explicit anatomical structure knowledge, addressing the
limitations of traditional methods that depend only on intensity similarity.
Based on this, we propose a novel SAM-assisted registration framework
incorporating prototype learning and contour awareness. The framework includes:
(1) Explicit anatomical information injection, where SAM-generated segmentation
masks are used as auxiliary inputs throughout training and testing to ensure
the consistency of anatomical information; (2) Prototype learning, which
leverages segmentation masks to extract prototype features and aligns
prototypes to optimize semantic correspondences between images; and (3)
Contour-aware loss, a contour-aware loss is designed that leverages the edges
of segmentation masks to improve the model's performance in fine-grained
deformation fields. Extensive experiments demonstrate that the proposed
framework significantly outperforms existing methods across multiple datasets,
particularly in challenging scenarios with complex anatomical structures and
ambiguous boundaries. Our code is available at
https://github.com/HaoXu0507/IPMI25-SAM-Assisted-Registration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Information Processing in Medical Imaging (IPMI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do we Really Need Visual Instructions? Towards Visual Instruction-Free
  Fine-tuning for Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning has become the predominant technology in eliciting
the multimodal task-solving capabilities of large vision-language models
(LVLMs). Despite the success, as visual instructions require images as the
input, it would leave the gap in inheriting the task-solving capabilities from
the backbone LLMs, and make it costly to collect a large-scale dataset. To
address it, we propose ViFT, a visual instruction-free fine-tuning framework
for LVLMs. In ViFT, we only require the text-only instructions and image
caption data during training, to separately learn the task-solving and visual
perception abilities. During inference, we extract and combine the
representations of the text and image inputs, for fusing the two abilities to
fulfill multimodal tasks. Experimental results demonstrate that ViFT can
achieve state-of-the-art performance on several visual reasoning and visual
instruction following benchmarks, with rather less training data. Our code and
data will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise GPS-Denied UAV Self-Positioning via Context-Enhanced Cross-View
  Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanze Xu, Ming Dai, Wenxiao Cai, Wankou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval has been employed as a robust complementary technique to
address the challenge of Unmanned Aerial Vehicles (UAVs) self-positioning.
However, most existing methods primarily focus on localizing objects captured
by UAVs through complex part-based representations, often overlooking the
unique challenges associated with UAV self-positioning, such as fine-grained
spatial discrimination requirements and dynamic scene variations. To address
the above issues, we propose the Context-Enhanced method for precise UAV
Self-Positioning (CEUSP), specifically designed for UAV self-positioning tasks.
CEUSP integrates a Dynamic Sampling Strategy (DSS) to efficiently select
optimal negative samples, while the Rubik's Cube Attention (RCA) module,
combined with the Context-Aware Channel Integration (CACI) module, enhances
feature representation and discrimination by exploiting interdimensional
interactions, inspired by the rotational mechanics of a Rubik's Cube. Extensive
experimental validate the effectiveness of the proposed method, demonstrating
notable improvements in feature representation and UAV self-positioning
accuracy within complex urban environments. Our approach achieves
state-of-the-art performance on the DenseUAV dataset, which is specifically
designed for dense urban contexts, and also delivers competitive results on the
widely recognized University-1652 benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARS: Mesh AutoRegressive Model for 3D Shape Detailization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingnan Gao, Weizhe Liu, Weixuan Sun, Senbo Wang, Xibin Song, Taizhang Shang, Shenzhou Chen, Hongdong Li, Xiaokang Yang, Yichao Yan, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for mesh detailization predominantly utilize
Generative Adversarial Networks (GANs) to generate detailed meshes from coarse
ones. These methods typically learn a specific style code for each category or
similar categories without enforcing geometry supervision across different
Levels of Detail (LODs). Consequently, such methods often fail to generalize
across a broader range of categories and cannot ensure shape consistency
throughout the detailization process. In this paper, we introduce MARS, a novel
approach for 3D shape detailization. Our method capitalizes on a novel
multi-LOD, multi-category mesh representation to learn shape-consistent mesh
representations in latent space across different LODs. We further propose a
mesh autoregressive model capable of generating such latent representations
through next-LOD token prediction. This approach significantly enhances the
realism of the generated shapes. Extensive experiments conducted on the
challenging 3D Shape Detailization benchmark demonstrate that our proposed MARS
model achieves state-of-the-art performance, surpassing existing methods in
both qualitative and quantitative assessments. Notably, the model's capability
to generate fine-grained details while preserving the overall shape integrity
is particularly commendable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Physics-Informed Blur Learning Framework for Imaging Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqun Chen, Yuxuan Li, Jun Dai, Jinwei Gu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate blur estimation is essential for high-performance imaging across
various applications. Blur is typically represented by the point spread
function (PSF). In this paper, we propose a physics-informed PSF learning
framework for imaging systems, consisting of a simple calibration followed by a
learning process. Our framework could achieve both high accuracy and universal
applicability. Inspired by the Seidel PSF model for representing spatially
varying PSF, we identify its limitations in optimization and introduce a novel
wavefront-based PSF model accompanied by an optimization strategy, both
reducing optimization complexity and improving estimation accuracy. Moreover,
our wavefront-based PSF model is independent of lens parameters, eliminate the
need for prior knowledge of the lens. To validate our approach, we compare it
with recent PSF estimation methods (Degradation Transfer and Fast Two-step)
through a deblurring task, where all the estimated PSFs are used to train
state-of-the-art deblurring algorithms. Our approach demonstrates improvements
in image quality in simulation and also showcases noticeable visual quality
improvements on real captured images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for
  UAV-View Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UAV-View Geo-Localization (UVGL) aims to ascertain the precise location of a
UAV by retrieving the most similar GPS-tagged satellite image. However,
existing methods predominantly rely on supervised learning paradigms that
necessitate annotated paired data for training, which incurs substantial
annotation costs and impedes large-scale deployment. To overcome this
limitation, we propose the Dynamic Memory-Driven and Neighborhood Information
Learning (DMNIL) network, a lightweight end-to-end self-supervised framework
for UAV-view geo-localization. The DMNIL framework utilizes a dual-path
clustering-based contrastive learning architecture as its baseline to model
intra-view structural relationships, enhancing feature consistency and
discriminability. Additionally, a dynamic memory-driven hierarchical learning
module is proposed to progressively mine local and global information,
reinforcing multi-level feature associations to improve model robustness. To
bridge the domain gap between UAV and satellite views, we design an
information-consistent evolutionary learning mechanism that systematically
explores latent correlations within intra-view neighborhoods and across
cross-view domains, ultimately constructing a unified cross-view feature
representation space. Extensive experiments on three benchmarks
(University-1652, SUES-200, and DenseUAV) demonstrate that DMNIL achieves
competitive performance against state-of-the-art supervised methods while
maintaining computational efficiency. Notably, this superiority is attained
without relying on paired training data, underscoring the framework's
practicality for real-world deployment. Codes will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GeoDANO, a geometric vision-language model (VLM) with a
domain-agnostic vision encoder, for solving plane geometry problems. Although
VLMs have been employed for solving geometry problems, their ability to
recognize geometric features remains insufficiently analyzed. To address this
gap, we propose a benchmark that evaluates the recognition of visual geometric
features, including primitives such as dots and lines, and relations such as
orthogonality. Our preliminary study shows that vision encoders often used in
general-purpose VLMs, e.g., OpenCLIP, fail to detect these features and
struggle to generalize across domains. We develop GeoCLIP, a CLIP based model
trained on synthetic geometric diagram-caption pairs to overcome the
limitation. Benchmark results show that GeoCLIP outperforms existing vision
encoders in recognizing geometric features. We then propose our VLM, GeoDANO,
which augments GeoCLIP with a domain adaptation strategy for unseen diagram
styles. GeoDANO outperforms specialized methods for plane geometry problems and
GPT-4o on MathVerse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld
  Radio<span class="highlight-title">graph</span>ic Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunyi Zhou, Kun Shi, Gang Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiographic testing is a fundamental non-destructive evaluation technique
for identifying weld defects and assessing quality in industrial applications
due to its high-resolution imaging capabilities. Over the past decade, deep
learning techniques have significantly advanced weld defect identification in
radiographic images. However, conventional approaches, which rely on training
small-scale, task-specific models on single-scenario datasets, exhibit poor
cross-scenario generalization. Recently, the Segment Anything Model (SAM), a
pre-trained visual foundation model trained on large-scale datasets, has
demonstrated exceptional zero-shot generalization capabilities. Fine-tuning SAM
with limited domain-specific data has yielded promising results in fields such
as medical image segmentation and anomaly detection. To the best of our
knowledge, this work is the first to introduce SAM-based segmentation for
general weld radiographic testing images. We propose WRT-SAM, a novel weld
radiographic defect segmentation model that leverages SAM through an
adapter-based integration with a specialized prompt generator architecture. To
improve adaptability to grayscale weld radiographic images, we introduce a
frequency prompt generator module, which enhances the model's sensitivity to
frequency-domain information. Furthermore, to address the multi-scale nature of
weld defects, we incorporate a multi-scale prompt generator module, enabling
the model to effectively extract and encode defect information across varying
scales. Extensive experimental evaluations demonstrate that WRT-SAM achieves a
recall of 78.87%, a precision of 84.04%, and an AUC of 0.9746, setting a new
state-of-the-art (SOTA) benchmark. Moreover, the model exhibits superior
zero-shot generalization performance, highlighting its potential for practical
deployment in diverse radiographic testing scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparison of Human and Machine Learning Errors in Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Estévez-Almenzar, Ricardo Baeza-Yates, Carlos Castillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning applications in high-stakes scenarios should always operate
under human oversight. Developing an optimal combination of human and machine
intelligence requires an understanding of their complementarities, particularly
regarding the similarities and differences in the way they make mistakes. We
perform extensive experiments in the area of face recognition and compare two
automated face recognition systems against human annotators through a
demographically balanced user study. Our research uncovers important ways in
which machine learning errors and human errors differ from each other, and
suggests potential strategies in which human-machine collaboration can improve
accuracy in face recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially private fine-tuned NF-Net to predict GI cancer type 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Venkatesh Chilukoti, Imran Hossen Md, Liqun Shan, Vijay Srinivas Tida, Xiali Hei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on global genomic status, the cancer tumor is classified as
Microsatellite Instable (MSI) and Microsatellite Stable (MSS). Immunotherapy is
used to diagnose MSI, whereas radiation and chemotherapy are used for MSS.
Therefore, it is significant to classify a gastro-intestinal (GI) cancer tumor
into MSI vs. MSS to provide appropriate treatment. The existing literature
showed that deep learning could directly predict the class of GI cancer tumors
from histological images. However, deep learning (DL) models are susceptible to
various threats, including membership inference attacks, model extraction
attacks, etc. These attacks render the use of DL models impractical in
real-world scenarios. To make the DL models useful and maintain privacy, we
integrate differential privacy (DP) with DL. In particular, this paper aims to
predict the state of GI cancer while preserving the privacy of sensitive data.
We fine-tuned the Normalizer Free Net (NF-Net) model. We obtained an accuracy
of 88.98\% without DP to predict (GI) cancer status. When we fine-tuned the
NF-Net using DP-AdamW and adaptive DP-AdamW, we got accuracies of 74.58% and
76.48%, respectively. Moreover, we investigate the Weighted Random Sampler
(WRS) and Class weighting (CW) to solve the data imbalance. We also evaluated
and analyzed the DP algorithms in different settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Whole-body Grasp Synthesis with Directional Controllability <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Paschalidis, Romana Wilschut, Dimitrije Antić, Omid Taheri, Dimitrios Tzionas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing 3D whole bodies that realistically grasp objects is useful for
animation, mixed reality, and robotics. This is challenging, because the hands
and body need to look natural w.r.t. each other, the grasped object, as well as
the local scene (i.e., a receptacle supporting the object). Moreover, training
data for this task is really scarce, while capturing new data is expensive.
Recent work goes beyond finite datasets via a divide-and-conquer approach; it
first generates a "guiding" right-hand grasp, and then searches for bodies that
match this. However, the guiding-hand synthesis lacks controllability and
receptacle awareness, so it likely has an implausible direction (i.e., a body
can't match this without penetrating the receptacle) and needs corrections
through major post-processing. Moreover, the body search needs exhaustive
sampling and is expensive. These are strong limitations. We tackle these with a
novel method called CWGrasp. Our key idea is that performing geometry-based
reasoning "early on," instead of "too late," provides rich "control" signals
for inference. To this end, CWGrasp first samples a plausible
reaching-direction vector (used later for both the arm and hand) from a
probabilistic model built via ray-casting from the object and collision
checking. Moreover, CWGrasp uniquely tackles both right and left-hand grasps.
We evaluate on the GRAB and ReplicaGrasp datasets. CWGrasp outperforms
baselines, at lower runtime and budget, while all components help performance.
Code and models are available at https://gpaschalidis.github.io/cwgrasp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Both Text and Images Leaked! A Systematic Analysis of <span class="highlight-title">Multimodal</span> LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaVILA: Legged Robot Vision-Language-Action Model for Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes to solve the problem of Vision-and-Language Navigation
with legged robots, which not only provides a flexible way for humans to
command but also allows the robot to navigate through more challenging and
cluttered scenes. However, it is non-trivial to translate human language
instructions all the way to low-level leg joint actions. We propose NaVILA, a
2-level framework that unifies a Vision-Language-Action model (VLA) with
locomotion skills. Instead of directly predicting low-level actions from VLA,
NaVILA first generates mid-level actions with spatial information in the form
of language, (e.g., "moving forward 75cm"), which serves as an input for a
visual locomotion RL policy for execution. NaVILA substantially improves
previous approaches on existing benchmarks. The same advantages are
demonstrated in our newly developed benchmarks with IsaacLab, featuring more
realistic scenes, low-level controls, and real-world robot experiments. We show
more results at https://navila-bot.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://navila-bot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generation and Detection of Sign Language Deepfakes - A Linguistic and
  Visual Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahzeb Naeem, Muhammad Riyyan Khan, Usman Tariq, Abhinav Dhall, Carlos Ivan Colon, Hasan Al-Nashash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research explores the positive application of deepfake technology for
upper body generation, specifically sign language for the Deaf and Hard of
Hearing (DHoH) community. Given the complexity of sign language and the
scarcity of experts, the generated videos are vetted by a sign language expert
for accuracy. We construct a reliable deepfake dataset, evaluating its
technical and visual credibility using computer vision and natural language
processing models. The dataset, consisting of over 1200 videos featuring both
seen and unseen individuals, is also used to detect deepfake videos targeting
vulnerable individuals. Expert annotations confirm that the generated videos
are comparable to real sign language content. Linguistic analysis, using
textual similarity scores and interpreter evaluations, shows that the
interpretation of generated videos is at least 90% similar to authentic sign
language. Visual analysis demonstrates that convincingly realistic deepfakes
can be produced, even for new subjects. Using a pose/style transfer model, we
pay close attention to detail, ensuring hand movements are accurate and align
with the driving video. We also apply machine learning algorithms to establish
a baseline for deepfake detection on this dataset, contributing to the
detection of fraudulent sign language videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures, IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL
  SYSTEM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLEAR: Character Unlearning in Textual and Visual Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) is critical for removing private or hazardous
information from deep learning models. While MU has advanced significantly in
unimodal (text or vision) settings, multimodal unlearning (MMU) remains
underexplored due to the lack of open benchmarks for evaluating cross-modal
data removal. To address this gap, we introduce CLEAR, the first open-source
benchmark designed specifically for MMU. CLEAR contains 200 fictitious
individuals and 3,700 images linked with corresponding question-answer pairs,
enabling a thorough evaluation across modalities. We conduct a comprehensive
analysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four
evaluation sets, demonstrating that jointly unlearning both modalities
outperforms single-modality approaches. The dataset is available at
https://huggingface.co/datasets/therem/CLEAR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision CNNs trained to estimate spatial latents learned similar
  ventral-stream-aligned representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies of the functional role of the primate ventral visual stream have
traditionally focused on object categorization, often ignoring -- despite much
prior evidence -- its role in estimating "spatial" latents such as object
position and pose. Most leading ventral stream models are derived by optimizing
networks for object categorization, which seems to imply that the ventral
stream is also derived under such an objective. Here, we explore an alternative
hypothesis: Might the ventral stream be optimized for estimating spatial
latents? And a closely related question: How different -- if at all -- are
representations learned from spatial latent estimation compared to
categorization? To ask these questions, we leveraged synthetic image datasets
generated by a 3D graphic engine and trained convolutional neural networks
(CNNs) to estimate different combinations of spatial and category latents. We
found that models trained to estimate just a few spatial latents achieve neural
alignment scores comparable to those trained on hundreds of categories, and the
spatial latent performance of models strongly correlates with their neural
alignment. Spatial latent and category-trained models have very similar -- but
not identical -- internal representations, especially in their early and middle
layers. We provide evidence that this convergence is partly driven by
non-target latent variability in the training data, which facilitates the
implicit learning of representations of those non-target latents. Taken
together, these results suggest that many training objectives, such as spatial
latents, can lead to similar models aligned neurally with the ventral stream.
Thus, one should not assume that the ventral stream is optimized for object
categorization only. As a field, we need to continue to sharpen our measures of
comparing models to brains to better understand the functional roles of the
ventral stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 21 figures, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Figurative Meaning through Explainable Visual Entailment <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in
tasks requiring a fine-grained understanding of literal meaning in images and
text, such as visual question-answering or visual entailment. However, there
has been little exploration of the capabilities of these models when presented
with images and captions containing figurative meaning, such as metaphors or
humor. To close this gap, we propose a new task framing the figurative meaning
understanding problem as an explainable visual entailment task, where the model
has to predict whether the image (premise) entails a caption (hypothesis) and
justify the predicted label with a textual explanation. The figurative
phenomena can be present in the image, in the caption, or both. Using a
human-AI collaboration approach, we build the accompanying expert-verified
dataset V-FLUTE, containing 6,027 {image, caption, label, explanation}
instances spanning five diverse figurative phenomena: metaphors, similes,
idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs
struggle to generalize from literal to figurative meaning, particularly when it
is present in images. Further, we identify common types of errors in VLM
reasoning (hallucination and incomplete or unsound reasoning) across classes of
models via human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthGPT: A Medical Large Vision-Language Model for Unifying
  Comprehension and Generation via Heterogeneous Knowledge Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HealthGPT, a powerful Medical Large Vision-Language Model
(Med-LVLM) that integrates medical visual comprehension and generation
capabilities within a unified autoregressive paradigm. Our bootstrapping
philosophy is to progressively adapt heterogeneous comprehension and generation
knowledge to pre-trained large language models (LLMs). This is achieved through
a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is
complemented by a tailored hierarchical visual perception approach and a
three-stage learning strategy. To effectively learn the HealthGPT, we devise a
comprehensive medical domain-specific comprehension and generation dataset
called VL-Health. Experimental results demonstrate exceptional performance and
scalability of HealthGPT in medical visual unified tasks. Our project can be
accessed at https://github.com/DCDmllm/HealthGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: added project page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advances in <span class="highlight-title">Multimodal</span> Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View
  Gaussian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onat Şahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly improved 3D
generation, enabling the use of assets generated from an image for embodied AI
simulations. However, the one-to-many nature of the image-to-3D problem limits
their use due to inconsistent content and quality across views. Previous models
optimize a 3D model by sampling views from a view-conditioned diffusion prior,
but diffusion models cannot guarantee view consistency. Instead, we present
ConsistentDreamer, where we first generate a set of fixed multi-view prior
images and sample random views between them with another diffusion model
through a score distillation sampling (SDS) loss. Thereby, we limit the
discrepancies between the views guided by the SDS loss and ensure a consistent
rough shape. In each iteration, we also use our generated multi-view prior
images for fine-detail reconstruction. To balance between the rough shape and
the fine-detail optimizations, we introduce dynamic task-dependent weights
based on homoscedastic uncertainty, updated automatically in each iteration.
Additionally, we employ opacity, depth distortion, and normal alignment losses
to refine the surface for mesh extraction. Our method ensures better view
consistency and visual quality compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted by Pattern Recognition Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large <span class="highlight-title">Multimodal</span> Models Solve Caption Generation for Scientific
  Figures? Lessons Learned from SCICAP Challenge 2023 <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the SCICAP datasets launch in 2021, the research community has made
significant progress in generating captions for scientific figures in scholarly
articles. In 2023, the first SCICAP Challenge took place, inviting global teams
to use an expanded SCICAP dataset to develop models for captioning diverse
figure types across various academic fields. At the same time, text generation
models advanced quickly, with many powerful pre-trained large multimodal models
(LMMs) emerging that showed impressive capabilities in various
vision-and-language tasks. This paper presents an overview of the first SCICAP
Challenge and details the performance of various models on its data, capturing
a snapshot of the fields state. We found that professional editors
overwhelmingly preferred figure captions generated by GPT-4V over those from
all other models and even the original captions written by authors. Following
this key finding, we conducted detailed analyses to answer this question: Have
advanced LMMs solved the task of generating captions for scientific figures?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Compressed Image Latents and <span class="highlight-title">Multimodal</span> Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever study of adapting compressed image latents
to suit the needs of downstream vision tasks that adopt Multimodal Large
Language Models (MLLMs). MLLMs have extended the success of large language
models to modalities (e.g. images) beyond text, but their billion scale hinders
deployment on resource-constrained end devices. While cloud-hosted MLLMs could
be available, transmitting raw, uncompressed images captured by end devices to
the cloud requires an efficient image compression system. To address this, we
focus on emerging neural image compression and propose a novel framework with a
lightweight transform-neck and a surrogate loss to adapt compressed image
latents for MLLM-based vision tasks. Given the huge scale of MLLMs, our
framework excludes the entire downstream MLLM except part of its visual encoder
from training our system. This stands out from most existing coding for machine
approaches that involve downstream networks in training and thus could be
impractical when the networks are MLLMs. The proposed framework is general in
that it is applicable to various MLLMs, neural image codecs, and multiple
application scenarios, where the neural image codec can be (1) pre-trained for
human perception without updating, (2) fully updated for joint human and
machine perception, or (3) fully updated for only machine perception. Extensive
experiments on different neural image codecs and various MLLMs show that our
method achieves great rate-accuracy performance with much less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iFormer: Integrating ConvNet and <span class="highlight-title">Transformer</span> for Mobile Application <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new family of mobile hybrid vision networks, called iFormer,
with a focus on optimizing latency and accuracy on mobile applications. iFormer
effectively integrates the fast local representation capacity of convolution
with the efficient global modeling ability of self-attention. The local
interactions are derived from transforming a standard convolutional network,
\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly
introduced mobile modulation attention removes memory-intensive operations in
MHA and employs an efficient modulation mechanism to boost dynamic global
representational capacity. We conduct comprehensive experiments demonstrating
that iFormer outperforms existing lightweight networks across various tasks.
Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\% on ImageNet-1k
with a latency of only 1.10 ms on an iPhone 13, surpassing the recently
proposed MobileNetV4 under similar latency constraints. Additionally, our
method shows significant improvements in downstream tasks, including COCO
object detection, instance segmentation, and ADE20k semantic segmentation,
while still maintaining low latency on mobile devices for high-resolution
inputs in these scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code:
  https://github.com/ChuanyangZheng/iFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Long Videos with <span class="highlight-title">Multimodal</span> Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16998v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16998v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have allowed recent LLM-based approaches to
achieve excellent performance on long-video understanding benchmarks. We
investigate how extensive world knowledge and strong reasoning skills of
underlying LLMs influence this strong performance. Surprisingly, we discover
that LLM-based approaches can yield surprisingly good accuracy on long-video
tasks with limited video information, sometimes even with no video specific
information. Building on this, we exploring injecting video-specific
information into an LLM-based framework. We utilize off-the-shelf vision tools
to extract three object-centric information modalities from videos and then
leverage natural language as a medium for fusing this information. Our
resulting Multimodal Video Understanding (MVU) framework demonstrates
state-of-the-art performance across multiple video understanding benchmarks.
Strong performance also on robotics domain tasks establish its strong
generality. Our code will be released publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/kahnchana/mvu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of End-to-End Continuous Spanish Lipreading in Different Data
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual speech recognition remains an open research problem where different
challenges must be considered by dispensing with the auditory sense, such as
visual ambiguities, the inter-personal variability among speakers, and the
complex modeling of silence. Nonetheless, recent remarkable results have been
achieved in the field thanks to the availability of large-scale databases and
the use of powerful attention mechanisms. Besides, multiple languages apart
from English are nowadays a focus of interest. This paper presents noticeable
advances in automatic continuous lipreading for Spanish. First, an end-to-end
system based on the hybrid CTC/Attention architecture is presented. Experiments
are conducted on two corpora of disparate nature, reaching state-of-the-art
results that significantly improve the best performance obtained to date for
both databases. In addition, a thorough ablation study is carried out, where it
is studied how the different components that form the architecture influence
the quality of speech recognition. Then, a rigorous error analysis is carried
out to investigate the different factors that could affect the learning of the
automatic system. Finally, a new Spanish lipreading benchmark is consolidated.
Code and trained models are available at
https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the "Language Resources and Evaluation" journal, Springer
  Nature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device
  Triggers for Insect Camera Traps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Gardiner, Sareh Rowands, Benno I. Simmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera traps, combined with AI, have emerged as a way to achieve automated,
scalable biodiversity monitoring. However, the passive infrared (PIR) sensors
that trigger camera traps are poorly suited for detecting small, fast-moving
ectotherms such as insects. Insects comprise over half of all animal species
and are key components of ecosystems and agriculture. The need for an
appropriate and scalable insect camera trap is critical in the wake of
concerning reports of declines in insect populations. This study proposes an
alternative to the PIR trigger: ultra-lightweight convolutional neural networks
running on low-powered hardware to detect insects in a continuous stream of
captured images. We train a suite of models to distinguish insect images from
backgrounds. Our design achieves zero latency between trigger and image
capture. Our models are rigorously tested and achieve high accuracy ranging
from 91.8% to 96.4% AUC on validation data and >87% AUC on data from
distributions unseen during training. The high specificity of our models
ensures minimal saving of false positive images, maximising deployment storage
efficiency. High recall scores indicate a minimal false negative rate,
maximising insect detection. Further analysis with saliency maps shows the
learned representation of our models to be robust, with low reliance on
spurious background features. Our system is also shown to operate deployed on
off-the-shelf, low-powered microcontroller units, consuming a maximum power
draw of less than 300mW. This enables longer deployment times using cheap and
readily available battery components. Overall we offer a step change in the
cost, efficiency and scope of insect monitoring. Solving the challenging
trigger problem, we demonstrate a system which can be deployed for far longer
than existing designs and budgets power and bandwidth effectively, moving
towards a generic insect camera trap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BitStack: Any-Size Compression of Large Language Models in Variable
  Memory Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel computational workflows for natural and biomedical image
  processing based on hypercomplex algebras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nektarios A. Valous, Eckhard Hitzer, Dragoş Duşe, Rodrigo Rojas Moraleda, Ferdinand Popp, Meggy Suarez-Carmona, Anna Berthel, Ismini Papageorgiou, Carlo Fremd, Alexander Rölle, Christina C. Westhoff, Bénédicte Lenoir, Niels Halama, Inka Zörnig, Dirk Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex image processing extends conventional techniques in a unified
paradigm encompassing algebraic and geometric principles. This work leverages
quaternions and the two-dimensional orthogonal planes split framework
(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D
planes) for natural/biomedical image analysis through the following
computational workflows and outcomes: natural/biomedical image re-colorization,
natural image de-colorization, natural/biomedical image contrast enhancement,
computational re-staining and stain separation in histological images, and
performance gains in machine/deep learning pipelines for histological images.
The workflows are analyzed separately for natural and biomedical images to
showcase the effectiveness of the proposed approaches. The proposed workflows
can regulate color appearance (e.g. with alternative renditions and grayscale
conversion) and image contrast, be part of automated image processing pipelines
(e.g. isolating stain components, boosting learning models), and assist in
digital pathology applications (e.g. enhancing biomarker visibility, enabling
colorblind-friendly renditions). Employing only basic arithmetic and matrix
operations, this work offers a computationally accessible methodology - in the
hypercomplex domain - that showcases versatility and consistency across image
processing tasks and a range of computer vision and biomedical applications.
The proposed non-data-driven methods achieve comparable or better results
(particularly in cases involving well-known methods) to those reported in the
literature, showcasing the potential of robust theoretical frameworks with
practical effectiveness. Results, methods, and limitations are detailed
alongside discussion of promising extensions, emphasizing the potential of
feature-rich mathematical/computational frameworks for natural and biomedical
images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Language Models Exhibit Higher Visual Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well do text-only Large Language Models (LLMs) naturally align with the
visual world? We provide the first direct analysis by utilizing frozen text
representations in a discriminative vision-language model framework and
measuring zero-shot generalization on unseen classes. We find decoder-based
LLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs
reliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs
leads to strong gains in cross-lingual settings, where our approach surpasses
CLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves
both robustness and generalization and also significantly reduces the need for
paired data and compute, making vision-language models more accessible and
adaptable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Meta-Learning from a Learning Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Wang, Wenwen Qiang, Chuxiong Sun, Changwen Zheng, Jiangmeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning has emerged as a powerful approach for leveraging knowledge
from previous tasks to solve new tasks. The mainstream methods focus on
training a well-generalized model initialization, which is then adapted to
different tasks with limited data and updates. However, it pushes the model
overfitting on the training tasks. Previous methods mainly attributed this to
the lack of data and used augmentations to address this issue, but they were
limited by sufficient training and effective augmentation strategies. In this
work, we focus on the more fundamental learning to learn strategy of
meta-learning to explore what causes errors and how to eliminate these errors
without changing the environment. Specifically, we first rethink the
algorithmic procedure of meta-learning from a learning lens. Through
theoretical and empirical analyses, we find that (i) this paradigm faces the
risk of both overfitting and underfitting and (ii) the model adapted to
different tasks promote each other where the effect is stronger if the tasks
are more similar. Based on this insight, we propose using task relations to
calibrate the optimization process of meta-learning and propose a plug-and-play
method called Task Relation Learner (TRLearner) to achieve this goal.
Specifically, it first obtains task relation matrices from the extracted
task-specific meta-data. Then, it uses the obtained matrices with
relation-aware consistency regularization to guide optimization. Extensive
theoretical and empirical analyses demonstrate the effectiveness of TRLearner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2VEval: T2V-generated Videos Benchmark <span class="highlight-title">Dataset</span> and Objective Evaluation
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelu Qi, Ping Shi, Shuqi Wang, Zhaoyang Zhang, Fei Zhao, Zefeng Ying, Da Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video (T2V) technology, as demonstrated by models
such as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the
applicability and popularity of the technology. This progress has created a
growing demand for accurate quality assessment metrics to evaluate the
perceptual quality of T2V-generated videos and optimize video generation
models. However, assessing the quality of text-to-video outputs remain
challenging due to the presence of highly complex distortions, such as
unnatural actions and phenomena that defy human cognition. To address these
challenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset
for text-to-video quality evaluation, comprising 148 textual prompts and 1,783
videos generated by 13 T2V models. To ensure a comprehensive evaluation, we
scored each video on four dimensions in the subjective experiment, which are
overall impression, text-video consistency, realness, and technical quality.
Based on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for
T2V quality evaluation. T2VEval assesses videos across three branches:
text-video consistency, realness, and technical quality. Using an
attention-based fusion module, T2VEval effectively integrates features from
each branch and predicts scores with the aid of a large language model.
Additionally, we implemented a progressive training strategy, enabling each
branch to learn targeted knowledge while maintaining synergy with the others.
Experimental results demonstrate that T2VEval achieves state-of-the-art
performance across multiple metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwen Wu, Lechao Cheng, Shengeng Tang, Xiaofeng Zhu, Chaowei Fang, Dingwen Zhang, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) seeks to enable a model to sequentially
learn new classes while retaining knowledge of previously learned ones.
Balancing flexibility and stability remains a significant challenge,
particularly when the task ID is unknown. To address this, our study reveals
that the gap in feature distribution between novel and existing tasks is
primarily driven by differences in mean and covariance moments. Building on
this insight, we propose a novel semantic drift calibration method that
incorporates mean shift compensation and covariance calibration. Specifically,
we calculate each class's mean by averaging its sample embeddings and estimate
task shifts using weighted embedding changes based on their proximity to the
previous mean, effectively capturing mean shifts for all learned classes with
each new task. We also apply Mahalanobis distance constraint for covariance
calibration, aligning class-specific embedding covariances between old and
current networks to mitigate the covariance shift. Additionally, we integrate a
feature-level self-distillation approach to enhance generalization.
Comprehensive experiments on commonly used datasets demonstrate the
effectiveness of our approach. The source code is available at
\href{https://github.com/fwu11/MACIL.git}{https://github.com/fwu11/MACIL.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Swapping via Learning and Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Xing, Lechao Cheng, Shengeng Tang, Yaxiong Wang, Zhun Zhong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce \textbf{Knowledge Swapping}, a novel task designed to
selectively regulate knowledge of a pretrained model by enabling the forgetting
of user\-specified information, retaining essential knowledge, and acquiring
new knowledge simultaneously. By delving into the analysis of knock-on feature
hierarchy, we find that incremental learning typically progresses from
low\-level representations to higher\-level semantics, whereas forgetting tends
to occur in the opposite direction\-starting from high-level semantics and
moving down to low-level features. Building upon this, we propose to benchmark
the knowledge swapping task with the strategy of \textit{Learning Before
Forgetting}. Comprehensive experiments on various tasks like image
classification, object detection, and semantic segmentation validate the
effectiveness of the proposed strategy. The source code is available at
\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives for Contrastive Visual Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning, but efficiently leveraging hard negatives, which are
samples closely resembling the anchor, remains challenging. We introduce SynCo
(Synthetic negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and strong representation learning,
surpassing MoCo-v2 by +0.4% and MoCHI by +1.0% on ImageNet ILSVRC-2012 linear
evaluation. It also transfers more effectively to detection tasks achieving
strong results on PASCAL VOC detection (57.2% AP) and significantly improving
over MoCo-v2 on COCO detection (+1.0% AP) and instance segmentation (+0.8% AP).
Our synthetic hard negative generation approach significantly enhances visual
representations learned through self-supervised contrastive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code: https://github.com/giakoumoglou/synco, Supplementary:
  https://giakoumoglou.com/src/synco_suppl.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Catastrophic Forgetting in Two-Stage Incremental Object
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qirui Wu, Shizhou Zhang, De Cheng, Yinghui Xing, Di Xu, Peng Wang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting is a critical chanllenge for incremental object
detection (IOD). Most existing methods treat the detector monolithically,
relying on instance replay or knowledge distillation without analyzing
component-specific forgetting. Through dissection of Faster R-CNN, we reveal a
key insight: Catastrophic forgetting is predominantly localized to the RoI Head
classifier, while regressors retain robustness across incremental stages. This
finding challenges conventional assumptions, motivating us to develop a
framework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates
classifier forgetting via replay of two types of prototypes: coarse prototypes
represent class-wise semantic centers of RoI features, while fine-grained
prototypes model intra-class variations. Null Space Gradient Projection (NSGP)
is further introduced to eliminate prototype-feature misalignment by updating
the feature extractor in directions orthogonal to subspace of old inputs via
gradient projection, aligning RePRE with incremental learning dynamics. Our
simple yet effective design allows NSGP-RePRE to achieve state-of-the-art
performance on the Pascal VOC and MS COCO datasets under various settings. Our
work not only advances IOD methodology but also provide pivotal insights for
catastrophic forgetting mitigation in IOD. Code will be available soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PrototypeFormer: Learning to Explore Prototype Relationships for
  Few-shot Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meijuan Su, Feihong He, Fanzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot image classification has received considerable attention for
overcoming the challenge of limited classification performance with limited
samples in novel classes. Most existing works employ sophisticated learning
strategies and feature learning modules to alleviate this challenge. In this
paper, we propose a novel method called PrototypeFormer, exploring the
relationships among category prototypes in the few-shot scenario. Specifically,
we utilize a transformer architecture to build a prototype extraction module,
aiming to extract class representations that are more discriminative for
few-shot classification. Besides, during the model training process, we propose
a contrastive learning-based optimization approach to optimize prototype
features in few-shot learning scenarios. Despite its simplicity, our method
performs remarkably well, with no bells and whistles. We have experimented with
our approach on several popular few-shot image classification benchmark
datasets, which shows that our method outperforms all current state-of-the-art
methods. In particular, our method achieves 97.07\% and 90.88\% on 5-way 5-shot
and 5-way 1-shot tasks of miniImageNet, which surpasses the state-of-the-art
results with accuracy of 0.57\% and 6.84\%, respectively. The code will be
released later.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Attribute-Relation Representation Based Video Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Du, Yiping Duan, Qianqian Yang, Xiaoming Tao, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of multimedia data volume, there is an increasing need
for efficient video transmission in applications such as virtual reality and
future video streaming services. Semantic communication is emerging as a vital
technique for ensuring efficient and reliable transmission in low-bandwidth,
high-noise settings. However, most current approaches focus on joint
source-channel coding (JSCC) that depends on end-to-end training. These methods
often lack an interpretable semantic representation and struggle with
adaptability to various downstream tasks. In this paper, we introduce the use
of object-attribute-relation (OAR) as a semantic framework for videos to
facilitate low bit-rate coding and enhance the JSCC process for more effective
video transmission. We utilize OAR sequences for both low bit-rate
representation and generative video reconstruction. Additionally, we
incorporate OAR into the image JSCC model to prioritize communication resources
for areas more critical to downstream tasks. Our experiments on traffic
surveillance video datasets assess the effectiveness of our approach in terms
of video transmission performance. The empirical findings demonstrate that our
OAR-based video coding method not only outperforms H.265 coding at lower
bit-rates but also synergizes with JSCC to deliver robust and efficient video
transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Vision Language Model Training via High Quality Data Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, Jiao Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning
via High QuaLity Data Curation), an open-source vision language model (VLM)
series achieving state-of-the-art (SOTA) performance in 2B and 8B parameters.
The following three key improvements contribute to SAIL-VL's leading
performance: (1) Scalable high-quality visual understanding data construction:
We implement a data construction pipeline to enable hundred-million-scale
high-quality recaption data annotation, and the resulted dataset SAIL-Caption
is validated to be of the highest data quality compared with opensource
alternatives. (2) Scalable Pretraining with High-Quality Visual Understanding
Data: We scale SAIL-VL's pretraining budget up to 655B tokens and show that
even a 2B VLM benefits from scaled up training data sizes, exhibiting expected
data size scaling laws in visual understanding and instruction following
performance. (3) Scalable SFT via data quantity and complexity scaling: We
curate a high-quality SFT dataset collection which outperforms opensource
alternatives in data quantity scaling effectiveness. We also demonstrate that
training with progressively higher-complexity data surpasses baseline one-stage
training by a large margin. SAIL-VL series models achieve the highest average
score in 18 widely used VLM benchmarks in our evaluation, with the 2B model
takes the top position over VLMs of comparable sizes on OpenCompass 2024
(https://rank.opencompass.org.cn/leaderboard-multimodal) demonstrating robust
visual comprehension abilities. SAIL-VL series models are released at
HuggingFace (https://huggingface.co/BytedanceDouyinContent).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Is That Talk About? A Video-to-Text Summarization <span class="highlight-title">Dataset</span> for
  Scientific Presentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming recorded videos into concise and accurate textual summaries is a
growing challenge in multimodal learning. This paper introduces VISTA, a
dataset specifically designed for video-to-text summarization in scientific
domains. VISTA contains 18,599 recorded AI conference presentations paired with
their corresponding paper abstracts. We benchmark the performance of
state-of-the-art large models and apply a plan-based framework to better
capture the structured nature of abstracts. Both human and automated
evaluations confirm that explicit planning enhances summary quality and factual
consistency. However, a considerable gap remains between models and human
performance, highlighting the challenges of scientific video summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global-Local Distillation Network-Based Audio-Visual Speaker Tracking
  with Incomplete Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidi Li, Yihan Li, Yixin Guo, Bin Ren, Zhenhuan Xu, Hao Guo, Hong Liu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In speaker tracking research, integrating and complementing multi-modal data
is a crucial strategy for improving the accuracy and robustness of tracking
systems. However, tracking with incomplete modalities remains a challenging
issue due to noisy observations caused by occlusion, acoustic noise, and sensor
failures. Especially when there is missing data in multiple modalities, the
performance of existing multi-modal fusion methods tends to decrease. To this
end, we propose a Global-Local Distillation-based Tracker (GLDTracker) for
robust audio-visual speaker tracking. GLDTracker is driven by a teacher-student
distillation model, enabling the flexible fusion of incomplete information from
each modality. The teacher network processes global signals captured by camera
and microphone arrays, and the student network handles local information
subject to visual occlusion and missing audio channels. By transferring
knowledge from teacher to student, the student network can better adapt to
complex dynamic scenes with incomplete observations. In the student network, a
global feature reconstruction module based on the generative adversarial
network is constructed to reconstruct global features from feature embedding
with missing local information. Furthermore, a multi-modal multi-level fusion
attention is introduced to integrate the incomplete feature and the
reconstructed feature, leveraging the complementarity and consistency of
audio-visual and global-local features. Experimental results on the AV16.3
dataset demonstrate that the proposed GLDTracker outperforms existing
state-of-the-art audio-visual trackers and achieves leading performance on both
standard and incomplete modalities datasets, highlighting its superiority and
robustness in complex conditions. The code and models will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We request to withdraw our paper from arXiv due to unresolved author
  disagreements about the data interpretation and study conclusions. To
  maintain scientific integrity, we believe withdrawing the paper is necessary.
  We regret any confusion caused</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Vila-Tomás, Pablo Hernández-Cámara, Valero Laparra, Jesús Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision models are at the core of image processing. For instance,
classical approaches to the problem of image quality are based on models that
include knowledge about human vision. However, nowadays, deep learning
approaches have obtained competitive results by simply approaching this problem
as regression of human decisions, and training an standard network on
human-rated datasets. These approaches have the advantages of being easily
adaptable to a particular problem and they fit very efficiently when data is
available. However, mainly due to the excess of parameters, they have the
problems of lack of interpretability, and over-fitting. Here we propose a
vision model that combines the best of both worlds by using a parametric neural
network architecture. We parameterize the layers to have bioplausible
functionality, and provide a set of bioplausible parameters. We analyzed
different versions of the model and compared it with the non-parametric
version. The parametric models achieve a three orders of magnitude reduction in
the number of parameters without suffering in regression performance.
Furthermore, we show that the parametric models behave better during training
and are easier to interpret as vision models. Interestingly, we find that, even
initialized with bioplausible trained for regression using human rated
datasets, which we call the feature-spreading problem. This suggests that the
deep learning approach is inherently flawed, and emphasizes the need to
evaluate and train models beyond regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for
  Traversability Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01395v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01395v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Santo, Juan J. Cabrera, David Valiente, Carlos Viegas, Arturo Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents TE-NeXt, a novel and efficient architecture for
Traversability Estimation (TE) from sparse LiDAR point clouds based on a
residual convolution block. TE-NeXt block fuses notions of current trends such
as attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate
high capacity for generalisation in a variety of urban and natural
environments, using well-known and accessible datasets such as SemanticKITTI,
Rellis-3D and SemanticUSL. Thus, the designed architecture ouperforms
state-of-the-art methods in the problem of semantic segmentation, demonstrating
better results in unstructured environments and maintaining high reliability
and robustness in urbans environments, which leads to better abstraction.
Implementation is available in a open repository to the scientific community
with the aim of ensuring the reproducibility of results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DINeuro: Distilling Knowledge from 2D Natural Images via Deformable
  Tubular Transferring Strategy for 3D Neuron Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yik San Cheng, Runkai Zhao, Heng Wang, Hanchuan Peng, Yui Lo, Yuqian Chen, Lauren J. O'Donnell, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing neuron morphology from 3D light microscope imaging data is
critical to aid neuroscientists in analyzing brain networks and neuroanatomy.
With the boost from deep learning techniques, a variety of learning-based
segmentation models have been developed to enhance the signal-to-noise ratio of
raw neuron images as a pre-processing step in the reconstruction workflow.
However, most existing models directly encode the latent representative
features of volumetric neuron data but neglect their intrinsic morphological
knowledge. To address this limitation, we design a novel framework that
distills the prior knowledge from a 2D Vision Transformer pre-trained on
extensive 2D natural images to facilitate neuronal morphological learning of
our 3D Vision Transformer. To bridge the knowledge gap between the 2D natural
image and 3D microscopic morphologic domains, we propose a deformable tubular
transferring strategy that adapts the pre-trained 2D natural knowledge to the
inherent tubular characteristics of neuronal structure in the latent embedding
space. The experimental results on the Janelia dataset of the BigNeuron project
demonstrate that our method achieves a segmentation performance improvement of
4.53% in mean Dice and 3.56% in mean 95% Hausdorff distance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, and 2 tables. This work has been accepted to 2025
  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-based Ensemble Learning in CMR Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Liu, Ziyi Wu, Liang Zhong, Lingyi Wen, Yuankai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing models typically segment either the entire 3D frame or 2D slices
independently to derive clinical functional metrics from ventricular
segmentation in cardiac cine sequences. While performing well overall, they
struggle at the end slices. To address this, we leverage spatial continuity to
extract global uncertainty from segmentation variance and use it as memory in
our ensemble learning method, Streaming, for classifier weighting, balancing
overall and end-slice performance. Additionally, we introduce the End
Coefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M&Ms
datasets show that our framework achieves near-state-of-the-art Dice Similarity
Coefficient (DSC) and outperforms all models on end-slice performance,
improving patient-specific segmentation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Language Prompting to Ease False Positives in Medical
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through
  Hierarchical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Jungqi Zhao, Allison Koenecke, Boyang Li, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision-language models may grasp basic spatial cues and simple
directions (e.g. left, right, front, back), but struggle with the
multi-dimensional spatial reasoning necessary for human-like understanding and
real-world applications. To address this gap, we develop SPHERE (Spatial
Perception and Hierarchical Evaluation of REasoning), a hierarchical evaluation
framework supported by a new human-annotated dataset. SPHERE systematically
probes models across increasing levels of complexity, from fundamental skills
to multi-skill integration and high-level reasoning that combines spatial,
visual, and logical understanding. Benchmark evaluation of state-of-the-art
models reveals significant deficiencies, especially in reasoning about distance
and proximity, understanding both egocentric and allocentric perspectives, and
applying spatial logic in physical contexts. These findings expose critical
blind spots in existing models and underscore the need for more advanced
spatial reasoning techniques, driving the development of vision-language models
that align more closely with human spatial cognition. The dataset will be
open-sourced upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion
  Models with Self-Augmented Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Liang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of AI-generated images necessitates effective
watermarking techniques to protect intellectual property and detect fraudulent
content. While existing training-based watermarking methods show promise, they
often struggle with generalizing across diverse prompts and tend to introduce
visible artifacts. To this end, we propose a novel, provably generalizable
image watermarking approach for Latent Diffusion Models, termed Self-Augmented
Training (SAT-LDM). Our method aligns the training and testing phases through a
free generation distribution, thereby enhancing the watermarking module's
generalization capabilities. We theoretically consolidate SAT-LDM by proving
that the free generation distribution contributes to its tight generalization
bound, without the need for additional data collection. Extensive experiments
show that SAT-LDM not only achieves robust watermarking but also significantly
improves the quality of watermarked images across a wide range of prompts.
Moreover, our experimental analyses confirm the strong generalization abilities
of SAT-LDM. We hope that our method provides a practical and efficient solution
for securing high-fidelity AI-generated content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoxMAC -- A Boxing <span class="highlight-title">Dataset</span> for Multi-label Action Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashikanta Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In competitive combat sports like boxing, analyzing a boxers's performance
statics is crucial for evaluating the quantity and variety of punches delivered
during bouts. These statistics provide valuable data and feedback, which are
routinely used for coaching and performance enhancement. We introduce BoxMAC, a
real-world boxing dataset featuring 15 professional boxers and encompassing 13
distinct action labels. Comprising over 60,000 frames, our dataset has been
meticulously annotated for multiple actions per frame with inputs from a boxing
coach. Since two boxers can execute different punches within a single
timestamp, this problem falls under the domain of multi-label action
classification. We propose a novel architecture for jointly recognizing
multiple actions in both individual images and videos. We investigate baselines
using deep neural network architectures to address both tasks. We believe that
BoxMAC will enable researchers and practitioners to develop and evaluate more
efficient models for performance analysis. With its realistic and diverse
nature, BoxMAC can serve as a valuable resource for the advancement of boxing
as a sport
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significant modifications are required to improve the clarity and
  accuracy of the findings and This submission was made without the full
  agreement of all co-authors. To ensure proper authorship attribution and
  compliance with ethical guidelines, we are withdrawing this version. A
  revised and more complete version will be submitted soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cluster and Predict Latent Patches for Improved Masked Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothée Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) offers a promising approach to self-supervised
representation learning, however existing MIM models still lag behind the
state-of-the-art. In this paper, we systematically analyze target
representations, loss functions, and architectures, to introduce CAPI - a novel
pure-MIM framework that relies on the prediction of latent clusterings. Our
approach leverages a clustering-based loss, which is stable to train, and
exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%
accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,
substantially outperforming previous MIM methods and approaching the
performance of the current state-of-the-art, DINOv2. We release all our code
and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, submitted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Fi: A Modality-Invariant Foundation Model for <span class="highlight-title">Multimodal</span> Human Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Chen, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human sensing, which employs various sensors and advanced deep learning
technologies to accurately capture and interpret human body information, has
significantly impacted fields like public security and robotics. However,
current human sensing primarily depends on modalities such as cameras and
LiDAR, each of which has its own strengths and limitations. Furthermore,
existing multi-modal fusion solutions are typically designed for fixed modality
combinations, requiring extensive retraining when modalities are added or
removed for diverse scenarios. In this paper, we propose a modality-invariant
foundation model for all modalities, X-Fi, to address this issue. X-Fi enables
the independent or combinatory use of sensor modalities without additional
training by utilizing a transformer structure to accommodate variable input
sizes and incorporating a novel "X-fusion" mechanism to preserve
modality-specific features during multimodal integration. This approach not
only enhances adaptability but also facilitates the learning of complementary
features across modalities. Extensive experiments conducted on the MM-Fi and
XRF55 datasets, employing six distinct modalities, demonstrate that X-Fi
achieves state-of-the-art performance in human pose estimation (HPE) and human
activity recognition (HAR) tasks. The findings indicate that our proposed model
can efficiently support a wide range of human sensing applications, ultimately
contributing to the evolution of scalable, multimodal sensing technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by
  Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kengo Uchida, Takashi Shibuya, Yuhta Takida, Naoki Murata, Julian Tanke, Shusuke Takahashi, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In text-to-motion generation, controllability as well as generation quality
and speed has become increasingly critical. The controllability challenges
include generating a motion of a length that matches the given textual
description and editing the generated motions according to control signals,
such as the start-end positions and the pelvis trajectory. In this paper, we
propose MoLA, which provides fast, high-quality, variable-length motion
generation and can also deal with multiple editing tasks in a single framework.
Our approach revisits the motion representation used as inputs and outputs in
the model, incorporating an activation variable to enable variable-length
motion generation. Additionally, we integrate a variational autoencoder and a
latent diffusion model, further enhanced through adversarial training, to
achieve high-quality and fast generation. Moreover, we apply a training-free
guided generation framework to achieve various editing tasks with motion
control inputs. We quantitatively show the effectiveness of adversarial
learning in text-to-motion generation, and demonstrate the applicability of our
editing framework to multiple editing tasks in the motion domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-Video-T2V Technical Report: The Practice, Challenges, and Future of
  Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-quality Unknown Object Instance Segmentation via Quadruple Boundary
  Error Refinement <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16132v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16132v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghyeok Back, Sangbeom Lee, Kangmin Kim, Joosoon Lee, Sungho Shin, Jemo Maeng, Kyoobin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient segmentation of unknown objects in unstructured
environments is essential for robotic manipulation. Unknown Object Instance
Segmentation (UOIS), which aims to identify all objects in unknown categories
and backgrounds, has become a key capability for various robotic tasks.
However, existing methods struggle with over-segmentation and
under-segmentation, leading to failures in manipulation tasks such as grasping.
To address these challenges, we propose QuBER (Quadruple Boundary Error
Refinement), a novel error-informed refinement approach for high-quality UOIS.
QuBER first estimates quadruple boundary errors-true positive, true negative,
false positive, and false negative pixels-at the instance boundaries of the
initial segmentation. It then refines the segmentation using an error-guided
fusion mechanism, effectively correcting both fine-grained and instance-level
segmentation errors. Extensive evaluations on three public benchmarks
demonstrate that QuBER outperforms state-of-the-art methods and consistently
improves various UOIS methods while maintaining a fast inference time of less
than 0.1 seconds. Furthermore, we show that QuBER improves the success rate of
grasping target objects in cluttered environments. Code and supplementary
materials are available at https://sites.google.com/view/uois-quber.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, accepted at ICRA 2025, project website:
  https://sites.google.com/view/uois-quber</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Growth Inhibitors for Suppressing Inappropriate Image Concepts in
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Die Chen, Zhiwen Li, Mingyuan Fan, Cen Chen, Wenmeng Zhou, Yanhao Wang, Yaliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their remarkable image generation capabilities, text-to-image
diffusion models inadvertently learn inappropriate concepts from vast and
unfiltered training data, which leads to various ethical and business risks.
Specifically, model-generated images may exhibit not safe for work (NSFW)
content and style copyright infringements. The prompts that result in these
problems often do not include explicit unsafe words; instead, they contain
obscure and associative terms, which are referred to as implicit unsafe
prompts. Existing approaches directly fine-tune models under textual guidance
to alter the cognition of the diffusion model, thereby erasing inappropriate
concepts. This not only requires concept-specific fine-tuning but may also
incur catastrophic forgetting. To address these issues, we explore the
representation of inappropriate concepts in the image space and guide them
towards more suitable ones by injecting growth inhibitors, which are tailored
based on the identified features related to inappropriate concepts during the
diffusion process. Additionally, due to the varying degrees and scopes of
inappropriate concepts, we train an adapter to infer the corresponding
suppression scale during the injection process. Our method effectively captures
the manifestation of subtle words at the image level, enabling direct and
efficient erasure of target concepts without the need for fine-tuning. Through
extensive experimentation, we demonstrate that our approach achieves superior
erasure results with little effect on other concepts while preserving image
quality and semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Triangle meshes are fundamental to 3D applications, enabling efficient
modification and rasterization while maintaining compatibility with standard
rendering pipelines. However, current automatic mesh generation methods
typically rely on intermediate representations that lack the continuous surface
quality inherent to meshes. Converting these representations into meshes
produces dense, suboptimal outputs. Although recent autoregressive approaches
demonstrate promise in directly modeling mesh vertices and faces, they are
constrained by the limitation in face count, scalability, and structural
fidelity. To address these challenges, we propose Nautilus, a locality-aware
autoencoder for artist-like mesh generation that leverages the local properties
of manifold meshes to achieve structural fidelity and efficient representation.
Our approach introduces a novel tokenization algorithm that preserves face
proximity relationships and compresses sequence length through locally shared
vertices and edges, enabling the generation of meshes with an unprecedented
scale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point
Conditioner that provides multi-scale geometric guidance, ensuring global
consistency and local structural fidelity by capturing fine-grained geometric
features. Extensive experiments demonstrate that Nautilus significantly
outperforms state-of-the-art methods in both fidelity and scalability. The
project page is at https://nautilusmeshgen.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingpeng Xie, Shiyu Tan, Yuanlei Wang, Tianle Du, Yifei Xue, Yizhen Lao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Neural Radiance Fields (NeRF) reconstructions are typically
hindered by the requirement for extensive image datasets and substantial
computational resources. This paper introduces IOVS4NeRF, a framework that
employs an uncertainty-guided incremental optimal view selection strategy
adaptable to various NeRF implementations. Specifically, by leveraging a hybrid
uncertainty model that combines rendering and positional uncertainties, the
proposed method calculates the most informative view from among the candidates,
thereby enabling incremental optimization of scene reconstruction. Our detailed
experiments demonstrate that IOVS4NeRF achieves high-fidelity NeRF
reconstruction with minimal computational resources, making it suitable for
large-scale scene applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variable Radiance Field for Real-World Category-Specific Reconstruction
  from Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Wang, Zhiqiang Yan, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing category-specific objects using Neural Radiance Field (NeRF)
from a single image is a promising yet challenging task. Existing approaches
predominantly rely on projection-based feature retrieval to associate 3D points
in the radiance field with local image features from the reference image.
However, this process is computationally expensive, dependent on known camera
intrinsics, and susceptible to occlusions. To address these limitations, we
propose Variable Radiance Field (VRF), a novel framework capable of efficiently
reconstructing category-specific objects without requiring known camera
intrinsics and demonstrating robustness against occlusions. First, we replace
the local feature retrieval with global latent representations, generated
through a single feed-forward pass, which improves efficiency and eliminates
reliance on camera intrinsics. Second, to tackle coordinate inconsistencies
inherent in real-world dataset, we define a canonical space by introducing a
learnable, category-specific shape template and explicitly aligning each
training object to this template using a learnable 3D transformation. This
approach also reduces the complexity of geometry prediction to modeling
deformations from the template to individual instances. Finally, we employ a
hyper-network-based method for efficient NeRF creation and enhance the
reconstruction performance through a contrastive learning-based pretraining
strategy. Evaluations on the CO3D dataset demonstrate that VRF achieves
state-of-the-art performance in both reconstruction quality and computational
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR: Scale-wise Text-conditioned AutoRegressive image generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Biye Li, Huaian Chen, Yi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce STAR, a text-to-image model that employs a scale-wise
auto-regressive paradigm. Unlike VAR, which is constrained to class-conditioned
synthesis for images up to 256$\times$256, STAR enables text-driven image
generation up to 1024$\times$1024 through three key designs. First, we
introduce a pre-trained text encoder to extract and adopt representations for
textual constraints, enhancing details and generalizability. Second, given the
inherent structural correlation across different scales, we leverage 2D Rotary
Positional Encoding (RoPE) and tweak it into a normalized version, ensuring
consistent interpretation of relative positions across token maps and
stabilizing the training process. Third, we observe that simultaneously
sampling all tokens within a single scale can disrupt inter-token
relationships, leading to structural instability, particularly in
high-resolution generation. To address this, we propose a novel stable sampling
method that incorporates causal relationships into the sampling process,
ensuring both rich details and stable structures. Compared to previous
diffusion models and auto-regressive models, STAR surpasses existing benchmarks
in fidelity, text-image consistency, and aesthetic quality, requiring just
2.21s for 1024$\times$1024 images on A100. This highlights the potential of
auto-regressive methods in high-quality image synthesis, offering new
directions for the text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compress image to patches for Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Zhao, Yaoru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Vision Transformer (ViT) has made significant strides in the field of
computer vision. However, as the depth of the model and the resolution of the
input images increase, the computational cost associated with training and
running ViT models has surged dramatically. This paper proposes a hybrid model
based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a
module called CI2P, which utilizes the CompressAI encoder to compress images
and subsequently generates a sequence of patches through a series of
convolutions. CI2P can replace the Patch Embedding component in the ViT model,
enabling seamless integration into existing ViT models. Compared to ViT-B/16,
CI2P-ViT has the number of patches input to the self-attention layer reduced to
a quarter of the original. This design not only significantly reduces the
computational cost of the ViT model but also effectively enhances the model's
accuracy by introducing the inductive bias properties of CNN. The ViT model's
precision is markedly enhanced. When trained from the ground up on the
Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing
a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's
computational operations, measured in floating-point operations per second
(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in
training velocity on identical hardware configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient-vDiT: Efficient Video Diffusion <span class="highlight-title">Transformer</span>s With Attention
  Tile 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promise of synthesizing high-fidelity videos, Diffusion
Transformers (DiTs) with 3D full attention suffer from expensive inference due
to the complexity of attention computation and numerous sampling steps. For
example, the popular Open-Sora-Plan model consumes more than 9 minutes for
generating a single video of 29 frames. This paper addresses the inefficiency
issue from two aspects: 1) Prune the 3D full attention based on the redundancy
within video data; We identify a prevalent tile-style repetitive pattern in the
3D attention maps for video data, and advocate a new family of sparse 3D
attention that holds a linear complexity w.r.t. the number of video frames. 2)
Shorten the sampling process by adopting existing multi-step consistency
distillation; We split the entire sampling trajectory into several segments and
perform consistency distillation within each one to activate few-step
generation capacities. We further devise a three-stage training pipeline to
conjoin the low-complexity attention and few-step generation capacities.
Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into
an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video
generation with a marginal performance trade-off in VBench. In addition, we
demonstrate that our approach is amenable to distributed inference, achieving
an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Vision-language Models for Self-supervised Remote
  Physiological Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Yue, Miaojing Shi, Hanli Wang, Shuai Ding, Qijun Chen, Shanlin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial video-based remote physiological measurement is a promising research
area for detecting human vital signs (e.g., heart rate, respiration frequency)
in a non-contact way. Conventional approaches are mostly supervised learning,
requiring extensive collections of facial videos and synchronously recorded
photoplethysmography (PPG) signals. To tackle it, self-supervised learning has
recently gained attentions; due to the lack of ground truth PPG signals, its
performance is however limited. In this paper, we propose a novel
self-supervised framework that successfully integrates the popular
vision-language models (VLMs) into the remote physiological measurement task.
Given a facial video, we first augment its positive and negative video samples
with varying rPPG signal frequencies. Next, we introduce a frequency-oriented
vision-text pair generation method by carefully creating contrastive
spatio-temporal maps from positive and negative samples and designing proper
text prompts to describe their relative ratios of signal frequencies. A
pre-trained VLM is employed to extract features for these formed vision-text
pairs and estimate rPPG signals thereafter. We develop a series of generative
and contrastive learning mechanisms to optimize the VLM, including the
text-guided visual map reconstruction task, the vision-text contrastive
learning task, and the frequency contrastive and ranking task. Overall, our
method for the first time adapts VLMs to digest and align the frequency-related
knowledge in vision and text modalities. Extensive experiments on four
benchmark datasets demonstrate that it significantly outperforms state of the
art self-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting <span class="highlight-title">Multi-modal</span> Large Language Model to Concept Drift From
  Pre-training Onwards <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Yang, Jie Lu, En Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) frequently face challenges from
concept drift when dealing with real-world streaming data, wherein
distributions change unpredictably. This mainly includes gradual drift due to
long-tailed data and sudden drift from Out-Of-Distribution (OOD) data, both of
which have increasingly drawn the attention of the research community. While
these issues have been extensively studied in the individual domain of vision
or language, their impacts on MLLMs in concept drift settings remain largely
underexplored. In this paper, we reveal the susceptibility and vulnerability of
Vision-Language (VL) models to significant biases arising from gradual drift
and sudden drift, particularly in the pre-training. To effectively address
these challenges, we propose a unified framework that extends concept drift
theory to the multi-modal domain, enhancing the adaptability of the VL model to
unpredictable distribution changes. Additionally, a T-distribution based drift
adapter is proposed to effectively mitigate the bias induced by the gradual
drift, which also facilitates the model in distinguishing sudden distribution
changes through explicit distribution modeling. Extensive experiments
demonstrate our method enhances the efficiency and accuracy of image-text
alignment in the pre-training of VL models, particularly in the concept drift
scenario. Moreover, various downstream tasks exhibit significant improvements
in our model's ability to adapt to the long-tailed open world. Furthermore, we
create a set of multi-modal datasets called OpenMMlo, specifically tailored for
the long-tailed open-world setting, to validate our findings. To foster the
development of the multi-modal community, we have made both OpenMMlo datasets
and our code publicly available at:
https://github.com/XiaoyuYoung/ConceptDriftMLLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object
  Detection and Motion Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05331v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05331v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahran Rahman Alve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project aims to develop a robust video surveillance system, which can
segment videos into smaller clips based on the detection of activities. It uses
CCTV footage, for example, to record only major events-like the appearance of a
person or a thief-so that storage is optimized and digital searches are easier.
It utilizes the latest techniques in object detection and tracking, including
Convolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well
as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks
(LSTMs), to achieve high accuracy in detection and capture temporal
dependencies. The approach incorporates adaptive background modeling through
Gaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to
detect motions. Multi-scale and contextual analysis are used to improve
detection across different object sizes and environments. A hybrid motion
segmentation strategy combines statistical and deep learning models to manage
complex movements, while optimizations for real-time processing ensure
efficient computation. Tracking methods, such as Kalman Filters and Siamese
networks, are employed to maintain smooth tracking even in cases of occlusion.
Detection is improved on various-sized objects for multiple scenarios by
multi-scale and contextual analysis. Results demonstrate high precision and
recall in detecting and tracking objects, with significant improvements in
processing times and accuracy due to real-time optimizations and
illumination-invariant features. The impact of this research lies in its
potential to transform video surveillance, reducing storage requirements and
enhancing security through reliable and efficient object detection and
tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Jiang, Shuang Chen, Boxi Wu, Xiaotong Guan, Jiahui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of generative artificial intelligence, previous studies
have achieved the task of generating aesthetic images from hand-drawn sketches,
fulfilling the public's needs for drawing. However, these methods are limited
to static images and lack the ability to control video animation generation
using hand-drawn sketches. To address this gap, we propose VidSketch, the first
method capable of generating high-quality video animations directly from any
number of hand-drawn sketches and simple text prompts, bridging the divide
between ordinary users and professional artists. Specifically, our method
introduces a Level-Based Sketch Control Strategy to automatically adjust the
guidance strength of sketches during the generation process, accommodating
users with varying drawing skills. Furthermore, a TempSpatial Attention
mechanism is designed to enhance the spatiotemporal consistency of generated
video animations, significantly improving the coherence across frames. You can
find more detailed cases on our official website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text4Seg: Reimagining Image Segmentation as Text Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have shown exceptional capabilities
in vision-language tasks; however, effectively integrating image segmentation
into these models remains a significant challenge. In this paper, we introduce
Text4Seg, a novel text-as-mask paradigm that casts image segmentation as a text
generation problem, eliminating the need for additional decoders and
significantly simplifying the segmentation process. Our key innovation is
semantic descriptors, a new textual representation of segmentation masks where
each image patch is mapped to its corresponding text label. This unified
representation allows seamless integration into the auto-regressive training
pipeline of MLLMs for easier optimization. We demonstrate that representing an
image with $16\times16$ semantic descriptors yields competitive segmentation
performance. To enhance efficiency, we introduce the Row-wise Run-Length
Encoding (R-RLE), which compresses redundant text sequences, reducing the
length of semantic descriptors by 74% and accelerating inference by $3\times$,
without compromising performance. Extensive experiments across various vision
tasks, such as referring expression segmentation and comprehension, show that
Text4Seg achieves state-of-the-art performance on multiple datasets by
fine-tuning different MLLM backbones. Our approach provides an efficient,
scalable solution for vision-centric tasks within the MLLM framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://mc-lan.github.io/Text4Seg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Image-to-Video Diffusion Models for Large-Motion Frame
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17042v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17042v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoxu Jin, Hiroshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of video generation models has advanced significantly in
recent years, we adopt large-scale image-to-video diffusion models for video
frame interpolation. We present a conditional encoder designed to adapt an
image-to-video model for large-motion frame interpolation. To enhance
performance, we integrate a dual-branch feature extractor and propose a
cross-frame attention mechanism that effectively captures both spatial and
temporal information, enabling accurate interpolations of intermediate frames.
Our approach demonstrates superior performance on the Fr\'echet Video Distance
(FVD) metric when evaluated against other state-of-the-art approaches,
particularly in handling large motion scenarios, highlighting advancements in
generative-based methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Reconstruction of Shoes for Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik Shrestha, Sujan Kapali, Swikar Gautam, Vishal Pokharel, Santosh Giri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a mobile-based solution that enhances online shoe
shopping through 3D modeling and Augmented Reality (AR), leveraging the
efficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D
images, the framework generates realistic 3D shoe models from 2D images,
achieving an average Peak Signal-to-Noise Ratio (PSNR) of 32, and enables
immersive AR interactions via smartphones. A custom shoe segmentation dataset
of 3120 images was created, with the best-performing segmentation model
achieving an Intersection over Union (IoU) score of 0.95. This paper
demonstrates the potential of 3D modeling and AR to revolutionize online
shopping by offering realistic virtual interactions, with applicability across
broader fashion categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via
  LLM Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02795v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02795v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Yan, Yiming Zhang, Baoyi He, Yuhao Fu, Qi Zhou, Zhijie Sang, Chunlin Ji, Shengyu Zhang, Fei Wu, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce InfiFusion, an efficient training pipeline designed to integrate
multiple domain-specialized Large Language Models (LLMs) into a single pivot
model, effectively harnessing the strengths of each source model. Traditional
fusion methods either merge model parameters directly or rely on knowledge
distillation with rigid assumptions, limiting their flexibility and efficiency.
InfiFusion overcomes these limitations by enhancing Universal Logit
Distillation (ULD) with Top-K selection and Logits Standardization. We propose
two fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source
model knowledge is distilled individually into the pivot model followed by
merging and Unified Fusion (InfiFusion$_u$), where knowledge from all source
models is distilled simultaneously into the pivot model. InfiFusion outperforms
the state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11
widely applied benchmarks covering reasoning, coding, mathematics, and
instruction-following tasks. Notably, InfiFusion achieves this superior
performance while significantly reduces computational costs, completing full
training with only 160 H800 GPU hours compared to the millions typically
required for traditional LLM training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significant performance improvements over the previous version; under
  review;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent rehearsal-free methods, guided by prompts, excel in vision-related
continual learning (CL) with drifting data but lack resource efficiency, making
real-world deployment challenging. In this paper, we introduce
Resource-Efficient Prompting (REP), which improves the computational and memory
efficiency of prompt-based rehearsal-free methods while minimizing accuracy
trade-offs. Our approach employs swift prompt selection to refine input data
using a carefully provisioned model and introduces adaptive token merging
(AToM) and layer dropping (ALD) for efficient prompt updates. AToM and ALD
selectively skip data and model layers while preserving task-specific features
during new-task learning. Extensive experiments on multiple image
classification datasets demonstrates REP's superior resource efficiency over
state-of-the-art ViT- and CNN-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexCAD: Unified and Versatile Controllable CAD Generation with
  Fine-tuned Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanwei Zhang, Shizhao Sun, Wenxiao Wang, Deng Cai, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there is a growing interest in creating computer-aided design (CAD)
models based on user intent, known as controllable CAD generation. Existing
work offers limited controllability and needs separate models for different
types of control, reducing efficiency and practicality. To achieve controllable
generation across all CAD construction hierarchies, such as sketch-extrusion,
extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by
fine-tuning large language models (LLMs). First, to enhance comprehension by
LLMs, we represent a CAD model as a structured text by abstracting each
hierarchy as a sequence of text tokens. Second, to address various controllable
generation tasks in a unified model, we introduce a hierarchy-aware masking
strategy. Specifically, during training, we mask a hierarchy-aware field in the
CAD text with a mask token. This field, composed of a sequence of tokens, can
be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to
predict this masked field. During inference, the user intent is converted into
a CAD text with a mask token replacing the part the user wants to modify, which
is then fed into FlexCAD to generate new CAD models. Comprehensive experiments
on public dataset demonstrate the effectiveness of FlexCAD in both generation
quality and controllability. Code will be available at
https://github.com/microsoft/FlexCAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounded Knowledge-Enhanced Medical Vision-Language Pre-training for
  Chest X-Ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Deng, Zhongzhen Huang, Yunqi Wang, Zhichuan Wang, Zhao Wang, Xiaofan Zhang, Qi Dou, Yeung Yu Hui, Edward S. Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical foundation models have the potential to revolutionize healthcare by
providing robust and generalized representations of medical data. Medical
vision-language pre-training has emerged as a promising approach for learning
domain-general representations of medical image and text. Current algorithms
that exploit global and local alignment between medical image and text could
however be marred by redundant information in medical data. To address this
issue, we propose a grounded knowledge-enhanced medical vision-language
pre-training (GK-MVLP) framework for chest X-ray. In this framework, medical
knowledge was grounded to the appropriate anatomical regions by using a
transformer-based grounded knowledge-enhanced module for fine-grained alignment
between textural features of medical knowledge and the corresponding anatomical
region-level visual features. The performance of GK-MVLP was competitive with
or exceeded the state of the art on downstream image understanding tasks (chest
X-ray disease classification, disease localization), generative task (report
generation), and vision-language understanding task (medical visual
question-answering). Our results demonstrate the advantage of incorporating
grounding mechanism to remove biases and improve the alignment between chest
X-ray image and radiology report.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhishma Dedhia, Niraj K. Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several accounts of human cognition posit that our intelligence is rooted in
our ability to form abstract composable concepts, ground them in our
environment, and reason over these grounded entities. This trifecta of human
thought has remained elusive in modern intelligent machines. In this work, we
investigate whether slot representations extracted from visual scenes serve as
appropriate compositional abstractions for grounding and reasoning. We present
the Neural Slot Interpreter (NSI), which learns to ground object semantics in
slots. At the core of NSI is an XML-like schema that uses simple syntax rules
to organize the object semantics of a scene into object-centric schema
primitives. Then, the NSI metric learns to ground primitives into slots through
a structured contrastive learning objective that reasons over the intermodal
alignment. Experiments with a bi-modal object-property and scene retrieval task
demonstrate the grounding efficacy and interpretability of correspondences
learned by NSI. From a scene representation standpoint, we find that emergent
NSI slots that move beyond the image grid by binding to spatial objects
facilitate improved visual grounding compared to conventional
bounding-box-based approaches. From a data efficiency standpoint, we
empirically validate that NSI learns more generalizable representations from a
fixed amount of annotation data than the traditional approach. We also show
that the grounded slots surpass unsupervised slots in real-world object
discovery and scale with scene complexity. Finally, we investigate the
reasoning abilities of the grounded slots. Vision Transformers trained on
grounding-aware NSI tokenizers using as few as ten tokens outperform
patch-based tokens on challenging few-shot classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magic 1-For-1: Generating One Minute Video Clips within One Minute 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, Daquan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we present Magic 1-For-1 (Magic141), an efficient
video generation model with optimized memory consumption and inference latency.
The key idea is simple: factorize the text-to-video generation task into two
separate easier tasks for diffusion step distillation, namely text-to-image
generation and image-to-video generation. We verify that with the same
optimization algorithm, the image-to-video task is indeed easier to converge
over the text-to-video task. We also explore a bag of optimization tricks to
reduce the computational cost of training the image-to-video (I2V) models from
three aspects: 1) model convergence speedup by using a multi-modal prior
condition injection; 2) inference latency speed up by applying an adversarial
step distillation, and 3) inference memory cost optimization with parameter
sparsification. With those techniques, we are able to generate 5-second video
clips within 3 seconds. By applying a test time sliding window, we are able to
generate a minute-long video within one minute with significantly improved
visual quality and motion dynamics, spending less than 1 second for generating
1 second video clips on average. We conduct a series of preliminary
explorations to find out the optimal tradeoff between computational cost and
video quality during diffusion step distillation and hope this could be a good
foundation model for open-source explorations. The code and the model weights
are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Serious updates are needed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFC-Bench: Benchmarking <span class="highlight-title">Multimodal</span> Fact-Checking with Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have significantly improved multimodal
reasoning tasks, such as visual question answering and image captioning. These
models embed multimodal facts within their parameters, rather than relying on
external knowledge bases to store factual information explicitly. However, the
content discerned by LVLMs may deviate from factuality due to inherent bias or
incorrect inference. To address this issue, we introduce MFC-Bench, a rigorous
and comprehensive benchmark designed to evaluate the factual accuracy of LVLMs
across three stages of verdict prediction for MFC: Manipulation,
Out-of-Context, and Veracity Classification. Through our evaluation on
MFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering
that current models still fall short in multimodal fact-checking and
demonstrate insensitivity to various forms of manipulated content. We hope that
MFC-Bench could raise attention to the trustworthy AI potentially assisted by
LVLMs in the future. The MFC-Bench and accompanying resources are publicly
accessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing
research in the multimodal fact-checking field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing <span class="highlight-title">Multimodal</span> Queries Representation via Fusion-Free
  Modality Interaction for <span class="highlight-title">Multimodal</span> Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet
  Transform Feature Modulation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared image super-resolution demands long-range dependency modeling and
multi-scale feature extraction to address challenges such as homogeneous
backgrounds, weak edges, and sparse textures. While Mamba-based state-space
models (SSMs) excel in global dependency modeling with linear complexity, their
block-wise processing disrupts spatial consistency, limiting their
effectiveness for IR image reconstruction. We propose IRSRMamba, a novel
framework integrating wavelet transform feature modulation for multi-scale
adaptation and an SSMs-based semantic consistency loss to restore fragmented
contextual information. This design enhances global-local feature fusion,
structural coherence, and fine-detail preservation while mitigating
block-induced artifacts. Experiments on benchmark datasets demonstrate that
IRSRMamba outperforms state-of-the-art methods in PSNR, SSIM, and perceptual
quality. This work establishes Mamba-based architectures as a promising
direction for high-fidelity IR image enhancement. Code are available at
https://github.com/yongsongH/IRSRMamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Text-Promptable Surgical Instrument Segmentation with Robust
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae-Min Choi, Juyoun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) is essential in computer-assisted
surgeries, with deep learning methods improving accuracy in complex
environments. Recently, text-promptable segmentation methods have been
introduced, generating masks based on textual descriptions. However, they
assume the text-described object is present and always generate an associated
mask even when the object is absent. Existing methods address this by using
prompts only for objects already known to exist in the scene, which relies on
inaccessible information. To address this, we rethink text-promptable SIS and
redefine it under robust conditions as Robust text-promptable SIS (R-SIS).
Unlike previous approaches, R-SIS is a process that analyzes text prompts for
all surgical instrument categories without relying on external knowledge,
identifies the instruments present in the scene, and segments them accordingly.
Building on this, we propose Robust Surgical Instrument Segmentation (RoSIS),
an optimized framework combining visual and language features for promptable
segmentation in the R-SIS setting. RoSIS employs an encoder-decoder
architecture with a Multi-Modal Fusion Block (MMFB) and a Selective Gate Block
(SGB) for balanced integration of vision and language features. Additionally,
an iterative refinement strategy enhances segmentation masks through a two-step
process: an initial pass with name-based prompts, followed by refinement with
location prompts. Experiments across multiple datasets and settings show that
RoSIS outperforms existing vision-based and promptable segmentation methods
under robust conditions. By rethinking text-promptable SIS, our work
establishes a fair and effective approach to surgical instrument segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 7 tables, submitted to IEEE Journal of
  Biomedical and Health Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Guide Dog: Egocentric Path Prediction on Smartphone <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AI Guide Dog (AIGD), a lightweight egocentric
(first-person) navigation system for visually impaired users, designed for
real-time deployment on smartphones. AIGD employs a vision-only multi-label
classification approach to predict directional commands, ensuring safe
navigation across diverse environments. We introduce a novel technique for
goal-based outdoor navigation by integrating GPS signals and high-level
directions, while also handling uncertain multi-path predictions for
destination-free indoor navigation. As the first navigation assistance system
to handle both goal-oriented and exploratory navigation across indoor and
outdoor settings, AIGD establishes a new benchmark in blind navigation. We
present methods, datasets, evaluations, and deployment insights to encourage
further innovations in assistive navigation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI 2025 Spring Symposium on Human-Compatible AI for
  Well-being: Harnessing Potential of GenAI for AI-Powered Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hiding and Recovering Knowledge in Text-to-Image Diffusion Models via
  Learnable Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12326v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12326v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Bui, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capability in generating
high-quality visual content from textual descriptions. However, since these
models are trained on large-scale internet data, they inevitably learn
undesirable concepts, such as sensitive content, copyrighted material, and
harmful or unethical elements. While previous works focus on permanently
removing such concepts, this approach is often impractical, as it can degrade
model performance and lead to irreversible loss of information. In this work,
we introduce a novel concept-hiding approach that makes unwanted concepts
inaccessible to public users while allowing controlled recovery when needed.
Instead of erasing knowledge from the model entirely, we incorporate a
learnable prompt into the cross-attention module, acting as a secure memory
that suppresses the generation of hidden concepts unless a secret key is
provided. This enables flexible access control -- ensuring that undesirable
content cannot be easily generated while preserving the option to reinstate it
under restricted conditions. Our method introduces a new paradigm where concept
suppression and controlled recovery coexist, which was not feasible in prior
works. We validate its effectiveness on the Stable Diffusion model,
demonstrating that hiding concepts mitigate the risks of permanent removal
while maintaining the model's overall capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision
  <span class="highlight-title">Transformer</span>s under Domain Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10973v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10973v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samar Khanna, Medhanie Irgau, David B. Lobell, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation
(LoRA) can effectively adapt large pre-trained foundation models to downstream
tasks using only a small fraction (0.1%-10%) of the original trainable weights.
An under-explored question of PEFT is in extending the pre-training phase
without supervised labels; that is, can we adapt a pre-trained foundation model
to a new domain via efficient self-supervised pre-training on this new domain?
In this work, we introduce ExPLoRA, a highly effective technique to improve
transfer learning of pre-trained vision transformers (ViTs) under domain
shifts. Initializing a ViT with pre-trained weights on large, natural-image
datasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised
pre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks
and tuning all other layers with LoRA. We then fine-tune the resulting model
only with LoRA on this new domain for supervised learning. Our experiments
demonstrate state-of-the-art results on satellite imagery, even outperforming
fully pre-training and fine-tuning ViTs. Using the DinoV2 training objective,
we demonstrate up to 8% improvement in linear probing top-1 accuracy on
downstream tasks while using <10% of the number of parameters that are used in
prior fully-tuned state-of-the art approaches. Our ablation studies confirm the
efficacy of our approach over other baselines, including PEFT and unfreezing
more ViT blocks. Code is available on the project website:
https://samar-khanna.github.io/ExPLoRA/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">31</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented
  Generation with Flexible User Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
mitigate large language model (LLM) hallucinations by incorporating external
knowledge retrieval. However, existing RAG frameworks often apply retrieval
indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or
failing to retrieve iteratively when required for complex reasoning. Recent
adaptive retrieval strategies, though adaptively navigates these retrieval
strategies, predict only based on query complexity and lacks user-driven
flexibility, making them infeasible for diverse user application needs. In this
paper, we introduce a novel user-controllable RAG framework that enables
dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two
classifiers: one trained to prioritize accuracy and another to prioritize
retrieval efficiency. Via an interpretable control parameter $\alpha$, users
can seamlessly navigate between minimal-cost retrieval and high-accuracy
retrieval based on their specific requirements. We empirically demonstrate that
our approach effectively balances accuracy, retrieval cost, and user
controllability, making it a practical and adaptable solution for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to
  Enhance Wikipedia Tail Bio<span class="highlight-title">graph</span>ies through Personal Narratives <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Pauras Mangesh Meher, Paramita Das, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wikipedia is an invaluable resource for factual information about a wide
range of entities. However, the quality of articles on less-known entities
often lags behind that of the well-known ones. This study proposes a novel
approach to enhancing Wikipedia's B and C category biography articles by
leveraging personal narratives such as autobiographies and biographies. By
utilizing a multi-staged retrieval-augmented generation technique -- REVerSum
-- we aim to enrich the informational content of these lesser-known articles.
Our study reveals that personal narratives can significantly improve the
quality of Wikipedia articles, providing a rich source of reliable information
that has been underutilized in previous studies. Based on crowd-based
evaluation, REVerSum generated content outperforms the best performing baseline
by 17% in terms of integrability to the original Wikipedia article and 28.5\%
in terms of informativeness. Code and Data are available at:
https://github.com/sayantan11995/wikipedia_enrichment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Evaluation of Fairness and Relevance in Recommender Systems with
  Pareto Frontier <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresia Veronika Rampisela, Tuukka Ruotsalo, Maria Maistro, Christina Lioma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness and relevance are two important aspects of recommender systems
(RSs). Typically, they are evaluated either (i) separately by individual
measures of fairness and relevance, or (ii) jointly using a single measure that
accounts for fairness with respect to relevance. However, approach (i) often
does not provide a reliable joint estimate of the goodness of the models, as it
has two different best models: one for fairness and another for relevance.
Approach (ii) is also problematic because these measures tend to be ad-hoc and
do not relate well to traditional relevance measures, like NDCG. Motivated by
this, we present a new approach for jointly evaluating fairness and relevance
in RSs: Distance to Pareto Frontier (DPFR). Given some user-item interaction
data, we compute their Pareto frontier for a pair of existing relevance and
fairness measures, and then use the distance from the frontier as a measure of
the jointly achievable fairness and relevance. Our approach is modular and
intuitive as it can be computed with existing measures. Experiments with 4 RS
models, 3 re-ranking strategies, and 6 datasets show that existing metrics have
inconsistent associations with our Pareto-optimal solution, making DPFR a more
robust and theoretically well-founded joint measure for assessing fairness and
relevance. Our code: https://github.com/theresiavr/DPFR-recsys-evaluation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TheWebConf/WWW 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information
  Retrieval Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Zhirui Deng, Clara Rus, Xiaopeng Ye, Yuanna Liu, Jun Xu, Zhicheng Dou, Ji-Rong Wen, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern information retrieval (IR). achieving more than just accuracy is
essential to sustaining a healthy ecosystem, especially when addressing
fairness and diversity considerations. To meet these needs, various datasets,
algorithms, and evaluation frameworks have been introduced. However, these
algorithms are often tested across diverse metrics, datasets, and experimental
setups, leading to inconsistencies and difficulties in direct comparisons. This
highlights the need for a comprehensive IR toolkit that enables standardized
evaluation of fairness- and diversity-aware algorithms across different IR
tasks. To address this challenge, we present FairDiverse, an open-source and
standardized toolkit. FairDiverse offers a framework for integrating fair and
diverse methods, including pre-processing, in-processing, and post-processing
techniques, at different stages of the IR pipeline. The toolkit supports the
evaluation of 28 fairness and diversity algorithms across 16 base models,
covering two core IR tasks (search and recommendation) thereby establishing a
comprehensive benchmark. Moreover, FairDiverse is highly extensible, providing
multiple APIs that empower IR researchers to swiftly develop and evaluate their
own fairness and diversity aware models, while ensuring fair comparisons with
existing baselines. The project is open-sourced and available on
https://github.com/XuChen0427/FairDiverse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio
  Chord Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-Modal</span> Retrieval Augmentation for Open-Ended and
  Knowledge-Intensive Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Zarif Ul Alam, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current video question answering systems perform well on some tasks
requiring only direct visual understanding, they struggle with questions
demanding knowledge beyond what is immediately observable in the video content.
We refer to this challenging scenario as knowledge-intensive video question
answering (KI-VideoQA), where models must retrieve and integrate external
information with visual understanding to generate accurate responses. This work
presents the first attempt to (1) study multi-modal retrieval-augmented
generation for KI-VideoQA, and (2) go beyond multi-choice questions by studying
open-ended questions in this task. Through an extensive empirical study of
state-of-the-art retrieval and vision language models in both zero-shot and
fine-tuned settings, we explore how different retrieval augmentation strategies
can enhance knowledge integration in KI-VideoQA. We analyze three key aspects:
(1) model's effectiveness across different information sources and modalities,
(2) the impact of heterogeneous multi-modal context integration, and (3)
model's effectiveness across different query formulation and retrieval result
consumption. Our results suggest that while retrieval augmentation generally
improves performance, its effectiveness varies significantly based on modality
choice and retrieval strategy. Additionally, we find that successful knowledge
integration often requires careful consideration of query formulation and
optimal retrieval depth. Our exploration advances state-of-the-art accuracy for
multiple choice questions by over 17.5% on the KnowIT VQA dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Recommendation Explanations through User-Centric Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingsen Zhang, Zihang Tian, Xueyang Feng, Xu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural language explanations for recommendations has become
increasingly important in recommender systems. Traditional approaches typically
treat user reviews as ground truth for explanations and focus on improving
review prediction accuracy by designing various model architectures. However,
due to limitations in data scale and model capability, these explanations often
fail to meet key user-centric aspects such as factuality, personalization, and
sentiment coherence, significantly reducing their overall helpfulness to users.
In this paper, we propose a novel paradigm that refines initial explanations
generated by existing explainable recommender models during the inference stage
to enhance their quality in multiple aspects. Specifically, we introduce a
multi-agent collaborative refinement framework based on large language models.
To ensure alignment between the refinement process and user demands, we employ
a plan-then-refine pattern to perform targeted modifications. To enable
continuous improvements, we design a hierarchical reflection mechanism that
provides feedback on the refinement process from both strategic and content
perspectives. Extensive experiments on three datasets demonstrate the
effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an
  LLM-Assisted Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyu Zhao, Yunxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In quantitative SciSci (science of science) studies, accurately identifying
individual scholars is paramount for scientific data analysis. However, the
variability in how names are represented-due to commonality, abbreviations, and
different spelling conventions-complicates this task. While identifier systems
like ORCID are being developed, many scholars remain unregistered, and numerous
publications are not included. Scholarly databases such as Clarivate and
OpenAlex have introduced their own ID systems as preliminary name
disambiguation solutions. This study evaluates the effectiveness of these
systems across different groups to determine their suitability for various
application scenarios. We sampled authors from the top quartile (Q1) of Web of
Science (WOS) journals based on country, discipline, and number of
corresponding author papers. For each group, we selected 100 scholars and
meticulously annotated all their papers using a Search-enhanced Large Language
Model method. Using these annotations, we identified the corresponding IDs in
OpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS
journals, and calculated precision and recall by comparing against the
annotated dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaMTEB: Massive Text Embedding Benchmark in Persian Language <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)
text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our
benchmark includes 63 datasets spanning seven different tasks: classification,
clustering, pair classification, reranking, retrieval, summary retrieval, and
semantic textual similarity. The datasets are formed as a combination of
existing, translated, and newly generated data, offering a diverse evaluation
framework for Persian language models. Given the increasing use of text
embedding models in chatbots, evaluation datasets are becoming inseparable
ingredients in chatbot challenges and Retrieval-Augmented Generation systems.
As a contribution, we include chatbot evaluation datasets in the MTEB benchmark
for the first time. In addition, in this paper, we introduce the new task of
summary retrieval which is not part of the tasks included in standard MTEB.
Another contribution of this paper is the introduction of a substantial number
of new Persian language NLP datasets suitable for training and evaluation, some
of which have no previous counterparts in Persian. We evaluate the performance
of several Persian and multilingual embedding models in a range of tasks. This
work introduces an open-source benchmark with datasets, code and a public
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU-accelerated Multi-relational Parallel <span class="highlight-title">Graph</span> Retrieval for Web-scale
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Guo, Guangxing Chen, Qian Gao, Xiaochao Liao, Jianjia Zheng, Lu Shen, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web recommendations provide personalized items from massive catalogs for
users, which rely heavily on retrieval stages to trade off the effectiveness
and efficiency of selecting a small relevant set from billion-scale candidates
in online digital platforms. As one of the largest Chinese search engine and
news feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based
Approximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance
estimation and efficient search for relevant items. However, current retrieval
at Baidu fails in comprehensive user-item relational understanding due to
dissected interaction modeling, and performs inefficiently in large-scale
graph-based ANNS because of suboptimal traversal navigation and the GPU
computational bottleneck under high concurrency. To this end, we propose a
GPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to
achieve effective yet efficient retrieval in web-scale recommendations. First,
we propose a multi-relational user-item relevance metric learning method that
unifies diverse user behaviors through multi-objective optimization and employs
a self-covariant loss to enhance pathfinding performance. Second, we develop a
hierarchical parallel graph-based ANNS to boost graph retrieval throughput,
which conducts breadth-depth-balanced searches on a large-scale item graph and
cost-effectively handles irregular neural computation via adaptive aggregation
on GPUs. In addition, we integrate system optimization strategies in the
deployment of GMP-GR in Baidu. Extensive experiments demonstrate the
superiority of GMP-GR in retrieval accuracy and efficiency. Deployed across
more than twenty applications at Baidu, GMP-GR serves hundreds of millions of
users with a throughput exceeding one hundred million requests per second.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLTW: Joint Improved <span class="highlight-title">Graph</span> <span class="highlight-title">Transformer</span> and LLM via Three-Word Language
  for Knowledge <span class="highlight-title">Graph</span> Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Turn <span class="highlight-title">Multi-Modal</span> Question Clarification for Enhanced
  Conversational Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimia Ramezan, Alireza Amiri Bavandpour, Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational query clarification enables users to refine their search
queries through interactive dialogue, improving search effectiveness.
Traditional approaches rely on text-based clarifying questions, which often
fail to capture complex user preferences, particularly those involving visual
attributes. While recent work has explored single-turn multi-modal
clarification with images alongside text, such methods do not fully support the
progressive nature of user intent refinement over multiple turns. Motivated by
this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,
which combines text and visual modalities to refine user queries in a
multi-turn conversation. To facilitate this task, we create a large-scale
dataset named ClariMM comprising over 13k multi-turn interactions and 33k
question-answer pairs containing multi-modal clarifying questions. We propose
Mario, a retrieval framework that employs a two-phase ranking strategy: initial
retrieval with BM25, followed by a multi-modal generative re-ranking model that
integrates textual and visual information from conversational history. Our
experiments show that multi-turn multi-modal clarification outperforms
uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are
most significant in longer interactions, demonstrating the value of progressive
refinement for complex queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Learning to Rank with Query-Level Click Propensity Estimation:
  Beyond Pointwise Observation and Relevance <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lulu Yu, Keping Bi, Jiafeng Guo, Shihao Liu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing unbiased learning-to-rank (ULTR) approaches are based on the
user examination hypothesis, which assumes that users will click a result only
if it is both relevant and observed (typically modeled by position). However,
in real-world scenarios, users often click only one or two results after
examining multiple relevant options, due to limited patience or because their
information needs have already been satisfied. Motivated by this, we propose a
query-level click propensity model to capture the probability that users will
click on different result lists, allowing for non-zero probabilities that users
may not click on an observed relevant result. We hypothesize that this
propensity increases when more potentially relevant results are present, and
refer to this user behavior as relevance saturation bias. Our method introduces
a Dual Inverse Propensity Weighting (DualIPW) mechanism -- combining
query-level and position-level IPW -- to address both relevance saturation and
position bias. Through theoretical derivation, we prove that DualIPW can learn
an unbiased ranking model. Experiments on the real-world Baidu-ULTR dataset
demonstrate that our approach significantly outperforms state-of-the-art ULTR
baselines. The code and dataset information can be found at
https://github.com/Trustworthy-Information-Access/DualIPW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, accepted by The ACM Web Conference (WWW) 2025
  Short Paper Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leave No One Behind: Enhancing Diversity While Maintaining Accuracy in
  Social Recommendation <span class="chip">DASFAA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Xiao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social recommendation, a branch of algorithms that utilizes social connection
information to construct recommender systems, has demonstrated its
effectiveness in enhancing recommendation accuracy. However, apart from
accuracy, the diversity of recommendations also plays a critical role in user
engagement. Unfortunately, the impact of social recommendation models on
recommendation diversity remains largely unexplored. In this study, we
investigate the dual performance of existing social recommendation algorithms
in terms of accuracy and diversity. Our empirical findings highlight a
concerning trend: social recommendation models tend to decrease diversity,
despite their accuracy improvements. To address this issue, we propose a novel
approach called Diversified Social Recommendation (DivSR), which leverages
relational knowledge distillation techniques to transfer high-diversity
structured knowledge from non-social recommendation models to social
recommendation models. DivSR is designed as a simple, model-agnostic framework
that integrates seamlessly with existing social recommendation architectures.
Experimental results on three benchmark datasets demonstrate that DivSR
significantly increases diversity without markedly compromising accuracy across
various social recommendation backbones, achieving a better accuracy-diversity
trade-off. Our code and data are publicly available at:
https://github.com/ll0ruc/DivSR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG vs. <span class="highlight-title">Graph</span>RAG: A Systematic Evaluation and Key Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across
various tasks by retrieving relevant information from external sources,
particularly on text-based data. For structured data, such as knowledge graphs,
GraphRAG has been widely used to retrieve relevant information. However, recent
studies have revealed that structuring implicit knowledge from text into graphs
can benefit certain tasks, extending the application of GraphRAG from graph
data to general text-based data. Despite their successful extensions, most
applications of GraphRAG for text data have been designed for specific tasks
and datasets, lacking a systematic evaluation and comparison between RAG and
GraphRAG on widely used text-based benchmarks. In this paper, we systematically
evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question
Answering and Query-based Summarization. Our results highlight the distinct
strengths of RAG and GraphRAG across different tasks and evaluation
perspectives. Inspired by these observations, we investigate strategies to
integrate their strengths to improve downstream tasks. Additionally, we provide
an in-depth discussion of the shortcomings of current GraphRAG approaches and
outline directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Ranking on Cascading Behavior <span class="highlight-title">Graph</span>s for Accurate
  Multi-Behavior Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonwoo Ko, Minseo Jeon, Jinhong Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-behavior recommendation predicts items a user may purchase by analyzing
diverse behaviors like viewing, adding to a cart, and purchasing. Existing
methods fall into two categories: representation learning and graph ranking.
Representation learning generates user and item embeddings to capture latent
interaction patterns, leveraging multi-behavior properties for better
generalization. However, these methods often suffer from over-smoothing and
bias toward frequent interactions, limiting their expressiveness. Graph ranking
methods, on the other hand, directly compute personalized ranking scores,
capturing user preferences more effectively. Despite their potential, graph
ranking approaches have been primarily explored in single-behavior settings and
remain underutilized for multi-behavior recommendation. In this paper, we
propose CascadingRank, a novel graph ranking method for multi-behavior
recommendation. It models the natural sequence of user behaviors (e.g.,
viewing, adding to cart, and purchasing) through a cascading behavior graph. An
iterative algorithm computes ranking scores, ensuring smoothness, query
fitting, and cascading alignment. Experiments on three real-world datasets
demonstrate that CascadingRank outperforms state-of-the-art methods, with up to
9.56% and 7.16% improvements in HR@10 and NDCG@10, respectively. Furthermore,
we provide theoretical analysis highlighting its effectiveness, convergence,
and scalability, showcasing the advantages of graph ranking in multi-behavior
recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RDSA: A Robust Deep <span class="highlight-title">Graph</span> Clustering Framework via Dual Soft Assignment <span class="chip">DASFAA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xiang, Li Fan, Tulika Saha, Xiaoying Pang, Yushan Pan, Haiyang Zhang, Chengtao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering is an essential aspect of network analysis that involves
grouping nodes into separate clusters. Recent developments in deep learning
have resulted in graph clustering, which has proven effective in many
applications. Nonetheless, these methods often encounter difficulties when
dealing with real-world graphs, particularly in the presence of noisy edges.
Additionally, many denoising graph clustering methods tend to suffer from lower
performance, training instability, and challenges in scaling to large datasets
compared to non-denoised models. To tackle these issues, we introduce a new
framework called the Robust Deep Graph Clustering Framework via Dual Soft
Assignment (RDSA). RDSA consists of three key components: (i) a node embedding
module that effectively integrates the graph's topological features and node
attributes; (ii) a structure-based soft assignment module that improves graph
modularity by utilizing an affinity matrix for node assignments; and (iii) a
node-based soft assignment module that identifies community landmarks and
refines node assignments to enhance the model's robustness. We assess RDSA on
various real-world datasets, demonstrating its superior performance relative to
existing state-of-the-art methods. Our findings indicate that RDSA provides
robust clustering across different graph types, excelling in clustering
effectiveness and robustness, including adaptability to noise, stability, and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2025; Complete version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Memory Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05558v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05558v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lu, Zheng Chai, Yuchao Zheng, Zhe Chen, Deping Xie, Peng Xu, Xun Zhou, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reason4Rec: Large Language Models for Recommendation with Deliberative
  User Preference Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Wenjie Wang, Yang Zhang, Fengbin Zhu, Qifan Wang, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent advancements in aligning Large Language Models (LLMs) with
recommendation tasks have shown great potential and promising performance
overall, these aligned recommendation LLMs still face challenges in complex
scenarios. This is primarily due to the current alignment approach focusing on
optimizing LLMs to generate user feedback directly, without incorporating
deliberation. To overcome this limitation and develop more reliable LLMs for
recommendations, we propose a new Deliberative Recommendation task, which
incorporates explicit reasoning about user preferences as an additional
alignment goal. We then introduce the Reasoning-powered Recommender framework
for deliberative user preference alignment, designed to enhance reasoning
capabilities by utilizing verbalized user feedback in a step-wise manner to
tackle this task. The framework employs collaborative step-wise experts and
tailored training strategies for each expert. Experimental results across three
real-world datasets demonstrate the rationality of the deliberative task
formulation and the superior performance of the proposed framework in improving
both prediction accuracy and reasoning quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Channel Multiplex <span class="highlight-title">Graph</span> Neural Networks for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interactive relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant challenges: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations within behavior patterns on the target relation in recommender system
scenarios. In this work, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interactive relations, and includes a relation chain representation
learner and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06% and 12.15% on average across all datasets in terms of
Recall@10 and NDCG@10 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciPIP: An LLM-based Scientific Paper Idea Proposer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has opened new
possibilities for automating the proposal of innovative scientific ideas. This
process involves two key phases: literature retrieval and idea generation.
However, existing approaches often fall short due to their reliance on
keyword-based search tools during the retrieval phase, which neglects crucial
semantic information and frequently results in incomplete retrieval outcomes.
Similarly, in the idea generation phase, current methodologies tend to depend
solely on the internal knowledge of LLMs or metadata from retrieved papers,
thereby overlooking significant valuable insights contained within the full
texts. To address these limitations, we introduce SciPIP, an innovative
framework designed to enhance the LLM-based proposal of scientific ideas
through improvements in both literature retrieval and idea generation. Our
approach begins with the construction of a comprehensive literature database
that supports advanced retrieval based not only on keywords but also on
semantics and citation relationships. This is complemented by the introduction
of a multi-granularity retrieval algorithm aimed at ensuring more thorough and
exhaustive retrieval results. For the idea generation phase, we propose a
dual-path framework that effectively integrates both the content of retrieved
papers and the extensive internal knowledge of LLMs. This integration
significantly boosts the novelty, feasibility, and practical value of proposed
ideas. Our experiments, conducted across various domains such as natural
language processing and computer vision, demonstrate SciPIP's capability to
generate a multitude of innovative and useful ideas. These findings underscore
SciPIP's potential as a valuable tool for researchers seeking to advance their
fields with groundbreaking concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 12 tables. The code has been availabel:
  https://github.com/cheerss/SciPIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Recommender Systems via <span class="highlight-title">Multimodal</span> Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adamya Shyam, Ramya Kamani, Venkateswara Rao Kagita, Vikas Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Filtering (CF) has emerged as one of the most prominent
implementation strategies for building recommender systems. The key idea is to
exploit the usage patterns of individuals to generate personalized
recommendations. CF techniques, especially for newly launched platforms, often
face a critical issue known as the data sparsity problem, which greatly limits
their performance. Cross-domain CF alleviates the problem of data sparsity by
finding a common set of entities (users or items) across the domains, which
then act as a conduit for knowledge transfer. Nevertheless, most real-world
datasets are collected from different domains, so they often lack information
about anchor points or reference information for entity alignment. This paper
introduces a domain adaptation technique to align the embeddings of entities
across domains. Our approach first exploits the available textual and visual
information to independently learn a multi-view latent representation for each
entity in the auxiliary and target domains. The different representations of
the entity are then fused to generate the corresponding unified representation.
A domain classifier is then trained to learn the embedding for the domain
alignment by fixing the unified features as the anchor points. Experiments on
\AS{four} publicly available benchmark datasets indicate the effectiveness of
our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multimodal</span> semantic retrieval for product search <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Esther Lopez Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval (also known as dense retrieval) based on textual data has
been extensively studied for both web search and product search application
fields, where the relevance of a query and a potential target document is
computed by their dense vector representation comparison. Product image is
crucial for e-commerce search interactions and is a key factor for customers at
product explorations. However, its impact on semantic retrieval has not been
well studied yet. In this research, we build a multimodal representation for
product items in e-commerce search in contrast to pure-text representation of
products, and investigate the impact of such representations. The models are
developed and evaluated on e-commerce datasets. We demonstrate that a
multimodal representation scheme for a product can show improvement either on
purchase recall or relevance accuracy in semantic retrieval. Additionally, we
provide numerical analysis for exclusive matches retrieved by a multimodal
semantic retrieval model versus a text-only semantic retrieval model, to
demonstrate the validation of multimodal solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EReL@MIR WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data and Decision Traceability for SDA TAP Lab's Prototype Battle
  Management System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Latha Pratti, Samya Bagchi, Yasir Latif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space Protocol is applying the principles derived from MITRE and NIST's
Supply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a
complex multi party system to achieve introspection, auditing, and replay of
data and decisions that ultimately lead to a end decision. The core goal of
decision traceability is to ensure transparency, accountability, and integrity
within the WA system. This is accomplished by providing a clear, auditable path
from the system's inputs all the way to the final decision. This traceability
enables the system to track the various algorithms and data flows that have
influenced a particular outcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CuriousLLM: Elevating Multi-Document Question Answering with
  LLM-Enhanced Knowledge <span class="highlight-title">Graph</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zukang Yang, Zixuan Zhu, Xuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success in open-domain
question answering. However, they continue to face challenges such as
hallucinations and knowledge cutoffs. These issues can be mitigated through
in-context learning by providing LLMs with relevant context before generating
answers. Recent literature proposes Knowledge Graph Prompting (KGP) which
integrates knowledge graphs with an LLM-based traversal agent to substantially
enhance document retrieval quality. However, KGP requires costly fine-tuning
with large datasets and remains prone to hallucination. In this paper, we
propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning
mechanism into an LLM agent. This mechanism enables the agent to generate
relevant follow-up questions, thereby guiding the information retrieval process
more efficiently. Central to our approach is the development of the new
Follow-upQA dataset, which includes questions and supporting evidence as input,
with follow-up questions serving as ground truths. These follow-up questions
either inquire about what is still missing to fully answer the user's query or
use special tokens to signify that the retrieved evidence is sufficient. Our
experiments show that CuriousLLM significantly boosts LLM performance in
multi-document question answering (MD-QA), circumventing the substantial
computational costs and latency from the original KGP framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Factors Paper-<span class="highlight-title">Review</span>er Matching <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid increase in paper submissions to academic conferences, the
need for automated and accurate paper-reviewer matching is more critical than
ever. Previous efforts in this area have considered various factors to assess
the relevance of a reviewer's expertise to a paper, such as the semantic
similarity, shared topics, and citation connections between the paper and the
reviewer's previous works. However, most of these studies focus on only one
factor, resulting in an incomplete evaluation of the paper-reviewer relevance.
To address this issue, we propose a unified model for paper-reviewer matching
that jointly considers semantic, topic, and citation factors. To be specific,
during training, we instruction-tune a contextualized language model shared
across all factors to capture their commonalities and characteristics; during
inference, we chain the three factors to enable step-by-step, coarse-to-fine
search for qualified reviewers given a submission. Experiments on four datasets
(one of which is newly contributed by us) spanning various fields such as
machine learning, computer vision, information retrieval, and data mining
consistently demonstrate the effectiveness of our proposed Chain-of-Factors
model in comparison with state-of-the-art paper-reviewer matching methods and
scientific pre-trained language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; Accepted to WWW 2025 (Code:
  https://github.com/yuzhimanhua/CoF)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It
mainly consists of retrieval and generation. The retrieval modules (a.k.a.
retrievers) aim to find useful information used to facilitate the generation
modules (a.k.a. generators). As such, generators' performance largely depends
on the effectiveness and efficiency of retrievers. However, the widely used
retrieval paradigm remains flat. It treats retrieval procedures as a one-off
deal with constant granularity. Despite effectiveness, we argue that they
suffer from two limitations: (1) flat retrieval exerts a significant burden on
one retriever; (2) constant granularity limits the ceiling of retrieval
performance. In this work, we propose a progressive retrieval paradigm with
coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance
effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive
retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small
quantity, and low-to-high capacity, which can relieve the burden on one
retriever and also promote the ceiling of retrieval performance. Extensive
experiments manifest that FunnelRAG achieves comparable retrieval performance
while the time overhead is reduced by nearly 40 percent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 13 tables. Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span> Foundation Models for Recommendation: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) serve as a fundamental tool for navigating the vast
expanse of online information, with deep learning advancements playing an
increasingly important role in improving ranking accuracy. Among these, graph
neural networks (GNNs) excel at extracting higher-order structural information,
while large language models (LLMs) are designed to process and comprehend
natural language, making both approaches highly effective and widely adopted.
Recent research has focused on graph foundation models (GFMs), which integrate
the strengths of GNNs and LLMs to model complex RS problems more efficiently by
leveraging the graph-based structure of user-item relationships alongside
textual understanding. In this survey, we provide a comprehensive overview of
GFM-based RS technologies by introducing a clear taxonomy of current
approaches, diving into methodological details, and highlighting key challenges
and future directions. By synthesizing recent advancements, we aim to offer
valuable insights into the evolving landscape of GFM-based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-granularity Interest Retrieval and Refinement Network for
  Long-Term User Behavior Modeling in CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15005v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15005v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong Yu, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through Rate (CTR) prediction is crucial for online personalization
platforms. Recent advancements have shown that modeling rich user behaviors can
significantly improve the performance of CTR prediction. Current long-term user
behavior modeling algorithms predominantly follow two cascading stages. The
first stage retrieves subsequence related to the target item from the long-term
behavior sequence, while the second stage models the relationship between the
subsequence and the target item. Despite significant progress, these methods
have two critical flaws. First, the retrieval query typically includes only
target item information, limiting the ability to capture the user's diverse
interests. Second, relational information, such as sequential and interactive
information within the subsequence, is frequently overlooked. Therefore, it
requires to be further mined to more accurately model user interests.
  To this end, we propose Multi-granularity Interest Retrieval and Refinement
Network (MIRRN). Specifically, we first construct queries based on behaviors
observed at different time scales to obtain subsequences, each capturing users'
interest at various granularities. We then introduce an noval multi-head
Fourier transformer to efficiently learn sequential and interactive information
within the subsequences, leading to more accurate modeling of user interests.
Finally, we employ multi-head target attention to adaptively assess the impact
of these multi-granularity interests on the target item. Extensive experiments
have demonstrated that MIRRN significantly outperforms state-of-the-art
baselines. Furthermore, an A/B test shows that MIRRN increases the average
number of listening songs by 1.32% and the average time of listening songs by
0.55% on the Huawei Music App. The implementation code is publicly available at
https://github.com/USTC-StarTeam/MIRRN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing <span class="highlight-title">Multimodal</span> Queries Representation via Fusion-Free
  Modality Interaction for <span class="highlight-title">Multimodal</span> Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-EXR: Controllable <span class="highlight-title">Review</span> Generation for Explainable
  Recommendation via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Li, Shaohua Li, Winda Marantika, Alex C. Kot, Huijing Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Model (DDPM) has shown great competence in
image and audio generation tasks. However, there exist few attempts to employ
DDPM in the text generation, especially review generation under recommendation
systems. Fueled by the predicted reviews explainability that justifies
recommendations could assist users better understand the recommended items and
increase the transparency of recommendation system, we propose a Diffusion
Model-based Review Generation towards EXplainable Recommendation named
Diffusion-EXR. Diffusion-EXR corrupts the sequence of review embeddings by
incrementally introducing varied levels of Gaussian noise to the sequence of
word embeddings and learns to reconstruct the original word representations in
the reverse process. The nature of DDPM enables our lightweight Transformer
backbone to perform excellently in the recommendation review generation task.
Extensive experimental results have demonstrated that Diffusion-EXR can achieve
state-of-the-art review generation for recommendation on two publicly available
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We request to withdraw our paper from the archive due to significant
  errors identified in the analysis and conclusions. Upon further review, we
  realized that these errors undermine the validity of our findings. We plan to
  conduct additional research to correct these issues and resubmit a revised
  version in the future</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">200</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models without Classifier-free Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Jianmin Bao, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Model-guidance (MG), a novel objective for training
diffusion model that addresses and removes of the commonly used Classifier-free
guidance (CFG). Our innovative approach transcends the standard modeling of
solely data distribution to incorporating the posterior probability of
conditions. The proposed technique originates from the idea of CFG and is easy
yet effective, making it a plug-and-play module for existing models. Our method
significantly accelerates the training process, doubles the inference speed,
and achieve exceptional quality that parallel and even surpass concurrent
diffusion models with CFG. Extensive experiments demonstrate the effectiveness,
efficiency, scalability on different models and datasets. Finally, we establish
state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.
Our code is available at https://github.com/tzco/Diffusion-wo-CFG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Getting-Up Policies for Real-World Humanoid Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xialin He, Runpei Dong, Zixuan Chen, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic fall recovery is a crucial prerequisite before humanoid robots can
be reliably deployed. Hand-designing controllers for getting up is difficult
because of the varied configurations a humanoid can end up in after a fall and
the challenging terrains humanoid robots are expected to operate on. This paper
develops a learning framework to produce controllers that enable humanoid
robots to get up from varying configurations on varying terrains. Unlike
previous successful applications of humanoid locomotion learning, the
getting-up task involves complex contact patterns, which necessitates
accurately modeling the collision geometry and sparser rewards. We address
these challenges through a two-phase approach that follows a curriculum. The
first stage focuses on discovering a good getting-up trajectory under minimal
constraints on smoothness or speed / torque limits. The second stage then
refines the discovered motions into deployable (i.e. smooth and slow) motions
that are robust to variations in initial configuration and terrains. We find
these innovations enable a real-world G1 humanoid robot to get up from two main
situations that we considered: a) lying face up and b) lying face down, both
tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass
and snowfield). To the best of our knowledge, this is the first successful
demonstration of learned getting-up policies for human-sized humanoid robots in
the real world. Project page: https://humanoid-getup.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://humanoid-getup.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Smooth and Expressive Interatomic Potentials for Physical
  Property Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fu, Brandon M. Wood, Luis Barroso-Luque, Daniel S. Levine, Meng Gao, Misko Dzamba, C. Lawrence Zitnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning interatomic potentials (MLIPs) have become increasingly
effective at approximating quantum mechanical calculations at a fraction of the
computational cost. However, lower errors on held out test sets do not always
translate to improved results on downstream physical property prediction tasks.
In this paper, we propose testing MLIPs on their practical ability to conserve
energy during molecular dynamic simulations. If passed, improved correlations
are found between test errors and their performance on physical property
prediction tasks. We identify choices which may lead to models failing this
test, and use these observations to improve upon highly-expressive models. The
resulting model, eSEN, provides state-of-the-art results on a range of physical
property prediction tasks, including materials stability prediction, thermal
conductivity prediction, and phonon calculations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked
  Entities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Sestak, Artur Toshev, Andreas Fürst, Günter Klambauer, Andreas Mayr, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models are spearheading recent progress in deep learning, showing
strong promise for trajectory sampling in dynamical systems as well. However,
while latent space modeling paradigms have transformed image and video
generation, similar approaches are more difficult for most dynamical systems.
Such systems -- from chemical molecule structures to collective human behavior
-- are described by interactions of entities, making them inherently linked to
connectivity patterns and the traceability of entities over time. Our approach,
LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked
Entities), combines the advantages of graph neural networks, i.e., the
traceability of entities across time-steps, with the efficiency and scalability
of recent advances in image and video generation, where pre-trained encoder and
decoder are frozen to enable generative modeling in the latent space. The core
idea of LaM-SLidE is to introduce identifier representations (IDs) to allow for
retrieval of entity properties, e.g., entity coordinates, from latent system
representations and thus enables traceability. Experimentally, across different
domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,
and generalizability. (Code is available at
https://github.com/ml-jku/LaM-SLidE)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ml-jku.github.io/LaM-SLidE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the
  Lens of Class Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Malashin, Valeria Yachnaya, Alexander Mullin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the training dynamics of deep classifiers by examining how
hierarchical relationships between classes evolve during training. Through
extensive experiments, we argue that the learning process in classification
problems can be understood through the lens of label clustering. Specifically,
we observe that networks tend to distinguish higher-level (hypernym) categories
in the early stages of training, and learn more specific (hyponym) categories
later. We introduce a novel framework to track the evolution of the feature
manifold during training, revealing how the hierarchy of class relations
emerges and refines across the network layers. Our analysis demonstrates that
the learned representations closely align with the semantic structure of the
dataset, providing a quantitative description of the clustering process.
Notably, we show that in the hypernym label space, certain properties of neural
collapse appear earlier than in the hyponym label space, helping to bridge the
gap between the initial and terminal phases of learning. We believe our
findings offer new insights into the mechanisms driving hierarchical learning
in deep networks, paving the way for future advancements in understanding deep
learning dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Query Complexity of Verifier-Assisted Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan T. Ash, Cyril Zhang, Andrej Risteski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a plethora of works have proposed inference-time algorithms (e.g.
best-of-n), which incorporate verifiers to assist the generation process. Their
quality-efficiency trade-offs have been empirically benchmarked on a variety of
constrained generation tasks, but the algorithmic design landscape is still
largely poorly understood. In this paper, we develop a mathematical framework
for reasoning about constrained generation using a pre-trained language model
generator oracle and a process verifier--which can decide whether a prefix can
be extended to a string which satisfies the constraints of choice. We show that
even in very simple settings, access to a verifier can render an intractable
problem (information-theoretically or computationally) to a tractable one. In
fact, we show even simple algorithms, like tokenwise rejection sampling, can
enjoy significant benefits from access to a verifier. Empirically, we show that
a natural modification of tokenwise rejection sampling, in which the sampler is
allowed to "backtrack" (i.e., erase the final few generated tokens) has robust
and substantive benefits over natural baselines (e.g. (blockwise) rejection
sampling, nucleus sampling)--both in terms of computational efficiency,
accuracy and diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty
  Quantification for LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patryk Marszałek, Klaudia Bałazy, Jacek Tabor, Tomasz Kuśmierczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large
language models by decomposing weight updates into low-rank matrices,
significantly reducing storage and computational overhead. While effective,
standard LoRA lacks mechanisms for uncertainty quantification, leading to
overconfident and poorly calibrated models. Bayesian variants of LoRA address
this limitation, but at the cost of a significantly increased number of
trainable parameters, partially offsetting the original efficiency gains.
Additionally, these models are harder to train and may suffer from unstable
convergence.
  In this work, we propose a novel parameter-efficient Bayesian LoRA,
demonstrating that effective uncertainty quantification can be achieved in very
low-dimensional parameter spaces. The proposed method achieves strong
performance with improved calibration and generalization while maintaining
computational efficiency. Our empirical findings show that, with the
appropriate projection of the weight space: (1) uncertainty can be effectively
modeled in a low-dimensional space, and (2) weight covariances exhibit low
ranks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws guide the development of large language models (LLMs) by
offering estimates for the optimal balance of model size, tokens, and compute.
More recently, loss-to-loss scaling laws that relate losses across pretraining
datasets and downstream tasks have emerged as a powerful tool for understanding
and improving LLM performance. In this work, we investigate which factors most
strongly influence loss-to-loss scaling. Our experiments reveal that the
pretraining data and tokenizer determine the scaling trend. In contrast, model
size, optimization hyperparameters, and even significant architectural
differences, such as between transformer-based models like Llama and
state-space models like Mamba, have limited impact. Consequently, practitioners
should carefully curate suitable pretraining datasets for optimal downstream
performance, while architectures and other settings can be freely optimized for
training efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Test-Time Compute Without Verification or RL is Suboptimal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial advances in scaling test-time compute, an ongoing debate
in the community is how it should be scaled up to enable continued and
efficient improvements with scaling. There are largely two approaches: first,
distilling successful search or thinking traces; and second, using verification
(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement
learning (RL) and search algorithms. In this paper, we prove that finetuning
LLMs with verifier-based (VB) methods based on RL or search is far superior to
verifier-free (VF) approaches based on distilling or cloning search traces,
given a fixed amount of compute/data budget. Further, we show that as we scale
test-time compute (measured as the output token length) and training data,
suboptimality of VF methods scales poorly compared to VB when the base
pre-trained LLM presents a heterogeneous distribution over correct solution
traces (e.g., different lengths, styles, etc.) and admits a non-sharp
distribution over rewards on traces sampled from it. We formalize this
condition using anti-concentration [Erd\H{o}s, 1945]. This implies a stronger
result that VB methods scale better asymptotically, with the performance gap
between VB and VF methods widening as test-time budget grows. We corroborate
our theory empirically on both didactic and math reasoning problems with
3/8/32B-sized pre-trained LLMs, where we find verification is crucial for
scaling test-time compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance
  Software Engineering? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SWE-Lancer, a benchmark of over 1,400 freelance software
engineering tasks from Upwork, valued at \$1 million USD total in real-world
payouts. SWE-Lancer encompasses both independent engineering tasks--ranging
from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks,
where models choose between technical implementation proposals. Independent
tasks are graded with end-to-end tests triple-verified by experienced software
engineers, while managerial decisions are assessed against the choices of the
original hired engineering managers. We evaluate model performance and find
that frontier models are still unable to solve the majority of tasks. To
facilitate future research, we open-source a unified Docker image and a public
evaluation split, SWE-Lancer Diamond
(https://github.com/openai/SWELancer-Benchmark). By mapping model performance
to monetary value, we hope SWE-Lancer enables greater research into the
economic impact of AI model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 24 pages appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using the Path of Least Resistance to Explain Deep Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Salek, Joseph Enguehard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated Gradients (IG), a widely used axiomatic path-based attribution
method, assigns importance scores to input features by integrating model
gradients along a straight path from a baseline to the input. While effective
in some cases, we show that straight paths can lead to flawed attributions. In
this paper, we identify the cause of these misattributions and propose an
alternative approach that treats the input space as a Riemannian manifold,
computing attributions by integrating gradients along geodesics. We call this
method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we
introduce two techniques: a k-Nearest Neighbours-based approach for smaller
models and a Stochastic Variational Inference-based method for larger ones.
Additionally, we propose a new axiom, Strong Completeness, extending the axioms
satisfied by IG. We show that this property is desirable for attribution
methods and that GIG is the only method that satisfies it. Through experiments
on both synthetic and real-world data, we demonstrate that GIG outperforms
existing explainability methods, including IG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How compositional generalization and creativity improve as diffusion
  models are trained 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Favero, Antonio Sclocchi, Francesco Cagnetta, Pascal Frossard, Matthieu Wyart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural data is often organized as a hierarchical composition of features.
How many samples do generative models need to learn the composition rules, so
as to produce a combinatorial number of novel data? What signal in the data is
exploited to learn? We investigate these questions both theoretically and
empirically. Theoretically, we consider diffusion models trained on simple
probabilistic context-free grammars - tree-like graphical models used to
represent the structure of data such as language and images. We demonstrate
that diffusion models learn compositional rules with the sample complexity
required for clustering features with statistically similar context, a process
similar to the word2vec algorithm. However, this clustering emerges
hierarchically: higher-level, more abstract features associated with longer
contexts require more data to be identified. This mechanism leads to a sample
complexity that scales polynomially with the said context size. As a result,
diffusion models trained on intermediate dataset size generate data coherent up
to a certain scale, but that lacks global coherence. We test these predictions
in different domains, and find remarkable agreement: both generated texts and
images achieve progressively larger coherence lengths as the training time or
dataset size grows. We discuss connections between the hierarchical clustering
mechanism we introduce here and the renormalization group in physics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Statistical Learning: Supervised Learning of Statistical Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Peyrard, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work demonstrates that the tools and principles driving the success of
large language models (LLMs) can be repurposed to tackle distribution-level
tasks, where the goal is to predict properties of the data-generating
distribution rather than labels for individual datapoints. These tasks
encompass statistical inference problems such as parameter estimation,
hypothesis testing, or mutual information estimation. Framing these tasks
within traditional machine learning pipelines is challenging, as supervision is
typically tied to individual datapoint. We propose meta-statistical learning, a
framework inspired by multi-instance learning that reformulates statistical
inference tasks as supervised learning problems. In this approach, entire
datasets are treated as single inputs to neural networks, which predict
distribution-level parameters. Transformer-based architectures, without
positional encoding, provide a natural fit due to their permutation-invariance
properties. By training on large-scale synthetic datasets, meta-statistical
models can leverage the scalability and optimization infrastructure of
Transformer-based LLMs. We demonstrate the framework's versatility with
applications in hypothesis testing and mutual information estimation, showing
strong performance, particularly for small datasets where traditional neural
methods struggle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Explainable Anomaly Detection and Root Cause Analysis in
  Dynamical Systems <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Sun, Rick S. Blum, Parv Venkitasubramaniam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical systems, prevalent in various scientific and engineering domains,
are susceptible to anomalies that can significantly impact their performance
and reliability. This paper addresses the critical challenges of anomaly
detection, root cause localization, and anomaly type classification in
dynamical systems governed by ordinary differential equations (ODEs). We define
two categories of anomalies: cyber anomalies, which propagate through
interconnected variables, and measurement anomalies, which remain localized to
individual variables. To address these challenges, we propose the Interpretable
Causality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic
explainable learning framework. ICODE leverages Neural ODEs for anomaly
detection while employing causality inference through an explanation channel to
perform root cause analysis (RCA), elucidating why specific time periods are
flagged as anomalous. ICODE is designed to simultaneously perform anomaly
detection, RCA, and anomaly type classification within a single, interpretable
framework. Our approach is grounded in the hypothesis that anomalies alter the
underlying ODEs of the system, manifesting as changes in causal relationships
between variables. We provide a theoretical analysis of how perturbations in
learned model parameters can be utilized to identify anomalies and their root
causes in time series data. Comprehensive experimental evaluations demonstrate
the efficacy of ICODE across various dynamical systems, showcasing its ability
to accurately detect anomalies, classify their types, and pinpoint their
origins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber
  Security (AICS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APB: Accelerating Distributed Long-Context Inference by Passing
  Compressed Context Blocks across GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Hao Zhou, Jie Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While long-context inference is crucial for advancing large language model
(LLM) applications, its prefill speed remains a significant bottleneck. Current
approaches, including sequence parallelism strategies and compute reduction
through approximate attention mechanisms, still fall short of delivering
optimal inference efficiency. This hinders scaling the inputs to longer
sequences and processing long-context queries in a timely manner. To address
this, we introduce APB, an efficient long-context inference framework that
leverages multi-host approximate attention to enhance prefill speed by reducing
compute and enhancing parallelism simultaneously. APB introduces a
communication mechanism for essential key-value pairs within a sequence
parallelism framework, enabling a faster inference speed while maintaining task
performance. We implement APB by incorporating a tailored FlashAttn kernel
alongside optimized distribution strategies, supporting diverse models and
parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x
compared with FlashAttn, RingAttn, and StarAttn, respectively, without any
observable task performance degradation. We provide the implementation and
experiment code of APB in https://github.com/thunlp/APB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaSplash: Adaptive Sparse Flash Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuno Gonçalves, Marcos Treviso, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computational cost of softmax-based attention in transformers limits
their applicability to long-context tasks. Adaptive sparsity, of which
$\alpha$-entmax attention is an example, offers a flexible data-dependent
alternative, but existing implementations are inefficient and do not leverage
the sparsity to obtain runtime and memory gains. In this work, we propose
AdaSplash, which combines the efficiency of GPU-optimized algorithms with the
sparsity benefits of $\alpha$-entmax. We first introduce a hybrid
Halley-bisection algorithm, resulting in a 7-fold reduction in the number of
iterations needed to compute the $\alpha$-entmax transformation. Then, we
implement custom Triton kernels to efficiently handle adaptive sparsity.
Experiments with RoBERTa and ModernBERT for text classification and
single-vector retrieval, along with GPT-2 for language modeling, show that our
method achieves substantial improvements in runtime and memory efficiency
compared to existing $\alpha$-entmax implementations. It approaches -- and in
some cases surpasses -- the efficiency of highly optimized softmax
implementations like FlashAttention-2, enabling long-context training while
maintaining strong task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication
  Facilities with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Xue Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating planning with LLMs presents transformative opportunities for
traditional industries, yet remains underexplored. In commercial construction,
the complexity of automated scheduling often requires manual intervention to
ensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to
optimize construction schedules in complex projects like semiconductor
fabrication. CONSTRUCTA addresses key challenges by: (1) integrating
construction-specific knowledge through static RAG; (2) employing
context-sampling techniques inspired by architectural expertise to provide
relevant input; and (3) deploying Construction DPO to align schedules with
expert preferences using RLHF. Experiments on proprietary data demonstrate
performance improvements of +42.3% in missing value prediction, +79.1% in
dependency analysis, and +28.9% in automated planning compared to baseline
methods, showcasing its potential to revolutionize construction workflows and
inspire domain-specific LLM advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Rank Thinning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal in thinning is to summarize a dataset using a small set of
representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel
Halving and Compress can match the quality of uniform subsampling while
substantially reducing the number of summary points. However, existing
guarantees cover only a restricted range of distributions and kernel-based
quality measures and suffer from pessimistic dimension dependence. To address
these deficiencies, we introduce a new low-rank analysis of sub-Gaussian
thinning that applies to any distribution and any kernel, guaranteeing
high-quality compression whenever the kernel or data matrix is approximately
low-rank. To demonstrate the broad applicability of the techniques, we design
practical sub-Gaussian thinning approaches that improve upon the best known
guarantees for approximating attention in transformers, accelerating stochastic
gradient training through reordering, and distinguishing distributions in
near-linear time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Upscale Neural Networks with Scaling Law? A <span class="highlight-title">Survey</span> and Practical
  Guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Sengupta, Yash Goel, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural scaling laws have revolutionized the design and optimization of
large-scale AI models by revealing predictable relationships between model
size, dataset volume, and computational resources. Early research established
power-law relationships in model performance, leading to compute-optimal
scaling strategies. However, recent studies highlighted their limitations
across architectures, modalities, and deployment contexts. Sparse models,
mixture-of-experts, retrieval-augmented learning, and multimodal models often
deviate from traditional scaling patterns. Moreover, scaling behaviors vary
across domains such as vision, reinforcement learning, and fine-tuning,
underscoring the need for more nuanced approaches. In this survey, we
synthesize insights from over 50 studies, examining the theoretical
foundations, empirical findings, and practical implications of scaling laws. We
also explore key challenges, including data efficiency, inference scaling, and
architecture-specific constraints, advocating for adaptive scaling strategies
tailored to real-world applications. We suggest that while scaling laws provide
a useful guide, they do not always generalize across all architectures and
training strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying the Stoichiometry of Virus-like Particles with Interpretable
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Zhang, Xianyuan Liu, Wei Wu, Sina Tabakhi, Wenrui Fan, Shuo Zhou, Kang Lan Tee, Tuck Seng Wong, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virus-like particles (VLPs) are valuable for vaccine development due to their
immune-triggering properties. Understanding their stoichiometry, the number of
protein subunits to form a VLP, is critical for vaccine optimisation. However,
current experimental methods to determine stoichiometry are time-consuming and
require highly purified proteins. To efficiently classify stoichiometry classes
in proteins, we curate a new dataset and propose an interpretable, data-driven
pipeline leveraging linear machine learning models. We also explore the impact
of feature encoding on model performance and interpretability, as well as
methods to identify key protein sequence features influencing classification.
The evaluation of our pipeline demonstrates that it can classify stoichiometry
while revealing protein features that possibly influence VLP assembly. The data
and code used in this work are publicly available at
https://github.com/Shef-AIRE/StoicIML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Bridging EEG Signals and Generative AI: From Image and Text
  to Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial
Intelligence (GenAI) has opened new frontiers in brain signal decoding,
enabling assistive communication, neural representation learning, and
multimodal integration. BCIs, particularly those leveraging
Electroencephalography (EEG), provide a non-invasive means of translating
neural activity into meaningful outputs. Recent advances in deep learning,
including Generative Adversarial Networks (GANs) and Transformer-based Large
Language Models (LLMs), have significantly improved EEG-based generation of
images, text, and speech. This paper provides a literature review of the
state-of-the-art in EEG-based multimodal generation, focusing on (i)
EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and
Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based
language models and contrastive learning methods. Additionally, we discuss the
emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We
highlight key datasets, use cases, challenges, and EEG feature encoding methods
that underpin generative approaches. By providing a structured overview of
EEG-based generative AI, this survey aims to equip researchers and
practitioners with insights to advance neural decoding, enhance assistive
technologies, and expand the frontiers of brain-computer interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The geometry of BERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bonino, Giorgia Ghione, Giansalvo Cirrincione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer neural networks, particularly Bidirectional Encoder
Representations from Transformers (BERT), have shown remarkable performance
across various tasks such as classification, text summarization, and question
answering. However, their internal mechanisms remain mathematically obscure,
highlighting the need for greater explainability and interpretability. In this
direction, this paper investigates the internal mechanisms of BERT proposing a
novel perspective on the attention mechanism of BERT from a theoretical
perspective. The analysis encompasses both local and global network behavior.
At the local level, the concept of directionality of subspace selection as well
as a comprehensive study of the patterns emerging from the self-attention
matrix are presented. Additionally, this work explores the semantic content of
the information stream through data distribution analysis and global
statistical measures including the novel concept of cone index. A case study on
the classification of SARS-CoV-2 variants using RNA which resulted in a very
high accuracy has been selected in order to observe these concepts in an
application. The insights gained from this analysis contribute to a deeper
understanding of BERT's classification process, offering potential avenues for
future architectural improvements in Transformer models and further analysis in
the training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atom of Thoughts for Markov LLM Test-Time Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) achieve superior performance through
training-time scaling, and test-time scaling further enhances their
capabilities by conducting effective reasoning during inference. However, as
the scale of reasoning increases, existing test-time scaling methods suffer
from accumulated historical information, which not only wastes computational
resources but also interferes with effective reasoning. To address this issue,
we observe that complex reasoning progress is often achieved by solving a
sequence of independent subquestions, each being self-contained and verifiable.
These subquestions are essentially atomic questions, relying primarily on their
current state rather than accumulated history, similar to the memoryless
transitions in a Markov process. Based on this observation, we propose Atom of
Thoughts (AoT), where each state transition in the reasoning process consists
of decomposing the current question into a dependency-based directed acyclic
graph and contracting its subquestions, forming a new atomic question state.
This iterative decomposition-contraction process continues until reaching
directly solvable atomic questions, naturally realizing Markov transitions
between question states. Furthermore, these atomic questions can be seamlessly
integrated into existing test-time scaling methods, enabling AoT to serve as a
plug-in enhancement for improving reasoning capabilities. Experiments across
six benchmarks demonstrate the effectiveness of AoT both as a standalone
framework and a plug-in enhancement. Notably, on HotpotQA, when applied to
gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and
DeepSeek-R1 by 10.6%. The code will be available at
https://github.com/qixucen/atom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Structural-Counterfactual Generation under Domain Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishn Vishwas Kher, Lokesh Venkata Siva Maruthi Badisa, Kusampudi Venkata Datta Sri Harsha, Chitneedi Geetha Sowmya, SakethaNath Jagarlapudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the burgeoning interest in cross-domain learning, we present a
novel generative modeling challenge: generating counterfactual samples in a
target domain based on factual observations from a source domain. Our approach
operates within an unsupervised paradigm devoid of parallel or joint datasets,
relying exclusively on distinct observational samples and causal graphs for
each domain. This setting presents challenges that surpass those of
conventional counterfactual generation. Central to our methodology is the
disambiguation of exogenous causes into effect-intrinsic and domain-intrinsic
categories. This differentiation facilitates the integration of domain-specific
causal graphs into a unified joint causal graph via shared effect-intrinsic
exogenous variables. We propose leveraging Neural Causal models within this
joint framework to enable accurate counterfactual generation under standard
identifiability assumptions. Furthermore, we introduce a novel loss function
that effectively segregates effect-intrinsic from domain-intrinsic variables
during model training. Given a factual observation, our framework combines the
posterior distribution of effect-intrinsic variables from the source domain
with the prior distribution of domain-intrinsic variables from the target
domain to synthesize the desired counterfactuals, adhering to Pearl's causal
hierarchy. Intriguingly, when domain shifts are restricted to alterations in
causal mechanisms without accompanying covariate shifts, our training regimen
parallels the resolution of a conditional optimal transport problem. Empirical
evaluations on a synthetic dataset show that our framework generates
counterfactuals in the target domain that very closely resemble the ground
truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconfigurable Intelligent Surfaces-Assisted Integrated Access and
  Backhaul 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charitha Madapatha, Behrooz Makki, Hao Guo, Tommy Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the impact of reconfigurable intelligent surfaces
(RISs) on the coverage extension of integrated access and backhaul (IAB)
networks. Particularly, using a finite stochastic geometry model, with random
distributions of user equipments (UEs) in a finite region, and planned
hierachical architecture for IAB, we study the service coverage probability
defined as the probability of the event that the UEs' minimum rate requirements
are satisfied. We present comparisons between different cases including
IAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network
controlled repeaters (NCRs). Our investigations focus on wide-area IAB assisted
with RIS through the lens of different design architectures and deployments,
revealing both conflicts and synergies for minimizing the effect of tree
foliage over seasonal changes. Our simulation results reveal both opportunities
and challenges towards the implementation of RIS in IAB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to 2025 European Conference on Networks and Communications
  (EuCNC) & 6G Summit, 2025, Poznan, Poland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging Language and Domain Specific Models: The Impact on Technical
  Vocabulary Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the integration of technical vocabulary in merged
language models. We explore the knowledge transfer mechanisms involved when
combining a general-purpose language-specific model with a domain-specific
model, focusing on the resulting model's comprehension of technical jargon. Our
experiments analyze the impact of this merging process on the target model's
proficiency in handling specialized terminology. We present a quantitative
evaluation of the performance of the merged model, comparing it with that of
the individual constituent models. The findings offer insights into the
effectiveness of different model merging methods for enhancing domain-specific
knowledge and highlight potential challenges and future directions in
leveraging these methods for cross-lingual knowledge transfer in Natural
Language Processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 263rd IPSJ-NL Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Task Group Updates for Multi-Task Optimization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseong Jeong, Kuk-Jin Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning enables the acquisition of task-generic knowledge by
training multiple tasks within a unified architecture. However, training all
tasks together in a single architecture can lead to performance degradation,
known as negative transfer, which is a main concern in multi-task learning.
Previous works have addressed this issue by optimizing the multi-task network
through gradient manipulation or weighted loss adjustments. However, their
optimization strategy focuses on addressing task imbalance in shared
parameters, neglecting the learning of task-specific parameters. As a result,
they show limitations in mitigating negative transfer, since the learning of
shared space and task-specific information influences each other during
optimization. To address this, we propose a different approach to enhance
multi-task performance by selectively grouping tasks and updating them for each
batch during optimization. We introduce an algorithm that adaptively determines
how to effectively group tasks and update them during the learning process. To
track inter-task relations and optimize multi-task networks simultaneously, we
propose proximal inter-task affinity, which can be measured during the
optimization process. We provide a theoretical analysis on how dividing tasks
into multiple groups and updating them sequentially significantly affects
multi-task performance by enhancing the learning of task-specific parameters.
Our methods substantially outperform previous multi-task optimization
approaches and are scalable to different architectures and various numbers of
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Should Maximize Welfare, Not (Only) Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nir Rosenfeld, Haifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decades of research in machine learning have given us powerful tools for
making accurate predictions. But when used in social settings and on human
inputs, better accuracy does not immediately translate to better social
outcomes. This may not be surprising given that conventional learning
frameworks are not designed to express societal preferences -- let alone
promote them. This position paper argues that machine learning is currently
missing, and can gain much from incorporating, a proper notion of social
welfare. The field of welfare economics asks: how should we allocate limited
resources to self-interested agents in a way that maximizes social benefit? We
argue that this perspective applies to many modern applications of machine
learning in social contexts, and advocate for its adoption. Rather than
disposing of prediction, we aim to leverage this forte of machine learning for
promoting social welfare. We demonstrate this idea by proposing a conceptual
framework that gradually transitions from accuracy maximization (with awareness
to welfare) to welfare maximization (via accurate prediction). We detail
applications and use-cases for which our framework can be effective, identify
technical challenges and practical opportunities, and highlight future avenues
worth pursuing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Generalizable Prompt for CLIP with Class Similarity Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehun Jung, Hyang-won Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vision-language models (VLMs), prompt tuning has shown its effectiveness
in adapting models to downstream tasks. However, learned prompts struggle to
generalize to unseen classes, as they tend to overfit to the classes that are
targeted during prompt tuning. Examining failure cases, we observed that
learned prompts disrupt the semantics of unseen classes, generating text
embeddings with incorrect semantic relationships among classes. To address
this, we propose Similarity Alignment Regularization (SAR), which regularizes
learnable prompts to preserve the semantic relationships among classes captured
by hand-crafted prompts. Specifically, we first obtain novel classes related to
base classes using ChatGPT-4o and utilize them as potential unseen classes
during prompt tuning. Then, by targeting both base and novel classes, SAR
aligns the similarity relationships among text embeddings generated by
learnable prompts with the similarity relationships from hand-crafted prompts.
Extensive experiments applying SAR to existing prompt tuning methods
demonstrate its effectiveness in improving generalization to unseen classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Barriers in Bellman-Based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brieuc Pinon, Raphaël Jungers, Jean-Charles Delvenne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning algorithms designed for high-dimensional spaces often
enforce the Bellman equation on a sampled subset of states, relying on
generalization to propagate knowledge across the state space. In this paper, we
identify and formalize a fundamental limitation of this common approach.
Specifically, we construct counterexample problems with a simple structure that
this approach fails to exploit. Our findings reveal that such algorithms can
neglect critical information about the problems, leading to inefficiencies.
Furthermore, we extend this negative result to another approach from the
literature: Hindsight Experience Replay learning state-to-state reachability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refined PAC-Bayes Bounds for Offline Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaury Gouverneur, Tobias J. Oechtering, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present refined probabilistic bounds on empirical reward
estimates for off-policy learning in bandit problems. We build on the
PAC-Bayesian bounds from Seldin et al. (2010) and improve on their results
using a new parameter optimization approach introduced by Rodr\'iguez et al.
(2024). This technique is based on a discretization of the space of possible
events to optimize the "in probability" parameter. We provide two
parameter-free PAC-Bayes bounds, one based on Hoeffding-Azuma's inequality and
the other based on Bernstein's inequality. We prove that our bounds are almost
optimal as they recover the same rate as would be obtained by setting the "in
probability" parameter after the realization of the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qubit-Based Framework for Quantum Machine Learning: Bridging Classical
  Data and Quantum Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavna Bose, Saurav Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper dives into the exciting and rapidly growing field of quantum
computing, explaining its core ideas, current progress, and how it could
revolutionize the way we solve complex problems. It starts by breaking down the
basics, like qubits, quantum circuits, and how principles like superposition
and entanglement make quantum computers fundamentally different-and far more
powerful for certain tasks-than the classical computers we use today. We also
explore how quantum computing deals with complex problems and why it is
uniquely suited for challenges classical systems struggle to handle. A big part
of this paper focuses on Quantum Machine Learning (QML), where the strengths of
quantum computing meet the world of artificial intelligence. By processing
massive datasets and optimizing intricate algorithms, quantum systems offer new
possibilities for machine learning. We highlight different approaches to
combining quantum and classical computing, showing how they can work together
to produce faster and more accurate results. Additionally, we explore the tools
and platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are
helping researchers and developers bring these theories to life. Of course,
quantum computing has its hurdles. Challenges like scaling up hardware,
correcting errors, and keeping qubits stable are significant roadblocks. Yet,
with rapid advancements in cloud-based platforms and innovative technologies,
the potential of quantum computing feels closer than ever. This paper aims to
offer readers a clear and comprehensive introduction to quantum computing, its
role in machine learning, and the immense possibilities it holds for the future
of technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Massively Scaling Explicit Policy-conditioned Value Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nico Bohlinger, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a scaling strategy for Explicit Policy-Conditioned Value
Functions (EPVFs) that significantly improves performance on challenging
continuous-control tasks. EPVFs learn a value function V({\theta}) that is
explicitly conditioned on the policy parameters, enabling direct gradient-based
updates to the parameters of any policy. However, EPVFs at scale struggle with
unrestricted parameter growth and efficient exploration in the policy parameter
space. To address these issues, we utilize massive parallelization with
GPU-based simulators, big batch sizes, weight clipping and scaled peturbations.
Our results show that EPVFs can be scaled to solve complex tasks, such as a
custom Ant environment, and can compete with state-of-the-art Deep
Reinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO)
and Soft Actor-Critic (SAC). We further explore action-based policy parameter
representations from previous work and specialized neural network architectures
to efficiently handle weight-space features, which have not been used in the
context of DRL before.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharp-PINNs: staggered hard-constrained physics-informed neural networks
  for phase field modelling of corrosion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanxi Chen, Chuanjie Cui, Rujin Ma, Airong Chen, Sifan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks have shown significant potential in solving
partial differential equations (PDEs) across diverse scientific fields.
However, their performance often deteriorates when addressing PDEs with
intricate and strongly coupled solutions. In this work, we present a novel
Sharp-PINN framework to tackle complex phase field corrosion problems. Instead
of minimizing all governing PDE residuals simultaneously, the Sharp-PINNs
introduce a staggered training scheme that alternately minimizes the residuals
of Allen-Cahn and Cahn-Hilliard equations, which govern the corrosion system.
To further enhance its efficiency and accuracy, we design an advanced neural
network architecture that integrates random Fourier features as coordinate
embeddings, employs a modified multi-layer perceptron as the primary backbone,
and enforces hard constraints in the output layer. This framework is
benchmarked through simulations of corrosion problems with multiple pits, where
the staggered training scheme and network architecture significantly improve
both the efficiency and accuracy of PINNs. Moreover, in three-dimensional
cases, our approach is 5-10 times faster than traditional finite element
methods while maintaining competitive accuracy, demonstrating its potential for
real-world engineering applications in corrosion prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Spatio-Temporal Neural Network for Air Quality Reanalysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammar Kheder, Benjamin Foreback, Lili Wang, Zhi-Song Liu, Michael Boy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air quality prediction is key to mitigating health impacts and guiding
decisions, yet existing models tend to focus on temporal trends while
overlooking spatial generalization. We propose AQ-Net, a spatiotemporal
reanalysis model for both observed and unobserved stations in the near future.
AQ-Net utilizes the LSTM and multi-head attention for the temporal regression.
We also propose a cyclic encoding technique to ensure continuous time
representation. To learn fine-grained spatial air quality estimation, we
incorporate AQ-Net with the neural kNN to explore feature-based interpolation,
such that we can fill the spatial gaps given coarse observation stations. To
demonstrate the efficiency of our model for spatiotemporal reanalysis, we use
data from 2013-2017 collected in northern China for PM2.5 analysis. Extensive
experiments show that AQ-Net excels in air quality reanalysis, highlighting the
potential of hybrid spatio-temporal models to better capture environmental
dynamics, especially in urban areas where both spatial and temporal variability
are critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FitLight: Federated Imitation Learning for Plug-and-Play Autonomous
  Traffic Signal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Ye, Yingbo Zhou, Zhusen Liu, Xiao Du, Hao Zhou, Xiang Lian, Mingsong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)
methods have been extensively studied, their practical applications still raise
some serious issues such as high learning cost and poor generalizability. This
is because the ``trial-and-error'' training style makes RL agents extremely
dependent on the specific traffic environment, which also requires a long
convergence time. To address these issues, we propose a novel Federated
Imitation Learning (FIL)-based framework for multi-intersection TSC, named
FitLight, which allows RL agents to plug-and-play for any traffic environment
without additional pre-training cost. Unlike existing imitation learning
approaches that rely on pre-training RL agents with demonstrations, FitLight
allows real-time imitation learning and seamless transition to reinforcement
learning. Due to our proposed knowledge-sharing mechanism and novel hybrid
pressure-based agent design, RL agents can quickly find a best control policy
with only a few episodes. Moreover, for resource-constrained TSC scenarios,
FitLight supports model pruning and heterogeneous model aggregation, such that
RL agents can work on a micro-controller with merely 16{\it KB} RAM and 32{\it
KB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art
methods, FitLight not only provides a superior starting point but also
converges to a better final solution on both real-world and synthetic datasets,
even under extreme resource limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning Should Move Beyond Incremental Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rupert Mitchell, Antonio Alliegro, Raffaello Camoriano, Dustin Carrión-Ojeda, Antonio Carta, Georgia Chalvatzaki, Nikhil Churamani, Carlo D'Eramo, Samin Hamidi, Robin Hesse, Fabian Hinder, Roshni Ramanna Kamath, Vincenzo Lomonaco, Subarnaduti Paul, Francesca Pistilli, Tinne Tuytelaars, Gido M van de Ven, Kristian Kersting, Simone Schaub-Meyer, Martin Mundt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) is the sub-field of machine learning concerned with
accumulating knowledge in dynamic environments. So far, CL research has mainly
focused on incremental classification tasks, where models learn to classify new
categories while retaining knowledge of previously learned ones. Here, we argue
that maintaining such a focus limits both theoretical development and practical
applicability of CL methods. Through a detailed analysis of concrete examples -
including multi-target classification, robotics with constrained output spaces,
learning in continuous task domains, and higher-level concept memorization - we
demonstrate how current CL approaches often fail when applied beyond standard
classification. We identify three fundamental challenges: (C1) the nature of
continuity in learning problems, (C2) the choice of appropriate spaces and
metrics for measuring similarity, and (C3) the role of learning objectives
beyond classification. For each challenge, we provide specific recommendations
to help move the field forward, including formalizing temporal dynamics through
distribution processes, developing principled approaches for continuous task
spaces, and incorporating density estimation and generative objectives. In so
doing, this position paper aims to broaden the scope of CL research while
strengthening its theoretical foundations, making it more applicable to
real-world problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GRAPH</span>GPT-O: Synergistic <span class="highlight-title">Multimodal</span> Comprehension and Generation on
  <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Multimodal Large Language Models (MLLMs) has enabled
the integration of multiple modalities, including texts and images, within the
large language model (LLM) framework. However, texts and images are usually
interconnected, forming a multimodal attributed graph (MMAG). It is
underexplored how MLLMs can incorporate the relational information
(\textit{i.e.}, graph structure) and semantic information (\textit{i.e.,} texts
and images) on such graphs for multimodal comprehension and generation. In this
paper, we propose GraphGPT-o, which supports omni-multimodal understanding and
creation on MMAGs. We first comprehensively study linearization variants to
transform semantic and structural information as input for MLLMs. Then, we
propose a hierarchical aligner that enables deep graph encoding, bridging the
gap between MMAGs and MLLMs. Finally, we explore the inference choices,
adapting MLLM to interleaved text and image generation in graph scenarios.
Extensive experiments on three datasets from different domains demonstrate the
effectiveness of our proposed method. Datasets and codes will be open-sourced
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLP: Vision-Language Preference Learning for Embodied Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Liu, Chenjia Bai, Jiafei Lyu, Shengjie Sun, Yali Du, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward engineering is one of the key challenges in Reinforcement Learning
(RL). Preference-based RL effectively addresses this issue by learning from
human feedback. However, it is both time-consuming and expensive to collect
human preference labels. In this paper, we propose a novel
\textbf{V}ision-\textbf{L}anguage \textbf{P}reference learning framework, named
\textbf{VLP}, which learns a vision-language preference model to provide
preference feedback for embodied manipulation tasks. To achieve this, we define
three types of language-conditioned preferences and construct a vision-language
preference dataset, which contains versatile implicit preference orders without
human annotations. The preference model learns to extract language-related
features, and then serves as a preference annotator in various downstream
tasks. The policy can be learned according to the annotated preferences via
reward learning or direct policy optimization. Extensive empirical results on
simulated embodied manipulation tasks demonstrate that our method provides
accurate preferences and generalizes to unseen tasks and unseen language
instructions, outperforming the baselines by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PreAdaptFWI: Pretrained-Based Adaptive Residual Learning for
  Full-Waveform Inversion Without <span class="highlight-title">Dataset</span> Dependency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Dong, Zhengyi Yuan, Jun Lin, Shiqi Dong, Xunqian Tong, Yue Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-waveform inversion (FWI) is a method that utilizes seismic data to
invert the physical parameters of subsurface media by minimizing the difference
between simulated and observed waveforms. Due to its ill-posed nature, FWI is
susceptible to getting trapped in local minima. Consequently, various research
efforts have attempted to combine neural networks with FWI to stabilize the
inversion process. This study presents a simple yet effective training
framework that is independent of dataset reliance and requires only moderate
pre-training on a simple initial model to stabilize network outputs. During the
transfer learning phase, the conventional FWI gradients will simultaneously
update both the neural network and the proposed adaptive residual learning
module, which learns the residual mapping of large-scale distribution features
in the network's output, rather than directly fitting the target mapping.
Through this synergistic training paradigm, the proposed algorithm effectively
infers the physically-informed prior knowledge into a global representation of
stratigraphic distribution, as well as capturing subtle variations in
inter-layer velocities within local details, thereby escaping local optima.
Evaluating the method on two benchmark models under various conditions,
including absent low-frequency data, noise interference, and differing initial
models, along with corresponding ablation experiments, consistently
demonstrates the superiority of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More
  Measurable Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Schwinn, Yan Scholten, Tom Wollschläger, Sophie Xhonneux, Stephen Casper, Stephan Günnemann, Gauthier Gidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misaligned research objectives have considerably hindered progress in
adversarial robustness research over the past decade. For instance, an
extensive focus on optimizing target metrics, while neglecting rigorous
standardized evaluation, has led researchers to pursue ad-hoc heuristic
defenses that were seemingly effective. Yet, most of these were exposed as
flawed by subsequent evaluations, ultimately contributing little measurable
progress to the field. In this position paper, we illustrate that current
research on the robustness of large language models (LLMs) risks repeating past
patterns with potentially worsened real-world implications. To address this, we
argue that realigned objectives are necessary for meaningful progress in
adversarial alignment. To this end, we build on established cybersecurity
taxonomy to formally define differences between past and emerging threat models
that apply to LLMs. Using this framework, we illustrate that progress requires
disentangling adversarial alignment into addressable sub-problems and returning
to core academic principles, such as measureability, reproducibility, and
comparability. Although the field presents significant challenges, the fresh
start on adversarial robustness offers the unique opportunity to build on past
experience while avoiding previous mistakes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Guided Diffusion Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gefan Yang, Frank van der Meulen, Stefan Sommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for simulating conditioned diffusion processes
(diffusion bridges) in Euclidean spaces. By training a neural network to
approximate bridge dynamics, our approach eliminates the need for
computationally intensive Markov Chain Monte Carlo (MCMC) methods or
reverse-process modeling. Compared to existing methods, it offers greater
robustness across various diffusion specifications and conditioning scenarios.
This applies in particular to rare events and multimodal distributions, which
pose challenges for score-learning- and MCMC-based approaches. We propose a
flexible variational family for approximating the diffusion bridge path measure
which is partially specified by a neural network. Once trained, it enables
efficient independent sampling at a cost comparable to sampling the
unconditioned (forward) process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ansatz-free Hamiltonian learning with Heisenberg-limited scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Ye Hu, Muzhou Ma, Weiyuan Gong, Qi Ye, Yu Tong, Steven T. Flammia, Susanne F. Yelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning the unknown interactions that govern a quantum system is crucial for
quantum information processing, device benchmarking, and quantum sensing. The
problem, known as Hamiltonian learning, is well understood under the assumption
that interactions are local, but this assumption may not hold for arbitrary
Hamiltonians. Previous methods all require high-order inverse polynomial
dependency with precision, unable to surpass the standard quantum limit and
reach the gold standard Heisenberg-limited scaling. Whether Heisenberg-limited
Hamiltonian learning is possible without prior assumptions about the
interaction structures, a challenge we term \emph{ansatz-free Hamiltonian
learning}, remains an open question. In this work, we present a quantum
algorithm to learn arbitrary sparse Hamiltonians without any structure
constraints using only black-box queries of the system's real-time evolution
and minimal digital controls to attain Heisenberg-limited scaling in estimation
error. Our method is also resilient to state-preparation-and-measurement
errors, enhancing its practical feasibility. Moreover, we establish a
fundamental trade-off between total evolution time and quantum control on
learning arbitrary interactions, revealing the intrinsic interplay between
controllability and total evolution time complexity for any learning algorithm.
These results pave the way for further exploration into Heisenberg-limited
Hamiltonian learning in complex quantum systems under minimal assumptions,
potentially enabling new benchmarking and verification protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure with Supplementary Materials (17 pages, 1 figure).
  HYH and MM contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMEL: Continuous Action Masking Enabled by Large Language Models for
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) in continuous action spaces encounters persistent
challenges, such as inefficient exploration and convergence to suboptimal
solutions. To address these limitations, we propose CAMEL, a novel framework
integrating LLM-generated suboptimal policies into the RL training pipeline.
CAMEL leverages dynamic action masking and an adaptive epsilon-masking
mechanism to guide exploration during early training stages while gradually
enabling agents to optimize policies independently. At the core of CAMEL lies
the integration of Python-executable suboptimal policies generated by LLMs
based on environment descriptions and task objectives. Although simplistic and
hard-coded, these policies offer valuable initial guidance for RL agents. To
effectively utilize these priors, CAMEL employs masking-aware optimization to
dynamically constrain the action space based on LLM outputs. Additionally,
epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling
agents to transition from constrained exploration to autonomous policy
refinement. Experimental validation on Gymnasium MuJoCo environments
demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated
policies significantly improve sample efficiency, achieving performance
comparable to or surpassing expert masking baselines. For Walker2d-v4, where
LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust
RL performance without notable degradation, highlighting the framework's
adaptability across diverse tasks. While CAMEL shows promise in enhancing
sample efficiency and mitigating convergence challenges, these issues remain
open for further research. Future work aims to generalize CAMEL to multimodal
LLMs for broader observation-action spaces and automate policy evaluation,
reducing human intervention and enhancing scalability in RL training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at RLDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Quantization-Aware Pre-Training: When to transition from
  16-bit to 1.58-bit pre-training for BitNet language models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Nielsen, Peter Schneider-Kamp, Lukas Galke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require immense resources for training and
inference. Quantization, a technique that reduces the precision of model
parameters, offers a promising solution for improving LLM efficiency and
sustainability. While post-training quantization methods typically achieve 4-8
bits per parameter, recent research suggests that training LLMs with 1.58 bits
per weight parameter from scratch can maintain model accuracy while greatly
reducing memory requirements and energy consumption at inference time. Here, we
investigate a training strategy for quantization-aware pre-training, where the
models are first trained with 16-bit precision and then transition into
1.58-bit quantization-aware training. Our results on 11 downstream tasks show
that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit
training and leaves models closer to those which have undergone 16-bit
training. We further investigate the effects of retaining the optimizer state
at the transition point and gradually phasing in quantization strength --
finding that both techniques alleviate the magnitude of loss spikes, but also
that these effects can be compensated through further training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Benign Overfitting in Two-Layer Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Xu, Kexin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed
a sharp phase transition from benign to harmful overfitting when the
noise-to-feature ratio exceeds a threshold-a situation common in long-tailed
data distributions where atypical data is prevalent. However, harmful
overfitting rarely happens in overparameterized neural networks. Further
experimental results suggested that memorization is necessary for achieving
near-optimal generalization error in long-tailed data distributions (Feldman &
Zhang, 2020). We argue that this discrepancy between theoretical predictions
and empirical observations arises because previous feature-noise data models
overlook the heterogeneous nature of noise across different data classes. In
this paper, we refine the feature-noise data model by incorporating
class-dependent heterogeneous noise and re-examine the overfitting phenomenon
in neural networks. Through a comprehensive analysis of the training dynamics,
we establish test loss bounds for the refined model. Our findings reveal that
neural networks can leverage "data noise", previously deemed harmful, to learn
implicit features that improve the classification accuracy for long-tailed
data. Experimental validation on both synthetic and real-world datasets
supports our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIMR: Less is More for RL Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefeng Li, Haoyang Zou, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we ask: what truly determines the effectiveness of RL training
data for enhancing language models' reasoning capabilities? While recent
advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack
of transparency about training data requirements has hindered systematic
progress. Starting directly from base models without distillation, we challenge
the assumption that scaling up RL training data inherently improves
performance. we demonstrate that a strategically selected subset of just 1,389
samples can outperform the full 8,523-sample dataset. We introduce Learning
Impact Measurement (LIM), an automated method to evaluate and prioritize
training samples based on their alignment with model learning trajectories,
enabling efficient resource utilization and scalable implementation. Our method
achieves comparable or even superior performance using only 1,389 samples
versus the full 8,523 samples dataset. Notably, while recent data-efficient
approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it
significantly underperforms at 7B-scale through supervised fine-tuning (SFT).
In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and
outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results
fundamentally reshape our understanding of RL scaling in LLMs, demonstrating
that precise sample selection, rather than data scale, may be the key to
unlocking enhanced reasoning capabilities. For reproducible research and future
innovation, we are open-sourcing LIMR, including implementation of LIM,
training and evaluation code, curated datasets, and trained models at
https://github.com/GAIR-NLP/LIMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. To the best of our
knowledge, DPT-Agent is the first language agent framework that achieves
successful real-time simultaneous human-AI collaboration autonomously. Code of
DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bitnet.cpp: Efficient Edge Inference for Ternary LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has
spurred interest in ternary LLMs. Despite this, research and practical
applications focusing on efficient edge inference for ternary LLMs remain
scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system
optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix
multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,
Bitnet.cpp incorporates a novel mpGEMM library to facilitate
sub-2-bits-per-weight, efficient and lossless inference. The library features
two core solutions: Ternary Lookup Table (TL), which addresses spatial
inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),
which ensures lossless edge inference, both enabling high-speed inference. Our
experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over
full-precision baselines and up to 2.32x over low-bit baselines, setting new
benchmarks in the field. Additionally, we expand TL to element-wise lookup
table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and
empirical evidence of its considerable potential. Bitnet.cpp is publicly
available at https://github.com/microsoft/BitNet/tree/paper , offering a
sophisticated solution for the efficient and practical deployment of edge LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aliaksandra Shysheya, John Bronskill, James Requeima, Shoaib Ahmed Siddiqui, Javier Gonzalez, David Duvenaud, Richard E. Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple method for probabilistic predictions on tabular data
based on Large Language Models (LLMs) called JoLT (Joint LLM Process for
Tabular data). JoLT uses the in-context learning capabilities of LLMs to define
joint distributions over tabular data conditioned on user-specified side
information about the problem, exploiting the vast repository of latent
problem-relevant knowledge encoded in LLMs. JoLT defines joint distributions
for multiple target variables with potentially heterogeneous data types without
any data conversion, data preprocessing, special handling of missing data, or
model training, making it accessible and efficient for practitioners. Our
experiments show that JoLT outperforms competitive methods on low-shot
single-target and multi-target tabular classification and regression tasks.
Furthermore, we show that JoLT can automatically handle missing data and
perform data imputation by leveraging textual side information. We argue that
due to its simplicity and generality, JoLT is an effective approach for a wide
variety of real prediction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedEAT: A Robustness Optimization Framework for Federated LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahao Pang, Xingyuan Wu, Xiaojin Zhang, Wei Chen, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advancements have been made by Large Language Models (LLMs) in
the domains of natural language understanding and automated content creation.
However, they still face persistent problems, including substantial
computational costs and inadequate availability of training data. The
combination of Federated Learning (FL) and LLMs (federated LLMs) offers a
solution by leveraging distributed data while protecting privacy, which
positions it as an ideal choice for sensitive domains. However, Federated LLMs
still suffer from robustness challenges, including data heterogeneity,
malicious clients, and adversarial attacks, which greatly hinder their
applications. We first introduce the robustness problems in federated LLMs, to
address these challenges, we propose FedEAT (Federated Embedding space
Adversarial Training), a novel framework that applies adversarial training in
the embedding space of client LLM and employs a robust aggregation approach,
specifically geometric median aggregation, to enhance the robustness of
Federated LLMs. Our experiments demonstrate that FedEAT effectively improves
the robustness of Federated LLMs with minimal performance loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Anomaly Detection in IoMT Networks using Ensemble AI Models on
  the CICIoMT2024 <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prathamesh Chandekar, Mansi Mehta, Swet Chandan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of Internet of Medical Things (IoMT) devices in
healthcare has introduced unique cybersecurity challenges, primarily due to the
diverse communication protocols and critical nature of these devices This
research aims to develop an advanced, real-time anomaly detection framework
tailored for IoMT network traffic, leveraging AI/ML models and the CICIoMT2024
dataset By integrating multi-protocol (MQTT, WiFi), attack-specific (DoS,
DDoS), time-series (active/idle states), and device-specific (Bluetooth) data,
our study captures a comprehensive range of IoMT interactions As part of our
data analysis, various machine learning techniques are employed which include
an ensemble model using XGBoost for improved performance against specific
attack types, sequential models comprised of LSTM and CNN-LSTM that leverage
time dependencies, and unsupervised models such as Autoencoders and Isolation
Forest that are good in general anomaly detection The results of the experiment
prove with an ensemble model lowers false positive rates and reduced
detections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StructTransform: A Scalable Attack Surface for Safety-Aligned Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shehel Yoosuf, Temoor Ali, Ahmed Lekssays, Mashael AlSabah, Issa Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a series of structure transformation attacks on LLM
alignment, where we encode natural language intent using diverse syntax spaces,
ranging from simple structure formats and basic query languages (e.g. SQL) to
new novel spaces and syntaxes created entirely by LLMs. Our extensive
evaluation shows that our simplest attacks can achieve close to 90% success
rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment
mechanisms. We improve the attack performance further by using an adaptive
scheme that combines structure transformations along with existing
\textit{content transformations}, resulting in over 96% ASR with 0% refusals.
  To generalize our attacks, we explore numerous structure formats, including
syntaxes purely generated by LLMs. Our results indicate that such novel
syntaxes are easy to generate and result in a high ASR, suggesting that
defending against our attacks is not a straightforward process. Finally, we
develop a benchmark and evaluate existing safety-alignment defenses against it,
showing that most of them fail with 100% ASR. Our results show that existing
safety alignment mostly relies on token-level patterns without recognizing
harmful concepts, highlighting and motivating the need for serious research
efforts in this direction. As a case study, we demonstrate how attackers can
use our attack to easily generate a sample malware, and a corpus of fraudulent
SMS messages, which perform well in bypassing detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aras Yurtman, Daan Van Wesenbeeck, Wannes Meert, Hendrik Blockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Motif Discovery (TSMD) identifies repeating patterns in time
series data, but its unsupervised nature might result in motifs that are not
interesting to the user. To address this, we propose a framework that allows
the user to impose constraints on the motifs to be discovered, where
constraints can easily be defined according to the properties of the desired
motifs in the application domain. We also propose an efficient implementation
of the framework, the LoCoMotif-DoK algorithm. We demonstrate that
LoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic
data, outperforming other TSMD techniques which only support a limited form of
domain knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BaxBench: Can LLMs Generate Correct and Secure Backends? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Vero, Niels Mündler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanović, Jingxuan He, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic generation of programs has long been a fundamental challenge in
computer science. Recent benchmarks have shown that large language models
(LLMs) can effectively generate code at the function level, make code edits,
and solve algorithmic coding tasks. However, to achieve full automation, LLMs
should be able to generate production-quality, self-contained application
modules. To evaluate the capabilities of LLMs in solving this challenge, we
introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for
the generation of backend applications. We focus on backends for three critical
reasons: (i) they are practically relevant, building the core components of
most modern web and cloud software, (ii) they are difficult to get right,
requiring multiple functions and files to achieve the desired functionality,
and (iii) they are security-critical, as they are exposed to untrusted
third-parties, making secure solutions that prevent deployment-time attacks an
imperative. BaxBench validates the functionality of the generated applications
with comprehensive test cases, and assesses their security exposure by
executing end-to-end exploits. Our experiments reveal key limitations of
current LLMs in both functionality and security: (i) even the best model,
OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could
successfully execute security exploits on more than half of the correct
programs generated by each LLM; and (iii) in less popular backend frameworks,
models further struggle to generate correct and secure applications. Progress
on BaxBench signifies important steps towards autonomous and secure software
development with LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio
  Chord Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Generalization on Text Attribute <span class="highlight-title">Graph</span>s: Principles with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Shikun Liu, Rongzhe Wei, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently been introduced to graph learning,
aiming to extend their zero-shot generalization success to tasks where labeled
graph data is scarce. Among these applications, inference over text-attributed
graphs (TAGs) presents unique challenges: existing methods struggle with LLMs'
limited context length for processing large node neighborhoods and the
misalignment between node embeddings and the LLM token space. To address these
issues, we establish two key principles for ensuring generalization and derive
the framework LLM-BP accordingly: (1) Unifying the attribute space with
task-adaptive embeddings, where we leverage LLM-based encoders and task-aware
prompting to enhance generalization of the text attribute embeddings; (2)
Developing a generalizable graph information aggregation mechanism, for which
we adopt belief propagation with LLM-estimated parameters that adapt across
graphs. Evaluations on 11 real-world TAG benchmarks demonstrate that LLM-BP
significantly outperforms existing approaches, achieving 8.10% improvement with
task-conditional embeddings and an additional 1.71% gain from adaptive
aggregation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intersectional Fairness in Reinforcement Learning with Large State and
  Constraint Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Eaton, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Bela Sengupta, Jessica Sorrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In traditional reinforcement learning (RL), the learner aims to solve a
single objective optimization problem: find the policy that maximizes expected
reward. However, in many real-world settings, it is important to optimize over
multiple objectives simultaneously. For example, when we are interested in
fairness, states might have feature annotations corresponding to multiple
(intersecting) demographic groups to whom reward accrues, and our goal might be
to maximize the reward of the group receiving the minimal reward. In this work,
we consider a multi-objective optimization problem in which each objective is
defined by a state-based reweighting of a single scalar reward function. This
generalizes the problem of maximizing the reward of the minimum reward group.
We provide oracle-efficient algorithms to solve these multi-objective RL
problems even when the number of objectives is exponentially large-for tabular
MDPs, as well as for large MDPs when the group functions have additional
structure. Finally, we experimentally validate our theoretical results and
demonstrate applications on a preferential attachment graph MDP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhou, Wenge Rong, Jianfei Zhang, Qing Sun, Yuanxin Ouyang, Zhang Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Tracing (KT) aims to predict students' future performances based on
their former exercises and additional information in educational settings. KT
has received significant attention since it facilitates personalized
experiences in educational situations. Simultaneously, the autoregressive
modeling on the sequence of former exercises has been proven effective for this
task. One of the primary challenges in autoregressive modeling for Knowledge
Tracing is effectively representing the anterior (pre-response) and posterior
(post-response) states of learners across exercises. Existing methods often
employ complex model architectures to update learner states using question and
response records. In this study, we propose a novel perspective on knowledge
tracing task by treating it as a generative process, consistent with the
principles of autoregressive models. We demonstrate that knowledge states can
be directly represented through autoregressive encodings on a question-response
alternate sequence, where model generate the most probable representation in
hidden state space by analyzing history interactions. This approach underpins
our framework, termed Alternate Autoregressive Knowledge Tracing (AAKT).
Additionally, we incorporate supplementary educational information, such as
question-related skills, into our framework through an auxiliary task, and
include extra exercise details, like response time, as additional inputs. Our
proposed framework is implemented using advanced autoregressive technologies
from Natural Language Generation (NLG) for both training and prediction.
Empirical evaluations on four real-world KT datasets indicate that AAKT
consistently outperforms all baseline models in terms of AUC, ACC, and RMSE.
Furthermore, extensive ablation studies and visualized analysis validate the
effectiveness of key components in AAKT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMTS-Mixer: Mixer-Networks for Irregular Multivariate Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Klötergens, Tim Dernedde, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting Irregular Multivariate Time Series (IMTS) has recently emerged as
a distinct research field, necessitating specialized models to address its
unique challenges. While most forecasting literature assumes regularly spaced
observations without missing values, many real-world datasets - particularly in
healthcare, climate research, and biomechanics - violate these assumptions.
Time Series (TS)-mixer models have achieved remarkable success in regular
multivariate time series forecasting. However, they remain unexplored for IMTS
due to their requirement for complete and evenly spaced observations. To bridge
this gap, we introduce IMTS-Mixer, a novel forecasting architecture designed
specifically for IMTS. Our approach retains the core principles of TS mixer
models while introducing innovative methods to transform IMTS into fixed-size
matrix representations, enabling their seamless integration with mixer modules.
We evaluate IMTS-Mixer on a benchmark of four real-world datasets from various
domains. Our results demonstrate that IMTS-Mixer establishes a new
state-of-the-art in forecasting accuracy while also improving computational
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning significantly improves the performance of Large Language Models
(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims
to provide an in-depth interpretation of the fine-tuning process through
circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike
previous studies
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
that focus on tasks where pre-trained models already perform well, we develop a
set of mathematical tasks where fine-tuning yields substantial performance
gains, which are closer to the practical setting. In our experiments, we
identify circuits at various checkpoints during fine-tuning and examine the
interplay between circuit analysis, fine-tuning methods, and task complexities.
First, we find that while circuits maintain high node similarity before and
after fine-tuning, their edges undergo significant changes, which is in
contrast to the previous work
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
that show circuits only add some additional components after fine-tuning. Based
on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)
method, which assigns ranks to layers based on edge changes in the circuits.
Experimental results demonstrate that our circuit-based LoRA algorithm achieves
an average performance improvement of 2.46\% over standard LoRA with similar
parameter sizes. Furthermore, we explore how combining circuits from subtasks
can enhance fine-tuning in compositional tasks, providing new insights into the
design of such tasks and deepening the understanding of circuit dynamics and
fine-tuning mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When performing 3D inpainting using novel-view rendering methods like Neural
Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture
and geometry consistency across camera views has been a challenge. In this
paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided
Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided
by the rendered depth information from each training view, our 3DGIC exploits
background pixels visible across different views for updating the inpainting
mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive
experiments on benchmark datasets, we confirm that our 3DGIC outperforms
current state-of-the-art 3D inpainting methods quantitatively and
qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Private Synthetic <span class="highlight-title">Graph</span> Generation and Fused Gromov-Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leoni Carla Wirth, Gholamali Aminian, Gesine Reinert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networks are popular for representing complex data. In particular,
differentially private synthetic networks are much in demand for method and
algorithm development. The network generator should be easy to implement and
should come with theoretical guarantees. Here we start with complex data as
input and jointly provide a network representation as well as a synthetic
network generator. Using a random connection model, we devise an effective
algorithmic approach for generating attributed synthetic graphs which is
$\epsilon$-differentially private at the vertex level, while preserving utility
under an appropriate notion of distance which we develop. We provide
theoretical guarantees for the accuracy of the private synthetic graphs using
the fused Gromov-Wasserstein distance, which extends the Wasserstein metric to
structured data. Our method draws inspiration from the PSMM method of
\citet{he2023}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Machine Learning for Kronecker Coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgi Butbaia, Kyu-Hwan Lee, Fabian Ruehle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the saliency of neural networks and employ interpretable machine
learning models to predict whether the Kronecker coefficients of the symmetric
group are zero or not. Our models use triples of partitions as input features,
as well as b-loadings derived from the principal component of an embedding that
captures the differences between partitions. Across all approaches, we achieve
an accuracy of approximately 83% and derive explicit formulas for a decision
function in terms of b-loadings. Additionally, we develop transformer-based
models for prediction, achieving the highest reported accuracy of over 99%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Selection to Generation: A <span class="highlight-title">Survey</span> of LLM-based Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning (AL) has been a powerful paradigm for improving model
efficiency and performance by selecting the most informative data points for
labeling and training. In recent active learning frameworks, Large Language
Models (LLMs) have been employed not only for selection but also for generating
entirely new data instances and providing more cost-effective annotations.
Motivated by the increasing importance of high-quality data and efficient model
training in the era of LLMs, we present a comprehensive survey on LLM-based
Active Learning. We introduce an intuitive taxonomy that categorizes these
techniques and discuss the transformative roles LLMs can play in the active
learning loop. We further examine the impact of AL on LLM learning paradigms
and its applications across various domains. Finally, we identify open
challenges and propose future research directions. This survey aims to serve as
an up-to-date resource for researchers and practitioners seeking to gain an
intuitive understanding of LLM-based AL techniques and deploy them to new
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Computation of the Fisher Information in Continual Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most popular methods for continual learning with deep neural
networks is Elastic Weight Consolidation (EWC), which involves computing the
Fisher Information. The exact way in which the Fisher Information is computed
is however rarely described, and multiple different implementations for it can
be found online. This blog post discusses and empirically compares several
often-used implementations, which highlights that many currently reported
results for EWC could likely be improved by changing the way the Fisher
Information is computed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the blogpost track at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Partial-Label Learning by Leveraging Class Activation Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Fuchs, Florian Kalinke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world training data is often noisy; for example, human annotators assign
conflicting class labels to the same instances. Partial-label learning (PLL) is
a weakly supervised learning paradigm that allows training classifiers in this
context without manual data cleaning. While state-of-the-art methods have good
predictive performance, their predictions are sensitive to high noise levels,
out-of-distribution data, and adversarial perturbations. We propose a novel PLL
method based on subjective logic, which explicitly represents uncertainty by
leveraging the magnitudes of the underlying neural network's class activation
values. Thereby, we effectively incorporate prior knowledge about the class
labels by using a novel label weight re-distribution strategy that we prove to
be optimal. We empirically show that our method yields more robust predictions
in terms of predictive performance under high PLL noise levels, handling
out-of-distribution examples, and handling adversarial perturbations on the
test instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via
  Modality-decoupled Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wu, Yuxin Xiong, Xintong Li, Yu Xia, Ruoyu Wang, Yu Wang, Tong Yu, Sungchul Kim, Ryan A. Rossi, Lina Yao, Jingbo Shang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent MLLMs have shown emerging visual understanding and reasoning abilities
after being pre-trained on large-scale multimodal datasets. Unlike
pre-training, where MLLMs receive rich visual-text alignment,
instruction-tuning is often text-driven with weaker visual supervision, leading
to the degradation of pre-trained visual understanding and causing visual
forgetting. Existing approaches, such as direct fine-tuning and continual
learning methods, fail to explicitly address this issue, often compressing
visual representations and prioritizing task alignment over visual retention,
which further worsens visual forgetting. To overcome this limitation, we
introduce a novel perspective leveraging effective rank to quantify the
degradation of visual representation richness, interpreting this degradation
through the information bottleneck principle as excessive compression that
leads to the degradation of crucial pre-trained visual knowledge. Building on
this view, we propose a modality-decoupled gradient descent (MDGD) method that
regulates gradient updates to maintain the effective rank of visual
representations while mitigating the over-compression effects described by the
information bottleneck. By explicitly disentangling the optimization of visual
understanding from task-specific alignment, MDGD preserves pre-trained visual
knowledge while enabling efficient task adaptation. To enable lightweight
instruction-tuning, we further develop a memory-efficient fine-tuning approach
using gradient masking, which selectively updates a subset of model parameters
to enable parameter-efficient fine-tuning (PEFT), reducing computational
overhead while preserving rich visual representations. Extensive experiments
across various downstream tasks and backbone MLLMs demonstrate that MDGD
effectively mitigates visual forgetting from pre-trained tasks while enabling
strong adaptation to new tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Croce, Christian Schlarmann, Naman Deep Singh, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring perceptual similarity is a key tool in computer vision. In recent
years perceptual metrics based on features extracted from neural networks with
large and diverse training sets, e.g. CLIP, have become popular. At the same
time, the metrics extracted from features of neural networks are not
adversarially robust. In this paper we show that adversarially robust CLIP
models, called R-CLIP$_\textrm{F}$, obtained by unsupervised adversarial
fine-tuning induce a better and adversarially robust perceptual metric that
outperforms existing metrics in a zero-shot setting, and further matches the
performance of state-of-the-art metrics while being robust after fine-tuning.
Moreover, our perceptual metric achieves strong performance on related tasks
such as robust image-to-image retrieval, which becomes especially relevant when
applied to "Not Safe for Work" (NSFW) content detection and dataset filtering.
While standard perceptual metrics can be easily attacked by a small
perturbation completely degrading NSFW detection, our robust perceptual metric
maintains high accuracy under an attack while having similar performance for
unperturbed images. Finally, perceptual metrics induced by robust CLIP models
have higher interpretability: feature inversion can show which images are
considered similar, while text inversion can find what images are associated to
a given prompt. This also allows us to visualize the very rich visual concepts
learned by a CLIP model, including memorized persons, paintings and complex
queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication in the IEEE Conference on
  Secure and Trustworthy Machine Learning (SaTML). The final version will be
  available on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Depot Discovery: A Generative Framework for Flexible
  Location-Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Site Qu, Guoqiang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Location-Routing Problem (LRP), which combines the challenges of facility
(depot) locating and vehicle route planning, is critically constrained by the
reliance on predefined depot candidates, limiting the solution space and
potentially leading to suboptimal outcomes. Previous research on LRP without
predefined depots is scant and predominantly relies on heuristic algorithms
that iteratively attempt depot placements across a planar area. Such approaches
lack the ability to proactively generate depot locations that meet specific
geographic requirements, revealing a notable gap in current research landscape.
To bridge this gap, we propose a data-driven generative DRL framework, designed
to proactively generate depots for LRP without predefined depot candidates,
solely based on customer requests data which include geographic and demand
information. It can operate in two distinct modes: direct generation of exact
depot locations, and the creation of a multivariate Gaussian distribution for
flexible depots sampling. By extracting depots' geographic pattern from
customer requests data, our approach can dynamically respond to logistical
needs, identifying high-quality depot locations that further reduce total
routing costs compared to traditional methods. Extensive experiments
demonstrate that, for a same group of customer requests, compared with those
depots identified through random attempts, our framework can proactively
generate depots that lead to superior solution routes with lower routing cost.
The implications of our framework potentially extend into real-world
applications, particularly in emergency medical rescue and disaster relief
logistics, where rapid establishment and adjustment of depot locations are
paramount, showcasing its potential in addressing LRP for dynamic and
unpredictable environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-aware contrastive heterogeneous molecular <span class="highlight-title">graph</span> learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mukun Chen, Jia Wu, Shirui Pan, Fu Lin, Bo Du, Xiuwen Gong, Wenbin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular representation learning is pivotal in predicting molecular
properties and advancing drug design. Traditional methodologies, which
predominantly rely on homogeneous graph encoding, are limited by their
inability to integrate external knowledge and represent molecular structures
across different levels of granularity. To address these limitations, we
propose a paradigm shift by encoding molecular graphs into heterogeneous
structures, introducing a novel framework: Knowledge-aware Contrastive
Heterogeneous Molecular Graph Learning (KCHML). This approach leverages
contrastive learning to enrich molecular representations with embedded external
knowledge. KCHML conceptualizes molecules through three distinct graph
views-molecular, elemental, and pharmacological-enhanced by heterogeneous
molecular graphs and a dual message-passing mechanism. This design offers a
comprehensive representation for property prediction, as well as for downstream
tasks such as drug-drug interaction (DDI) prediction. Extensive benchmarking
demonstrates KCHML's superiority over state-of-the-art molecular property
prediction models, underscoring its ability to capture intricate molecular
features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Agents Making Agent Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool use has turned large language models (LLMs) into powerful agents that
can perform complex multi-step tasks by dynamically utilising external software
components. However, these tools must be implemented in advance by human
developers, hindering the applicability of LLM agents in domains which demand
large numbers of highly specialised tools, like in life sciences and medicine.
Motivated by the growing trend of scientific studies accompanied by public code
repositories, we propose ToolMaker, a novel agentic framework that autonomously
transforms papers with code into LLM-compatible tools. Given a short task
description and a repository URL, ToolMaker autonomously installs required
dependencies and generates code to perform the task, using a closed-loop
self-correction mechanism to iteratively diagnose and rectify errors. To
evaluate our approach, we introduce a benchmark comprising 15 diverse and
complex computational tasks spanning both medical and non-medical domains with
over 100 unit tests to objectively assess tool correctness and robustness.
ToolMaker correctly implements 80% of the tasks, substantially outperforming
current state-of-the-art software engineering agents. ToolMaker therefore is a
step towards fully autonomous agent-based scientific workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks
  using Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manaar Alam, Hithem Lamri, Michail Maniatakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks embed hidden functionalities in deep neural networks (DNN),
triggering malicious behavior with specific inputs. Advanced defenses monitor
anomalous DNN inferences to detect such attacks. However, concealed backdoors
evade detection by maintaining a low pre-deployment attack success rate (ASR)
and restoring high ASR post-deployment via machine unlearning. Existing
concealed backdoors are often constrained by requiring white-box or black-box
access or auxiliary data, limiting their practicality when such access or data
is unavailable. This paper introduces ReVeil, a concealed backdoor attack
targeting the data collection phase of the DNN training pipeline, requiring no
model access or auxiliary data. ReVeil maintains low pre-deployment ASR across
four datasets and four trigger patterns, successfully evades three popular
backdoor detection methods, and restores high ASR post-deployment through
machine unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at 62nd Design Automation Conference (DAC)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Momentum and Error Feedback for Clipping with Fast Rates and
  Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustem Islamov, Samuel Horvath, Aurelien Lucchi, Peter Richtarik, Eduard Gorbunov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strong Differential Privacy (DP) and Optimization guarantees are two
desirable properties for a method in Federated Learning (FL). However, existing
algorithms do not achieve both properties at once: they either have optimal DP
guarantees but rely on restrictive assumptions such as bounded
gradients/bounded data heterogeneity, or they ensure strong optimization
performance but lack DP guarantees. To address this gap in the literature, we
propose and analyze a new method called Clip21-SGD2M based on a novel
combination of clipping, heavy-ball momentum, and Error Feedback. In
particular, for non-convex smooth distributed problems with clients having
arbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal
convergence rate and also near optimal (local-)DP neighborhood. Our numerical
experiments on non-convex logistic regression and training of neural networks
highlight the superiority of Clip21-SGD2M over baselines in terms of the
optimization performance for a given DP-budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral structure learning for clinical time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Lerner, Anita Burgun, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop and evaluate a structure learning algorithm for clinical time
series. Clinical time series are multivariate time series observed in multiple
patients and irregularly sampled, challenging existing structure learning
algorithms. We assume that our times series are realizations of StructGP, a
k-dimensional multi-output or multi-task stationary Gaussian process (GP), with
independent patients sharing the same covariance function. StructGP encodes
ordered conditional relations between time series, represented in a directed
acyclic graph. We implement an adapted NOTEARS algorithm, which based on a
differentiable definition of acyclicity, recovers the graph by solving a series
of continuous optimization problems. Simulation results show that up to mean
degree 3 and 20 tasks, we reach a median recall of 0.93% [IQR, 0.86, 0.97]
while keeping a median precision of 0.71% [0.57-0.84], for recovering directed
edges. We further show that the regularization path is key to identifying the
graph. With StructGP, we proposed a model of time series dependencies, that
flexibly adapt to different time series regularity, while enabling us to learn
these dependencies from observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best of Both Worlds: Regret Minimization versus Minimax Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Müller, Jon Schneider, Stratis Skoulakis, Luca Viano, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the existence of online learning algorithms
with bandit feedback that simultaneously guarantee $O(1)$ regret compared to a
given comparator strategy, and $O(\sqrt{T})$ regret compared to the best
strategy in hindsight, where $T$ is the number of rounds. We provide the first
affirmative answer to this question. In the context of symmetric zero-sum
games, both in normal- and extensive form, we show that our results allow us to
guarantee to risk at most $O(1)$ loss while being able to gain $\Omega(T)$ from
exploitable opponents, thereby combining the benefits of both no-regret
algorithms and minimax play.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact Upper and Lower Bounds for the Output Distribution of Neural
  Networks with Random Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive exact upper and lower bounds for the cumulative distribution
function (cdf) of the output of a neural network over its entire support
subject to noisy (stochastic) inputs. The upper and lower bounds converge to
the true cdf over its domain as the resolution increases. Our method applies to
any feedforward NN using continuous monotonic piecewise differentiable
activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs,
which were beyond the scope of competing approaches. The novelty and an
instrumental tool of our approach is to bound general NNs with ReLU NNs. The
ReLU NN based bounds are then used to derive upper and lower bounds of the cdf
of the NN output. Experiments demonstrate that our method delivers guaranteed
bounds of the predictive output distribution over its support, thus providing
exact error guarantees, in contrast to competing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-Oriented Data Augmentation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is an essential technique in natural language processing
(NLP) for enriching training datasets by generating diverse samples. This
process is crucial for improving the robustness and generalization capabilities
of NLP models. However, a significant challenge remains: \textit{Insufficient
Attention to Sample Distribution Diversity}. Most existing methods focus on
increasing the sample numbers while neglecting the sample distribution
diversity, which can lead to model overfitting. In response, we explore data
augmentation's impact on dataset diversity and propose a
\textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data
\textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). %
\(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning
approach to train an LLM as a diverse paraphraser, which is capable of
augmenting textual datasets by generating diversified paraphrases. Then, we
apply the LLM paraphraser to a selected coreset of highly informative samples
and integrate the paraphrases with the original data to create a more diverse
augmented dataset. Finally, we conduct extensive experiments on 12 real-world
textual datasets. The results show that our fine-tuned LLM augmenter improves
diversity while preserving label consistency, thereby enhancing the robustness
and performance of downstream tasks. Specifically, it achieves an average
performance gain of \(10.52\%\), surpassing the runner-up baseline with more
than three percentage points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Subspace Learning for Surface Anomaly Classification Based on 3D
  Point Cloud Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Cao, Chengyu Tao, Juan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface anomaly classification is critical for manufacturing system fault
diagnosis and quality control. However, the following challenges always hinder
accurate anomaly classification in practice: (i) Anomaly patterns exhibit
intra-class variation and inter-class similarity, presenting challenges in the
accurate classification of each sample. (ii) Despite the predefined classes,
new types of anomalies can occur during production that require to be detected
accurately. (iii) Anomalous data is rare in manufacturing processes, leading to
limited data for model learning. To tackle the above challenges simultaneously,
this paper proposes a novel deep subspace learning-based 3D anomaly
classification model. Specifically, starting from a lightweight encoder to
extract the latent representations, we model each class as a subspace to
account for the intra-class variation, while promoting distinct subspaces of
different classes to tackle the inter-class similarity. Moreover, the explicit
modeling of subspaces offers the capability to detect out-of-distribution
samples, i.e., new types of anomalies, and the regularization effect with much
fewer learnable parameters of our proposed subspace classifier, compared to the
popular Multi-Layer Perceptions (MLPs). Extensive numerical experiments
demonstrate our method achieves better anomaly classification results than
benchmark methods, and can effectively identify the new types of anomalies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the kernel learning problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Feng Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical kernel ridge regression problem aims to find the best fit for
the output $Y$ as a function of the input data $X\in \mathbb{R}^d$, with a
fixed choice of regularization term imposed by a given choice of a reproducing
kernel Hilbert space, such as a Sobolev space. Here we consider a
generalization of the kernel ridge regression problem, by introducing an extra
matrix parameter $U$, which aims to detect the scale parameters and the feature
variables in the data, and thereby improve the efficiency of kernel ridge
regression. This naturally leads to a nonlinear variational problem to optimize
the choice of $U$. We study various foundational mathematical aspects of this
variational problem, and in particular how this behaves in the presence of
multiscale structures in the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How does ion temperature gradient turbulence depend on magnetic
  geometry? Insights from data and machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Landreman, Jong Youl Choi, Caio Alves, Prasanna Balaprakash, R. Michael Churchill, Rory Conlin, Gareth Roberg-Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic geometry has a significant effect on the level of turbulent
transport in fusion plasmas. Here, we model and analyze this dependence using
multiple machine learning methods and a dataset of > 200,000 nonlinear
simulations of ion-temperature-gradient turbulence in diverse non-axisymmetric
geometries. The dataset is generated using a large collection of both optimized
and randomly generated stellarator equilibria. At fixed gradients, the
turbulent heat flux varies between geometries by several orders of magnitude.
Trends are apparent among the configurations with particularly high or low heat
flux. Regression and classification techniques from machine learning are then
applied to extract patterns in the dataset. Due to a symmetry of the
gyrokinetic equation, the heat flux and regressions thereof should be invariant
to translations of the raw features in the parallel coordinate, similar to
translation invariance in computer vision applications. Multiple regression
models including convolutional neural networks (CNNs) and decision trees can
achieve reasonable predictive power for the heat flux in held-out test
configurations, with highest accuracy for the CNNs. Using Spearman correlation,
sequential feature selection, and Shapley values to measure feature importance,
it is consistently found that the most important geometric lever on the heat
flux is the flux surface compression in regions of bad curvature. The second
most important feature relates to the magnitude of geodesic curvature. These
two features align remarkably with surrogates that have been proposed based on
theory, while the methods here allow a natural extension to more features for
increased accuracy. The dataset, released with this publication, may also be
used to test other proposed surrogates, and we find many previously published
proxies do correlate well with both the heat flux and stability boundary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperspherical Energy <span class="highlight-title">Transformer</span> with Recurrent Depth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhe Hu, Difan Zou, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based foundation models have achieved unprecedented success with
a gigantic amount of parameters and computational resources. Yet, the core
building blocks of these models, the Transformer layers, and how they are
arranged and configured are primarily engineered from the bottom up and driven
by heuristics. For advancing next-generation architectures, it demands
exploring a prototypical model that is amenable to high interpretability and of
practical competence. To this end, we take a step from the top-down view and
design neural networks from an energy minimization perspective. Specifically,
to promote isotropic token distribution on the sphere, we formulate a modified
Hopfield energy function on the subspace-embedded hypersphere, based on which
Transformer layers with symmetric structures are designed as the iterative
optimization for the energy function. By integrating layers with the same
parameters, we propose \textit{Hyper-Spherical Energy Transformer} (Hyper-SET),
an alternative to the vanilla Transformer with recurrent depth. This design
inherently provides greater interpretability and allows for scaling to deeper
layers without a significant increase in the number of parameters. We also
empirically demonstrate that Hyper-SET achieves comparable or even superior
performance on both synthetic and real-world tasks, such as solving Sudoku and
masked image modeling, while utilizing fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Interpretable Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Barbiero, Giuseppe Marra, Gabriele Ciravegna, David Debot, Francesco De Santis, Michelangelo Diligenti, Mateo Espinosa Zarlenga, Francesco Giannini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formalize a novel modeling framework for achieving interpretability in
deep learning, anchored in the principle of inference equivariance. While the
direct verification of interpretability scales exponentially with the number of
variables of the system, we show that this complexity can be mitigated by
treating interpretability as a Markovian property and employing neural
re-parametrization techniques. Building on these insights, we propose a new
modeling paradigm -- neural generation and interpretable execution -- that
enables scalable verification of equivariance. This paradigm provides a general
approach for designing Neural Interpretable Reasoners that are not only
expressive but also transparent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> In-Context Parametric Inference: Point or Distribution Estimators? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Mittal, <span class="highlight-author">Yoshua Bengio</span>, Nikolay Malkin, Guillaume Lajoie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian and frequentist inference are two fundamental paradigms in
statistical estimation. Bayesian methods treat hypotheses as random variables,
incorporating priors and updating beliefs via Bayes' theorem, whereas
frequentist methods assume fixed but unknown hypotheses, relying on estimators
like maximum likelihood. While extensive research has compared these
approaches, the frequentist paradigm of obtaining point estimates has become
predominant in deep learning, as Bayesian inference is challenging due to the
computational complexity and the approximation gap of posterior estimation
methods. However, a good understanding of trade-offs between the two approaches
is lacking in the regime of amortized estimators, where in-context learners are
trained to estimate either point values via maximum likelihood or maximum a
posteriori estimation, or full posteriors using normalizing flows, score-based
diffusion samplers, or diagonal Gaussian approximations, conditioned on
observations. To help resolve this, we conduct a rigorous comparative analysis
spanning diverse problem settings, from linear models to shallow neural
networks, with a robust evaluation framework assessing both in-distribution and
out-of-distribution generalization on tractable tasks. Our experiments indicate
that amortized point estimators generally outperform posterior inference,
though the latter remain competitive in some low-dimensional problems, and we
further discuss why this might be the case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximum Entropy Reinforcement Learning with Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi Dong, Jian Cheng, Xi Sheryl Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a
mainstream implementation for realizing the Maximum Entropy Reinforcement
Learning (MaxEnt RL) objective, which incorporates entropy maximization to
encourage exploration and enhance policy robustness. While the Gaussian policy
performs well on simpler tasks, its exploration capacity and potential
performance in complex multi-goal RL environments are limited by its inherent
unimodality. In this paper, we employ the diffusion model, a powerful
generative model capable of capturing complex multimodal distributions, as the
policy representation to fulfill the MaxEnt RL objective, developing a method
named MaxEnt RL with Diffusion Policy (MaxEntDP). Our method enables efficient
exploration and brings the policy closer to the optimal MaxEnt policy.
Experimental results on Mujoco benchmarks show that MaxEntDP outperforms the
Gaussian policy and other generative models within the MaxEnt RL framework, and
performs comparably to other state-of-the-art diffusion-based online RL
algorithms. Our code is available at https://github.com/diffusionyes/MaxEntDP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Task Relationships for Continual Learning Using
  Transferability-Aware Task Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanru Wu, Xiangyu Chen, Jianning Wang, Enming Zhang, Hanbing Liu, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) has been an essential topic in the contemporary
application of deep neural networks, where catastrophic forgetting (CF) can
impede a model's ability to acquire knowledge progressively. Existing CL
strategies primarily address CF by regularizing model updates or separating
task-specific and shared components. However, these methods focus on task model
elements while overlooking the potential of leveraging inter-task relationships
for learning enhancement. To address this, we propose a transferability-aware
task embedding named H-embedding and train a hypernet under its guidance to
learn task-conditioned model weights for CL tasks. Particularly, H-embedding is
introduced based on an information theoretical transferability measure and is
designed to be online and easy to compute. The framework is also characterized
by notable practicality, which only requires storing a low-dimensional task
embedding for each task, and can be efficiently trained in an end-to-end way.
Extensive evaluations and experimental analyses on datasets including Permuted
MNIST, Cifar10/100, and ImageNet-R demonstrate that our framework performs
prominently compared to various baseline methods, displaying great potential in
exploiting intrinsic task relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Graph</span>Thought: <span class="highlight-title">Graph</span> Combinatorial Optimization with Thought Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiao Huang, Lifeng Guo, Junjie Sheng, Haosheng Chen, Wenhao Li, Bo Jin, Changhong Lu, Xiangfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, especially in text processing and generative tasks. Recent
advancements in the reasoning capabilities of state-of-the-art LLMs, such as
OpenAI-o1, have significantly broadened their applicability, particularly in
complex problem-solving and logical inference. However, most existing LLMs
struggle with notable limitations in handling graph combinatorial optimization
(GCO) problems. To bridge this gap, we formally define the Optimal Thoughts
Design (OTD) problem, including its state and action thought space. We then
introduce a novel framework, GraphThought, designed to generate high-quality
thought datasets for GCO problems. Leveraging these datasets, we fine-tune the
Llama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact
8B-parameter architecture, Llama-GT matches the performance of state-of-the-art
LLMs on the GraphArena benchmark. Experimental results show that our approach
outperforms both proprietary and open-source models, even rivaling specialized
models like o1-mini. This work sets a new state-of-the-art benchmark while
challenging the prevailing notion that model scale is the primary driver of
reasoning capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 5 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Actor-Critic Algorithm with Function Approximation for Risk Sensitive
  Cost Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyajit Guin, Vivek S. Borkar, Shalabh Bhatnagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the risk-sensitive cost criterion with
exponentiated costs for Markov decision processes and develop a model-free
policy gradient algorithm in this setting. Unlike additive cost criteria such
as average or discounted cost, the risk-sensitive cost criterion is less
studied due to the complexity resulting from the multiplicative structure of
the resulting Bellman equation. We develop an actor-critic algorithm with
function approximation in this setting and provide its asymptotic convergence
analysis. We also show the results of numerical experiments that demonstrate
the superiority in performance of our algorithm over other recent algorithms in
the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Embeddings for Deep Learning on Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshko Koloski, Andrei Margeloiu, Xiangjian Jiang, Blaž Škrlj, Nikola Simidjievski, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular deep-learning methods require embedding numerical and categorical
input features into high-dimensional spaces before processing them. Existing
methods deal with this heterogeneous nature of tabular data by employing
separate type-specific encoding approaches. This limits the cross-table
transfer potential and the exploitation of pre-trained knowledge. We propose a
novel approach that first transforms tabular data into text, and then leverages
pre-trained representations from LLMs to encode this data, resulting in a
plug-and-play solution to improv ing deep-learning tabular methods. We
demonstrate that our approach improves accuracy over competitive models, such
as MLP, ResNet and FT-Transformer, by validating on seven classification
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributional autoencoders know the score 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrej Leban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents novel and desirable properties of a recently introduced
class of autoencoders -- the Distributional Principal Autoencoder (DPA) -- that
combines distributionally correct reconstruction with principal components-like
interpretability of the encodings.
  First, we show that the level sets of the encoder orient themselves exactly
with regard to the score of the data distribution. This both explains the
method's often remarkable performance in disentangling the the factors of
variation of the data, as well as opens up possibilities of recovering its
distribution while having access to samples only. In settings where the score
itself has physical meaning -- such as when the data obey the Boltzmann
distribution -- we demonstrate that the method can recover scientifically
important quantities such as the \textit{minimum free energy path}.
  Second, we show that if the data lie on a manifold that can be approximated
by the encoder, the optimal encoder's components beyond the dimension of the
manifold will carry absolutely no additional information about the data
distribution. This promises new ways of determining the number of relevant
dimensions of the data beyond common heuristics such as the scree plot.
  Finally, the fact that the method is learning the score means that it could
have promise as a generative model, potentially rivaling approaches such as
diffusion, which similarly attempts to approximate the score of the data
distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaMTEB: Massive Text Embedding Benchmark in Persian Language <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)
text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our
benchmark includes 63 datasets spanning seven different tasks: classification,
clustering, pair classification, reranking, retrieval, summary retrieval, and
semantic textual similarity. The datasets are formed as a combination of
existing, translated, and newly generated data, offering a diverse evaluation
framework for Persian language models. Given the increasing use of text
embedding models in chatbots, evaluation datasets are becoming inseparable
ingredients in chatbot challenges and Retrieval-Augmented Generation systems.
As a contribution, we include chatbot evaluation datasets in the MTEB benchmark
for the first time. In addition, in this paper, we introduce the new task of
summary retrieval which is not part of the tasks included in standard MTEB.
Another contribution of this paper is the introduction of a substantial number
of new Persian language NLP datasets suitable for training and evaluation, some
of which have no previous counterparts in Persian. We evaluate the performance
of several Persian and multilingual embedding models in a range of tasks. This
work introduces an open-source benchmark with datasets, code and a public
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Trustworthy Anomaly Detection for Critical Applications
  through Approximated Partial AUC Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Bougaham, Benoît Frénay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly Detection is a crucial step for critical applications such in the
industrial, medical or cybersecurity domains. These sectors share the same
requirement of handling differently the different types of classification
errors. Indeed, even if false positives are acceptable, false negatives are
not, because it would reflect a missed detection of a quality issue, a disease
or a cyber threat. To fulfill this requirement, we propose a method that
dynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A
binary classifier is trained to optimize the specific range of the AUC ROC
curve that prevents the True Positive Rate (TPR) to reach 100% while minimizing
the False Positive Rate (FPR). The optimal threshold that does not trigger any
false negative is then kept and used at the test step. The results show a TPR
of 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR
improvement of 4.3% for a FPR cost of 12.2% against other state-of-the-art
methods. The code is available at https://github.com/ArnaudBougaham/tapAUC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reasoning Ability of Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Srivastava, Shuxiang Cao, Xuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning has long been viewed as an emergent property of large language
models (LLMs), appearing at or above a certain scale ($\sim$100B parameters).
However, recent studies challenge this assumption, showing that small language
models (SLMs) can also achieve competitive reasoning performance. SLMs are
increasingly favored for their efficiency and deployability. However, there is
a lack of systematic study on the reasoning abilities of diverse SLMs,
including those trained from scratch or derived from LLMs through quantization,
pruning, and distillation. This raises a critical question: Can SLMs achieve
reasoning abilities comparable to LLMs? In this work, we systematically survey,
benchmark, and analyze 72 SLMs from six model families across 14 reasoning
benchmarks. For reliable evaluation, we examine four evaluation methods and
compare four LLM judges against human evaluations on 800 data points. We repeat
all experiments three times to ensure a robust performance assessment.
Additionally, we analyze the impact of different prompting strategies in small
models. Beyond accuracy, we also evaluate model robustness under adversarial
conditions and intermediate reasoning steps. Our findings challenge the
assumption that scaling is the only way to achieve strong reasoning. Instead,
we foresee a future where SLMs with strong reasoning capabilities can be
developed through structured training or post-training compression. They can
serve as efficient alternatives to LLMs for reasoning-intensive tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Diffusion Model for Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyeong Jo, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as a promising alternative to autoregressive
models in modeling discrete categorical data. Yet diffusion models that
directly work on discrete data space do not fully exploit the power of
iterative refinement, as the signals are lost during the transition between
discrete states. Existing continuous diffusion models for discrete data have
limited performance compared to discrete approaches, and the unclear link
between them restricts the development of diffusion models for discrete data.
In this work, we propose a continuous diffusion model for language modeling
that incorporates the geometry of the underlying categorical distribution. We
establish a connection between the discrete diffusion and continuous flow on
the statistical manifold, and building on the analogy, we introduce a simple
design for the diffusion process that generalizes previous discrete diffusion
models. We further propose a simulation-free training framework based on radial
symmetry and a simple technique to address the high dimensionality of the
manifold. Comprehensive experiments on language modeling benchmarks and other
modalities show that our method outperforms existing discrete diffusion models
and approaches the performance of autoregressive models. Codes available at
\href{https://github.com/harryjo97/RDLM}{https://github.com/harryjo97/RDLM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Automatic Prompt Engineering: An Optimization Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwu Li, Xiangfeng Wang, Wenhao Li, Bo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of foundation models has shifted focus from resource-intensive
fine-tuning to prompt engineering, a paradigm that steers model behavior
through input design rather than weight updates. While manual prompt
engineering faces limitations in scalability, adaptability, and cross-modal
alignment, automated methods, spanning foundation model (FM) based
optimization, evolutionary methods, gradient-based optimization, and
reinforcement learning, offer promising solutions. Existing surveys, however,
remain fragmented across modalities and methodologies. This paper presents the
first comprehensive survey on automated prompt engineering through a unified
optimization-theoretic lens. We formalize prompt optimization as a maximization
problem over discrete, continuous, and hybrid prompt spaces, systematically
organizing methods by their optimization variables (instructions, soft prompts,
exemplars), task-specific objectives, and computational frameworks. By bridging
theoretical formulation with practical implementations across text, vision, and
multimodal domains, this survey establishes a foundational framework for both
researchers and practitioners, while highlighting underexplored frontiers in
constrained optimization and agent-oriented prompt design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\text{M}^{\text{3}}$: A Modular World Model over Streams of Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lior Cohen, Kaixin Wang, Bingyi Kang, Uri Gadot, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token-based world models emerged as a promising modular framework, modeling
dynamics over token streams while optimizing tokenization separately. While
successful in visual environments with discrete actions (e.g., Atari games),
their broader applicability remains uncertain. In this paper, we introduce
$\text{M}^{\text{3}}$, a $\textbf{m}$odular $\textbf{w}$orld $\textbf{m}$odel
that extends this framework, enabling flexible combinations of observation and
action modalities through independent modality-specific components.
$\text{M}^{\text{3}}$ integrates several improvements from existing literature
to enhance agent performance. Through extensive empirical evaluation across
diverse benchmarks, $\text{M}^{\text{3}}$ achieves state-of-the-art sample
efficiency for planning-free world models. Notably, among these methods, it is
the first to reach a human-level median score on Atari 100K, with superhuman
performance on 13 games. We
$\href{https://github.com/leor-c/M3}{\text{open-source our code and weights}}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-Agent Collaboration in Embodied AI: A Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wu, Xian Wei, Guang Chen, Hao Shen, Xiangfeng Wang, Wenhao Li, Bo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied multi-agent systems (EMAS) have attracted growing attention for
their potential to address complex, real-world challenges in areas such as
logistics and robotics. Recent advances in foundation models pave the way for
generative agents capable of richer communication and adaptive problem-solving.
This survey provides a systematic examination of how EMAS can benefit from
these generative capabilities. We propose a taxonomy that categorizes EMAS by
system architectures and embodiment modalities, emphasizing how collaboration
spans both physical and virtual contexts. Central building blocks, perception,
planning, communication, and feedback, are then analyzed to illustrate how
generative techniques bolster system robustness and flexibility. Through
concrete examples, we demonstrate the transformative effects of integrating
foundation models into embodied, multi-agent frameworks. Finally, we discuss
challenges and future directions, underlining the significant promise of EMAS
to reshape the landscape of AI-driven collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Keep a Promise: Scaling Language Model Decoding Parallelism
  with Learned Asynchronous Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding with autoregressive large language models (LLMs) traditionally
occurs sequentially, generating one token after another. An emerging line of
work explored parallel decoding by identifying and simultaneously generating
semantically independent chunks of LLM responses. However, these techniques
rely on hand-crafted heuristics tied to syntactic structures like lists and
paragraphs, making them rigid and imprecise. We present PASTA, a learning-based
system that teaches LLMs to identify semantic independence and express parallel
decoding opportunities in their own responses. At its core are PASTA-LANG and
its interpreter: PASTA-LANG is an annotation language that enables LLMs to
express semantic independence in their own responses; the language interpreter
acts on these annotations to orchestrate parallel decoding on-the-fly at
inference time. Through a two-stage finetuning process, we train LLMs to
generate PASTA-LANG annotations that optimize both response quality and
decoding speed. Evaluation on AlpacaEval, an instruction following benchmark,
shows that our approach Pareto-dominates existing methods in terms of decoding
speed and response quality; our results demonstrate geometric mean speedups
ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to
-7.1%, measured by length-controlled win rates against sequential decoding
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zhang, Yifan Yang, Kai Zhen, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated exceptional capabilities across
diverse tasks, but their fine-tuning demands significant memory, posing
challenges for resource-constrained environments. Zeroth-order (ZO)
optimization provides a memory-efficient alternative by eliminating the need
for backpropagation. However, ZO optimization suffers from high gradient
variance, and prior research has largely focused on single-task learning,
leaving its application to multi-task learning unexplored. Multi-task learning
is crucial for leveraging shared knowledge across tasks to improve
generalization, yet it introduces unique challenges under ZO settings, such as
amplified gradient variance and collinearity. In this paper, we present MaZO,
the first framework specifically designed for multi-task LLM fine-tuning under
ZO optimization. MaZO tackles these challenges at the parameter level through
two key innovations: a weight importance metric to identify critical parameters
and a multi-task weight update mask to selectively update these parameters,
reducing the dimensionality of the parameter space and mitigating task
conflicts. Experiments demonstrate that MaZO achieves state-of-the-art
performance, surpassing even multi-task learning methods designed for
first-order optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DifCluE: Generating Counterfactual Explanations with Diffusion
  Autoencoders and modal clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suparshva Jain, Amit Sangroya, Lovekesh Vig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating multiple counterfactual explanations for different modes within a
class presents a significant challenge, as these modes are distinct yet
converge under the same classification. Diffusion probabilistic models (DPMs)
have demonstrated a strong ability to capture the underlying modes of data
distributions. In this paper, we harness the power of a Diffusion Autoencoder
to generate multiple distinct counterfactual explanations. By clustering in the
latent space, we uncover the directions corresponding to the different modes
within a class, enabling the generation of diverse and meaningful
counterfactuals. We introduce a novel methodology, DifCluE, which consistently
identifies these modes and produces more reliable counterfactual explanations.
Our experimental results demonstrate that DifCluE outperforms the current
state-of-the-art in generating multiple counterfactual explanations, offering a
significant advance- ment in model interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Surrogate Potential Mean Field Games via Gaussian Processes: A
  Data-Driven Approach to Ill-Posed Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingguo Zhang, Xianjin Yang, Chenchen Mou, Chao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mean field games (MFGs) describe the collective behavior of large populations
of interacting agents. In this work, we tackle ill-posed inverse problems in
potential MFGs, aiming to recover the agents' population, momentum, and
environmental setup from limited, noisy measurements and partial observations.
These problems are ill-posed because multiple MFG configurations can explain
the same data, or different parameters can yield nearly identical observations.
Nonetheless, they remain crucial in practice for real-world scenarios where
data are inherently sparse or noisy, or where the MFG structure is not fully
determined. Our focus is on finding surrogate MFGs that accurately reproduce
the observed data despite these challenges. We propose two Gaussian process
(GP)-based frameworks: an inf-sup formulation and a bilevel approach. The
choice between them depends on whether the unknown parameters introduce
concavity in the objective. In the inf-sup framework, we use the linearity of
GPs and their parameterization structure to maintain convex-concave properties,
allowing us to apply standard convex optimization algorithms. In the bilevel
framework, we employ a gradient-descent-based algorithm and introduce two
methods for computing the outer gradient. The first method leverages an
existing solver for the inner potential MFG and applies automatic
differentiation, while the second adopts an adjoint-based strategy that
computes the outer gradient independently of the inner solver. Our numerical
experiments show that when sufficient prior information is available, the
unknown parameters can be accurately recovered. Otherwise, if prior information
is limited, the inverse problem is ill-posed, but our frameworks can still
produce surrogate MFG models that closely match observed data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">GNN</span>-based Spectral Filtering Mechanism for Imbalance Classification in
  Network Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abubakar Isah, Ibrahim Aliyu, Sulaiman Muhammad Rashid, Jaehyung Park, Minsoo Hahn, Jinsul Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks are gaining attention in Fifth-Generation (5G) core
network digital twins, which are data-driven complex systems with numerous
components. Analyzing these data can be challenging due to rare failure types,
leading to imbalanced classification in multiclass settings. Digital twins of
5G networks increasingly employ graph classification as the main method for
identifying failure types. However, the skewed distribution of failure
occurrences is a major class imbalance issue that prevents effective graph data
mining. Previous studies have not sufficiently tackled this complex problem. In
this paper, we propose Class-Fourier Graph Neural Network (CF-GNN) introduces a
class-oriented spectral filtering mechanism that ensures precise classification
by estimating a unique spectral filter for each class. We employ eigenvalue and
eigenvector spectral filtering to capture and adapt to variations in the
minority classes, ensuring accurate class-specific feature discrimination, and
adept at graph representation learning for complex local structures among
neighbors in an end-to-end setting. Extensive experiments have demonstrated
that the proposed CF-GNN could help with both the creation of new techniques
for enhancing classifiers and the investigation of the characteristics of the
multi-class imbalanced data in a network digital twin system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.06595</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Gradient-based Design Optimization Via Differentiable
  Physics-Informed Neural Operator: A Composites Autoclave Processing Case
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janak M. Patel, Milad Ramezankhani, Anirudh Deodhar, Dagnachew Birru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation and optimization are crucial for advancing the engineering design
of complex systems and processes. Traditional optimization methods require
substantial computational time and effort due to their reliance on
resource-intensive simulations, such as finite element analysis, and the
complexity of rigorous optimization algorithms. Data-agnostic AI-based
surrogate models, such as Physics-Informed Neural Operators (PINOs), offer a
promising alternative to these conventional simulations, providing drastically
reduced inference time, unparalleled data efficiency, and zero-shot
super-resolution capability. However, the predictive accuracy of these models
is often constrained to small, low-dimensional design spaces or systems with
relatively simple dynamics. To address this, we introduce a novel
Physics-Informed DeepONet (PIDON) architecture, which extends the capabilities
of conventional neural operators to effectively model the nonlinear behavior of
complex engineering systems across high-dimensional design spaces and a wide
range of dynamic design configurations. This new architecture outperforms
existing SOTA models, enabling better predictions across broader design spaces.
Leveraging PIDON's differentiability, we integrate a gradient-based
optimization approach using the Adam optimizer to efficiently determine optimal
design variables. This forms an end-to-end gradient-based optimization
framework that accelerates the design process while enhancing scalability and
efficiency. We demonstrate the effectiveness of this framework in the
optimization of aerospace-grade composites curing processes achieving a 3x
speedup in obtaining optimal design variables compared to gradient-free
methods. Beyond composites processing, the proposed model has the potential to
be used as a scalable and efficient optimization tool for broader applications
in advanced engineering and digital twin systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU-accelerated Multi-relational Parallel <span class="highlight-title">Graph</span> Retrieval for Web-scale
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Guo, Guangxing Chen, Qian Gao, Xiaochao Liao, Jianjia Zheng, Lu Shen, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web recommendations provide personalized items from massive catalogs for
users, which rely heavily on retrieval stages to trade off the effectiveness
and efficiency of selecting a small relevant set from billion-scale candidates
in online digital platforms. As one of the largest Chinese search engine and
news feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based
Approximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance
estimation and efficient search for relevant items. However, current retrieval
at Baidu fails in comprehensive user-item relational understanding due to
dissected interaction modeling, and performs inefficiently in large-scale
graph-based ANNS because of suboptimal traversal navigation and the GPU
computational bottleneck under high concurrency. To this end, we propose a
GPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to
achieve effective yet efficient retrieval in web-scale recommendations. First,
we propose a multi-relational user-item relevance metric learning method that
unifies diverse user behaviors through multi-objective optimization and employs
a self-covariant loss to enhance pathfinding performance. Second, we develop a
hierarchical parallel graph-based ANNS to boost graph retrieval throughput,
which conducts breadth-depth-balanced searches on a large-scale item graph and
cost-effectively handles irregular neural computation via adaptive aggregation
on GPUs. In addition, we integrate system optimization strategies in the
deployment of GMP-GR in Baidu. Extensive experiments demonstrate the
superiority of GMP-GR in retrieval accuracy and efficiency. Deployed across
more than twenty applications at Baidu, GMP-GR serves hundreds of millions of
users with a throughput exceeding one hundred million requests per second.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dictionary-Learning-Based Data Pruning for System Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingna Wang, Sikai Zhang, Limin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System identification is normally involved in augmenting time series data by
time shifting and nonlinearisation (via polynomial basis), which introduce
redundancy both feature-wise and sample-wise. Many research works focus on
reducing redundancy feature-wise, while less attention is paid to sample-wise
redundancy. This paper proposes a novel data pruning method, called
(mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary
learning. Time series data is represented by some representative samples,
called atoms, via dictionary learning. The useful samples are selected based on
their correlation with the atoms. The method is tested on one simulated dataset
and two benchmark datasets. The R-squared between the coefficients of models
trained on the full and the coefficients of models trained on pruned datasets
is adopted to evaluate the performance of data pruning methods. It is found
that the proposed method significantly outperforms the random pruning method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No-regret incentive-compatible online learning under exact truthfulness
  with non-myopic experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpei Komiyama, Nishant A. Mehta, Ali Mortazavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an online forecasting setting in which, over $T$ rounds, $N$
strategic experts each report a forecast to a mechanism, the mechanism selects
one forecast, and then the outcome is revealed. In any given round, each expert
has a belief about the outcome, but the expert wishes to select its report so
as to maximize the total number of times it is selected. The goal of the
mechanism is to obtain low belief regret: the difference between its cumulative
loss (based on its selected forecasts) and the cumulative loss of the best
expert in hindsight (as measured by the experts' beliefs). We consider exactly
truthful mechanisms for non-myopic experts, meaning that truthfully reporting
its belief strictly maximizes the expert's subjective probability of being
selected in any future round. Even in the full-information setting, it is an
open problem to obtain the first no-regret exactly truthful mechanism in this
setting. We develop the first no-regret mechanism for this setting via an
online extension of the Independent-Event Lotteries Forecasting Competition
Mechanism (I-ELF). By viewing this online I-ELF as a novel instance of Follow
the Perturbed Leader (FPL) with noise based on random walks with loss-dependent
perturbations, we obtain $\tilde{O}(\sqrt{T N})$ regret. Our results are fueled
by new tail bounds for Poisson binomial random variables that we develop. We
extend our results to the bandit setting, where we give an exactly truthful
mechanism obtaining $\tilde{O}(T^{2/3} N^{1/3})$ regret; this is the first
no-regret result even among approximately truthful mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanxuan Liao, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) is essential for Large Language Models (LLMs) to
adapt to evolving real-world demands, yet they are susceptible to catastrophic
forgetting (CF). While traditional CF solutions rely on expensive data
rehearsal, recent rehearsal-free methods employ model-based and
regularization-based strategies to address this issue. However, these
approaches often neglect the model's plasticity, which is crucial to achieving
optimal performance on newly learned tasks. Consequently, a key challenge in CL
is striking a balance between preserving plasticity and mitigating CF. To
tackle this challenge, we propose the $\textbf{D}$ecomposed
$\textbf{A}$ttention-based $\textbf{T}$ask $\textbf{A}$daptation (DATA), which
explicitly decouples and learns both task-specific and task-shared knowledge
using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA
dynamically adjusts the weights of adapters of different ranks based on their
relevance and distinction from previous tasks, allowing the model to acquire
new task-specific skills while effectively retaining previously learned
knowledge. Specifically, we implement a decomposed component weighting strategy
comprising learnable components that collectively generate attention-based
weights, allowing the model to integrate and utilize diverse knowledge from
each DATA. Extensive experiments on three widely used benchmarks demonstrate
that our proposed method achieves state-of-the-art performance. Notably, our
approach significantly enhances model plasticity and mitigates CF by extending
learnable components and employing stochastic restoration during training
iterations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian
  Optimization Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Wei Yang, Yun-Ming Chan, Wei Hung, Xi Liu, Ping-Chun Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline model-based reinforcement learning (MBRL) serves as a competitive
framework that can learn well-performing policies solely from pre-collected
data with the help of learned dynamics models. To fully unleash the power of
offline MBRL, model selection plays a pivotal role in determining the dynamics
model utilized for downstream policy learning. However, offline MBRL
conventionally relies on validation or off-policy evaluation, which are rather
inaccurate due to the inherent distribution shift in offline RL. To tackle
this, we propose BOMS, an active model selection framework that enhances model
selection in offline MBRL with only a small online interaction budget, through
the lens of Bayesian optimization (BO). Specifically, we recast model selection
as BO and enable probabilistic inference in BOMS by proposing a novel
model-induced kernel, which is theoretically grounded and computationally
efficient. Through extensive experiments, we show that BOMS improves over the
baseline methods with a small amount of online interaction comparable to only
$1\%$-$2.5\%$ of offline training data on various RL tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAPS: Throat and Acoustic Paired Speech <span class="highlight-title">Dataset</span> for Deep Learning-Based
  Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunsik Kim, Yonghun Song, Yoonyoung Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-noise environments such as factories, subways, and busy streets,
capturing clear speech is challenging due to background noise. Throat
microphones provide a solution with their noise-suppressing properties,
reducing the noise while recording speech. However, a significant limitation
remains: high-frequency information is attenuated as sound waves pass through
skin and tissue, reducing speech clarity. Recent deep learning approaches have
shown promise in enhancing throat microphone recordings, but further progress
is constrained by the absence of standardized dataset. We introduce a throat
and acoustic paired speech dataset (TAPS), a collection of paired utterances
recorded from 60 native Korean speakers using throat and acoustic microphones.
To demonstrate the TAPS's utility, we tested three baseline deep learning
models and identified the mapping-based approach as superior in improving
speech quality and restoring content. Additionally, we propose an optimal
method to mitigate the signal mismatch between throat and acoustic microphones,
ensuring model performance. These results highlight the potential of TAPS to
serve as a standardized dataset and advance research in throat microphone-based
speech enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximation of Permutation Invariant Polynomials by <span class="highlight-title">Transformer</span>s:
  Efficient Construction in Column-Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Takeshita, Masaaki Imaizumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are a type of neural network that have demonstrated remarkable
performance across various domains, particularly in natural language processing
tasks. Motivated by this success, research on the theoretical understanding of
transformers has garnered significant attention. A notable example is the
mathematical analysis of their approximation power, which validates the
empirical expressive capability of transformers. In this study, we investigate
the ability of transformers to approximate column-symmetric polynomials, an
extension of symmetric polynomials that take matrices as input. Consequently,
we establish an explicit relationship between the size of the transformer
network and its approximation capability, leveraging the parameter efficiency
of transformers and their compatibility with symmetry by focusing on the
algebraic properties of symmetric polynomials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GiFT: Gibbs Fine-Tuning for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Wanjin Feng, Xin Zhou, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) with synthetic data is a prevalent
practice in code generation. A key approach is self-training, where LLMs are
iteratively trained on self-generated correct code snippets. In this case, the
self-generated codes are drawn from a conditional distribution, conditioned on
a specific seed description. However, the seed description is not the only
valid representation that aligns with its intended meaning. With all valid
descriptions and codes forming a joint space, codes drawn from the conditional
distribution would lead to an underrepresentation of the full description-code
space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training
method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn
from the marginal distribution of the joint space, thereby mitigating the
biases inherent in conditional sampling. We provide a theoretical analysis
demonstrating the potential benefits of fine-tuning LLMs with code derived from
the marginal distribution. Furthermore, we propose a perplexity-based code
selection method to mitigate the imbalanced long-tail distribution of the
self-generated codes. Empirical evaluation of two LLMs across four datasets
demonstrates that GiFT achieves superior performance, particularly on more
challenging benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All Models Are Miscalibrated, But Some Less So: Comparing Calibration
  with Conditional Mean Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Moskvichev, Dino Sejdinovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When working in a high-risk setting, having well calibrated probabilistic
predictive models is a crucial requirement. However, estimators for calibration
error are not always able to correctly distinguish which model is better
calibrated. We propose the \emph{conditional kernel calibration error} (CKCE)
which is based on the Hilbert-Schmidt norm of the difference between
conditional mean operators. By working directly with the definition of strong
calibration as the distance between conditional distributions, which we
represent by their embeddings in reproducing kernel Hilbert spaces, the CKCE is
less sensitive to the marginal distribution of predictive models. This makes it
more effective for relative comparisons than previously proposed calibration
metrics. Our experiments, using both synthetic and real data, show that CKCE
provides a more consistent ranking of models by their calibration error and is
more robust against distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMFCA-Net: A Lightweight Model for Multi-Channel Speech Enhancement with
  Efficient Narrow-Band and Cross-Band Attention <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaokai Zhang, Hanchen Pei, Wanqi Wang, Gongping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based end-to-end multi-channel speech enhancement methods have
achieved impressive performance by leveraging sub-band, cross-band, and spatial
information. However, these methods often demand substantial computational
resources, limiting their practicality on terminal devices. This paper presents
a lightweight multi-channel speech enhancement network with decoupled fully
connected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis
decoupled fully-connected attention (T-FCA) and frequency-axis decoupled
fully-connected attention (F-FCA) mechanisms to effectively capture long-range
narrow-band and cross-band information without recurrent units. Experimental
results show that LMFCA-Net performs comparably to state-of-the-art methods
while significantly reducing computational complexity and latency, making it a
promising solution for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient Pre-training: Exploring FP4 Precision in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, Weiming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning computational demands for training large language models
(LLMs) necessitate efficient methods, including quantized training, which
leverages low-bit arithmetic operations to reduce costs. While FP8 precision
has shown potential, leveraging FP4 remains challenging due to inherent
quantization errors and limited representation capability. Based on the
Transformer architecture, we present an FP4 training scheme for LLMs,
overcoming these obstacles through mixed-precision quantization strategies
tailed for different modules and training stages. This allows us to apply the
precision level suitable to distinct components within the model, ensuring that
multi-head attention and linear layers are handled appropriately. Our
pretraining recipe ensures stability in backpropagation by incorporating
fine-grained quantization methods with a target precision training schedule.
Experimental results demonstrate that our FP4 training scheme achieves accuracy
comparable to BF16 and FP8, with smaller theoretical computational cost. With
the advent of next-generation hardware supporting FP4, our method sets the
foundation for efficient ultra-low precision training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connector-S: A <span class="highlight-title">Survey</span> of Connectors in <span class="highlight-title">Multi-modal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Zhu, Zheng Zhang, Xi Chen, Yiming Shi, Miao Li, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancements in multi-modal large language models (MLLMs),
connectors play a pivotal role in bridging diverse modalities and enhancing
model performance. However, the design and evolution of connectors have not
been comprehensively analyzed, leaving gaps in understanding how these
components function and hindering the development of more powerful connectors.
In this survey, we systematically review the current progress of connectors in
MLLMs and present a structured taxonomy that categorizes connectors into atomic
operations (mapping, compression, mixture of experts) and holistic designs
(multi-layer, multi-encoder, multi-modal scenarios), highlighting their
technical contributions and advancements. Furthermore, we discuss several
promising research frontiers and challenges, including high-resolution input,
dynamic compression, guide information selection, combination strategy, and
interpretability. This survey is intended to serve as a foundational reference
and a clear roadmap for researchers, providing valuable insights into the
design and optimization of next-generation connectors to enhance the
performance and adaptability of MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fishing For Cheap And Efficient Pruners At Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivo Gollini Navarrete, Nicolas Mauricio Cuadrado, Jose Renato Restom, Martin Takáč, Samuel Horváth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning offers a promising solution to mitigate the associated costs and
environmental impact of deploying large deep neural networks (DNNs).
Traditional approaches rely on computationally expensive trained models or
time-consuming iterative prune-retrain cycles, undermining their utility in
resource-constrained settings. To address this issue, we build upon the
established principles of saliency (LeCun et al., 1989) and connection
sensitivity (Lee et al., 2018) to tackle the challenging problem of one-shot
pruning neural networks (NNs) before training (PBT) at initialization. We
introduce Fisher-Taylor Sensitivity (FTS), a computationally cheap and
efficient pruning criterion based on the empirical Fisher Information Matrix
(FIM) diagonal, offering a viable alternative for integrating first- and
second-order information to identify a model's structurally important
parameters. Although the FIM-Hessian equivalency only holds for convergent
models that maximize the likelihood, recent studies (Karakida et al., 2019)
suggest that, even at initialization, the FIM captures essential geometric
information of parameters in overparameterized NNs, providing the basis for our
method. Finally, we demonstrate empirically that layer collapse, a critical
limitation of data-dependent pruning methodologies, is easily overcome by
pruning within a single training epoch after initialization. We perform
experiments on ResNet18 and VGG19 with CIFAR-10 and CIFAR-100, widely used
benchmarks in pruning research. Our method achieves competitive performance
against state-of-the-art techniques for one-shot PBT, even under extreme
sparsity conditions. Our code is made available to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages of main content (excluding references), 2 figures, 2 tables,
  1 algorithm, and 11 pages of appendix. Code available at
  https://github.com/Gollini/Fisher_Taylor_Sensitivity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Editing Provide Evidence for Localization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Victor Veitch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A basic aspiration for interpretability research in large language models is
to "localize" semantically meaningful behaviors to particular components within
the LLM. There are various heuristics for finding candidate locations within
the LLM. Once a candidate localization is found, it can be assessed by editing
the internal representations at the corresponding localization and checking
whether this induces model behavior that is consistent with the semantic
interpretation of the localization. The question we address here is: how strong
is the evidence provided by such edits? To assess localization, we want to
assess the effect of the optimal intervention at a particular location. The key
new technical tool is a way of adapting LLM alignment techniques to find such
optimal localized edits. With this tool in hand, we give an example where the
edit-based evidence for localization appears strong, but where localization
clearly fails. Indeed, we find that optimal edits at random localizations can
be as effective as aligning the full model. In aggregate, our results suggest
that merely observing that localized edits induce targeted changes in behavior
provides little to no evidence that these locations actually encode the target
behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Turn <span class="highlight-title">Multi-Modal</span> Question Clarification for Enhanced
  Conversational Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimia Ramezan, Alireza Amiri Bavandpour, Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational query clarification enables users to refine their search
queries through interactive dialogue, improving search effectiveness.
Traditional approaches rely on text-based clarifying questions, which often
fail to capture complex user preferences, particularly those involving visual
attributes. While recent work has explored single-turn multi-modal
clarification with images alongside text, such methods do not fully support the
progressive nature of user intent refinement over multiple turns. Motivated by
this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,
which combines text and visual modalities to refine user queries in a
multi-turn conversation. To facilitate this task, we create a large-scale
dataset named ClariMM comprising over 13k multi-turn interactions and 33k
question-answer pairs containing multi-modal clarifying questions. We propose
Mario, a retrieval framework that employs a two-phase ranking strategy: initial
retrieval with BM25, followed by a multi-modal generative re-ranking model that
integrates textual and visual information from conversational history. Our
experiments show that multi-turn multi-modal clarification outperforms
uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are
most significant in longer interactions, demonstrating the value of progressive
refinement for complex queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Row-Based Sparse Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cen-Jhih Li, Aditya Bhaskara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning is an important step in adapting foundation models such as large
language models to downstream tasks. To make this step more accessible to users
with limited computational budgets, it is crucial to develop fine-tuning
methods that are memory and computationally efficient. Sparse Fine-tuning (SFT)
and Low-rank adaptation (LoRA) are two frameworks that have emerged for
addressing this problem and have been adopted widely in practice. In this work,
we develop a new SFT framework, based on ideas from neural network pruning. At
a high level, we first identify "important" neurons/nodes using feature
importance metrics from network pruning (specifically, we use the structural
pruning method), and then perform fine-tuning by restricting to weights
involving these neurons. Using experiments on common language tasks, we
demonstrate that our method significantly improves the memory efficiency of SFT
without increasing training time complexity and implementation complexity,
while achieving accuracy comparable to state-of-the-art methods such as LoRA
and its variants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADO: Automatic Data Optimization for Inputs in LLM Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Lin, Wenyue Hua, Lingyao Li, Zhenting Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a novel approach to enhance the performance of Large
Language Models (LLMs) through the optimization of input data within prompts.
While previous research has primarily focused on refining instruction
components and augmenting input data with in-context examples, our work
investigates the potential benefits of optimizing the input data itself. We
introduce a two-pronged strategy for input data optimization: content
engineering and structural reformulation. Content engineering involves imputing
missing values, removing irrelevant attributes, and enriching profiles by
generating additional information inferred from existing attributes. Subsequent
to content engineering, structural reformulation is applied to optimize the
presentation of the modified content to LLMs, given their sensitivity to input
format. Our findings suggest that these optimizations can significantly improve
the performance of LLMs in various tasks, offering a promising avenue for
future research in prompt engineering. The source code is available at
https://anonymous.4open.science/r/ADO-6BC5/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMART: Self-Aware Agent for Tool Overuse Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Model (LLM) agents demonstrate strong reasoning and
tool use capabilities, but often lack self-awareness, failing to balance these
approaches effectively. This imbalance leads to Tool Overuse, where models
unnecessarily rely on external tools for tasks solvable with parametric
knowledge, increasing computational overhead. Inspired by human metacognition,
we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm
that enhances an agent's self-awareness to optimize task handling and reduce
tool overuse. To support this paradigm, we introduce SMART-ER, a dataset
spanning three domains, where reasoning alternates between parametric knowledge
and tool-dependent steps, with each step enriched by rationales explaining when
tools are necessary. Through supervised training, we develop SMARTAgent, a
family of models that dynamically balance parametric knowledge and tool use.
Evaluations show that SMARTAgent reduces tool use by 24% while improving
performance by over 37%, enabling 7B-scale models to match its 70B counterpart
and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test
data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool
calls. These highlight the potential of strategic tool use to enhance
reasoning, mitigate overuse, and bridge the gap between model size and
performance, advancing intelligent and resource-efficient agent designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Splitting criteria for ordinal decision trees: an experimental study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Ayllón-Gavilán, Francisco José Martínez-Estudillo, David Guijo-Rubio, César Hervás-Martínez, Pedro Antonio Gutiérrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ordinal Classification (OC) is a machine learning field that addresses
classification tasks where the labels exhibit a natural order. Unlike nominal
classification, which treats all classes as equally distinct, OC takes the
ordinal relationship into account, producing more accurate and relevant
results. This is particularly critical in applications where the magnitude of
classification errors has implications. Despite this, OC problems are often
tackled using nominal methods, leading to suboptimal solutions. Although
decision trees are one of the most popular classification approaches, ordinal
tree-based approaches have received less attention when compared to other
classifiers. This work conducts an experimental study of tree-based
methodologies specifically designed to capture ordinal relationships. A
comprehensive survey of ordinal splitting criteria is provided, standardising
the notations used in the literature for clarity. Three ordinal splitting
criteria, Ordinal Gini (OGini), Weighted Information Gain (WIG), and Ranking
Impurity (RI), are compared to the nominal counterparts of the first two (Gini
and information gain), by incorporating them into a decision tree classifier.
An extensive repository considering 45 publicly available OC datasets is
presented, supporting the first experimental comparison of ordinal and nominal
splitting criteria using well-known OC evaluation metrics. Statistical analysis
of the results highlights OGini as the most effective ordinal splitting
criterion to date. Source code, datasets, and results are made available to the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-LLM Coevolution: Evidence from Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a statistical analysis of arXiv paper abstracts, we report a marked drop
in the frequency of several words previously identified as overused by ChatGPT,
such as "delve", starting soon after they were pointed out in early 2024. The
frequency of certain other words favored by ChatGPT, such as "significant", has
instead kept increasing. These phenomena suggest that some authors of academic
papers have adapted their use of large language models (LLMs), for example, by
selecting outputs or applying modifications to the LLM-generated content. Such
coevolution and cooperation of humans and LLMs thus introduce additional
challenges to the detection of machine-generated text in real-world scenarios.
Estimating the impact of LLMs on academic writing by examining word frequency
remains feasible, and more attention should be paid to words that were already
frequently employed, including those that have decreased in frequency due to
LLMs' disfavor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Point of View of a Sentiment: Towards Clinician Bias Detection in
  Psychiatric Notes <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alissa A. Valentine, Lauren A. Lepow, Lili Chan, Alexander W. Charney, Isotta Landi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative patient descriptions and stigmatizing language can contribute to
generating healthcare disparities in two ways: (1) read by patients, they can
harm their trust and engagement with the medical center; (2) read by
physicians, they may negatively influence their perspective of a future
patient. In psychiatry, the patient-clinician therapeutic alliance is a major
determinant of clinical outcomes. Therefore, language usage in psychiatric
clinical notes may not only create healthcare disparities, but also perpetuate
them. Recent advances in NLP systems have facilitated the efforts to detect
discriminatory language in healthcare. However, such attempts have only focused
on the perspectives of the medical center and its physicians. Considering both
physicians and non-physicians' point of view is a more translatable approach to
identifying potentially harmful language in clinical notes. By leveraging
pre-trained and large language models (PLMs and LLMs), this work aims to
characterize potentially harmful language usage in psychiatric notes by
identifying the sentiment expressed in sentences describing patients based on
the reader's point of view. Extracting 39 sentences from the Mount Sinai Health
System containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa,
GatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and
few-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to
classify the sentiment of the sentences according to the physician or
non-physician point of view. Results showed that GPT-3.5 aligned best to
physician point of view and Mistral aligned best to non-physician point of
view. These results underline the importance of recognizing the reader's point
of view, not only for improving the note writing process, but also for the
quantification, identification, and reduction of bias in computational systems
for downstream analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral presentation at NAACL 2024 Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Acoustic Side-Channel Attacks on Keyboards Using <span class="highlight-title">Transformer</span>s
  and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of microphones in everyday devices and the growing
reliance on online services have amplified the risk of acoustic side-channel
attacks (ASCAs) targeting keyboards. This study explores deep learning
techniques, specifically vision transformers (VTs) and large language models
(LLMs), to enhance the effectiveness and applicability of such attacks. We
present substantial improvements over prior research, with the CoAtNet model
achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement
for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via
Zoom compared to previous benchmarks. We also evaluate transformer
architectures and language models, with the best VT model matching CoAtNet's
performance. A key advancement is the introduction of a noise mitigation method
for real-world scenarios. By using LLMs for contextual understanding, we detect
and correct erroneous keystrokes in noisy environments, enhancing ASCA
performance. Additionally, fine-tuned lightweight language models with Low-Rank
Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X
more parameters. This integration of VTs and LLMs improves the practical
applicability of ASCA mitigation, marking the first use of these technologies
to address ASCAs and error correction in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We will reflect comments from the reviewers and re-submit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELL your Model: Contrastive Explanations for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronny Luss, Erik Miehling, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of black-box deep neural network classification models has sparked
the need to explain their decisions. However, in the case of generative AI,
such as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing a contrastive explanation
method requiring simply black-box/query access. Our explanations suggest that
an LLM outputs a reply to a given prompt because if the prompt was slightly
modified, the LLM would have given a different response that is either less
preferable or contradicts the original response. The key insight is that
contrastive explanations simply require a scoring function that has meaning to
the user and not necessarily a specific real valued quantity (viz. class
label). To this end, we offer a novel budgeted algorithm, our main algorithmic
contribution, which intelligently creates contrasts based on such a scoring
function while adhering to a query budget, necessary for longer contexts. We
show the efficacy of our method on important natural language tasks such as
open-text generation and chatbot conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision CNNs trained to estimate spatial latents learned similar
  ventral-stream-aligned representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies of the functional role of the primate ventral visual stream have
traditionally focused on object categorization, often ignoring -- despite much
prior evidence -- its role in estimating "spatial" latents such as object
position and pose. Most leading ventral stream models are derived by optimizing
networks for object categorization, which seems to imply that the ventral
stream is also derived under such an objective. Here, we explore an alternative
hypothesis: Might the ventral stream be optimized for estimating spatial
latents? And a closely related question: How different -- if at all -- are
representations learned from spatial latent estimation compared to
categorization? To ask these questions, we leveraged synthetic image datasets
generated by a 3D graphic engine and trained convolutional neural networks
(CNNs) to estimate different combinations of spatial and category latents. We
found that models trained to estimate just a few spatial latents achieve neural
alignment scores comparable to those trained on hundreds of categories, and the
spatial latent performance of models strongly correlates with their neural
alignment. Spatial latent and category-trained models have very similar -- but
not identical -- internal representations, especially in their early and middle
layers. We provide evidence that this convergence is partly driven by
non-target latent variability in the training data, which facilitates the
implicit learning of representations of those non-target latents. Taken
together, these results suggest that many training objectives, such as spatial
latents, can lead to similar models aligned neurally with the ventral stream.
Thus, one should not assume that the ventral stream is optimized for object
categorization only. As a field, we need to continue to sharpen our measures of
comparing models to brains to better understand the functional roles of the
ventral stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 21 figures, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiTTo-TTS: Diffusion <span class="highlight-title">Transformer</span>s for Scalable Text-to-Speech without
  Domain-Specific Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon Lee, Dong Won Kim, Jaehyeon Kim, Seungjun Chung, Jaewoong Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale latent diffusion models (LDMs) excel in content generation across
various modalities, but their reliance on phonemes and durations in
text-to-speech (TTS) limits scalability and access from other fields. While
recent studies show potential in removing these domain-specific factors,
performance remains suboptimal. In this work, we introduce DiTTo-TTS, a
Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based
TTS can achieve state-of-the-art performance without domain-specific factors.
Through rigorous analysis and empirical exploration, we find that (1) DiT with
minimal modifications outperforms U-Net, (2) variable-length modeling with a
speech length predictor significantly improves results over fixed-length
approaches, and (3) conditions like semantic alignment in speech latent
representations are key to further enhancement. By scaling our training data to
82K hours and the model size to 790M parameters, we achieve superior or
comparable zero-shot performance to state-of-the-art TTS models in naturalness,
intelligibility, and speaker similarity, all without relying on domain-specific
factors. Speech samples are available at https://ditto-tts.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a novel language model architecture that is capable of scaling
test-time computation by implicitly reasoning in latent space. Our model works
by iterating a recurrent block, thereby unrolling to arbitrary depth at
test-time. This stands in contrast to mainstream reasoning models that scale up
compute by producing more tokens. Unlike approaches based on chain-of-thought,
our approach does not require any specialized training data, can work with
small context windows, and can capture types of reasoning that are not easily
represented in words. We scale a proof-of-concept model to 3.5 billion
parameters and 800 billion tokens. We show that the resulting model can improve
its performance on reasoning benchmarks, sometimes dramatically, up to a
computation load equivalent to 50 billion parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The model is available at
  https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can
  be found at https://github.com/seal-rg/recurrent-pretraining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Equivalence of Bayesian Neural Networks and Gaussian
  Processes: On the Importance of Learning Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Sendera, Amin Sorkhei, Tomasz Kuśmierczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Processes (GPs) provide a convenient framework for specifying
function-space priors, making them a natural choice for modeling uncertainty.
In contrast, Bayesian Neural Networks (BNNs) offer greater scalability and
extendability but lack the advantageous properties of GPs. This motivates the
development of BNNs capable of replicating GP-like behavior. However, existing
solutions are either limited to specific GP kernels or rely on heuristics.
  We demonstrate that trainable activations are crucial for effective mapping
of GP priors to wide BNNs. Specifically, we leverage the closed-form
2-Wasserstein distance for efficient gradient-based optimization of
reparameterized priors and activations. Beyond learned activations, we also
introduce trainable periodic activations that ensure global stationarity by
design, and functional priors conditioned on GP hyperparameters to allow
efficient model selection.
  Empirically, our method consistently outperforms existing approaches or
matches performance of the heuristic methods, while offering stronger
theoretical foundations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advances in <span class="highlight-title">Multimodal</span> Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the importance of social vulnerability in opioid-related
  mortality across the United States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Deas, Adam Spannaus, Dakotah D. Maguire, Jodie Trafton, Anuj J. Kapadia, Vasileios Maroulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The opioid crisis remains a critical public health challenge in the United
States. Despite national efforts to reduce opioid prescribing rates by nearly
45\% between 2011 and 2021, opioid overdose deaths more than tripled during
this same period. This alarming trend reflects a major shift in the crisis,
with illegal opioids now driving the majority of overdose deaths instead of
prescription opioids. Although much attention has been given to supply-side
factors fueling this transition, the underlying socioeconomic conditions that
perpetuate and exacerbate opioid misuse remain less understood. Moreover, the
COVID-19 pandemic intensified the opioid crisis through widespread social
isolation and record-high unemployment; consequently, understanding the
socioeconomic drivers of this epidemic has become even more crucial in recent
years. To address this need, our study examines the correlation between
opioid-related mortality and thirteen components of the Social Vulnerability
Index (SVI). Leveraging a nationwide county-level dataset spanning consecutive
years from 2010 to 2022, this study integrates empirical insights from
exploratory data analysis with feature importance metrics derived from machine
learning models. Our findings highlight critical social factors strongly
correlated with opioid-related mortality, emphasizing their potential roles in
worsening the epidemic when their levels are high and mitigating it when their
levels are low.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Expressive Power of Sparse Geometric MPNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02025v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02025v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Sverdlov, Nadav Dym
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in chemistry and other sciences, we study the
expressive power of message-passing neural networks for geometric graphs, whose
node features correspond to 3-dimensional positions. Recent work has shown that
such models can separate generic pairs of non-isomorphic geometric graphs,
though they may fail to separate some rare and complicated instances. However,
these results assume a fully connected graph, where each node possesses
complete knowledge of all other nodes. In contrast, often, in application,
every node only possesses knowledge of a small number of nearest neighbors.
  This paper shows that generic pairs of non-isomorphic geometric graphs can be
separated by message-passing networks with rotation equivariant features as
long as the underlying graph is connected. When only invariant intermediate
features are allowed, generic separation is guaranteed for generically globally
rigid graphs. We introduce a simple architecture, EGENNET, which achieves our
theoretical guarantees and compares favorably with alternative architecture on
synthetic and chemical benchmarks. Our code is available at
https://github.com/yonatansverdlov/E-GenNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Multi-Permutation Equivariance through the Lens of
  Irreducible Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Sverdlov, Ido Springer, Nadav Dym
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the characterization of equivariant linear layers for
representations of permutations and related groups. Unlike traditional
approaches, which address these problems using parameter-sharing, we consider
an alternative methodology based on irreducible representations and Schur's
lemma. Using this methodology, we obtain an alternative derivation for existing
models like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space
(DWS) networks. The derivation for DWS networks is significantly simpler than
that of previous results.
  Next, we extend our approach to unaligned symmetric sets, where equivariance
to the wreath product of groups is required. Previous works have addressed this
problem in a rather restrictive setting, in which almost all wreath equivariant
layers are Siamese. In contrast, we give a full characterization of layers in
this case and show that there is a vast number of additional non-Siamese layers
in some settings. We also show empirically that these additional non-Siamese
layers can improve performance in tasks like graph anomaly detection, weight
space alignment, and learning Wasserstein distances. Our code is available at
\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Valuation using Neural Networks for Efficient Instruction
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Dilek Hakkani-Tür
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide crucial insights into model training, but
existing methods suffer from large computational costs and limited
generalization. Particularly, recent works have proposed various metrics and
algorithms to calculate the influence of data using language models, which do
not scale well with large models and datasets. This is because of the expensive
forward and backward passes required for computation, substantial memory
requirements to store large models, and poor generalization of influence
estimates to new data. In this paper, we explore the use of small neural
networks -- which we refer to as the InfluenceNetwork -- to estimate influence
values, achieving up to 99% cost reduction. Our evaluation demonstrates that
influence values can be estimated with models just 0.0027% the size of full
language models (we use 7B and 8B versions). We apply our algorithm of
estimating influence values (called NN-CIFT: Neural Networks for effiCient
Instruction Fine-Tuning) to the downstream task of subset selection for general
instruction fine-tuning. In our study, we include four state-of-the-art
influence functions and show no compromise in performance, despite large
speedups, between NN-CIFT and the original influence functions. We provide an
in-depth hyperparameter analyses of NN-CIFT. The code for our method can be
found here: https://github.com/agarwalishika/NN-CIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RDSA: A Robust Deep <span class="highlight-title">Graph</span> Clustering Framework via Dual Soft Assignment <span class="chip">DASFAA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xiang, Li Fan, Tulika Saha, Xiaoying Pang, Yushan Pan, Haiyang Zhang, Chengtao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering is an essential aspect of network analysis that involves
grouping nodes into separate clusters. Recent developments in deep learning
have resulted in graph clustering, which has proven effective in many
applications. Nonetheless, these methods often encounter difficulties when
dealing with real-world graphs, particularly in the presence of noisy edges.
Additionally, many denoising graph clustering methods tend to suffer from lower
performance, training instability, and challenges in scaling to large datasets
compared to non-denoised models. To tackle these issues, we introduce a new
framework called the Robust Deep Graph Clustering Framework via Dual Soft
Assignment (RDSA). RDSA consists of three key components: (i) a node embedding
module that effectively integrates the graph's topological features and node
attributes; (ii) a structure-based soft assignment module that improves graph
modularity by utilizing an affinity matrix for node assignments; and (iii) a
node-based soft assignment module that identifies community landmarks and
refines node assignments to enhance the model's robustness. We assess RDSA on
various real-world datasets, demonstrating its superior performance relative to
existing state-of-the-art methods. Our findings indicate that RDSA provides
robust clustering across different graph types, excelling in clustering
effectiveness and robustness, including adaptability to noise, stability, and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2025; Complete version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Path Planning for Masked Diffusion Model Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid Rector-Brooks, Sherwood Yao, Alexander Tong, Pranam Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore how token unmasking order influences generative
quality in masked diffusion models (MDMs). We derive an expanded evidence lower
bound (ELBO) that introduces a planner to select which tokens to unmask at each
step. Our analysis reveals that alternative unmasking strategies can enhance
generation performance. Building on this, we propose Path Planning (P2), a
sampling framework that uses a pre-trained BERT model or the denoiser itself to
guide unmasking decisions. P2 generalizes all known MDM sampling strategies and
significantly improves performance across diverse domains, including language
generation (in-context learning, code generation, story infilling, mathematical
reasoning, reverse curse correction) and biological sequence generation
(protein and RNA sequences).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention as a Hypernetwork <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05816v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05816v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Schug, Seijin Kobayashi, Yassir Akram, João Sacramento, Razvan Pascanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers can under some circumstances generalize to novel problem
instances whose constituent parts might have been encountered during training,
but whose compositions have not. What mechanisms underlie this ability for
compositional generalization? By reformulating multi-head attention as a
hypernetwork, we reveal that a composable, low-dimensional latent code
specifies key-query specific operations. We find empirically that this latent
code is predictive of the subtasks the network performs on unseen task
compositions, revealing that latent codes acquired during training are reused
to solve unseen problem instances. To further examine the hypothesis that the
intrinsic hypernetwork of multi-head attention supports compositional
generalization, we ablate whether making the hypernetwork-generated linear
value network nonlinear strengthens compositionality. We find that this
modification improves compositional generalization on abstract reasoning tasks.
In particular, we introduce a symbolic version of the Raven's Progressive
Matrices human intelligence test, which gives us precise control over the
problem compositions encountered during training and evaluation. We demonstrate
on this task how scaling model size and data enables compositional
generalization in transformers and gives rise to a functionally structured
latent space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Oral); Code available at
  https://github.com/smonsays/hypernetwork-attention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-Budget-Aware LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18547v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18547v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is critical for large language models (LLMs) to excel in a wide
range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM
performance by decomposing problems into intermediate steps, they also incur
significant overhead in token usage, leading to increased costs. We find that
the reasoning process of current LLMs is unnecessarily lengthy and it can be
compressed by including a reasonable token budget in the prompt, but the choice
of token budget plays a crucial role in the actual compression effectiveness.
We then propose a token-budget-aware LLM reasoning framework, which dynamically
estimates token budgets for different problems based on reasoning complexity
and uses the estimated token budgets to guide the reasoning process.
Experiments show that our method effectively reduces token costs in CoT
reasoning with only a slight performance reduction, offering a practical
solution to balance efficiency and accuracy in LLM reasoning. Code:
https://github.com/GeniusHTX/TALE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheuk Kit Lee, Paul Jeha, Jes Frellsen, Pietro Lio, Michael Samuel Albergo, Francisco Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete diffusion models are a class of generative models that produce
samples from an approximated data distribution within a discrete state space.
Often, there is a need to target specific regions of the data distribution.
Current guidance methods aim to sample from a distribution with mass
proportional to $p_0(x_0) p(\zeta|x_0)^\alpha$ but fail to achieve this in
practice. We introduce a Sequential Monte Carlo algorithm that generates
unbiasedly from this target distribution, utilising the learnt unconditional
and guided process. We validate our approach on low-dimensional distributions,
controlled images and text generations. For text generation, our method
provides strong control while maintaining low perplexity compared to
guidance-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization capabilities and robustness of hybrid models grounded in
  physics compared to purely deep learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17884v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17884v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Abadía-Heredia, Adrián Corrochano, Manuel Lopez-Martin, Soledad Le Clainche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the generalization capabilities and robustness of
purely deep learning (DL) models and hybrid models based on physical principles
in fluid dynamics applications, specifically focusing on iteratively
forecasting the temporal evolution of flow dynamics. Three autoregressive
models were compared: a hybrid model (POD-DL) that combines proper orthogonal
decomposition (POD) with a long-short term memory (LSTM) layer, a convolutional
autoencoder combined with a convolutional LSTM (ConvLSTM) layer and a
variational autoencoder (VAE) combined with a ConvLSTM layer. These models were
tested on two high-dimensional, nonlinear datasets representing the velocity
field of flow past a circular cylinder in both laminar and turbulent regimes.
The study used latent dimension methods, enabling a bijective reduction of
high-dimensional dynamics into a lower-order space to facilitate future
predictions. While the VAE and ConvLSTM models accurately predicted laminar
flow, the hybrid POD-DL model outperformed the others across both laminar and
turbulent flow regimes. This success is attributed to the model's ability to
incorporate modal decomposition, reducing the dimensionality of the data, by a
non-parametric method, and simplifying the forecasting component. By leveraging
POD, the model not only gained insight into the underlying physics, improving
prediction accuracy with less training data, but also reduce the number of
trainable parameters as POD is non-parametric. The findings emphasize the
potential of hybrid models, particularly those integrating modal decomposition
and deep learning, in predicting complex flow dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, two column, 26 figures and 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Compressed Image Latents and <span class="highlight-title">Multimodal</span> Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever study of adapting compressed image latents
to suit the needs of downstream vision tasks that adopt Multimodal Large
Language Models (MLLMs). MLLMs have extended the success of large language
models to modalities (e.g. images) beyond text, but their billion scale hinders
deployment on resource-constrained end devices. While cloud-hosted MLLMs could
be available, transmitting raw, uncompressed images captured by end devices to
the cloud requires an efficient image compression system. To address this, we
focus on emerging neural image compression and propose a novel framework with a
lightweight transform-neck and a surrogate loss to adapt compressed image
latents for MLLM-based vision tasks. Given the huge scale of MLLMs, our
framework excludes the entire downstream MLLM except part of its visual encoder
from training our system. This stands out from most existing coding for machine
approaches that involve downstream networks in training and thus could be
impractical when the networks are MLLMs. The proposed framework is general in
that it is applicable to various MLLMs, neural image codecs, and multiple
application scenarios, where the neural image codec can be (1) pre-trained for
human perception without updating, (2) fully updated for joint human and
machine perception, or (3) fully updated for only machine perception. Extensive
experiments on different neural image codecs and various MLLMs show that our
method achieves great rate-accuracy performance with much less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iFormer: Integrating ConvNet and <span class="highlight-title">Transformer</span> for Mobile Application <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new family of mobile hybrid vision networks, called iFormer,
with a focus on optimizing latency and accuracy on mobile applications. iFormer
effectively integrates the fast local representation capacity of convolution
with the efficient global modeling ability of self-attention. The local
interactions are derived from transforming a standard convolutional network,
\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly
introduced mobile modulation attention removes memory-intensive operations in
MHA and employs an efficient modulation mechanism to boost dynamic global
representational capacity. We conduct comprehensive experiments demonstrating
that iFormer outperforms existing lightweight networks across various tasks.
Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\% on ImageNet-1k
with a latency of only 1.10 ms on an iPhone 13, surpassing the recently
proposed MobileNetV4 under similar latency constraints. Additionally, our
method shows significant improvements in downstream tasks, including COCO
object detection, instance segmentation, and ADE20k semantic segmentation,
while still maintaining low latency on mobile devices for high-resolution
inputs in these scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code:
  https://github.com/ChuanyangZheng/iFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable Insect Monitoring: Ultra-Lightweight CNNs as On-Device
  Triggers for Insect Camera Traps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Gardiner, Sareh Rowands, Benno I. Simmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera traps, combined with AI, have emerged as a way to achieve automated,
scalable biodiversity monitoring. However, the passive infrared (PIR) sensors
that trigger camera traps are poorly suited for detecting small, fast-moving
ectotherms such as insects. Insects comprise over half of all animal species
and are key components of ecosystems and agriculture. The need for an
appropriate and scalable insect camera trap is critical in the wake of
concerning reports of declines in insect populations. This study proposes an
alternative to the PIR trigger: ultra-lightweight convolutional neural networks
running on low-powered hardware to detect insects in a continuous stream of
captured images. We train a suite of models to distinguish insect images from
backgrounds. Our design achieves zero latency between trigger and image
capture. Our models are rigorously tested and achieve high accuracy ranging
from 91.8% to 96.4% AUC on validation data and >87% AUC on data from
distributions unseen during training. The high specificity of our models
ensures minimal saving of false positive images, maximising deployment storage
efficiency. High recall scores indicate a minimal false negative rate,
maximising insect detection. Further analysis with saliency maps shows the
learned representation of our models to be robust, with low reliance on
spurious background features. Our system is also shown to operate deployed on
off-the-shelf, low-powered microcontroller units, consuming a maximum power
draw of less than 300mW. This enables longer deployment times using cheap and
readily available battery components. Overall we offer a step change in the
cost, efficiency and scope of insect monitoring. Solving the challenging
trigger problem, we demonstrate a system which can be deployed for far longer
than existing designs and budgets power and bandwidth effectively, moving
towards a generic insect camera trap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impactful Bit-Flip Search on Full-precision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08133v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08133v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Benedek, Matan Levy, Mahmood Sharif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have shown remarkable performance in various tasks, yet they
remain susceptible to subtle changes in their input or model parameters. One
particularly impactful vulnerability arises through the Bit-Flip Attack (BFA),
where flipping a small number of critical bits in a model's parameters can
severely degrade its performance. A common technique for inducing bit flips in
DRAM is the Row-Hammer attack, which exploits frequent uncached memory accesses
to alter data. Identifying susceptible bits can be achieved through exhaustive
search or progressive layer-by-layer analysis, especially in quantized
networks. In this work, we introduce Impactful Bit-Flip Search (IBS), a novel
method for efficiently pinpointing and flipping critical bits in full-precision
networks. Additionally, we propose a Weight-Stealth technique that
strategically modifies the model's parameters in a way that maintains the float
values within the original distribution, thereby bypassing simple range checks
often used in tamper detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BitStack: Any-Size Compression of Large Language Models in Variable
  Memory Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized numerous applications, yet
their deployment remains challenged by memory constraints on local devices.
While scaling laws have enhanced LLM capabilities, the primary bottleneck has
shifted from \textit{capability} to \textit{availability}, emphasizing the need
for efficient memory management. Traditional compression methods, such as
quantization, often require predefined compression ratios and separate
compression processes for each setting, complicating deployment in variable
memory environments. In this paper, we introduce \textbf{BitStack}, a novel,
training-free weight compression approach that enables megabyte-level
trade-offs between memory usage and model performance. By leveraging weight
decomposition, BitStack can dynamically adjust the model size with minimal
transmission between running memory and storage devices. Our approach
iteratively decomposes weight matrices while considering the significance of
each parameter, resulting in an approximately 1-bit per parameter residual
block in each decomposition iteration. These blocks are sorted and stacked in
storage as basic transmission units, with different quantities loaded based on
current memory availability. Extensive experiments across a wide range of tasks
demonstrate that, despite offering fine-grained size control, BitStack
consistently matches or surpasses strong quantization baselines, particularly
at extreme compression ratios. To the best of our knowledge, this is the first
decomposition-based method that effectively bridges the gap to practical
compression techniques like quantization. Code is available at
https://github.com/xinghaow99/BitStack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Online Confidence Bounds for Multinomial Logistic Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongkyu Lee, Min-hwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an improved online confidence bound for multinomial
logistic (MNL) models and apply this result to MNL bandits, achieving
variance-dependent optimal regret. Recently, Lee & Oh (2024) established an
online confidence bound for MNL models and achieved nearly minimax-optimal
regret in MNL bandits. However, their results still depend on the
norm-boundedness of the unknown parameter $B$ and the maximum size of possible
outcomes $K$. To address this, we first derive an online confidence bound of
$O\left(\sqrt{d \log t} + B \right)$, which is a significant improvement over
the previous bound of $O (B \sqrt{d} \log t \log K )$ (Lee & Oh, 2024). This is
mainly achieved by establishing tighter self-concordant properties of the MNL
loss and introducing a novel intermediary term to bound the estimation error.
Using this new online confidence bound, we propose a constant-time algorithm,
OFU-MNL++, which achieves a variance-dependent regret bound of $O \Big( d \log
T \sqrt{ \smash[b]{\sum_{t=1}^T} \sigma_t^2 } \Big) $ for sufficiently large
$T$, where $\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$
is the dimension of the contexts, and $T$ is the total number of rounds.
Furthermore, we introduce a Maximum Likelihood Estimation (MLE)-based
algorithm, OFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O
\Big( d \log (BT) \sqrt{ \smash[b]{\sum_{t=1}^T} \sigma_t^2 } \Big) $.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cost-aware simulation-based inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Bharti, Daolang Huang, Samuel Kaski, François-Xavier Briol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference (SBI) is the preferred framework for estimating
parameters of intractable models in science and engineering. A significant
challenge in this context is the large computational cost of simulating data
from complex models, and the fact that this cost often depends on parameter
values. We therefore propose \textit{cost-aware SBI methods} which can
significantly reduce the cost of existing sampling-based SBI methods, such as
neural SBI and approximate Bayesian computation. This is achieved through a
combination of rejection and self-normalised importance sampling, which
significantly reduces the number of expensive simulations needed. Our approach
is studied extensively on models from epidemiology to telecommunications
engineering, where we obtain significant reductions in the overall cost of
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel computational workflows for natural and biomedical image
  processing based on hypercomplex algebras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nektarios A. Valous, Eckhard Hitzer, Dragoş Duşe, Rodrigo Rojas Moraleda, Ferdinand Popp, Meggy Suarez-Carmona, Anna Berthel, Ismini Papageorgiou, Carlo Fremd, Alexander Rölle, Christina C. Westhoff, Bénédicte Lenoir, Niels Halama, Inka Zörnig, Dirk Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex image processing extends conventional techniques in a unified
paradigm encompassing algebraic and geometric principles. This work leverages
quaternions and the two-dimensional orthogonal planes split framework
(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D
planes) for natural/biomedical image analysis through the following
computational workflows and outcomes: natural/biomedical image re-colorization,
natural image de-colorization, natural/biomedical image contrast enhancement,
computational re-staining and stain separation in histological images, and
performance gains in machine/deep learning pipelines for histological images.
The workflows are analyzed separately for natural and biomedical images to
showcase the effectiveness of the proposed approaches. The proposed workflows
can regulate color appearance (e.g. with alternative renditions and grayscale
conversion) and image contrast, be part of automated image processing pipelines
(e.g. isolating stain components, boosting learning models), and assist in
digital pathology applications (e.g. enhancing biomarker visibility, enabling
colorblind-friendly renditions). Employing only basic arithmetic and matrix
operations, this work offers a computationally accessible methodology - in the
hypercomplex domain - that showcases versatility and consistency across image
processing tasks and a range of computer vision and biomedical applications.
The proposed non-data-driven methods achieve comparable or better results
(particularly in cases involving well-known methods) to those reported in the
literature, showcasing the potential of robust theoretical frameworks with
practical effectiveness. Results, methods, and limitations are detailed
alongside discussion of promising extensions, emphasizing the potential of
feature-rich mathematical/computational frameworks for natural and biomedical
images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HRP: High-Rank Preheating for Superior LoRA Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhu Chen, Yingjie Wang, Shi Fu, Li Shen, Yongcheng Jing, Xinmei Tian, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the crucial impact of initialization on the convergence
properties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that
random initialization, a widely used schema, will likely lead LoRA to random
low-rank results, rather than the best low-rank result. While this issue can be
mitigated by adjusting initialization towards a well-informed direction, it
relies on prior knowledge of the target, which is typically unknown in
real-world scenarios. To approximate this well-informed initial direction, we
propose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few
steps and uses the singular value decomposition of the preheated result as a
superior initialization. HRP initialization is theory-supported to combine the
convergence strengths of high-rank LoRA and the generalization strengths of
low-rank LoRA. Extensive experiments demonstrate that HRP significantly
enhances LoRA's effectiveness across various models and tasks, achieving
performance comparable to full-parameter fine-tuning and outperforming other
initialization strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Circuit Compositions: Exploring Modular Structures in <span class="highlight-title">Transformer</span>-Based
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Mondorf, Sondre Wold, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in interpretability research is to what extent neural
networks, particularly language models, implement reusable functions through
subnetworks that can be composed to perform more complex tasks. Recent advances
in mechanistic interpretability have made progress in identifying
$\textit{circuits}$, which represent the minimal computational subgraphs
responsible for a model's behavior on specific tasks. However, most studies
focus on identifying circuits for individual tasks without investigating how
functionally similar circuits $\textit{relate}$ to each other. To address this
gap, we study the modularity of neural networks by analyzing circuits for
highly compositional subtasks within a transformer-based language model.
Specifically, given a probabilistic context-free grammar, we identify and
compare circuits responsible for ten modular string-edit operations. Our
results indicate that functionally similar circuits exhibit both notable node
overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits
identified can be reused and combined through set operations to represent more
complex functional model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time Verification and Refinement of Language Model Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonho Ko, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance across a wide
range of natural language tasks. However, a critical challenge remains in that
they sometimes generate factually incorrect answers. To address this, while
many previous work has focused on identifying errors in their generation and
further refining them, they are slow in deployment since they are designed to
verify the response from LLMs only after their entire generation (from the
first to last tokens) is done. Further, we observe that once LLMs generate
incorrect tokens early on, there is a higher likelihood that subsequent tokens
will also be factually incorrect. To this end, in this work, we propose
Streaming-VR (Streaming Verification and Refinement), a novel approach designed
to enhance the efficiency of verification and refinement of LLM outputs.
Specifically, the proposed Streaming-VR enables on-the-fly verification and
correction of tokens as they are being generated, similar to a streaming
process, ensuring that each subset of tokens is checked and refined in
real-time by another LLM as the LLM constructs its response. Through
comprehensive evaluations on multiple datasets, we demonstrate that our
approach not only enhances the factual accuracy of LLMs, but also offers a more
efficient solution compared to prior refinement methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Meta-Learning from a Learning Lens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Wang, Wenwen Qiang, Chuxiong Sun, Changwen Zheng, Jiangmeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning has emerged as a powerful approach for leveraging knowledge
from previous tasks to solve new tasks. The mainstream methods focus on
training a well-generalized model initialization, which is then adapted to
different tasks with limited data and updates. However, it pushes the model
overfitting on the training tasks. Previous methods mainly attributed this to
the lack of data and used augmentations to address this issue, but they were
limited by sufficient training and effective augmentation strategies. In this
work, we focus on the more fundamental learning to learn strategy of
meta-learning to explore what causes errors and how to eliminate these errors
without changing the environment. Specifically, we first rethink the
algorithmic procedure of meta-learning from a learning lens. Through
theoretical and empirical analyses, we find that (i) this paradigm faces the
risk of both overfitting and underfitting and (ii) the model adapted to
different tasks promote each other where the effect is stronger if the tasks
are more similar. Based on this insight, we propose using task relations to
calibrate the optimization process of meta-learning and propose a plug-and-play
method called Task Relation Learner (TRLearner) to achieve this goal.
Specifically, it first obtains task relation matrices from the extracted
task-specific meta-data. Then, it uses the obtained matrices with
relation-aware consistency regularization to guide optimization. Extensive
theoretical and empirical analyses demonstrate the effectiveness of TRLearner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models Struggle to Achieve a Consistent Temporal Representation
  of Facts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) have shown substantial improvements in handling factual
knowledge, yet their capability to consistently represent temporal facts, which
are valid only within specific timeframes, remains underexplored. To
investigate this, we introduce TimeStress, a novel dataset comprising 521K
statements on 2003 of the most popular temporal facts in Wikidata. Each
statement contextualizes a fact with correct and incorrect dates across three
precisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to
discern between correct and incorrect temporal statements based on their
probability of being generated. We assess 18 LMs across various architectures
using two metrics: the win rate, indicating how often correct dates outperform
incorrect ones, and robustness, reflecting consistent performance across all
dates. Our findings reveal that while some LMs achieve a win rate exceeding
80\%, robustness remains low, with the best model achieving only 6\%.
Furthermore, robust knowledge at one date precision does not reliably transfer
to others, highlighting a significant generalization gap. These results
underscore the struggle of LMs to maintain a consistent temporal
representation, supporting their limitations as reliable sources of temporal
knowledge. We provide all data and code for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Discretize Denoising Diffusion ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models (DPMs) are generative models showing
competitive performance in various domains, including image synthesis and 3D
point cloud generation. Sampling from pre-trained DPMs involves multiple neural
function evaluations (NFEs) to transform Gaussian noise samples into images,
resulting in higher computational costs compared to single-step generative
models such as GANs or VAEs. Therefore, reducing the number of NFEs while
preserving generation quality is crucial. To address this, we propose LD3, a
lightweight framework designed to learn the optimal time discretization for
sampling. LD3 can be combined with various samplers and consistently improves
generation quality without having to retrain resource-intensive neural
networks. We demonstrate analytically and empirically that LD3 improves
sampling efficiency with much less computational overhead. We evaluate our
method with extensive experiments on 7 pre-trained models, covering
unconditional and conditional sampling in both pixel-space and latent-space
DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional
CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient
approach to sampling from pre-trained diffusion models. Code is available at
https://github.com/vinhsuhi/LD3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Policy Gradient in Reproducing Kernel Hilbert Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Bossens, Kishor Bharti, Jayne Thompson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parametrised quantum circuits offer expressive and data-efficient
representations for machine learning. Due to quantum states residing in a
high-dimensional Hilbert space, parametrised quantum circuits have a natural
interpretation in terms of kernel methods. The representation of quantum
circuits in terms of quantum kernels has been studied widely in quantum
supervised learning, but has been overlooked in the context of quantum RL. This
paper proposes parametric and non-parametric policy gradient and actor-critic
algorithms with quantum kernel policies in quantum environments. This approach,
implemented with both numerical and analytical quantum policy gradient
techniques, allows exploiting the many advantages of kernel methods, including
available analytic forms for the gradient of the policy and tunable
expressiveness. The proposed approach is suitable for vector-valued action
spaces and each of the formulations demonstrates a quadratic reduction in query
complexity compared to their classical counterparts. Two actor-critic
algorithms, one based on stochastic policy gradient and one based on
deterministic policy gradient (comparable to the popular DDPG algorithm),
demonstrate additional query complexity reductions compared to quantum policy
gradient algorithms under favourable conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel-Based Distributed Q-Learning: A Scalable Reinforcement Learning
  Approach for Dynamic Treatment Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wang, Yao Wang, Shao-Bo Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large amounts of electronic health records (EHRs) concerning
chronic diseases have been collected to facilitate medical diagnosis. Modeling
the dynamic properties of EHRs related to chronic diseases can be efficiently
done using dynamic treatment regimes (DTRs). While reinforcement learning (RL)
is a widely used method for creating DTRs, there is ongoing research in
developing RL algorithms that can effectively handle large amounts of data. In
this paper, we present a scalable kernel-based distributed Q-learning algorithm
for generating DTRs. We perform both theoretical assessments and numerical
analysis for the proposed approach. The results demonstrate that our algorithm
significantly reduces the computational complexity associated with the
state-of-the-art deep reinforcement learning methods, while maintaining
comparable generalization performance in terms of accumulated rewards across
stages, such as survival time or cumulative survival probability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text2Chart31: Instruction Tuning for Chart Generation with Automatic
  Feedback <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Pesaran Zadeh, Juyeon Kim, Jin-Hwa Kim, Gunhee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong capabilities across
various language tasks, notably through instruction-tuning methods. However,
LLMs face challenges in visualizing complex, real-world data through charts and
plots. Firstly, existing datasets rarely cover a full range of chart types,
such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning
methods do not fully leverage the intricate relationships within rich datasets,
including text, code, and figures. To address these challenges, we propose a
hierarchical pipeline and a new dataset for chart generation. Our dataset,
Text2Chart31, includes 31 unique plot types referring to the Matplotlib
library, with 11.1K tuples of descriptions, code, data tables, and plots.
Moreover, we introduce a reinforcement learning-based instruction tuning
technique for chart generation tasks without requiring human feedback. Our
experiments show that this approach significantly enhances the model
performance, enabling smaller models to outperform larger open-source models
and be comparable to state-of-the-art proprietary models in data visualization
tasks. We make the code and dataset available at
https://github.com/fatemehpesaran310/Text2Chart31.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Oral. Code and dataset are released at
  https://github.com/fatemehpesaran310/Text2Chart31</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Universality of Self-Supervised Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01053v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01053v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwen Qiang, Jingyao Wang, Lingyu Si, Chuxiong Sun, Fuchun Sun, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the characteristics that define a good
representation or model. We propose that such a representation or model should
possess universality, characterized by: (i) discriminability: performing well
on training samples; (ii) generalization: performing well on unseen datasets;
and (iii) transferability: performing well on unseen tasks with distribution
shifts. Despite its importance, current self-supervised learning (SSL) methods
lack explicit modeling of universality, and theoretical analysis remains
underexplored. To address these issues, we aim to explore and incorporate
universality into SSL. Specifically, we first revisit SSL from a task
perspective and find that each mini-batch can be viewed as a multi-class
classification task. We then propose that a universal SSL model should achieve:
(i) learning universality by minimizing loss across all training samples, and
(ii) evaluation universality by learning causally invariant representations
that generalize well to unseen tasks. To quantify this, we introduce a
$\sigma$-measurement that assesses the gap between the performance of SSL model
and optimal task-specific models. Furthermore, to model universality, we
propose the GeSSL framework. It first learns task-specific models by minimizing
SSL loss, then incorporates future updates to enhance discriminability, and
finally integrates these models to learn from multiple tasks. Theoretical and
empirical evidence supports the effectiveness of GeSSL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms:
  Exploring Tuning Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshko Koloski, Blaž Škrlj, Marko Robnik-Šikonja, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cross-lingual transfer is a promising technique to solve tasks in
less-resourced languages. In this empirical study, we compare two fine-tuning
approaches combined with zero-shot and full-shot learning approaches for large
language models in a cross-lingual setting. As fine-tuning strategies, we
compare parameter-efficient adapter methods with fine-tuning of all parameters.
As cross-lingual transfer strategies, we compare the intermediate-training
(\textit{IT}) that uses each language sequentially and cross-lingual validation
(\textit{CLV}) that uses a target language already in the validation phase of
fine-tuning. We assess the success of transfer and the extent of catastrophic
forgetting in a source language due to cross-lingual transfer, i.e., how much
previously acquired knowledge is lost when we learn new information in a
different language. The results on two different classification problems, hate
speech detection and product reviews, each containing datasets in several
languages, show that the \textit{IT} cross-lingual strategy outperforms
\textit{CLV} for the target language. Our findings indicate that, in the
majority of cases, the \textit{CLV} strategy demonstrates superior retention of
knowledge in the base language (English) compared to the \textit{IT} strategy,
when evaluating catastrophic forgetting in multiple cross-lingual transfers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonhee Kim, Marco Valentino, André Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on logical reasoning in Language Models (LMs) have sparked a
debate on whether they can learn systematic reasoning principles during
pre-training or merely exploit superficial patterns in the training data. This
paper presents a mechanistic interpretation of syllogistic reasoning in LMs to
advance the understanding of internal dynamics. Specifically, we present a
methodology for circuit discovery aimed at interpreting content-independent
reasoning mechanisms. Through two distinct intervention methods, we uncover a
sufficient and necessary circuit involving middle-term suppression that
elucidates how LMs transfer information to derive valid conclusions from
premises. Furthermore, we investigate how belief biases manifest in syllogistic
reasoning, finding evidence of partial contamination from additional attention
heads responsible for encoding commonsense and contextualized knowledge.
Finally, we explore the generalization of the discovered mechanisms across
various syllogistic schemes, model sizes and architectures, finding that the
identified circuit is sufficient and necessary for the schemes on which the
models achieve high downstream accuracy (> 60%), and that the activation
patterns apply to models of different families. Overall, our findings suggest
that LMs indeed learn transferable content-independent reasoning mechanisms,
but that, at the same time, such mechanisms do not involve generalizable and
abstract logical primitives, being susceptible to contamination by the same
world knowledge acquired during pre-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Off-Policy Learning for High-Dimensional Action Spaces <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Otto, Philipp Becker, Ngo Anh Vien, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing off-policy reinforcement learning algorithms often rely on an
explicit state-action-value function representation, which can be problematic
in high-dimensional action spaces due to the curse of dimensionality. This
reliance results in data inefficiency as maintaining a state-action-value
function in such spaces is challenging. We present an efficient approach that
utilizes only a state-value function as the critic for off-policy deep
reinforcement learning. This approach, which we refer to as Vlearn, effectively
circumvents the limitations of existing methods by eliminating the necessity
for an explicit state-action-value function. To this end, we leverage a
weighted importance sampling loss for learning deep value functions from
off-policy data. While this is common for linear methods, it has not been
combined with deep value function networks. This transfer to deep methods is
not straightforward and requires novel design choices such as robust policy
updates, twin value function networks to avoid an optimization bias, and
importance weight clipping. We also present a novel analysis of the variance of
our estimate compared to commonly used importance sampling estimators such as
V-trace. Our approach improves sample complexity as well as final performance
and ensures consistent and robust performance across various benchmark tasks.
Eliminating the state-action-value function in Vlearn facilitates a streamlined
learning process, yielding high-return agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AffinityFlow: Guided Flows for Antibody Affinity Maturation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Chen, Karla-Luise Herpoldt, Chenchao Zhao, Zichen Wang, Marcus Collins, Shang Shang, Ron Benson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are widely used as therapeutics, but their development requires
costly affinity maturation, involving iterative mutations to enhance binding
affinity.This paper explores a sequence-only scenario for affinity maturation,
using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold
within flow matching to generate diverse protein structures, enabling a
sequence-conditioned generative model of structure. Building on this, we
propose an alternating optimization framework that (1) fixes the sequence to
guide structure generation toward high binding affinity using a structure-based
affinity predictor, then (2) applies inverse folding to create sequence
mutations, refined by a sequence-based affinity predictor for post selection. A
key challenge is the lack of labeled data for training both predictors. To
address this, we develop a co-teaching module that incorporates valuable
information from noisy biophysical energies into predictor refinement. The
sequence-based predictor selects consensus samples to teach the structure-based
predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art
performance in affinity maturation experiments. We plan to open-source our code
after acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRIS: An Immersive Robot Interaction System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinkai Jiang, Qihao Yuan, Enes Ulas Dincer, Hongyi Zhou, Ge Li, Xueyin Li, Julius Haag, Nicolas Schreiber, Kailai Li, Gerhard Neumann, Rudolf Lioutikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces IRIS, an immersive Robot Interaction System leveraging
Extended Reality (XR), designed for robot data collection and interaction
across multiple simulators, benchmarks, and real-world scenarios. While
existing XR-based data collection systems provide efficient and intuitive
solutions for large-scale data collection, they are often challenging to
reproduce and reuse. This limitation arises because current systems are highly
tailored to simulator-specific use cases and environments. IRIS is a novel,
easily extendable framework that already supports multiple simulators,
benchmarks, and even headsets. Furthermore, IRIS is able to include additional
information from real-world sensors, such as point clouds captured through
depth cameras. A unified scene specification is generated directly from
simulators or real-world sensors and transmitted to XR headsets, creating
identical scenes in XR. This specification allows IRIS to support any of the
objects, assets, and robots provided by the simulators. In addition, IRIS
introduces shared spatial anchors and a robust communication protocol that
links simulations between multiple XR headsets. This feature enables multiple
XR headsets to share a synchronized scene, facilitating collaborative and
multi-user data collection. IRIS can be deployed on any device that supports
the Unity Framework, encompassing the vast majority of commercially available
headsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and
the HoloLens 2. IRIS showcased its versatility across a wide range of
real-world and simulated scenarios, using current popular robot simulators such
as MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study
evaluates IRIS on a data collection task for the LIBERO benchmark. The study
shows that IRIS significantly outperforms the baseline in both objective and
subjective metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DP-DyLoRA: Fine-Tuning <span class="highlight-title">Transformer</span>-Based Models On-Device under
  Differentially Private Federated Learning using Dynamic Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06368v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06368v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Karthikeyan Saravanan, Rogier van Dalen, Haaris Mehmood, David Tuckey, Mete Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) allows clients to collaboratively train a global
model without sharing their local data with a server. However, clients'
contributions to the server can still leak sensitive information. Differential
privacy (DP) addresses such leakage by providing formal privacy guarantees,
with mechanisms that add randomness to the clients' contributions. The
randomness makes it infeasible to train large transformer-based models, common
in modern federated learning systems. In this work, we empirically evaluate the
practicality of fine-tuning large scale on-device transformer-based models with
differential privacy in a federated learning system. We conduct comprehensive
experiments on various system properties for tasks spanning a multitude of
domains: speech recognition, computer vision (CV) and natural language
understanding (NLU). Our results show that full fine-tuning under
differentially private federated learning (DP-FL) generally leads to huge
performance degradation which can be alleviated by reducing the dimensionality
of contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks
of existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA)
consistently outperforms other methods. An even more promising approach,
DyLoRA, which makes the low rank variable, when naively combined with FL would
straightforwardly break differential privacy. We therefore propose an
adaptation method that can be combined with differential privacy and call it
DP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word
error rate (WER) increase due to DP to less than 2% and 7% respectively with 1
million clients and a stringent privacy budget of $\epsilon=2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper
  Limb Multi-Joint Movement Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajnish Kumar, Anand Gupta, Suriya Prakash Muthukrishnan, Lalan Kumar, Sitikantha Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeletons and rehabilitation systems have the potential to improve human
strength and recovery by using adaptive human-machine interfaces. Achieving
precise and responsive control in these systems depends on accurately
estimating joint movement dynamics, such as joint angle, velocity,
acceleration, external mass, and torque. While machine learning (ML) approaches
have been employed to predict joint kinematics from surface electromyography
(sEMG) data, traditional ML models often struggle to generalize across dynamic
movements. In contrast, physics-informed neural networks integrate
biomechanical principles, but their effectiveness in predicting full movement
dynamics has not been thoroughly explored. To address this, we introduce the
Physics-informed Gated Recurrent Network (PiGRN), a novel model designed to
predict multi-joint movement dynamics from sEMG data. PiGRN uses a Gated
Recurrent Unit (GRU) to process time-series sEMG inputs, estimate multi-joint
kinematics and external loads, and predict joint torque while incorporating
physics-based constraints during training. Experimental validation, using sEMG
data from five participants performing elbow flexion-extension tasks with 0 kg,
2 kg, and 4 kg loads, showed that PiGRN accurately predicted joint torques for
10 novel movements. RMSE values ranged from 4.02\% to 11.40\%, with correlation
coefficients between 0.87 and 0.98. These results underscore PiGRN's potential
for real-time applications in exoskeletons and rehabilitation. Future work will
focus on expanding datasets, improving musculoskeletal models, and
investigating unsupervised learning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Autoregression: Discrete Diffusion for Complex Reasoning and
  Planning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models, despite their impressive capabilities,
struggle with complex reasoning and long-term planning tasks. We introduce
discrete diffusion models as a novel solution to these challenges. Through the
lens of subgoal imbalance, we demonstrate how diffusion models effectively
learn difficult subgoals that elude autoregressive approaches. We propose
Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on
difficulty during learning. On complex tasks like Countdown, Sudoku, and
Boolean Satisfiability Problems, MDM significantly outperforms autoregressive
models without using search techniques. For instance, MDM achieves 91.5\% and
100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and
20.7\% for autoregressive models. Our work highlights the potential of
diffusion-based approaches in advancing AI capabilities for sophisticated
language understanding and problem-solving tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Deductive and Inductive Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkun Cai, Xu Zhao, Haoliang Liu, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Serge Belongie, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning tasks, yet their reliance on static prompt structures and limited
adaptability to complex scenarios remains a significant challenge. In this
paper, we propose the Deductive and InDuctive(DID) method, a novel framework
that enhances LLM reasoning by dynamically integrating both deductive and
inductive reasoning approaches. Drawing from cognitive science principles, DID
implements a dual-metric complexity evaluation system that combines Littlestone
dimension and information entropy to precisely assess task difficulty and guide
decomposition strategies. DID enables the model to progressively adapt its
reasoning pathways based on problem complexity, mirroring human cognitive
processes. We evaluate DID's effectiveness across multiple benchmarks,
including the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset
for temporal reasoning. Our results demonstrate significant improvements in
reasoning quality and solution accuracy - achieving 70.3% accuracy on AIW
(compared to 62.2% for Tree of Thought) while maintaining lower computational
costs. The success of DID in improving LLM performance while preserving
computational efficiency suggests promising directions for developing more
cognitively aligned and capable language models. Our work contributes a
theoretically grounded, input-centric approach to enhancing LLM reasoning
capabilities, offering an efficient alternative to traditional
output-exploration methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion
  Models with Self-Augmented Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Liang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of AI-generated images necessitates effective
watermarking techniques to protect intellectual property and detect fraudulent
content. While existing training-based watermarking methods show promise, they
often struggle with generalizing across diverse prompts and tend to introduce
visible artifacts. To this end, we propose a novel, provably generalizable
image watermarking approach for Latent Diffusion Models, termed Self-Augmented
Training (SAT-LDM). Our method aligns the training and testing phases through a
free generation distribution, thereby enhancing the watermarking module's
generalization capabilities. We theoretically consolidate SAT-LDM by proving
that the free generation distribution contributes to its tight generalization
bound, without the need for additional data collection. Extensive experiments
show that SAT-LDM not only achieves robust watermarking but also significantly
improves the quality of watermarked images across a wide range of prompts.
Moreover, our experimental analyses confirm the strong generalization abilities
of SAT-LDM. We hope that our method provides a practical and efficient solution
for securing high-fidelity AI-generated content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span>Eval36K: Benchmarking Coding and Reasoning Capabilities of Large
  Language Models on <span class="highlight-title">Graph</span> <span class="highlight-title">Dataset</span>s <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success in natural
language processing (NLP), demonstrating significant capabilities in processing
and understanding text data. However, recent studies have identified
limitations in LLMs' ability to manipulate, program, and reason about
structured data, especially graphs. We introduce GraphEval36K, the first
comprehensive graph dataset, comprising 40 graph coding problems and 36,900
test cases to evaluate the ability of LLMs on graph problem-solving. Our
dataset is categorized into eight primary and four sub-categories to ensure a
thorough evaluation across different types of graphs. We benchmark ten LLMs,
finding that private models outperform open-source ones, though the gap is
narrowing. We also analyze the performance of LLMs across directed vs
undirected graphs, different kinds of graph concepts, and network models.
Furthermore, to improve the usability of our evaluation framework, we propose
Structured Symbolic Decomposition (SSD), an instruction-based method designed
to enhance LLM performance on complex graph tasks. Results show that SSD
improves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and
Claude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. This paper
  has been accepted by NAACL 2025. GraphEval36K is available at
  https://grapheval36k.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can humans teach machines to code? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Céline Hocquette, Johannes Langer, Andrew Cropper, Ute Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of inductive program synthesis is for a machine to automatically
generate a program from user-supplied examples. A key underlying assumption is
that humans can provide sufficient examples to teach a concept to a machine. To
evaluate the validity of this assumption, we conduct a study where human
participants provide examples for six programming concepts, such as finding the
maximum element of a list. We evaluate the generalisation performance of five
program synthesis systems trained on input-output examples (i) from non-expert
humans, (ii) from a human expert, and (iii) randomly sampled. Our results
suggest that non-experts typically do not provide sufficient examples for a
program synthesis system to learn an accurate program.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynthSOD: Developing an Heterogeneous <span class="highlight-title">Dataset</span> for Orchestra Music Source
  Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaime Garcia-Martinez, David Diaz-Guerra, Archontis Politis, Tuomas Virtanen, Julio J. Carabias-Orti, Pedro Vera-Candeas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in music source separation have significantly progressed,
particularly in isolating vocals, drums, and bass elements from mixed tracks.
These developments owe much to the creation and use of large-scale, multitrack
datasets dedicated to these specific components. However, the challenge of
extracting similarly sounding sources from orchestra recordings has not been
extensively explored, largely due to a scarcity of comprehensive and clean (i.e
bleed-free) multitrack datasets. In this paper, we introduce a novel multitrack
dataset called SynthSOD, developed using a set of simulation techniques to
create a realistic (i.e. using high-quality soundfonts), musically motivated,
and heterogeneous training set comprising different dynamics, natural tempo
changes, styles, and conditions. Moreover, we demonstrate the application of a
widely used baseline music separation model trained on our synthesized dataset
w.r.t to the well-known EnsembleSet, and evaluate its performance under both
synthetic and real-world conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The SynthSOD dataset can be downloaded from
  https://doi.org/10.5281/zenodo.13759492</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Intrinsically Motivated Feedback <span class="highlight-title">Graph</span> for
  Lost-sales Inventory Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Liu, Xinran Li, Shibo Chen, Gen Li, Jiashuo Jiang, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has proven to be well-performed and
general-purpose in the inventory control (IC). However, further improvement of
RL algorithms in the IC domain is impeded due to two limitations of online
experience. First, online experience is expensive to acquire in real-world
applications. With the low sample efficiency nature of RL algorithms, it would
take extensive time to train the RL policy to convergence. Second, online
experience may not reflect the true demand due to the lost sales phenomenon
typical in IC, which makes the learning process more challenging. To address
the above challenges, we propose a decision framework that combines
reinforcement learning with feedback graph (RLFG) and intrinsically motivated
exploration (IME) to boost sample efficiency. In particular, we first take
advantage of the inherent properties of lost-sales IC problems and design the
feedback graph (FG) specially for lost-sales IC problems to generate abundant
side experiences aid RL updates. Then we conduct a rigorous theoretical
analysis of how the designed FG reduces the sample complexity of RL methods.
Based on the theoretical insights, we design an intrinsic reward to direct the
RL agent to explore to the state-action space with more side experiences,
further exploiting FG's power. Experimental results demonstrate that our method
greatly improves the sample efficiency of applying RL in IC. Our code is
available at https://anonymous.4open.science/r/RLIMFG4IC-811D/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Channel Multiplex <span class="highlight-title">Graph</span> Neural Networks for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interactive relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant challenges: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations within behavior patterns on the target relation in recommender system
scenarios. In this work, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interactive relations, and includes a relation chain representation
learner and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06% and 12.15% on average across all datasets in terms of
Recall@10 and NDCG@10 respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15046v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15046v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Till Freihaut, Giorgia Ramponi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent systems, agent behavior is driven by utility functions that
encapsulate their individual goals and interactions. Inverse Reinforcement
Learning (IRL) seeks to uncover these utilities by analyzing expert behavior,
offering insights into the underlying decision-making processes. However,
multi-agent settings pose significant challenges, particularly when rewards are
inferred from equilibrium observations. A key obstacle is that single (Nash)
equilibrium observations often fail to adequately capture critical game
properties, leading to potential misrepresentations. This paper offers a
rigorous analysis of the feasible reward set in multi-agent IRL and addresses
these limitations by introducing entropy-regularized games, ensuring
equilibrium uniqueness and enhancing interpretability. Furthermore, we examine
the effects of estimation errors and present the first sample complexity
results for multi-agent IRL across diverse scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16205v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16205v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Lin, Hongming Yang, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Large Language Models (LLMs) has brought significant
advancements across various tasks. However, despite these achievements, LLMs
still exhibit inherent safety vulnerabilities, especially when confronted with
jailbreak attacks. Existing jailbreak methods suffer from two main limitations:
reliance on complicated prompt engineering and iterative optimization, which
lead to low attack success rate (ASR) and attack efficiency (AE). In this work,
we propose an efficient jailbreak attack method, Analyzing-based Jailbreak
(ABJ), which leverages the advanced reasoning capability of LLMs to
autonomously generate harmful content, revealing their underlying safety
vulnerabilities during complex reasoning process. We conduct comprehensive
experiments on ABJ across various open-source and closed-source LLMs. In
particular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional
AE among all target LLMs, showcasing its remarkable attack effectiveness,
transferability, and efficiency. Our findings underscore the urgent need to
prioritize and improve the safety of LLMs to mitigate the risks of misuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciPIP: An LLM-based Scientific Paper Idea Proposer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has opened new
possibilities for automating the proposal of innovative scientific ideas. This
process involves two key phases: literature retrieval and idea generation.
However, existing approaches often fall short due to their reliance on
keyword-based search tools during the retrieval phase, which neglects crucial
semantic information and frequently results in incomplete retrieval outcomes.
Similarly, in the idea generation phase, current methodologies tend to depend
solely on the internal knowledge of LLMs or metadata from retrieved papers,
thereby overlooking significant valuable insights contained within the full
texts. To address these limitations, we introduce SciPIP, an innovative
framework designed to enhance the LLM-based proposal of scientific ideas
through improvements in both literature retrieval and idea generation. Our
approach begins with the construction of a comprehensive literature database
that supports advanced retrieval based not only on keywords but also on
semantics and citation relationships. This is complemented by the introduction
of a multi-granularity retrieval algorithm aimed at ensuring more thorough and
exhaustive retrieval results. For the idea generation phase, we propose a
dual-path framework that effectively integrates both the content of retrieved
papers and the extensive internal knowledge of LLMs. This integration
significantly boosts the novelty, feasibility, and practical value of proposed
ideas. Our experiments, conducted across various domains such as natural
language processing and computer vision, demonstrate SciPIP's capability to
generate a multitude of innovative and useful ideas. These findings underscore
SciPIP's potential as a valuable tool for researchers seeking to advance their
fields with groundbreaking concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 12 tables. The code has been availabel:
  https://github.com/cheerss/SciPIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Interpretable Hierarchical Dynamical Systems Models from Time
  Series Data <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In science, we are often interested in obtaining a generative model of the
underlying system dynamics from observed time series. While powerful methods
for dynamical systems reconstruction (DSR) exist when data come from a single
domain, how to best integrate data from multiple dynamical regimes and leverage
it for generalization is still an open question. This becomes particularly
important when individual time series are short, and group-level information
may help to fill in for gaps in single-domain data. Here we introduce a
hierarchical framework that enables to harvest group-level (multi-domain)
information while retaining all single-domain characteristics, and showcase it
on popular DSR benchmarks, as well as on neuroscience and medical data. In
addition to faithful reconstruction of all individual dynamical regimes, our
unsupervised methodology discovers common low-dimensional feature spaces in
which datasets with similar dynamics cluster. The features spanning these
spaces were further dynamically highly interpretable, surprisingly in often
linear relation to control parameters that govern the dynamics of the
underlying system. Finally, we illustrate transfer learning and generalization
to new parameter regimes, paving the way toward DSR foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the Thirteenth International Conference on Learning
  Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Discrete Diffusion Samplers: Combinatorial Optimization and
  Statistical Physics <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Sanokowski, Wilhelm Berghammer, Martin Ennemoser, Haoyu Peter Wang, Sepp Hochreiter, Sebastian Lehner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to sample from complex unnormalized distributions over discrete
domains emerged as a promising research direction with applications in
statistical physics, variational inference, and combinatorial optimization.
Recent work has demonstrated the potential of diffusion models in this domain.
However, existing methods face limitations in memory scaling and thus the
number of attainable diffusion steps since they require backpropagation through
the entire generative process. To overcome these limitations we introduce two
novel training methods for discrete diffusion samplers, one grounded in the
policy gradient theorem and the other one leveraging Self-Normalized Neural
Importance Sampling (SN-NIS). These methods yield memory-efficient training and
achieve state-of-the-art results in unsupervised combinatorial optimization.
Numerous scientific applications additionally require the ability of unbiased
sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte
Carlo that enable for the first time the application of discrete diffusion
models to this problem. We validate our methods on Ising model benchmarks and
find that they outperform popular autoregressive approaches. Our work opens new
avenues for applying diffusion models to a wide range of scientific
applications in discrete domains that were hitherto restricted to exact
likelihood models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multimodal</span> semantic retrieval for product search <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Esther Lopez Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval (also known as dense retrieval) based on textual data has
been extensively studied for both web search and product search application
fields, where the relevance of a query and a potential target document is
computed by their dense vector representation comparison. Product image is
crucial for e-commerce search interactions and is a key factor for customers at
product explorations. However, its impact on semantic retrieval has not been
well studied yet. In this research, we build a multimodal representation for
product items in e-commerce search in contrast to pure-text representation of
products, and investigate the impact of such representations. The models are
developed and evaluated on e-commerce datasets. We demonstrate that a
multimodal representation scheme for a product can show improvement either on
purchase recall or relevance accuracy in semantic retrieval. Additionally, we
provide numerical analysis for exclusive matches retrieved by a multimodal
semantic retrieval model versus a text-only semantic retrieval model, to
demonstrate the validation of multimodal solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EReL@MIR WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabuLa: Harnessing Language Models for Tabular Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilong Zhao, Robert Birke, Lydia Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data synthesis is crucial for addressing privacy and security
concerns in industries reliant on tabular data. While recent advancements adopt
large language models (LLMs) for realistic tabular data generation, their long
training times and limited reusability hinder practical applications. In this
paper, we propose Tabula, a tabular data synthesizer that leverages the
structure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data
synthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained
weights originally designed for natural language tasks, focusing instead on a
tailored approach for tabular data. In addition, Tabula introduces a token
sequence compression strategy that significantly reduces training time while
maintaining data quality, alongside a novel token padding method that improves
sequence alignment across training batches. Experiments on six datasets show
that Tabula achieves superior synthetic data utility compared to current SOTA
methods. Additionally, the results demonstrate that Tabula model trained on
tabular datasets serves effectively as a foundational model for synthesizing
new tabular datasets. Furthermore, the proposed padding method outperforms the
conventional left and right padding strategies. Finally, the results highlight
that Tabula averagely reduces training time per epoch by 46.2% compared to
state-of-the-art LLM approaches while achieving higher data utility. Our code
is available at https://github.com/zhao-zilong/Tabula
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Discovery Inspired Unsupervised Domain Adaptation for
  Emotion-Cause Pair Extraction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Hua, Yujin Huang, Shuo Huang, Tao Feng, Lizhen Qu, Chris Bain, Richard Bassed, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the task of emotion-cause pair extraction in the
unsupervised domain adaptation setting. The problem is challenging as the
distributions of the events causing emotions in target domains are dramatically
different than those in source domains, despite the distributions of emotional
expressions between domains are overlapped. Inspired by causal discovery, we
propose a novel deep latent model in the variational autoencoder (VAE)
framework, which not only captures the underlying latent structures of data but
also utilizes the easily transferable knowledge of emotions as the bridge to
link the distributions of events in different domains. To facilitate knowledge
transfer across domains, we also propose a novel variational posterior
regularization technique to disentangle the latent representations of emotions
from those of events in order to mitigate the damage caused by the spurious
correlations related to the events in source domains. Through extensive
experiments, we demonstrate that our model outperforms the strongest baseline
by approximately 11.05\% on a Chinese benchmark and 2.45\% on a English
benchmark in terms of weighted-average F1 score. We have released our source
code and the generated dataset publicly at:
https://github.com/tk1363704/CAREL-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 5 tables. The paper has been published in the
  Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APOLLO: SGD-like Memory, AdamW-level Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05270v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05270v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z. Pan, Zhangyang Wang, Jinwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are notoriously memory-intensive during
training, particularly with the popular AdamW optimizer. This memory burden
necessitates using more or higher-end GPUs or reducing batch sizes, limiting
training scalability and throughput. To address this, various memory-efficient
optimizers have been proposed to reduce optimizer memory usage. However, they
face critical challenges: (i) reliance on costly SVD operations; (ii)
significant performance trade-offs compared to AdamW; and (iii) still
substantial optimizer memory overhead to maintain competitive performance.
  In this work, we identify that AdamW's learning rate adaptation rule can be
effectively coarsened as a structured learning rate update. Based on this
insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM
Optimization (APOLLO), which approximates learning rate scaling using an
auxiliary low-rank optimizer state based on pure random projection. This
structured learning rate update rule makes APOLLO highly tolerant to further
memory reductions while delivering comparable pre-training performance. Even
its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance
compared to AdamW with SGD-level memory costs.
  Extensive experiments demonstrate that the APOLLO series performs on-par with
or better than AdamW, while achieving greater memory savings by nearly
eliminating the optimization states of AdamW. These savings provide significant
system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB
setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model
Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without
system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training
LLaMA-7B on a single GPU using less than 12 GB of memory with weight
quantization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MLSys 2025; the newest version with new experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Generate Diverse Molecules? Towards Alignment with Structural
  Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyosoon Jang, Yunhui Jang, Jaehyung Kim, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated
impressive performance in molecular generation, which offers potential to
accelerate drug discovery. However, the current LLMs overlook a critical
requirement for drug discovery: proposing a diverse set of molecules. This
diversity is essential for improving the chances of finding a viable drug, as
it provides alternative molecules that may succeed where others fail in
real-world validations. Nevertheless, the LLMs often output structurally
similar molecules. While decoding schemes like diverse beam search may enhance
textual diversity, this often does not align with molecular structural
diversity. In response, we propose a new method for fine-tuning molecular
generative LLMs to autoregressively generate a set of structurally diverse
molecules, where each molecule is generated by conditioning on the previously
generated molecules. Our approach consists of two stages: (1) supervised
fine-tuning to adapt LLMs to autoregressively generate molecules in a sequence
and (2) reinforcement learning to maximize structural diversity within the
generated molecules. Our experiments show that the proposed approach enables
LLMs to generate diverse molecules better than existing approaches for diverse
sequence generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MING: A Functional Approach to Learning Molecular Generative Models <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Khoa Nguyen, Maciej Falkiewicz, Giangiacomo Mercatali, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional molecule generation methods often rely on sequence- or
graph-based representations, which can limit their expressive power or require
complex permutation-equivariant architectures. This paper introduces a novel
paradigm for learning molecule generative models based on functional
representations. Specifically, we propose Molecular Implicit Neural Generation
(MING), a diffusion-based model that learns molecular distributions in the
function space. Unlike standard diffusion processes in the data space, MING
employs a novel functional denoising probabilistic process, which jointly
denoises information in both the function's input and output spaces by
leveraging an expectation-maximization procedure for latent implicit neural
representations of data. This approach enables a simple yet effective model
design that accurately captures underlying function distributions. Experimental
results on molecule-related datasets demonstrate MING's superior performance
and ability to generate plausible molecular samples, surpassing
state-of-the-art data-space methods while offering a more streamlined
architecture and significantly faster generation times. The code is available
at https://github.com/v18nguye/MING.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Spectral Bias on Real-World Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itay Lavie, Zohar Ringel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental
tools in statistics and machine learning, with recent applications to highly
over-parameterized deep neural networks. The ability of these tools to learn a
target function is directly related to the eigenvalues of their kernel sampled
on the input data distribution. Targets that have support on higher eigenvalues
are more learnable. However, solving such eigenvalue problems on real-world
data remains a challenge. Here, we consider cross-dataset learnability and show
that one may use eigenvalues and eigenfunctions associated with highly
idealized data measures to reveal spectral bias on complex datasets and bound
learnability on real-world data. This allows us to leverage various symmetries
that realistic kernels manifest to unravel their spectral bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot generation of synthetic neurosurgical data with large language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin A. Barr, Eddie Guo, Emre Sezgin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical data is fundamental to advance neurosurgical research, but access is
often constrained by data availability, small sample sizes, privacy
regulations, and resource-intensive preprocessing and de-identification
procedures. Synthetic data offers a potential solution to challenges associated
with accessing and using real-world data (RWD). This study aims to evaluate the
capability of zero-shot generation of synthetic neurosurgical data with a large
language model (LLM), GPT-4o, by benchmarking with the conditional tabular
generative adversarial network (CTGAN). Synthetic datasets were compared to
real-world neurosurgical data to assess fidelity (means, proportions,
distributions, and bivariate correlations), utility (ML classifier performance
on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated
datasets matched or exceeded CTGAN performance, despite no fine-tuning or
access to RWD for pre-training. Datasets demonstrated high univariate and
bivariate fidelity to RWD without directly exposing any real patient records,
even at amplified sample size. Training an ML classifier on GPT-4o-generated
data and testing on RWD for a binary prediction task showed an F1 score (0.706)
with comparable performance to training on the CTGAN data (0.705) for
predicting postoperative functional status deterioration. GPT-4o demonstrated a
promising ability to generate high-fidelity synthetic neurosurgical data. These
findings also indicate that data synthesized with GPT-4o can effectively
augment clinical data with small sample sizes, and train ML models for
prediction of neurosurgical outcomes. Further investigation is necessary to
improve the preservation of distributional characteristics and boost classifier
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 4 tables (updated version, fixed typos and
  formatting)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space-aware Socioeconomic Indicator Inference with Heterogeneous <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Zou, Jiani Huang, Xixuan Hao, Yuhao Yang, Haomin Wen, Yibo Yan, Chao Huang, Chao Chen, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regional socioeconomic indicators are critical across various domains, yet
their acquisition can be costly. Inferring global socioeconomic indicators from
a limited number of regional samples is essential for enhancing management and
sustainability in urban areas and human settlements. Current inference methods
typically rely on spatial interpolation based on the assumption of spatial
continuity, which does not adequately address the complex variations present
within regional spaces. In this paper, we present GeoHG, the first space-aware
socioeconomic indicator inference method that utilizes a heterogeneous
graph-based structure to represent geospace for non-continuous inference.
Extensive experiments demonstrate the effectiveness of GeoHG in comparison to
existing methods, achieving an $R^2$ score exceeding 0.8 under extreme data
scarcity with a masked ratio of 95\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of Operational Records in Aviation Using Deep Learning
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety in the aviation industry is critical, even minor anomalies
can lead to severe consequences. This study evaluates the performance of four
different models for DP (deep learning), including: Bidirectional Long
Short-Term Memory (BLSTM), Convolutional Neural Networks (CNN), Long Short-Term
Memory (LSTM), and Simple Recurrent Neural Networks (sRNN), on a multi-class
classification task involving Commercial, Military, and Private categories
using the Socrata aviation dataset of 4,864 records. The models were assessed
using a classification report, confusion matrix analysis, accuracy metrics,
validation loss and accuracy curves. Among the models, BLSTM achieved the
highest overall accuracy of 72%, demonstrating superior performance in
stability and balanced classification, while LSTM followed closely with 71%,
excelling in recall for the Commercial class. CNN and sRNN exhibited lower
accuracies of 67% and 69%, with significant misclassifications in the Private
class. While the results highlight the strengths of BLSTM and LSTM in handling
sequential dependencies and complex classification tasks, all models faced
challenges with class imbalance, particularly in predicting the Military and
Private categories. Addressing these limitations through data augmentation,
advanced feature engineering, and ensemble learning techniques could enhance
classification accuracy and robustness. This study underscores the importance
of selecting appropriate architectures for domain specific tasks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference paper; aviation safety, NLP, DL, operational record
  classification, Socrata</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive NAD: Online and Self-adaptive Unsupervised Network Anomaly
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yachao Yuan, Yu Huang, Yali Yuan, Jin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread usage of the Internet of Things (IoT) has raised the risks of
cyber threats, thus developing Anomaly Detection Systems (ADSs) that can adapt
to evolving or new attacks is critical. Previous studies primarily focused on
offline unsupervised learning methods to safeguard ADSs, which is not
applicable in practical real-world applications. Besides, most of them strongly
rely on assumptions of known legitimates and fail to satisfy the interpretable
requirements in security applications, creating barriers to the adoption in
practice. In this paper, we design Adaptive NAD, a general framework to improve
and interpret online unsupervised anomaly detection in security domains. An
interpretable two-layer anomaly detection strategy is proposed to generate
reliable high-confidence pseudo-labels. Then, an online learning scheme is
introduced to update Adaptive NAD by a novel threshold calculation technique to
adapt to new threats. Experimental results demonstrate that Adaptive NAD
achieves more than 5.4%, 23.0%, and 3.2% improvements in SPAUC compared with
state-of-the-art solutions on the CIC-Darknet2020, CIC-DoHBrw-2020, and
Edge-IIoTset datasets, respectively. The code is released at
https://github.com/MyLearnCodeSpace/Adaptive-NAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Risk to Uncertainty: Generating Predictive Uncertainty Measures via
  Bayesian Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10727v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10727v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Kotelevskii, Vladimir Kondratyev, Martin Takáč, Éric Moulines, Maxim Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are various measures of predictive uncertainty in the literature, but
their relationships to each other remain unclear. This paper uses a
decomposition of statistical pointwise risk into components, associated with
different sources of predictive uncertainty, namely aleatoric uncertainty
(inherent data variability) and epistemic uncertainty (model-related
uncertainty). Together with Bayesian methods, applied as an approximation, we
build a framework that allows one to generate different predictive uncertainty
measures.
  We validate our method on image datasets by evaluating its performance in
detecting out-of-distribution and misclassified instances using the AUROC
metric. The experimental results confirm that the measures derived from our
framework are useful for the considered downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KcMF: A Knowledge-compliant Framework for Schema and Entity Matching
  with Fine-tuning-free LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqin Xu, Huan Li, Ke Chen, Lidan Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema matching (SM) and entity matching (EM) tasks are crucial for data
integration. While large language models (LLMs) have shown promising results in
these tasks, they suffer from hallucinations and confusion about task
instructions. This study presents the Knowledge-Compliant Matching Framework
(KcMF), an LLM-based approach that addresses these issues without the need for
domain-specific fine-tuning. KcMF employs a once-and-for-all pseudo-code-based
task decomposition strategy to adopt natural language statements that guide LLM
reasoning and reduce confusion across various task types. We also propose two
mechanisms, Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build
domain knowledge sets when unstructured domain knowledge is lacking. Moreover,
we introduce a result-ensemble strategy to leverage multiple knowledge sources
and suppress badly formatted outputs. Extensive evaluations confirm that KcMF
clearly enhances five LLM backbones in both SM and EM tasks while outperforming
the non-LLM competitors by an average F1-score of 17.93%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under reveiw; new results and analysis added, typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GINopic: Topic Modeling with <span class="highlight-title">Graph</span> Isomorphism Network <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Adhya, Debarshi Kumar Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is a widely used approach for analyzing and exploring large
document collections. Recent research efforts have incorporated pre-trained
contextualized language models, such as BERT embeddings, into topic modeling.
However, they often neglect the intrinsic informational value conveyed by
mutual dependencies between words. In this study, we introduce GINopic, a topic
modeling framework based on graph isomorphism networks to capture the
correlation between words. By conducting intrinsic (quantitative as well as
qualitative) and extrinsic evaluations on diverse benchmark datasets, we
demonstrate the effectiveness of GINopic compared to existing topic models and
highlight its potential for advancing topic modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper for NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation with Estimation of Source Reliability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongyeon Hwang, Junyoung Park, Hyejin Park, Sangdon Park, Jungseul Ok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) addresses key limitations of large
language models (LLMs), such as hallucinations and outdated knowledge, by
incorporating external databases. These databases typically consult multiple
sources to encompass up-to-date and various information. However, standard RAG
methods often overlook the heterogeneous source reliability in the multi-source
database and retrieve documents solely based on relevance, making them prone to
propagating misinformation. To address this, we propose Reliability-Aware RAG
(RA-RAG) which estimates the reliability of multiple sources and incorporates
this information into both retrieval and aggregation processes. Specifically,
it iteratively estimates source reliability and true answers for a set of
queries with no labelling. Then, it selectively retrieves relevant documents
from a few of reliable sources and aggregates them using weighted majority
voting, where the selective retrieval ensures scalability while not
compromising the performance. We also introduce a benchmark designed to reflect
real-world scenarios with heterogeneous source reliability and demonstrate the
effectiveness of RA-RAG compared to a set of baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PHLP: Sole Persistent Homology for Link Prediction - Interpretable
  Feature Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon You, Eunwoo Heo, Jae-Hun Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction (LP), inferring the connectivity between nodes, is a
significant research area in graph data, where a link represents essential
information on relationships between nodes. Although graph neural network
(GNN)-based models have achieved high performance in LP, understanding why they
perform well is challenging because most comprise complex neural networks. We
employ persistent homology (PH), a topological data analysis method that helps
analyze the topological information of graphs, to interpret the features used
for prediction. We propose a novel method that employs PH for LP (PHLP)
focusing on how the presence or absence of target links influences the overall
topology. The PHLP utilizes the angle hop subgraph and new node labeling called
degree double radius node labeling (Degree DRNL), distinguishing the
information of graphs better than DRNL. Using only a classifier, PHLP performs
similarly to state-of-the-art (SOTA) models on most benchmark datasets.
Incorporating the outputs calculated using PHLP into the existing GNN-based
SOTA models improves performance across all benchmark datasets. To the best of
our knowledge, PHLP is the first method of applying PH to LP without GNNs. The
proposed approach, employing PH while not relying on neural networks, enables
the identification of crucial factors for improving performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Singularities in point clouds with the <span class="highlight-title">graph</span> Laplacian: An
  explicit approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Andersson, Benny Avelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop theory and methods that use the graph Laplacian to analyze the
geometry of the underlying manifold of datasets. Our theory provides
theoretical guarantees and explicit bounds on the functional forms of the graph
Laplacian when it acts on functions defined close to singularities of the
underlying manifold. We use these explicit bounds to develop tests for
singularities and propose methods that can be used to estimate geometric
properties of singularities in the datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoostStep: Boosting mathematical capability of Large Language Models via
  improved single-step reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive ability in solving
complex mathematical problems with multi-step reasoning and can be further
enhanced with well-designed in-context learning (ICL) examples. However, this
potential is often constrained by two major challenges in ICL: granularity
mismatch and irrelevant information. We observe that while LLMs excel at
decomposing mathematical problems, they often struggle with reasoning errors in
fine-grained steps. Moreover, ICL examples retrieved at the question level may
omit critical steps or even mislead the model with irrelevant details. To
address this issue, we propose BoostStep, a method that enhances reasoning
accuracy through step-aligned ICL, a novel mechanism that carefully aligns
retrieved reference steps with the corresponding reasoning steps. Additionally,
BoostStep incorporates an effective "first-try" strategy to deliver exemplars
highly relevant to the current state of reasoning. BoostStep is a flexible and
powerful method that integrates seamlessly with chain-of-thought (CoT) and tree
search algorithms, refining both candidate selection and decision-making.
Empirical results show that BoostStep improves GPT-4o's CoT performance by 4.6%
across mathematical benchmarks, significantly surpassing traditional few-shot
learning's 1.2%. Moreover, it can achieve an additional 7.5\% gain combined
with tree search. Surprisingly, it enhances state-of-the-art LLMs to solve
challenging math problems using simpler examples. It improves
DeepSeek-R1-671B's performance on AIME by 2.2%, leveraging simple examples only
from the MATH dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and Data are available at
  https://github.com/beichenzbc/BoostStep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLAD: Improving Latent <span class="highlight-title">Graph</span> Generative Modeling with Simple
  Quantization <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16883v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16883v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning graph generative models over latent spaces has received less
attention compared to models that operate on the original data space and has so
far demonstrated lacklustre performance. We present GLAD a latent space graph
generative model. Unlike most previous latent space graph generative models,
GLAD operates on a discrete latent space that preserves to a significant extent
the discrete nature of the graph structures making no unnatural assumptions
such as latent space continuity. We learn the prior of our discrete latent
space by adapting diffusion bridges to its structure. By operating over an
appropriately constructed latent space we avoid relying on decompositions that
are often used in models that operate in the original data space. We present
experiments on a series of graph benchmark datasets that demonstrates GLAD as
the first equivariant latent graph generative method achieves competitive
performance with the state of the art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 (Previously accepted at SPIGM ICML 2024 workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CuriousLLM: Elevating Multi-Document Question Answering with
  LLM-Enhanced Knowledge <span class="highlight-title">Graph</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zukang Yang, Zixuan Zhu, Xuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success in open-domain
question answering. However, they continue to face challenges such as
hallucinations and knowledge cutoffs. These issues can be mitigated through
in-context learning by providing LLMs with relevant context before generating
answers. Recent literature proposes Knowledge Graph Prompting (KGP) which
integrates knowledge graphs with an LLM-based traversal agent to substantially
enhance document retrieval quality. However, KGP requires costly fine-tuning
with large datasets and remains prone to hallucination. In this paper, we
propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning
mechanism into an LLM agent. This mechanism enables the agent to generate
relevant follow-up questions, thereby guiding the information retrieval process
more efficiently. Central to our approach is the development of the new
Follow-upQA dataset, which includes questions and supporting evidence as input,
with follow-up questions serving as ground truths. These follow-up questions
either inquire about what is still missing to fully answer the user's query or
use special tokens to signify that the retrieved evidence is sufficient. Our
experiments show that CuriousLLM significantly boosts LLM performance in
multi-document question answering (MD-QA), circumventing the substantial
computational costs and latency from the original KGP framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Iterative Algorithm for Rescaled Hyperbolic Functions Regression <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeqi Gao, Zhao Song, Junze Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have numerous real-life applications across
various domains, such as natural language translation, sentiment analysis,
language modeling, chatbots and conversational agents, creative writing, text
classification, summarization, and generation. LLMs have shown great promise in
improving the accuracy and efficiency of these tasks, and have the potential to
revolutionize the field of natural language processing (NLP) in the years to
come. Exponential function based attention unit is a fundamental element in
LLMs. Several previous works have studied the convergence of exponential
regression and softmax regression.
  In this paper, we propose an iterative algorithm to solve a rescaled version
of the slightly different formulation of the softmax regression problem that
arises in attention mechanisms of large language models. Specifically, we
consider minimizing the squared loss between a certain function, which can be
either the exponential function, hyperbolic sine function, or hyperbolic cosine
function, and its inner product with a target $n$-dimensional vector $b$,
scaled by the normalization term. This ``rescaled softmax regression'' differs
from classical softmax regression in the location of the normalization factor.
  The efficiency and generalizability of this framework to multiple hyperbolic
functions make it relevant for optimizing attention mechanisms. The analysis
also leads to a corollary bounding solution changes under small perturbations
for in-context learning. Limitations and societal impact are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Communications: A Unified Framework for Cross-modal Context-aware
  Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce token communications (TokCom), a unified
framework to leverage cross-modal context information in generative semantic
communications (GenSC). TokCom is a new paradigm, motivated by the recent
success of generative foundation models and multimodal large language models
(GFM/MLLMs), where the communication units are tokens, enabling efficient
transformer-based token processing at the transmitter and receiver. In this
paper, we introduce the potential opportunities and challenges of leveraging
context in GenSC, explore how to integrate GFM/MLLMs-based token processing
into semantic communication systems to leverage cross-modal context
effectively, present the key principles for efficient TokCom at various layers
in future wireless networks. We demonstrate the corresponding TokCom benefits
in a GenSC setup for image, leveraging cross-modal context information, which
increases the bandwidth efficiency by 70.8% with negligible loss of
semantic/perceptual quality. Finally, the potential research directions are
identified to facilitate adoption of TokCom in future wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Both Text and Images Leaked! A Systematic Analysis of <span class="highlight-title">Multimodal</span> LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code Available: https://github.com/MLLM-Data-Contamination/MM-Detect</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Compressed Image Latents and <span class="highlight-title">Multimodal</span> Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever study of adapting compressed image latents
to suit the needs of downstream vision tasks that adopt Multimodal Large
Language Models (MLLMs). MLLMs have extended the success of large language
models to modalities (e.g. images) beyond text, but their billion scale hinders
deployment on resource-constrained end devices. While cloud-hosted MLLMs could
be available, transmitting raw, uncompressed images captured by end devices to
the cloud requires an efficient image compression system. To address this, we
focus on emerging neural image compression and propose a novel framework with a
lightweight transform-neck and a surrogate loss to adapt compressed image
latents for MLLM-based vision tasks. Given the huge scale of MLLMs, our
framework excludes the entire downstream MLLM except part of its visual encoder
from training our system. This stands out from most existing coding for machine
approaches that involve downstream networks in training and thus could be
impractical when the networks are MLLMs. The proposed framework is general in
that it is applicable to various MLLMs, neural image codecs, and multiple
application scenarios, where the neural image codec can be (1) pre-trained for
human perception without updating, (2) fully updated for joint human and
machine perception, or (3) fully updated for only machine perception. Extensive
experiments on different neural image codecs and various MLLMs show that our
method achieves great rate-accuracy performance with much less complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Attribute-Relation Representation Based Video Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Du, Yiping Duan, Qianqian Yang, Xiaoming Tao, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of multimedia data volume, there is an increasing need
for efficient video transmission in applications such as virtual reality and
future video streaming services. Semantic communication is emerging as a vital
technique for ensuring efficient and reliable transmission in low-bandwidth,
high-noise settings. However, most current approaches focus on joint
source-channel coding (JSCC) that depends on end-to-end training. These methods
often lack an interpretable semantic representation and struggle with
adaptability to various downstream tasks. In this paper, we introduce the use
of object-attribute-relation (OAR) as a semantic framework for videos to
facilitate low bit-rate coding and enhance the JSCC process for more effective
video transmission. We utilize OAR sequences for both low bit-rate
representation and generative video reconstruction. Additionally, we
incorporate OAR into the image JSCC model to prioritize communication resources
for areas more critical to downstream tasks. Our experiments on traffic
surveillance video datasets assess the effectiveness of our approach in terms
of video transmission performance. The empirical findings demonstrate that our
OAR-based video coding method not only outperforms H.265 coding at lower
bit-rates but also synergizes with JSCC to deliver robust and efficient video
transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRe: Enhancing <span class="highlight-title">Multimodal</span> Queries Representation via Fusion-Free
  Modality Interaction for <span class="highlight-title">Multimodal</span> Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal retrieval methods have endowed text-based retrievers with
multimodal capabilities by utilizing pre-training strategies for visual-text
alignment. They often directly fuse the two modalities for cross-reference
during the alignment to understand multimodal queries. However, existing
methods often overlook crucial visual information due to a text-dominant issue,
which overly depends on text-driven signals. In this paper, we introduce MIRe,
a retrieval framework that achieves modality interaction without fusing textual
features during the alignment. Our method allows the textual query to attend to
visual embeddings while not feeding text-driven signals back into the visual
representations. Additionally, we construct a pre-training dataset for
multimodal query retrieval by transforming concise question-answer pairs into
extended passages. Our experiments demonstrate that our pre-training strategy
significantly enhances the understanding of multimodal queries, resulting in
strong performance across four multimodal retrieval benchmarks under zero-shot
settings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-16T00:00:00Z">2025-02-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">49</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Wang, Haote Xu, Xiaolu Chen, Haodi Xu, Yue Huang, Xinghao Ding, Xiaotong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) in 3D point clouds is crucial in a wide range of
industrial applications, especially in various forms of precision
manufacturing. Considering the industrial demand for reliable 3D AD, several
methods have been developed. However, most of these approaches typically
require training separate models for each category, which is memory-intensive
and lacks flexibility. In this paper, we propose a novel Point-Language model
with dual-prompts for 3D ANomaly dEtection (PLANE). The approach leverages
multi-modal prompts to extend the strong generalization capabilities of
pre-trained Point-Language Models (PLMs) to the domain of 3D point cloud AD,
achieving impressive detection performance across multiple categories using a
single model. Specifically, we propose a dual-prompt learning method,
incorporating both text and point cloud prompts. The method utilizes a dynamic
prompt creator module (DPCM) to produce sample-specific dynamic prompts, which
are then integrated with class-specific static prompts for each modality,
effectively driving the PLMs. Additionally, based on the characteristics of
point cloud data, we propose a pseudo 3D anomaly generation method (Ano3D) to
improve the model's detection capabilities in an unsupervised setting.
Experimental results demonstrate that the proposed method, which is under the
multi-class-one-model paradigm, achieves a +8.7%/+17% gain on anomaly detection
and localization performance as compared to the state-of-the-art
one-class-one-model methods for the Anomaly-ShapeNet dataset, and obtains
+4.3%/+4.1% gain for the Real3D-AD dataset. Code will be available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging <span class="highlight-title">Multimodal</span>-LLMs Assisted by Instance Segmentation for
  Intelligent Traffic Monitoring <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robust and efficient traffic monitoring system is essential for smart
cities and Intelligent Transportation Systems (ITS), using sensors and cameras
to track vehicle movements, optimize traffic flow, reduce congestion, enhance
road safety, and enable real-time adaptive traffic control. Traffic monitoring
models must comprehensively understand dynamic urban conditions and provide an
intuitive user interface for effective management. This research leverages the
LLaVA visual grounding multimodal large language model (LLM) for traffic
monitoring tasks on the real-time Quanser Interactive Lab simulation platform,
covering scenarios like intersections, congestion, and collisions. Cameras
placed at multiple urban locations collect real-time images from the
simulation, which are fed into the LLaVA model with queries for analysis. An
instance segmentation model integrated into the cameras highlights key elements
such as vehicles and pedestrians, enhancing training and throughput. The system
achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in
determining steering direction, outperforming traditional models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, submitted to 30th IEEE International Symposium on
  Computers and Communications (ISCC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORDIAL: Can <span class="highlight-title">Multimodal</span> Large Language Models Effectively Understand
  Coherence Relationships? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) are renowned for their superior
instruction-following and reasoning capabilities across diverse problem
domains. However, existing benchmarks primarily focus on assessing factual and
logical correctness in downstream tasks, with limited emphasis on evaluating
MLLMs' ability to interpret pragmatic cues and intermodal relationships. To
address this gap, we assess the competency of MLLMs in performing Multimodal
Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL,
encompasses a broad spectrum of Coherence Relations across 3 different
discourse domains at varying levels of granularity. Through our experiments on
10+ MLLMs employing different prompting strategies, we show that even top
models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple
classifier-based baselines. This study emphasizes the need to move beyond
similarity-based metrics and adopt a discourse-driven framework for evaluating
MLLMs, providing a more nuanced assessment of their capabilities. The benchmark
and code are available at: https://github.com/aashish2000/CORDIAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for
  Traffic Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpitsinh Vaghela, Duo Lu, Aayush Atul Verma, Bharatesh Chakravarthi, Hua Wei, Yezhou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single camera 3D perception for traffic monitoring faces significant
challenges due to occlusion and limited field of view. Moreover, fusing
information from multiple cameras at the image feature level is difficult
because of different view angles. Further, the necessity for practical
implementation and compatibility with existing traffic infrastructure compounds
these challenges. To address these issues, this paper introduces a novel
Bird's-Eye-View road occupancy detection framework that leverages multiple
roadside cameras to overcome the aforementioned limitations. To facilitate the
framework's development and evaluation, a synthetic dataset featuring diverse
scenes and varying camera configurations is generated using the CARLA
simulator. A late fusion and three early fusion methods were implemented within
the proposed framework, with performance further enhanced by integrating
backgrounds. Extensive evaluations were conducted to analyze the impact of
multi-camera inputs and varying BEV occupancy map sizes on model performance.
Additionally, a real-world data collection pipeline was developed to assess the
model's ability to generalize to real-world environments. The sim-to-real
capabilities of the model were evaluated using zero-shot and few-shot
fine-tuning, demonstrating its potential for practical application. This
research aims to advance perception systems in traffic monitoring, contributing
to improved traffic management, operational efficiency, and road safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OctoTools: An Agentic Framework with Extensible Tools for Complex
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving complex reasoning tasks may involve visual understanding, domain
knowledge retrieval, numerical calculation, and multi-step reasoning. Existing
methods augment large language models (LLMs) with external tools but are
restricted to specialized domains, limited tool types, or require additional
training data. In this paper, we introduce OctoTools, a training-free,
user-friendly, and easily extensible open-source agentic framework designed to
tackle complex reasoning across diverse domains. OctoTools introduces
standardized tool cards to encapsulate tool functionality, a planner for both
high-level and low-level planning, and an executor to carry out tool usage. We
validate OctoTools' generality across 16 diverse tasks (including MathVista,
MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains
of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions
and LangChain by up to 10.6% when given the same set of tools. Through
comprehensive analysis and ablations, OctoTools demonstrates advantages in task
planning, effective tool usage, and multi-step problem solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>89 pages, 18 figures. Project website: https://octotools.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automatic Identification of Missing Tissues using a
  Geometric-Learning Correspondence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliana M. Vasquez Osorio, Edward Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing tissue presents a big challenge for dose mapping, e.g., in the
reirradiation setting. We propose a pipeline to identify missing tissue on
intra-patient structure meshes using a previously trained geometric-learning
correspondence model. For our application, we relied on the prediction
discrepancies between forward and backward correspondences of the input meshes,
quantified using a correspondence-based Inverse Consistency Error (cICE). We
optimised the threshold applied to cICE to identify missing points in a dataset
of 35 simulated mandible resections. Our identified threshold, 5.5 mm, produced
a balanced accuracy score of 0.883 in the training data, using an ensemble
approach. This pipeline produced plausible results for a real case where ~25%
of the mandible was removed after a surgical intervention. The pipeline,
however, failed on a more extreme case where ~50% of the mandible was removed.
This is the first time geometric-learning modelling is proposed to identify
missing points in corresponding anatomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in XXth International Conference on the use of Computers in
  Radiation therapy. Pages 759-762 in XXth ICCR Proceedings, found in
  https://udl.hal.science/hal-04720234v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting network optimization stability for enhanced PET image
  denoising using deep image prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fumio Hashimoto, Kibo Ote, Yuya Onishi, Hideaki Tashima, Go Akamatsu, Yuma Iwao, Miwako Takahashi, Taiga Yamaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PET is affected by statistical noise due to constraints on tracer dose and
scan duration, impacting both diagnostic performance and quantitative accuracy.
While deep learning (DL)-based PET denoising methods have been used to improve
image quality, they may introduce over-smoothing, compromising quantitative
accuracy. We propose a method for making a DL solution more reliable and apply
it to the conditional deep image prior (DIP). We introduce the idea of
stability information in the optimization process of conditional DIP, enabling
the identification of unstable regions within the network's optimization
trajectory. Our method incorporates a stability map, which is derived from
multiple intermediate outputs of moderate network at different optimization
steps. The final denoised image is then obtained by computing linear
combination of the DIP output and the original reconstructed image, weighted by
the stability map. Our method effectively reduces noise while preserving small
structure details in brain FDG images. Results demonstrated that our approach
outperformed existing methods in peak-to-valley ratio and noise suppression
across various low-dose levels. Region-of-interest analysis confirmed that the
proposed method maintains quantitative accuracy without introducing under- or
over-estimation. We applied our method to full-dose PET data to assess its
impact on image quality. The results revealed that the proposed method
significantly reduced background noise while preserving the peak-to-valley
ratio at a level comparable to that of unfiltered full-dose PET images. The
proposed method introduces a robust approach to DL-based PET denoising,
enhancing its reliability and preserving quantitative accuracy. This strategy
has the potential to advance performance in high-sensitivity PET scanners,
demonstrating that DL can extend PET imaging capabilities beyond low-dose
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaskFlow: Discrete Flows For Flexible and Efficient Long Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Fuest, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating long, high-quality videos remains a challenge due to the complex
interplay of spatial and temporal dynamics and hardware limitations. In this
work, we introduce \textbf{MaskFlow}, a unified video generation framework that
combines discrete representations with flow-matching to enable efficient
generation of high-quality long videos. By leveraging a frame-level masking
strategy during training, MaskFlow conditions on previously generated unmasked
frames to generate videos with lengths ten times beyond that of the training
sequences. MaskFlow does so very efficiently by enabling the use of fast Masked
Generative Model (MGM)-style sampling and can be deployed in both fully
autoregressive as well as full-sequence generation modes. We validate the
quality of our method on the FaceForensics (FFS) and Deepmind Lab (DMLab)
datasets and report Fr\'echet Video Distance (FVD) competitive with
state-of-the-art approaches. We also provide a detailed analysis on the
sampling efficiency of our method and demonstrate that MaskFlow can be applied
to both timestep-dependent and timestep-independent models in a training-free
manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of LLM-based Agents in Medicine: How far are we from Baymax? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming healthcare through the
development of LLM-based agents that can understand, reason about, and assist
with medical tasks. This survey provides a comprehensive review of LLM-based
agents in medicine, examining their architectures, applications, and
challenges. We analyze the key components of medical agent systems, including
system profiles, clinical planning mechanisms, medical reasoning frameworks,
and external capacity enhancement. The survey covers major application
scenarios such as clinical decision support, medical documentation, training
simulations, and healthcare service optimization. We discuss evaluation
frameworks and metrics used to assess these agents' performance in healthcare
settings. While LLM-based agents show promise in enhancing healthcare delivery,
several challenges remain, including hallucination management, multimodal
integration, implementation barriers, and ethical considerations. The survey
concludes by highlighting future research directions, including advances in
medical reasoning inspired by recent developments in LLM architectures,
integration with physical systems, and improvements in training simulations.
This work provides researchers and practitioners with a structured overview of
the current state and future prospects of LLM-based agents in medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on
  Continual Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite exceptional capabilities in knowledge-intensive tasks, Large Language
Models (LLMs) face a critical gap in understanding how they internalize new
knowledge, particularly how to structurally embed acquired knowledge in their
neural computations. We address this issue through the lens of knowledge
circuit evolution, identifying computational subgraphs that facilitate
knowledge storage and processing. Our systematic analysis of circuit evolution
throughout continual pre-training reveals several key findings: (1) the
acquisition of new knowledge is influenced by its relevance to pre-existing
knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase
shift from formation to optimization; (3) the evolution of knowledge circuits
follows a deep-to-shallow pattern. These insights not only advance our
theoretical understanding of the mechanisms of new knowledge acquisition in
LLMs, but also provide potential implications for improving continual
pre-training strategies to enhance model performance. Code and data will be
available at https://github.com/zjunlp/DynamicKnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Deception to Perception: The Surprising Benefits of Deepfakes for
  Detecting, Measuring, and Mitigating Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Liu, Balaji Padmanabhan, Siva Viswanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deepfake technologies have predominantly been criticized for potential
misuse, our study demonstrates their significant potential as tools for
detecting, measuring, and mitigating biases in key societal domains. By
employing deepfake technology to generate controlled facial images, we extend
the scope of traditional correspondence studies beyond mere textual
manipulations. This enhancement is crucial in scenarios such as pain
assessments, where subjective biases triggered by sensitive features in facial
images can profoundly affect outcomes. Our results reveal that deepfakes not
only maintain the effectiveness of correspondence studies but also introduce
groundbreaking advancements in bias measurement and correction techniques. This
study emphasizes the constructive role of deepfake technologies as essential
tools for advancing societal equity and fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReLearn: Unlearning via Learning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current unlearning methods for large language models usually rely on reverse
optimization to reduce target token probabilities. However, this paradigm
disrupts the subsequent tokens prediction, degrading model performance and
linguistic coherence. Moreover, existing evaluation metrics overemphasize
contextual forgetting while inadequately assessing response fluency and
relevance. To address these challenges, we propose ReLearn, a data augmentation
and fine-tuning pipeline for effective unlearning, along with a comprehensive
evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)
and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and
Linguistic Score (LS) to evaluate generation quality. Our experiments show that
ReLearn successfully achieves targeted forgetting while preserving high-quality
output. Through mechanistic analysis, we further demonstrate how reverse
optimization disrupts coherent text generation, while ReLearn preserves this
essential capability. Code is available at https://github.com/zjunlp/unlearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can't See the Forest for the Trees: Benchmarking <span class="highlight-title">Multimodal</span> Safety
  Awareness for <span class="highlight-title">Multimodal</span> LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have expanded the capabilities of
traditional language models by enabling interaction through both text and
images. However, ensuring the safety of these models remains a significant
challenge, particularly in accurately identifying whether multimodal content is
safe or unsafe-a capability we term safety awareness. In this paper, we
introduce MMSafeAware, the first comprehensive multimodal safety awareness
benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500
carefully curated image-prompt pairs. MMSafeAware includes both unsafe and
over-safety subsets to assess models abilities to correctly identify unsafe
content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine
widely used MLLMs using MMSafeAware reveals that current models are not
sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies
36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further
explore three methods to improve safety awareness-prompting-based approaches,
visual contrastive decoding, and vision-centric reasoning fine-tuning-but find
that none achieve satisfactory performance. Our findings highlight the profound
challenges in developing MLLMs with robust safety awareness, underscoring the
need for further research in this area. All the code and data will be publicly
available to facilitate future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-DEMT: A hybrid real-time acupoint detection model combining mamba and
  <span class="highlight-title">transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Yang, Qi Zang, Chulong Zhang, Lingfeng Huang, Yaoqin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Chinese acupuncture methods often face controversy in clinical
practice due to their high subjectivity. Additionally, current
intelligent-assisted acupuncture systems have two major limitations: slow
acupoint localization speed and low accuracy. To address these limitations, a
new method leverages the excellent inference efficiency of the state-space
model Mamba, while retaining the advantages of the attention mechanism in the
traditional DETR architecture, to achieve efficient global information
integration and provide high-quality feature information for acupoint
localization tasks. Furthermore, by employing the concept of residual
likelihood estimation, it eliminates the need for complex upsampling processes,
thereby accelerating the acupoint localization task. Our method achieved
state-of-the-art (SOTA) accuracy on a private dataset of acupoints on the human
back, with an average Euclidean distance pixel error (EPE) of 7.792 and an
average time consumption of 10.05 milliseconds per localization task. Compared
to the second-best algorithm, our method improved both accuracy and speed by
approximately 14\%. This significant advancement not only enhances the efficacy
of acupuncture treatment but also demonstrates the commercial potential of
automated acupuncture robot systems. Access to our method is available at
https://github.com/Sohyu1/RT-DEMT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAViMNet: SSMs-Based Domain Adaptive Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Enes Doruk, Hasan F. Ates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation (UDA) for object detection adapts models
trained on labeled source domains to unlabeled target domains, ensuring robust
performance across domain shifts. Transformer-based architectures excel at
capturing long-range dependencies but face efficiency challenges due to their
quadratic attention complexity, which limits scalability in UDA tasks. To
address these issues, we propose a hybrid domain-adaptive Mamba Transformer
architecture that combines Mamba's efficient state-space modeling with
attention mechanisms to tackle domain-specific spatial and channel-wise
variations. Each hybrid block integrates domain-adaptive Mamba blocks and
attention mechanisms: Domain-Adaptive Mamba employs spatial and channel
state-space models to adaptively model domain variations, while attention
mechanisms leverage self-attention for intra-domain feature enhancement and
cross-attention for effective source-target alignment. Our approach processes
both shallow and deeper features, employing an entropy-based knowledge
distillation framework with margin ReLU to emphasize discriminative features
and suppress noise. Gradient Reversal Layers enable adversarial alignment
across network layers, while entropy-driven gating attention with random
perturbations refines target features and mitigates overfitting. By unifying
these components, our architecture achieves state-of-the-art performance in UDA
object detection, balancing efficiency with robust generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowing Your Target: Target-Aware <span class="highlight-title">Transformer</span> Makes Better
  Spatio-Temporal Video Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Yaojie Shen, Chenxi Luo, Tiejian Luo, Yan Huang, Yuewei Lin, Heng Fan, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has attracted increasing interest in STVG, owing to its
end-to-end pipeline and promising result. Existing Transformer-based STVG
approaches often leverage a set of object queries, which are initialized simply
using zeros and then gradually learn target position information via iterative
interactions with multimodal features, for spatial and temporal localization.
Despite simplicity, these zero object queries, due to lacking target-specific
cues, are hard to learn discriminative target information from interactions
with multimodal features in complicated scenarios (\e.g., with distractors or
occlusion), resulting in degradation. Addressing this, we introduce a novel
Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate
object queries via exploring target-specific cues from the given video-text
pair, for improving STVG. The key lies in two simple yet effective modules,
comprising text-guided temporal sampling (TTS) and attribute-aware spatial
activation (ASA), working in a cascade. The former focuses on selecting
target-relevant temporal cues from a video utilizing holistic text information,
while the latter aims at further exploiting the fine-grained visual attribute
information of the object from previous target-aware temporal cues, which is
applied for object query initialization. Compared to existing methods
leveraging zero-initialized queries, object queries in our TA-STVG, directly
generated from a given video-text pair, naturally carry target-specific cues,
making them adaptive and better interact with multimodal features for learning
more discriminative information to improve STVG. In our experiments on three
benchmarks, TA-STVG achieves state-of-the-art performance and significantly
outperforms the baseline, validating its efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and
  Privacy Risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-Language Models (VLMs) have shown remarkable performance across
various tasks, particularly in recognizing geographic information from images.
However, significant challenges remain, including biases and privacy concerns.
To systematically address these issues in the context of geographic information
recognition, we introduce a benchmark dataset consisting of 1,200 images paired
with detailed geographic metadata. Evaluating four VLMs, we find that while
these models demonstrate the ability to recognize geographic information from
images, achieving up to $53.8\%$ accuracy in city prediction, they exhibit
significant regional biases. Specifically, performance is substantially higher
for economically developed and densely populated regions compared to less
developed ($-12.5\%$) and sparsely populated ($-17.0\%$) areas. Moreover, the
models exhibit regional biases, frequently overpredicting certain locations;
for instance, they consistently predict Sydney for images taken in Australia.
The strong performance of VLMs also raises privacy concerns, particularly for
users who share images online without the intent of being identified. Our code
and dataset are publicly available at
https://github.com/uscnlp-lime/FairLocator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BFA: Best-Feature-Aware Fusion for Multi-View Fine-grained Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Lan, Weixin Mao, Haosheng Li, Le Wang, Tiancai Wang, Haoqiang Fan, Osamu Yoshie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, multi-view cameras are typically employed for
fine-grained manipulation tasks. Existing approaches (e.g., ACT) tend to treat
multi-view features equally and directly concatenate them for policy learning.
However, it will introduce redundant visual information and bring higher
computational costs, leading to ineffective manipulation. For a fine-grained
manipulation task, it tends to involve multiple stages while the most
contributed view for different stages is varied over time. In this paper, we
propose a plug-and-play best-feature-aware (BFA) fusion strategy for multi-view
manipulation tasks, which is adaptable to various policies. Built upon the
visual backbone of the policy network, we design a lightweight network to
predict the importance score of each view. Based on the predicted importance
scores, the reweighted multi-view features are subsequently fused and input
into the end-to-end policy network, enabling seamless integration. Notably, our
method demonstrates outstanding performance in fine-grained manipulations.
Experimental results show that our approach outperforms multiple baselines by
22-46% success rate on different tasks. Our work provides new insights and
inspiration for tackling key challenges in fine-grained manipulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyRefill: A Unified, Data-Efficient Framework for Left-Prompt-Guided
  Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Xie, Chenjie Cao, Yunuo Cai, Xiangyang Xue, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to
address a diverse range of reference-based vision tasks. Inspired by the human
creative process, we reformulate these tasks using a left-right stitching
formulation to construct contextual input. Building upon this foundation, we
propose AnyRefill, an extension of LeftRefill, that effectively adapts
Text-to-Image (T2I) models to various vision tasks. AnyRefill leverages the
inpainting priors of advanced T2I model based on the Diffusion Transformer
(DiT) architecture, and incorporates flexible components to enhance its
capabilities. By combining task-specific LoRAs with the stitching input,
AnyRefill unlocks its potential across diverse tasks, including conditional
generation, visual perception, and image editing, without requiring additional
visual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency,
requiring minimal task-specific fine-tuning while maintaining high generative
performance. Through extensive ablation studies, we demonstrate that AnyRefill
outperforms other image condition injection methods and achieves competitive
results compared to state-of-the-art open-source methods. Notably, AnyRefill
delivers results comparable to advanced commercial tools, such as IC-Light and
SeedEdit, even in challenging scenarios. Comprehensive experiments and ablation
studies across versatile tasks validate the strong generation of the proposed
simple yet effective LPG formulation, establishing AnyRefill as a unified,
highly data-efficient solution for reference-based vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, submitted to TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavRAG: Generating User Demand Instructions for Embodied Navigation
  through Retrieval-Augmented LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) is an essential skill for embodied
agents, allowing them to navigate in 3D environments following natural language
instructions. High-performance navigation models require a large amount of
training data, the high cost of manually annotating data has seriously hindered
this field. Therefore, some previous methods translate trajectory videos into
step-by-step instructions for expanding data, but such instructions do not
match well with users' communication styles that briefly describe destinations
or state specific needs. Moreover, local navigation trajectories overlook
global context and high-level task planning. To address these issues, we
propose NavRAG, a retrieval-augmented generation (RAG) framework that generates
user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical
scene description tree for 3D scene understanding from global layout to local
details, then simulates various user roles with specific demands to retrieve
from the scene tree, generating diverse instructions with LLM. We annotate over
2 million navigation instructions across 861 scenes and evaluate the data
quality and navigation performance of trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-promptable Propagation for Referring Medical Image Sequence
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runtian Yuan, Jilan Xu, Mohan Chen, Qingqiu Li, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image sequences, generated by both 2D video-based examinations and 3D
imaging techniques, consist of sequential frames or slices that capture the
same anatomical entities (e.g., organs or lesions) from multiple perspectives.
Existing segmentation studies typically process medical images using either 2D
or 3D methods in isolation, often overlooking the inherent consistencies among
these images. Additionally, interactive segmentation, while highly beneficial
in clinical scenarios, faces the challenge of integrating text prompts
effectively across multi-modalities. To address these issues, we introduce an
innovative task, Referring Medical Image Sequence Segmentation for the first
time, which aims to segment the referred anatomical entities corresponding to
medical text prompts. We develop a strong baseline model, Text-Promptable
Propagation (TPP), designed to exploit the intrinsic relationships among
sequential images and their associated textual descriptions. TPP supports the
segmentation of arbitrary objects of interest based on cross-modal prompt
fusion. Carefully designed medical prompts are fused and employed as queries to
guide image sequence segmentation through triple-propagation. We curate a large
and comprehensive benchmark covering 4 modalities and 20 different organs and
lesions. Experimental results consistently demonstrate the superior performance
of our approach compared to previous methods across these datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phantom: Subject-consistent video generation via cross-modal alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, Xinglong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous development of foundational models for video generation is
evolving into various applications, with subject-consistent video generation
still in the exploratory stage. We refer to this as Subject-to-Video, which
extracts subject elements from reference images and generates
subject-consistent video through textual instructions. We believe that the
essence of subject-to-video lies in balancing the dual-modal prompts of text
and image, thereby deeply and simultaneously aligning both text and visual
content. To this end, we propose Phantom, a unified video generation framework
for both single and multi-subject references. Building on existing
text-to-video and image-to-video architectures, we redesign the joint
text-image injection model and drive it to learn cross-modal alignment via
text-image-video triplet data. In particular, we emphasize subject consistency
in human generation, covering existing ID-preserving video generation while
offering enhanced advantages. The project homepage is here
https://phantom-video.github.io/Phantom/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faces of Fairness: Examining Bias in Facial Expression Recognition
  <span class="highlight-title">Dataset</span>s and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mehdi Hosseini, Ali Pourramezan Fard, Mohammad H. Mahoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building AI systems, including Facial Expression Recognition (FER), involves
two critical aspects: data and model design. Both components significantly
influence bias and fairness in FER tasks. Issues related to bias and fairness
in FER datasets and models remain underexplored. This study investigates bias
sources in FER datasets and models. Four common FER datasets--AffectNet, ExpW,
Fer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and
ExpW exhibit high generalizability despite data imbalances. Additionally, this
research evaluates the bias and fairness of six deep models, including three
state-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet,
XceptionNet, as well as three transformer-based models: ViT, CLIP, and
GPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve
the highest accuracy scores, they also display the highest levels of bias.
These findings underscore the urgent need for developing new methodologies to
mitigate bias and ensure fairness in datasets and models, particularly in
affective computing applications. See our implementation details at
https://github.com/MMHosseini/bias_in_FER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Real-Time Generation of Delay-Compensated Video Feeds for
  Outdoor Mobile Robot Teleoperation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Yixiao Fang, Andre Schreiber, Tianchen Ji, Zhe Huang, Aganze Mihigo, Cassidy Wall, Abdulrahman Almana, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperation is an important technology to enable supervisors to control
agricultural robots remotely. However, environmental factors in dense crop rows
and limitations in network infrastructure hinder the reliability of data
streamed to teleoperators. These issues result in delayed and variable frame
rate video feeds that often deviate significantly from the robot's actual
viewpoint. We propose a modular learning-based vision pipeline to generate
delay-compensated images in real-time for supervisors. Our extensive offline
evaluations demonstrate that our method generates more accurate images compared
to state-of-the-art approaches in our setting. Additionally, ours is one of the
few works to evaluate a delay-compensation method in outdoor field environments
with complex terrain on data from a real robot in real-time. Resulting videos
and code are provided at https://sites.google.com/illinois.edu/comp-teleop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICRA 2025; 8 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of anatomical structures and pathological regions in medical
images is essential for modern clinical diagnosis, disease research, and
treatment planning. While significant advancements have been made in deep
learning-based segmentation techniques, many of these methods still suffer from
limitations in data efficiency, generalizability, and interactivity. As a
result, developing precise segmentation methods that require fewer labeled
datasets remains a critical challenge in medical image analysis. Recently, the
introduction of foundation models like CLIP and Segment-Anything-Model (SAM),
with robust cross-domain representations, has paved the way for interactive and
universal image segmentation. However, further exploration of these models for
data-efficient segmentation in medical imaging is still needed and highly
relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that
integrates the CLIP and SAM models to perform segmentation on clinical scans
using text prompts, in both zero-shot and weakly supervised settings. Our
approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard
Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the
Multi-modal Information Bottleneck (M2IB) to create visual prompts for
generating segmentation masks from SAM in the zero-shot setting. We also
investigate using zero-shot segmentation labels within a weakly supervised
paradigm to enhance segmentation quality further. Extensive testing across four
diverse segmentation tasks and medical imaging modalities (breast tumor
ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high
accuracy of our proposed framework. Our code is available at
https://github.com/HealthX-Lab/MedCLIP-SAMv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusing DeBias: a Recipe for Turning a Bug into a Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning model effectiveness in classification tasks is often challenged
by the quality and quantity of training data which, whenever containing strong
spurious correlations between specific attributes and target labels, can result
in unrecoverable biases in model predictions. Tackling these biases is crucial
in improving model generalization and trust, especially in real-world
scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting
as a plug-in for common methods in model debiasing while exploiting the
inherent bias-learning tendency of diffusion models. Our approach leverages
conditional diffusion models to generate synthetic bias-aligned images, used to
train a bias amplifier model, to be further employed as an auxiliary method in
different unsupervised debiasing approaches. Our proposed method, which also
tackles the common issue of training set memorization typical of this type of
tech- niques, beats current state-of-the-art in multiple benchmark datasets by
significant margins, demonstrating its potential as a versatile and effective
tool for tackling dataset bias in deep learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 Pages, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EC-DIT: Scaling Diffusion <span class="highlight-title">Transformer</span>s with Adaptive Expert-Choice
  Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, Nan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have been widely adopted for text-to-image synthesis.
While scaling these models up to billions of parameters shows promise, the
effectiveness of scaling beyond current sizes remains underexplored and
challenging. By explicitly exploiting the computational heterogeneity of image
generations, we develop a new family of Mixture-of-Experts (MoE) models
(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns
to adaptively optimize the compute allocated to understand the input texts and
generate the respective image patches, enabling heterogeneous computation
aligned with varying text-image complexities. This heterogeneity provides an
efficient way of scaling EC-DIT up to 97 billion parameters and achieving
significant improvements in training convergence, text-to-image alignment, and
overall generation quality over dense models and conventional MoE models.
Through extensive ablations, we show that EC-DIT demonstrates superior
scalability and adaptive compute allocation by recognizing varying textual
importance through end-to-end training. Notably, in text-to-image alignment
evaluation, our largest models achieve a state-of-the-art GenEval score of
71.68% and still maintain competitive inference speed with intuitive
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Backdoor Consistency Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengen Wang, Murat Kantarcioglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency models are a new class of models that generate images by directly
mapping noise to data, allowing for one-step generation and significantly
accelerating the sampling process. However, their robustness against
adversarial attacks has not yet been thoroughly investigated. In this work, we
conduct the first study on the vulnerability of consistency models to backdoor
attacks. While previous research has explored backdoor attacks on diffusion
models, those studies have primarily focused on conventional diffusion models,
employing a customized backdoor training process and objective, whereas
consistency models have distinct training processes and objectives. Our
proposed framework demonstrates the vulnerability of consistency models to
backdoor attacks. During image generation, poisoned consistency models produce
images with a Fr\'echet Inception Distance (FID) comparable to that of a clean
model when sampling from Gaussian noise. However, once the trigger is
activated, they generate backdoor target images. We explore various trigger and
target configurations to evaluate the vulnerability of consistency models,
including the use of random noise as a trigger. This novel trigger is visually
inconspicuous, more challenging to detect, and aligns well with the sampling
process of consistency models. Across all configurations, our framework
successfully compromises the consistency models while maintaining high utility
and specificity. We also examine the stealthiness of our proposed attack, which
is attributed to the unique properties of consistency models and the elusive
nature of the Gaussian noise trigger. Our code is available at
\href{https://github.com/chengenw/backdoorCM}{https://github.com/chengenw/backdoorCM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of
  Thought Reasoning with <span class="highlight-title">Multimodal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahraa Al Sahili, Ioannis Patras, Matthew Purver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of text-to-image generative models, biases inherent in training
datasets often propagate into generated content, posing significant ethical
challenges, particularly in socially sensitive contexts. We introduce FairCoT,
a novel framework that enhances fairness in text to image models through Chain
of Thought (CoT) reasoning within multimodal generative large language models.
FairCoT employs iterative CoT refinement to systematically mitigate biases, and
dynamically adjusts textual prompts in real time, ensuring diverse and
equitable representation in generated images. By integrating iterative
reasoning processes, FairCoT addresses the limitations of zero shot CoT in
sensitive scenarios, balancing creativity with ethical responsibility.
Experimental evaluations across popular text-to-image systems including DALLE
and various Stable Diffusion variants, demonstrate that FairCoT significantly
enhances fairness and diversity without sacrificing image quality or semantic
fidelity. By combining robust reasoning, lightweight deployment, and
extensibility to multiple models, FairCoT represents a promising step toward
more socially responsible and transparent AI driven content generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^2$IST: <span class="highlight-title">Multi-Modal</span> Interactive Side-Tuning for Efficient Referring
  Expression Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01131v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01131v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Liu, Ting Liu, Siteng Huang, Yi Xin, Yue Hu, Quanjun Yin, Donglin Wang, Yuanyuan Wu, Honggang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring expression comprehension (REC) is a vision-language task to locate
a target object in an image based on a language expression. Fully fine-tuning
general-purpose pre-trained vision-language foundation models for REC yields
impressive performance but becomes increasingly costly. Parameter-efficient
transfer learning (PETL) methods have shown strong performance with fewer
tunable parameters. However, directly applying PETL to REC faces two
challenges: (1) insufficient multi-modal interaction between pre-trained
vision-language foundation models, and (2) high GPU memory usage due to
gradients passing through the heavy vision-language foundation models. To this
end, we present M$^2$IST: Multi-Modal Interactive Side-Tuning with M$^3$ISAs:
Mixture of Multi-Modal Interactive Side-Adapters. During fine-tuning, we fix
the pre-trained uni-modal encoders and update M$^3$ISAs to enable efficient
vision-language alignment for REC. Empirical results reveal that M$^2$IST
achieves better performance-efficiency trade-off than full fine-tuning and
other PETL methods, requiring only 2.11% tunable parameters, 39.61% GPU memory,
and 63.46% training time while maintaining competitive performance. Our code is
released at https://github.com/xuyang-liu16/M2IST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is released at https://github.com/xuyang-liu16/M2IST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression with Global Guidance: Towards Training-free High-Resolution
  MLLMs Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05179v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05179v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Bo Zheng, Linfeng Zhang, Siteng Huang, Honggang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have attracted considerable
attention due to their exceptional performance in visual content understanding
and reasoning. However, their inference efficiency has been a notable concern,
as the increasing length of multimodal contexts leads to quadratic complexity.
Token compression techniques, which reduce the number of visual tokens, have
demonstrated their effectiveness in reducing computational costs. Yet, these
approaches have struggled to keep pace with the rapid advancements in MLLMs,
especially the AnyRes strategy in the context of high-resolution image
understanding. In this paper, we propose a novel token compression method,
GlobalCom$^2$, tailored for high-resolution MLLMs that receive both the
thumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the
thumbnail as the "commander" of the entire token compression process, directing
the allocation of retention ratios and the specific compression for each crop.
In this way, redundant tokens are eliminated while important local details are
adaptively preserved to the highest extent feasible. Empirical results across
10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between
performance and efficiency, and consistently outperforms state-of-the-art token
compression methods with LLaVA-NeXT-7B/13B models. Our code is released at
https://github.com/xuyang-liu16/GlobalCom2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Intent Understanding for Ambiguous Prompts through
  Human-Machine Co-Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangfan He, Jianhui Wang, Yijin Wang, Kun Li, Li Sun, Jiayi Su, Jingyuan Lu, Jinhua Song, Haoyuan Li, Sida Li, Tianyu Shi, Miao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's image generation systems are capable of producing realistic and
high-quality images. However, user prompts often contain ambiguities, making it
difficult for these systems to interpret users' actual intentions.
Consequently, many users must modify their prompts several times to ensure the
generated images meet their expectations. While some methods focus on enhancing
prompts to make the generated images fit user needs, the model is still hard to
understand users' real needs, especially for non-expert users. In this
research, we aim to enhance the visual parameter-tuning process, making the
model user-friendly for individuals without specialized knowledge and better
understand user needs. We propose a human-machine co-adaption strategy using
mutual information between the user's prompts and the pictures under
modification as the optimizing target to make the system better adapt to user
needs. We find that an improved model can reduce the necessity for multiple
rounds of adjustments. We also collect multi-round dialogue datasets with
prompts and images pairs and user intent. Various experiments demonstrate the
effectiveness of the proposed method in our proposed dataset. Our annotation
tools and several examples of our dataset are available at
https://zenodo.org/records/14876029 for easier review. And we will open source
our full dataset and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Libra: Leveraging Temporal Images for Biomedical Radiology Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology report generation (RRG) requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. While multimodal
large language models (MLLMs) align with pre-trained vision encoders to enhance
visual-language understanding, most existing methods rely on single-image
analysis or rule-based heuristics to process multiple images, failing to fully
leverage temporal information in multi-modal medical datasets. In this paper,
we introduce Libra, a temporal-aware MLLM tailored for chest X-ray report
generation. Libra combines a radiology-specific image encoder with a novel
Temporal Alignment Connector (TAC), designed to accurately capture and
integrate temporal differences between paired current and prior images.
Extensive experiments on the MIMIC-CXR dataset demonstrate that Libra
establishes a new state-of-the-art benchmark among similarly scaled MLLMs,
setting new standards in both clinical relevance and lexical accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures, Adding Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Previous Steps: A Training-free Fast Solver for Flow
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Song, Hanjiang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow diffusion models (FDMs) have recently shown potential in generation
tasks due to the high generation quality. However, the current ordinary
differential equation (ODE) solver for FDMs, e.g., the Euler solver, still
suffers from slow generation since ODE solvers need many number function
evaluations (NFE) to keep high-quality generation. In this paper, we propose a
novel training-free flow-solver to reduce NFE while maintaining high-quality
generation. The key insight for the flow-solver is to leverage the previous
steps to reduce the NFE, where a cache is created to reuse these results from
the previous steps. Specifically, the Taylor expansion is first used to
approximate the ODE. To calculate the high-order derivatives of Taylor
expansion, the flow-solver proposes to use the previous steps and a polynomial
interpolation to approximate it, where the number of orders we could
approximate equals the number of previous steps we cached. We also prove that
the flow-solver has a more minor approximation error and faster generation
speed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,
LSUN-Church, ImageNet, and real text-to-image generation prove the efficiency
of the flow-solver. Specifically, the flow-solver improves the FID-30K from
13.79 to 6.75, from 46.64 to 19.49 with $\text{NFE}=10$ on CIFAR-10 and
LSUN-Church, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human alignment of neural network representations <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01201v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01201v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Muttenthaler, Jonas Dippel, Lorenz Linhardt, Robert A. Vandermeulen, Simon Kornblith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's computer vision models achieve human or near-human level performance
across a wide variety of vision tasks. However, their architectures, data, and
learning algorithms differ in numerous ways from those that give rise to human
vision. In this paper, we investigate the factors that affect the alignment
between the representations learned by neural networks and human mental
representations inferred from behavioral responses. We find that model scale
and architecture have essentially no effect on the alignment with human
behavioral responses, whereas the training dataset and objective function both
have a much larger impact. These findings are consistent across three datasets
of human similarity judgments collected using two different tasks. Linear
transformations of neural network representations learned from behavioral
responses from one dataset substantially improve alignment with human
similarity judgments on the other two datasets. In addition, we find that some
human concepts such as food and animals are well-represented by neural networks
whereas others such as royal or sports-related objects are not. Overall,
although models trained on larger, more diverse datasets achieve better
alignment with humans than models trained on ImageNet alone, our results
indicate that scaling alone is unlikely to be sufficient to train neural
networks with conceptual representations that match those used by humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mammo-Clustering: A Weakly Supervised Multi-view Tri-level Information
  Fusion Context Clustering Network for Localization and Classification in
  Mammo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Yang, Chulong Zhang, Qi Zang, Juan Yu, Liang Zeng, Xiao Luo, Yexuan Xing, Xin Pan, Qi Li, Xiaokun Liang, Yaoqin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a significant global health issue, and the diagnosis of
breast imaging has always been challenging. Mammography images typically have
extremely high resolution, with lesions occupying only a very small area.
Down-sampling in neural networks can easily lead to the loss of
microcalcifications or subtle structures, making it difficult for traditional
neural network architectures to address these issues. To tackle these
challenges, we propose a Context Clustering Network with triple information
fusion. Firstly, compared to CNNs or transformers, we find that Context
clustering methods (1) are more computationally efficient and (2) can more
easily associate structural or pathological features, making them suitable for
the clinical tasks of mammography. Secondly, we propose a triple information
fusion mechanism that integrates global information, feature-based local
information, and patch-based local information. The proposed approach is
rigorously evaluated on two public datasets, Vindr-Mammo and CBIS-DDSM, using
five independent splits to ensure statistical robustness. Our method achieves
an AUC of 0.828 on Vindr-Mammo and 0.805 on CBIS-DDSM, outperforming the next
best method by 3.1% and 2.4%, respectively. These improvements are
statistically significant (p<0.05), underscoring the benefits of Context
Clustering Network with triple information fusion. Overall, our Context
Clustering framework demonstrates strong potential as a scalable and
cost-effective solution for large-scale mammography screening, enabling more
efficient and accurate breast cancer detection. Access to our method is
available at https://github.com/Sohyu1/Mammo_Clustering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCGAN: Enhancing GAN Training with Regression-Based Generator Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17191v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17191v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoren Xiao, Hao Ni, Weixin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial networks (GANs) have emerged as a powerful tool for
generating high-fidelity data. However, the main bottleneck of existing
approaches is the lack of supervision on the generator training, which often
results in undamped oscillation and unsatisfactory performance. To address this
issue, we propose an algorithm called Monte Carlo GAN (MCGAN). This approach,
utilizing an innovative generative loss function, termly the regression loss,
reformulates the generator training as a regression task and enables the
generator training by minimizing the mean squared error between the
discriminator's output of real data and the expected discriminator of fake
data. We demonstrate the desirable analytic properties of the regression loss,
including discriminability and optimality, and show that our method requires a
weaker condition on the discriminator for effective generator training. These
properties justify the strength of this approach to improve the training
stability while retaining the optimality of GAN by leveraging strong
supervision of the regression loss. Extensive experiments on diverse datasets,
including image data (CIFAR-10/100, FFHQ256, ImageNet, and LSUN Bedroom), time
series data (VAR and stock data) and video data, are conducted to demonstrate
the flexibility and effectiveness of our proposed MCGAN. Numerical results show
that the proposed MCGAN is versatile in enhancing a variety of backbone GAN
models and achieves consistent and significant improvement in terms of quality,
accuracy, training stability, and learned latent space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Calorimeter: Migrating Visual Object Detector to High-energy
  Particle Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongtian Yu, Yangu Li, Yunfan Liu, Yunxuan Song, Xiaorui Lyu, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-energy physics, accurately estimating the kinematic parameters
(position and momentum) of anti-neutrons ($\bar{n}$) is essential for exploring
the fundamental governing principles. However, this process is particularly
challenging when using an electromagnetic calorimeter (EMC) as the energy
detector, due to their limited accuracy and efficiency in interacting with
$\bar{n}$. To address this issue, we propose Vision Calorimeter (ViC), a
data-driven framework which migrates visual object detection techniques to
high-energy particle images. To accommodate the unique characteristics of
particle images, we introduce the heat-conduction operator (HCO) into both the
backbone and the head of the conventional object detector and conduct
significant structural improvements. HCO enjoys the advantage of both radial
prior and global attention, as it is inspired by physical heat conduction which
naturally aligns with the pattern of particle incidence. Implemented via the
Discrete Cosine Transform (DCT), HCO extracts frequency-domain features,
bridging the distribution gap between the particle images and the natural
images on which visual object detectors are pre-trained. Experimental results
demonstrate that ViC significantly outperforms traditional approaches, reducing
the incident position prediction error by 46.16% (from 17.31$^{\circ}$ to
9.32$^{\circ}$) and providing the first baseline result with an incident
momentum regression error of 21.48%. This study underscores ViC's great
potential as a general-purpose particle parameter estimator in high-energy
physics. Code is available at https://github.com/yuhongtian17/ViC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D
  Facial Animation Synthesis Using VQ-VAE <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07966v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07966v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven 3D facial animation synthesis has been an active field of
research with attention from both academia and industry. While there are
promising results in this area, recent approaches largely focus on lip-sync and
identity control, neglecting the role of emotions and emotion control in the
generative process. That is mainly due to the lack of emotionally rich facial
animation data and algorithms that can synthesize speech animations with
emotional expressions at the same time. In addition, majority of the models are
deterministic, meaning given the same audio input, they produce the same output
motion. We argue that emotions and non-determinism are crucial to generate
diverse and emotionally-rich facial animations. In this paper, we propose
ProbTalk3D a non-deterministic neural network approach for emotion controllable
speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and
an emotionally rich facial animation dataset 3DMEAD. We provide an extensive
comparative analysis of our model against the recent 3D facial animation
synthesis approaches, by evaluating the results objectively, qualitatively, and
with a perceptual user study. We highlight several objective metrics that are
more suitable for evaluating stochastic outputs and use both in-the-wild and
ground truth data for subjective evaluation. To our knowledge, that is the
first non-deterministic 3D facial animation synthesis method incorporating a
rich emotion dataset and emotion control with emotion labels and intensity
levels. Our evaluation demonstrates that the proposed model achieves superior
performance compared to state-of-the-art emotion-controlled, deterministic and
non-deterministic models. We recommend watching the supplementary video for
quality judgement. The entire codebase is publicly available
(https://github.com/uuembodiedsocialai/ProbTalk3D/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM
  SIGGRAPH MIG 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cost-Effective Attention Mechanisms for Low Resource Settings: Necessity
  & Sufficiency of Linear Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Hosseini, Mehran Hosseini, Ignacio Castro, Matthew Purver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From natural language processing to vision, Scaled Dot Product Attention
(SDPA) is the backbone of most modern deep learning applications.
Unfortunately, its memory and computational requirements can be prohibitive in
low-resource settings. In this paper, we improve its efficiency without
sacrificing its versatility. We propose three attention variants where we
remove consecutive linear transformations or add a novel one, and evaluate them
on a range of standard NLP and vision tasks. Our proposed models are
substantially lighter than standard SDPA (and have 25-50% fewer parameters). We
show that the performance cost of these changes is negligible relative to size
reduction and that in one case (Super Attention) we succeed in outperforming
SDPA by up to 10% while improving its speed and reducing its parameters by 25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refinement Module based on Parse <span class="highlight-title">Graph</span> of Feature Map for Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibang Liu, Xuemei Xie, Guangming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parse graphs of the human body can be obtained in the human brain to help
humans complete the human pose estimation (HPE). It contains a hierarchical
structure, like a tree structure, and context relations among nodes. Many
researchers predefine the parse graph of body structure to design HPE
frameworks. However, these frameworks struggle to adapt to instances that
deviate from the predefined parse graph and are often parameter-heavy. Unlike
them, we view the feature map holistically, much like the human body. It can be
optimized using parse graphs, where each node's feature is an implicit
expression rather than a fixed one. This allows it to adapt to more instances,
unconstrained by rigid structural features. In this paper, we design the
Refinement Module based on the Parse Graph of feature map (RMPG), which
includes two stages: top-down decomposition and bottom-up combination. In the
first stage, the feature map is decomposed into multiple sub-feature maps along
the channel. In the second stage, the context relations of sub-feature maps are
calculated to obtain their respective context information and the sub-feature
maps with context information are concatenated along channels to obtain the
refined feature map. Additionally, we design a hierarchical network with fewer
parameters using multiple RMPG modules for HPE according to the parse graph of
body structure, some of which are supervised to obtain context relations among
body parts. Our network achieves excellent results on multiple mainstream human
pose datasets. More importantly, the effectiveness of RMPG is proven on
different methods. The code of RMPG will be open.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Enhancement of CNN Algorithm for Rice Leaf Disease Image
  Classification in Mobile Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kayne Uriel K. Rodrigo, Jerriane Hillary Heart S. Marcial, Samuel C. Brillo, Khatalyn E. Mata, Jonathan C. Morano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on enhancing rice leaf disease image classification
algorithms, which have traditionally relied on Convolutional Neural Network
(CNN) models. We employed transfer learning with MobileViTV2_050 using
ImageNet-1k weights, a lightweight model that integrates CNN's local feature
extraction with Vision Transformers' global context learning through a
separable self-attention mechanism. Our approach resulted in a significant
15.66% improvement in classification accuracy for MobileViTV2_050-A, our first
enhanced model trained on the baseline dataset, achieving 93.14%. Furthermore,
MobileViTV2_050-B, our second enhanced model trained on a broader rice leaf
dataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy.
Additionally, MobileViTV2-A attained an F1-score of 93% across four rice labels
and a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In
terms of resource consumption, our enhanced models reduced the total parameters
of the baseline CNN model by up to 92.50%, from 14 million to 1.1 million.
These results indicate that MobileViTV2_050 not only improves computational
efficiency through its separable self-attention mechanism but also enhances
global context learning. Consequently, it offers a lightweight and robust
solution suitable for mobile deployment, advancing the interpretability and
practicality of models in precision agriculture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at 46th World Conference on Applied Science, Engineering &
  Technology (WCASET) from Institute for Educational Research and Publication
  (IFERP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian multi-target filtering with target dynamics driven by a
  stochastic differential equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ángel F. García-Fernández, Simo Särkkä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes multi-target filtering algorithms in which target
dynamics are given in continuous time and measurements are obtained at discrete
time instants. In particular, targets appear according to a Poisson point
process (PPP) in time with a given Gaussian spatial distribution, targets move
according to a general time-invariant linear stochastic differential equation,
and the life span of each target is modelled with an exponential distribution.
For this multi-target dynamic model, we derive the distribution of the set of
new born targets and calculate closed-form expressions for the best fitting
mean and covariance of each target at its time of birth by minimising the
Kullback-Leibler divergence via moment matching. This yields a novel Gaussian
continuous-discrete Poisson multi-Bernoulli mixture (PMBM) filter, and its
approximations based on Poisson multi-Bernoulli and probability hypothesis
density filtering. These continuous-discrete multi-target filters are also
extended to target dynamics driven by nonlinear stochastic differential
equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Matlab code available at https://github.com/Agarciafernandez</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan, Nan Duan, Yibo Zhu, Daxin Jiang, Hong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have shown remarkable performance in modeling
and generating high-quality videos. However, the quadratic computational
complexity of 3D full attention mechanism presents significant challenges in
scaling video DiT training, especially for high-definition and lengthy videos,
where attention can dominate up to 95% of the end-to-end time and necessitate
specialized communication paradigms to handle large input sizes.
  This paper introduces DSV, a novel framework designed to accelerate and scale
the training of video DiTs by leveraging the inherent dynamic attention
sparsity throughout the training process. DSV employs a two-stage training
algorithm that exploits sparsity patterns, focusing on critical elements
supported by efficient, tailored kernels. To accommodate the new sparsity
dimension, we develop a hybrid sparsity-aware context parallelism that
effectively scales to large inputs by addressing the heterogeneity of sparsity
across attention heads and blocks, resulting in optimized sparse computation
and communication. Extensive evaluations demonstrate that DSV achieves up to
3.02x gain in training throughput with nearly no quality degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Spatiotemporal Approach to Tri-Perspective Representation for 3D
  Semantic Occupancy Prediction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathira Silva, Savindu Bhashitha Wannigama, Gihan Jayatilaka, Muhammad Haris Khan, Roshan Ragel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Holistic understanding and reasoning in 3D scenes are crucial for the success
of autonomous driving systems. The evolution of 3D semantic occupancy
prediction as a pretraining task for autonomous driving and robotic
applications captures finer 3D details compared to traditional 3D detection
methods. Vision-based 3D semantic occupancy prediction is increasingly
overlooked in favor of LiDAR-based approaches, which have shown superior
performance in recent years. However, we present compelling evidence that there
is still potential for enhancing vision-based methods. Existing approaches
predominantly focus on spatial cues such as tri-perspective view (TPV)
embeddings, often overlooking temporal cues. This study introduces S2TPVFormer,
a spatiotemporal transformer architecture designed to predict temporally
coherent 3D semantic occupancy. By introducing temporal cues through a novel
Temporal Cross-View Hybrid Attention mechanism (TCVHA), we generate
Spatiotemporal TPV (S2TPV) embeddings that enhance the prior process.
Experimental evaluations on the nuScenes dataset demonstrate a significant
+4.1% of absolute gain in mean Intersection over Union (mIoU) for 3D semantic
occupancy compared to baseline TPVFormer, validating the effectiveness of
S2TPVFormer in advancing 3D scene perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2025 Workshop on Machine Learning for Autonomous
  Driving at AAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving Skeletons: Motion Dynamics in Action Recognition <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushang Qiu, Lei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition has gained significant attention for its
ability to efficiently represent spatiotemporal information in a lightweight
format. Most existing approaches use graph-based models to process skeleton
sequences, where each pose is represented as a skeletal graph structured around
human physical connectivity. Among these, the Spatiotemporal Graph
Convolutional Network (ST-GCN) has become a widely used framework.
Alternatively, hypergraph-based models, such as the Hyperformer, capture
higher-order correlations, offering a more expressive representation of complex
joint interactions. A recent advancement, termed Taylor Videos, introduces
motion-enhanced skeleton sequences by embedding motion concepts, providing a
fresh perspective on interpreting human actions in skeleton-based action
recognition. In this paper, we conduct a comprehensive evaluation of both
traditional skeleton sequences and Taylor-transformed skeletons using ST-GCN
and Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal
graph and hypergraph representations, analyzing static poses against
motion-injected poses. Our findings highlight the strengths and limitations of
Taylor-transformed skeletons, demonstrating their potential to enhance motion
dynamics while exposing current challenges in fully using their benefits. This
study underscores the need for innovative skeletal modelling techniques to
effectively handle motion-rich data and advance the field of action
recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Companion Proceedings of the ACM Web Conference (WWW
  Companion 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03677v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03677v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Huang, Ziyu Xu, Hai Wu, Jinlong Wang, Qiming Xia, Yan Xia, Jonathan Li, Kyle Gao, Chenglu Wen, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based vision systems are integral for 3D object detection, which is
crucial for autonomous navigation. However, they suffer from performance
degradation in adverse weather conditions due to the quality deterioration of
LiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is
expected to solve this problem. However, the fusion of LiDAR and 4D radar is
challenging because they differ significantly in terms of data quality and the
degree of degradation in adverse weather. To address these issues, we introduce
L4DR, a weather-robust 3D object detection method that effectively achieves
LiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and
Foreground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is
the first exploration of the complementarity of early fusion between LiDAR and
4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )
parallel feature extraction backbone coupled with a Multi-Scale Gated Fusion
(MSGF) module to counteract the varying degrees of sensor degradation under
adverse weather conditions. Experimental evaluation on a VoD dataset with
simulated fog proves that L4DR is more adaptable to changing weather
conditions. It delivers a significant performance increase under different fog
levels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only
approach. Moreover, the results on the K-Radar dataset validate the consistent
performance improvement of L4DR in real-world adverse weather conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025(Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G$^2$V$^2$former: <span class="highlight-title">Graph</span> Guided Video Vision <span class="highlight-title">Transformer</span> for Face
  Anti-Spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In videos containing spoofed faces, we may uncover the spoofing evidence
based on either photometric or dynamic abnormality, even a combination of both.
Prevailing face anti-spoofing (FAS) approaches generally concentrate on the
single-frame scenario, however, purely photometric-driven methods overlook the
dynamic spoofing clues that may be exposed over time. This may lead FAS systems
to conclude incorrect judgments, especially in cases where it is easily
distinguishable in terms of dynamics but challenging to discern in terms of
photometrics. To this end, we propose the Graph Guided Video Vision Transformer
(G$^2$V$^2$former), which combines faces with facial landmarks for photometric
and dynamic feature fusion. We factorize the attention into space and time, and
fuse them via a spatiotemporal block. Specifically, we design a novel temporal
attention called Kronecker temporal attention, which has a wider receptive
field, and is beneficial for capturing dynamic information. Moreover, we
leverage the low-semantic motion of facial landmarks to guide the high-semantic
change of facial expressions based on the motivation that regions containing
landmarks may reveal more dynamic clues. Extensive experiments on nine
benchmark datasets demonstrate that our method achieves superior performance
under various scenarios. The codes will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask Approximation Net: A Novel Diffusion Model Approach for Remote
  Sensing Change Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwei Sun, Jing Yao, Changsheng Zhou, Xiangyong Cao, Pedram Ghamisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image change description represents an innovative multimodal
task within the realm of remote sensing processing. This task not only
facilitates the detection of alterations in surface conditions, but also
provides comprehensive descriptions of these changes, thereby improving human
interpretability and interactivity.Generally, existing deep-learning-based
methods predominantly utilized a three-stage framework that successively
perform feature extraction, feature fusion, and localization from bitemporal
images before text generation. However, this reliance often leads to an
excessive focus on the design of specific network architectures and restricts
the feature distributions to the dataset at hand, which in turn results in
limited generalizability and robustness during application.To address these
limitations, this paper proposes a novel approach for remote sensing image
change detection and description that incorporates diffusion models, aiming to
transition the emphasis of modeling paradigms from conventional feature
learning to data distribution learning. The proposed method primarily includes
a simple multi-scale change detection module, whose output features are
subsequently refined by an well-designed diffusion model. Furthermore, we
introduce a frequency-guided complex filter module to boost the model
performance by managing high-frequency noise throughout the diffusion process.
We validate the effectiveness of our proposed method across several datasets
for remote sensing change detection and description, showcasing its superior
performance compared to existing techniques. The code will be available at
\href{https://github.com/sundongwei}{MaskApproxNet} after a possible
publication.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MemeSense: An Adaptive In-Context Framework for Social Commonsense
  Driven Meme Moderation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Adak, Somnath Banerjee, Rajarshi Mandal, Avik Halder, Sayan Layek, Rima Hazra, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memes present unique moderation challenges due to their subtle, multimodal
interplay of images, text, and social context. Standard systems relying
predominantly on explicit textual cues often overlook harmful content
camouflaged by irony, symbolism, or cultural references. To address this gap,
we introduce MemeSense, an adaptive in-context learning framework that fuses
social commonsense reasoning with visually and semantically related reference
examples. By encoding crucial task information into a learnable cognitive shift
vector, MemeSense effectively balances lexical, visual, and ethical
considerations, enabling precise yet context-aware meme intervention. Extensive
evaluations on a curated set of implicitly harmful memes demonstrate that
MemeSense substantially outperforms strong baselines, paving the way for safer
online communities. Code and data available at:
https://github.com/sayantan11995/MemeSense
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at:
  https://github.com/sayantan11995/MemeSense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSP: A Simulator For Multi-Agent Ranking Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommy Mordo, Tomer Kordonsky, Haya Nachimovsky, Moshe Tennenholtz, Oren Kurland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In ranking competitions, document authors compete for the highest rankings by
modifying their content in response to past rankings. Previous studies focused
on human participants, primarily students, in controlled settings. The rise of
generative AI, particularly Large Language Models (LLMs), introduces a new
paradigm: using LLMs as document authors. This approach addresses scalability
constraints in human-based competitions and reflects the growing role of
LLM-generated content on the web-a prime example of ranking competition. We
introduce a highly configurable ranking competition simulator that leverages
LLMs as document authors. It includes analytical tools to examine the resulting
datasets. We demonstrate its capabilities by generating multiple datasets and
conducting an extensive analysis. Our code and datasets are publicly available
for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Scientific Document Retrieval with Concept Coverage-based
  Query Set Generation <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeongKu Kang, Bowen Jin, Wonbin Kweon, Yu Zhang, Dongha Lee, Jiawei Han, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In specialized fields like the scientific domain, constructing large-scale
human-annotated datasets poses a significant challenge due to the need for
domain expertise. Recent methods have employed large language models to
generate synthetic queries, which serve as proxies for actual user queries.
However, they lack control over the content generated, often resulting in
incomplete coverage of academic concepts in documents. We introduce Concept
Coverage-based Query set Generation (CCQGen) framework, designed to generate a
set of queries with comprehensive coverage of the document's concepts. A key
distinction of CCQGen is that it adaptively adjusts the generation process
based on the previously generated queries. We identify concepts not
sufficiently covered by previous queries, and leverage them as conditions for
subsequent query generation. This approach guides each new query to complement
the previous ones, aiding in a thorough understanding of the document.
Extensive experiments demonstrate that CCQGen significantly enhances query
quality and retrieval performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gumbel Reranking: Differentiable End-to-End Reranker Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG systems rely on rerankers to identify relevant documents. However,
fine-tuning these models remains challenging due to the scarcity of annotated
query-document pairs. Existing distillation-based approaches suffer from
training-inference misalignment and fail to capture interdependencies among
candidate documents. To overcome these limitations, we reframe the reranking
process as an attention-mask problem and propose Gumbel Reranking, an
end-to-end training framework for rerankers aimed at minimizing the
training-inference gap. In our approach, reranker optimization is reformulated
as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel
Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end
optimization by minimizing the overall language loss. Experiments across
various settings consistently demonstrate performance gains, including a 10.4\%
improvement in recall on HotpotQA for distinguishing indirectly relevant
documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graceful forgetting: Memory as a process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alain de Cheveigné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A rational theory of memory is proposed to explain how we can accommodate
unbounded sensory input within bounded storage space. Memory is stored as
statistics, organized into complex structures that are constantly summarized
and compressed to make room for new input. This process, driven by space
constraints, is guided by heuristics that optimize the memory for future needs.
Sensory input is rapidly encoded as simple statistics that are more slowly
elaborated into more abstract constructs. This theory differs from previous
accounts of memory by (a) its reliance on statistics, (b) its use of heuristics
to guide the choice of statistics, and (c) the emphasis on memory as a process
that is intensive, complex, and expensive. The theory is intended as an aid to
make sense of our extensive knowledge of memory, and bring us closer to an
understanding of memory in functional and mechanistic terms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinMTEB: Finance Massive Text Embedding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advances in large language
models (LLMs) have further enhanced the performance of embedding models. While
these models are often benchmarked on general-purpose datasets, real-world
applications demand domain-specific evaluation. In this work, we introduce the
Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart
to MTEB designed for the financial domain. FinMTEB comprises 64 financial
domain-specific embedding datasets across 7 tasks that cover diverse textual
types in both Chinese and English, such as financial news articles, corporate
annual reports, ESG reports, regulatory filings, and earnings call transcripts.
We also develop a finance-adapted model, FinPersona-E5, using a persona-based
data synthetic method to cover diverse financial embedding tasks for training.
Through extensive evaluation of 15 embedding models, including FinPersona-E5,
we show three key findings: (1) performance on general-purpose benchmarks shows
limited correlation with financial domain tasks; (2) domain-adapted models
consistently outperform their general-purpose counterparts; and (3)
surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated
dense embeddings in financial Semantic Textual Similarity (STS) tasks,
underscoring current limitations in dense embedding techniques. Our work
establishes a robust evaluation framework for financial NLP applications and
provides crucial insights for developing domain-specific embedding models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/FinMTEB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuOTE: Question-Oriented Text Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Neeser, Kaylen Latimer, Aadyant Khatri, Chris Latimer, Naren Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to
retrieval-augmented generation (RAG) systems, aimed at improving document
representation for accurate and nuanced retrieval. Unlike traditional RAG
pipelines, which rely on embedding raw text chunks, QuOTE augments chunks with
hypothetical questions that the chunk can potentially answer, enriching the
representation space. This better aligns document embeddings with user query
semantics, and helps address issues such as ambiguity and context-dependent
relevance. Through extensive experiments across diverse benchmarks, we
demonstrate that QuOTE significantly enhances retrieval accuracy, including in
multi-hop question-answering tasks. Our findings highlight the versatility of
question generation as a fundamental indexing strategy, opening new avenues for
integrating question generation into retrieval-based AI pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashCheck: Exploration of Efficient Evidence Retrieval for Fast
  Fact-Checking <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Nanekhan, Venktesh V, Erik Martin, Henrik Vatndal, Vinay Setty, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advances in digital tools have led to the rampant spread of
misinformation. While fact-checking aims to combat this, manual fact-checking
is cumbersome and not scalable. It is essential for automated fact-checking to
be efficient for aiding in combating misinformation in real-time and at the
source. Fact-checking pipelines primarily comprise a knowledge retrieval
component which extracts relevant knowledge to fact-check a claim from large
knowledge sources like Wikipedia and a verification component. The existing
works primarily focus on the fact-verification part rather than evidence
retrieval from large data collections, which often face scalability issues for
practical applications such as live fact-checking. In this study, we address
this gap by exploring various methods for indexing a succinct set of factual
statements from large collections like Wikipedia to enhance the retrieval phase
of the fact-checking pipeline. We also explore the impact of vector
quantization to further improve the efficiency of pipelines that employ dense
retrieval approaches for first-stage retrieval. We study the efficiency and
effectiveness of the approaches on fact-checking datasets such as HoVer and
WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the
real-world utility of the efficient retrieval approaches by fact-checking 2024
presidential debate and also open source the collection of claims with
corresponding labels identified in the debate. Through a combination of indexed
facts together with Dense retrieval and Index compression, we achieve up to a
10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the
classical fact-checking pipelines over large collections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECIR 2025, 15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion
  in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonduk Seo, Seunghyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion is widely used in Information Retrieval (IR) to improve
search outcomes by enriching queries with additional contextual information.
Although recent Large Language Model (LLM) based methods generate
pseudo-relevant content and expanded terms via multiple prompts, they often
yield repetitive, narrow expansions that lack the diverse context needed to
retrieve all relevant information. In this paper, we introduce QA-Expand, a
novel and effective framework for query expansion. It first generates multiple
relevant questions from the initial query and subsequently produces
corresponding pseudo-answers as surrogate documents. A feedback model further
rewrites and filters these answers to ensure only the most informative
augmentations are incorporated. Extensive experiments on benchmarks such as
BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up
to 13% over state-of-the-art methods, offering a robust solution for modern
retrieval challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models
  for Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen Huang, Yong Dou, Xuhui Jiang, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown promising potential in
knowledge intensive question answering (QA). However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-guided
Retrieval-Augmented Generation with LMs, which explicitly introduces rules for
in-context learning (RuleRAG-ICL) to guide retrievers to recall related
documents in the directions of rules and uniformly guide generators to reason
attributed by the same rules. Moreover, most existing RAG datasets were
constructed without considering rules and Knowledge Graphs (KGs) are recognized
as providing high-quality rules. Therefore, we construct five rule-aware RAG
benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval
and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL
improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of
+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,
experiments on four existing RAG datasets show RuleRAG is also effective by
offering rules in RuleQA to them, further proving the generalization of rule
guidance in RuleRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A
  Benchmark and Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13694v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13694v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yu, Mingyue Cheng, Jiqian Yang, Jie Ouyang, Yucong Luo, Chenyi Lei, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is increasingly recognized as an
effective approach to mitigating the hallucination of large language models
(LLMs) through the integration of external knowledge. While numerous efforts,
most studies focus on a single type of external knowledge source. In contrast,
most real-world applications involve diverse knowledge from various sources, a
scenario that has been relatively underexplored. The main dilemma is the lack
of a suitable dataset incorporating multiple knowledge sources and
pre-exploration of the associated issues. To address these challenges, we
standardize a benchmark dataset that combines structured and unstructured
knowledge across diverse and complementary domains. Building upon the dataset,
we identify the limitations of existing methods under such conditions.
Therefore, we develop PruningRAG, a plug-and-play RAG framework that uses
multi-granularity pruning strategies to more effectively incorporate relevant
context and mitigate the negative impact of misleading information. Extensive
experimental results demonstrate superior performance of PruningRAG and our
insightful findings are also reported. Our dataset and code are publicly
available\footnote{https://github.com/USTCAGI/PruningRAG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triple Modality Fusion: Aligning Visual, Textual, and <span class="highlight-title">Graph</span> Data with
  Large Language Models for Multi-Behavior Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyi Ma, Xiaohan Li, Zezhong Fan, Kai Zhao, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sushant Kumar, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating diverse data modalities is crucial for enhancing the performance
of personalized recommendation systems. Traditional models, which often rely on
singular data sources, lack the depth needed to accurately capture the
multifaceted nature of item features and user behaviors. This paper introduces
a novel framework for multi-behavior recommendations, leveraging the fusion of
triple-modality, which is visual, textual, and graph data through alignment
with large language models (LLMs). By incorporating visual information, we
capture contextual and aesthetic item characteristics; textual data provides
insights into user interests and item features in detail; and graph data
elucidates relationships within the item-behavior heterogeneous graphs. Our
proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs
to align and integrate these three modalities, achieving a comprehensive
representation of user behaviors. The LLM models the user's interactions
including behaviors and item features in natural languages. Initially, the LLM
is warmed up using only natural language-based prompts. We then devise the
modality fusion module based on cross-attention and self-attention mechanisms
to integrate different modalities from other models into the same embedding
space and incorporate them into an LLM. Extensive experiments demonstrate the
effectiveness of our approach in improving recommendation accuracy. Further
ablation studies validate the effectiveness of our model design and benefits of
the TMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Data to Decisions: The Transformational Power of Machine Learning
  in Business Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kapilya Gangadharan, K. Malathi, Anoop Purandaran, Barathi Subramanian, Rathinaraja Jeyaraj, Soon Ki Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research aims to explore the impact of Machine Learning (ML) on the
evolution and efficacy of Recommendation Systems (RS), particularly in the
context of their growing significance in commercial business environments.
Methodologically, the study delves into the role of ML in crafting and refining
these systems, focusing on aspects such as data sourcing, feature engineering,
and the importance of evaluation metrics, thereby highlighting the iterative
nature of enhancing recommendation algorithms. The deployment of Recommendation
Engines (RE), driven by advanced algorithms and data analytics, is explored
across various domains, showcasing their significant impact on user experience
and decision-making processes. These engines not only streamline information
discovery and enhance collaboration but also accelerate knowledge acquisition,
proving vital in navigating the digital landscape for businesses. They
contribute significantly to sales, revenue, and the competitive edge of
enterprises by offering improved recommendations that align with individual
customer needs. The research identifies the increasing expectation of users for
a seamless, intuitive online experience, where content is personalized and
dynamically adapted to changing preferences. Future research directions include
exploring advancements in deep learning models, ethical considerations in the
deployment of RS, and addressing scalability challenges. This study emphasizes
the indispensability of comprehending and leveraging ML in RS for researchers
and practitioners, to tap into the full potential of personalized
recommendation in commercial business prospects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Embedding Spectrum for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowen Peng, Kazunari Sugiyama, Xin Liu, Tsunenori Mine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommender systems heavily rely on high-quality representations
learned from high-dimensional sparse data. While significant efforts have been
invested in designing powerful algorithms for extracting user preferences, the
factors contributing to good representations have remained relatively
unexplored. In this work, we shed light on an issue in the existing pair-wise
learning paradigm (i.e., the embedding collapse problem), that the
representations tend to span a subspace of the whole embedding space, leading
to a suboptimal solution and reducing the model capacity. Specifically,
optimization on observed interactions is equivalent to a low pass filter
causing users/items to have the same representations and resulting in a
complete collapse. While negative sampling acts as a high pass filter to
alleviate the collapse by balancing the embedding spectrum, its effectiveness
is only limited to certain losses, which still leads to an incomplete collapse.
To tackle this issue, we propose a novel method called DirectSpec, acting as a
reliable all pass filter to balance the spectrum distribution of the embeddings
during training, ensuring that users/items effectively span the entire
embedding space. Additionally, we provide a thorough analysis of DirectSpec
from a decorrelation perspective and propose an enhanced variant, DirectSpec+,
which employs self-paced gradients to optimize irrelevant samples more
effectively. Moreover, we establish a close connection between DirectSpec+ and
uniformity, demonstrating that contrastive learning (CL) can alleviate the
collapse issue by indirectly balancing the spectrum. Finally, we implement
DirectSpec and DirectSpec+ on two popular recommender models: MF and LightGCN.
Our experimental results demonstrate its effectiveness and efficiency over
competitive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Trans on Recommender Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can't See the Forest for the Trees: Benchmarking <span class="highlight-title">Multimodal</span> Safety
  Awareness for <span class="highlight-title">Multimodal</span> LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have expanded the capabilities of
traditional language models by enabling interaction through both text and
images. However, ensuring the safety of these models remains a significant
challenge, particularly in accurately identifying whether multimodal content is
safe or unsafe-a capability we term safety awareness. In this paper, we
introduce MMSafeAware, the first comprehensive multimodal safety awareness
benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500
carefully curated image-prompt pairs. MMSafeAware includes both unsafe and
over-safety subsets to assess models abilities to correctly identify unsafe
content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine
widely used MLLMs using MMSafeAware reveals that current models are not
sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies
36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further
explore three methods to improve safety awareness-prompting-based approaches,
visual contrastive decoding, and vision-centric reasoning fine-tuning-but find
that none achieve satisfactory performance. Our findings highlight the profound
challenges in developing MLLMs with robust safety awareness, underscoring the
need for further research in this area. All the code and data will be publicly
available to facilitate future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering
  without Font Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao, Lyle Ungar, Camillo J. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work demonstrates that diffusion models can achieve font-controllable
multilingual text rendering using just raw images without font label
annotations. Visual text rendering remains a significant challenge. While
recent methods condition diffusion on glyphs, it is impossible to retrieve
exact font annotations from large-scale, real-world datasets, which prevents
user-specified font control. To address this, we propose a data-driven solution
that integrates the conditional diffusion model with a text segmentation model,
utilizing segmentation masks to capture and represent fonts in pixel space in a
self-supervised manner, thereby eliminating the need for any ground-truth
labels and enabling users to customize text rendering with any multilingual
font of their choice. The experiment provides a proof of concept of our
algorithm in zero-shot text and font editing across diverse fonts and
languages, providing valuable insights for the community and industry toward
achieving generalized visual text rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is preliminary work and code will be released at
  github.com/bowen-upenn/ControlText</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advances in Discrete Speech Tokens: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of speech generation technologies in the era of large
language models (LLMs) has established discrete speech tokens as a foundational
paradigm for speech representation. These tokens, characterized by their
discrete, compact, and concise nature, are not only advantageous for efficient
transmission and storage, but also inherently compatible with the language
modeling framework, enabling seamless integration of speech into text-dominated
LLM architectures. Current research categorizes discrete speech tokens into two
principal classes: acoustic tokens and semantic tokens, each of which has
evolved into a rich research domain characterized by unique design philosophies
and methodological approaches. This survey systematically synthesizes the
existing taxonomy and recent innovations in discrete speech tokenization,
conducts a critical examination of the strengths and limitations of each
paradigm, and presents systematic experimental comparisons across token types.
Furthermore, we identify persistent challenges in the field and propose
potential research directions, aiming to offer actionable insights to inspire
future advancements in the development and application of discrete speech
tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, 3 tables. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking Backward: Streaming Video-to-Video Translation with Feature
  Banks <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, Diana Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces StreamV2V, a diffusion model that achieves real-time
streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V
methods using batches to process limited frames, we opt to process frames in a
streaming fashion, to support unlimited frames. At the heart of StreamV2V lies
a backward-looking principle that relates the present to the past. This is
realized by maintaining a feature bank, which archives information from past
frames. For incoming frames, StreamV2V extends self-attention to include banked
keys and values and directly fuses similar past features into the output. The
feature bank is continually updated by merging stored and new features, making
it compact but informative. StreamV2V stands out for its adaptability and
efficiency, seamlessly integrating with image diffusion models without
fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x
faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative
metrics and user studies confirm StreamV2V's exceptional ability to maintain
temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page:
  https://jeff-liangf.github.io/projects/streamv2v</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-15T00:00:00Z">2025-02-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Conversational Agents from Open-Source Large Language Models
  with Illocutionary Force and Document-Based Knowledge Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Godfrey Inyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first present a novel way of computationally analysing and
extracting illocutionary forces from dialogue using Bert-based Large Language
Models, and demonstrate how these features impact the response of a
conversational agent guided by a document-based knowledge bank demonstrated by
a bespoke web conversational chat agent system developed. Our proposed
illocutionary force extraction and classification technique is the first of its
kind using the Argument Interchange Format (AIF) Dataset, showing an improved
performance compared to two methods for carrying out similar tasks with a macro
F1 of approximately 45%. When we evaluated the system based on 2 knowledge
files, with 2 user queries each, across 5 open-source large language models
(LLMs) using 10 standard metrics we found out that larger open-source models,
such as Llama2:13b and Llama3-chatqa-latest, demonstrated an improved alignment
when the user illocutionary force was included with their query, achieving
higher QA and linguistic similarity scores. The smaller models on the other
hand like Tinyllama:latest showed an increased perplexity and mixed
performance, which explicitly indicated struggles in processing queries that
explicitly included illocutionary forces. The results from the analysis
highlight the potential of illocutionary force to enhance conversational depth
while underscoring the need for model-specific optimizations to address
increased computational costs and response times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 1 figure, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Geometric Approach to Personalized Recommendation with Set-Theoretic
  Constraints Using Box Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shib Dasgupta, Michael Boratko, Andrew McCallum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized item recommendation typically suffers from data sparsity, which
is most often addressed by learning vector representations of users and items
via low-rank matrix factorization. While this effectively densifies the matrix
by assuming users and movies can be represented by linearly dependent latent
features, it does not capture more complicated interactions. For example,
vector representations struggle with set-theoretic relationships, such as
negation and intersection, e.g. recommending a movie that is "comedy and
action, but not romance". In this work, we formulate the problem of
personalized item recommendation as matrix completion where rows are
set-theoretically dependent. To capture this set-theoretic dependence we
represent each user and attribute by a hyper-rectangle or box (i.e. a Cartesian
product of intervals). Box embeddings can intuitively be understood as
trainable Venn diagrams, and thus not only inherently represent similarity (via
the Jaccard index), but also naturally and faithfully support arbitrary
set-theoretic relationships. Queries involving set-theoretic constraints can be
efficiently computed directly on the embedding space by performing geometric
operations on the representations. We empirically demonstrate the superiority
of box embeddings over vector-based neural methods on both simple and complex
item recommendation queries by up to 30 \% overall.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Order-agnostic Identifier for Large Language Model-based Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lin, Haihan Shi, Wenjie Wang, Fuli Feng, Qifan Wang, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Large Language Models (LLMs) for generative recommendation has
attracted significant research interest, where item tokenization is a critical
step. It involves assigning item identifiers for LLMs to encode user history
and generate the next item. Existing approaches leverage either token-sequence
identifiers, representing items as discrete token sequences, or single-token
identifiers, using ID or semantic embeddings. Token-sequence identifiers face
issues such as the local optima problem in beam search and low generation
efficiency due to step-by-step generation. In contrast, single-token
identifiers fail to capture rich semantics or encode Collaborative Filtering
(CF) information, resulting in suboptimal performance.
  To address these issues, we propose two fundamental principles for item
identifier design: 1) integrating both CF and semantic information to fully
capture multi-dimensional item information, and 2) designing order-agnostic
identifiers without token dependency, mitigating the local optima issue and
achieving simultaneous generation for generation efficiency. Accordingly, we
introduce a novel set identifier paradigm for LLM-based generative
recommendation, representing each item as a set of order-agnostic tokens. To
implement this paradigm, we propose SETRec, which leverages CF and semantic
tokenizers to obtain order-agnostic multi-dimensional tokens. To eliminate
token dependency, SETRec uses a sparse attention mask for user history encoding
and a query-guided generation mechanism for simultaneous token generation. We
instantiate SETRec on T5 and Qwen (from 1.5B to 7B). Extensive experiments
demonstrate its effectiveness under various scenarios (e.g., full ranking,
warm- and cold-start ranking, and various item popularity groups). Moreover,
results validate SETRec's superior efficiency and show promising scalability on
cold-start items as model sizes increase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating improvements on using Large Language Models (LLMs) for
  property extraction in the Open Research Knowledge <span class="highlight-title">Graph</span> (ORKG) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandra Schaftner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research highlights the great potential of Large Language Models
(LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly
complex step in this process is relation extraction, aimed at identifying
suitable properties to describe the content of research. This study builds
directly on previous research of three Open Research Knowledge Graph (ORKG)
team members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and
Mistral for property extraction in scientific literature. Given the moderate
performance observed, the previous work concluded that fine-tuning is needed to
improve these models' alignment with scientific tasks and their emulation of
human expertise. Expanding on this prior experiment, this study evaluates the
impact of advanced prompt engineering techniques and demonstrates that these
techniques can highly significantly enhance the results. Additionally, this
study extends the property extraction process to include property matching to
existing ORKG properties, which are retrieved via the API. The evaluation
reveals that results generated through advanced prompt engineering achieve a
higher proportion of matches with ORKG properties, further emphasizing the
enhanced alignment achieved. Moreover, this lays the groundwork for addressing
challenges such as the inconsistency of ORKG properties, an issue highlighted
in prior studies. By assigning unique URIs and using standardized terminology,
this work increases the consistency of the properties, fulfilling a crucial
aspect of Linked Data and FAIR principles - core commitments of ORKG. This, in
turn, significantly enhances the applicability of ORKG content for subsequent
tasks such as comparisons of research publications. Finally, the study
concludes with recommendations for future improvements in the overall property
extraction process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTM-based Selective Dense Text Retrieval Guided by Sparse Lexical
  Retrieval <span class="chip">ECIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingrui Yang, Parker Carlson, Yifan Qiao, Wentai Xie, Shanxiu He, Tao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies fast fusion of dense retrieval and sparse lexical
retrieval, and proposes a cluster-based selective dense retrieval method called
CluSD guided by sparse lexical retrieval. CluSD takes a lightweight
cluster-based approach and exploits the overlap of sparse retrieval results and
embedding clusters in a two-stage selection process with an LSTM model to
quickly identify relevant clusters while incurring limited extra memory space
overhead. CluSD triggers partial dense retrieval and performs cluster-based
block disk I/O if needed. This paper evaluates CluSD and compares it with
several baselines for searching in-memory and on-disk MS MARCO and BEIR
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ECIR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposition Dilemmas: Does Claim Decomposition Boost or Burden
  Fact-Checking Performance? <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qisheng Hu, Quanyu Long, Wenya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking pipelines increasingly adopt the Decompose-Then-Verify
paradigm, where texts are broken down into smaller claims for individual
verification and subsequently combined for a veracity decision. While
decomposition is widely-adopted in such pipelines, its effects on final
fact-checking performance remain underexplored. Some studies have reported
improvements from decompostition, while others have observed performance
declines, indicating its inconsistent impact. To date, no comprehensive
analysis has been conducted to understand this variability. To address this
gap, we present an in-depth analysis that explicitly examines the impact of
decomposition on downstream verification performance. Through error case
inspection and experiments, we introduce a categorization of decomposition
errors and reveal a trade-off between accuracy gains and the noise introduced
through decomposition. Our analysis provides new insights into understanding
current system's instability and offers guidance for future studies toward
improving claim decomposition in fact-checking pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ USM: Unbiased <span class="highlight-title">Survey</span> Modeling for Limiting Negative User Experiences in
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10674v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10674v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghui Yu, Peiyi Li, Haoze Wu, Yiri Wen, Bingfeng Deng, Hongyu Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing negative user experiences is essential for the success of
recommendation platforms. Exposing users to inappropriate content could not
only adversely affect users' psychological well-beings, but also potentially
drive users away from the platform, sabotaging the platform's long-term
success. However, recommendation algorithms tend to weigh more heavily on
positive feedback signals due to the scarcity of negative ones, which may
result in the neglect of valuable negative user feedback. In this paper, we
propose an approach aimed at limiting negative user experiences. Our method
primarily relies on distributing in-feed surveys to the users, modeling the
users' feedback collected from the survey, and integrating the model
predictions into the recommendation system. We further enhance the baseline
survey model by integrating the Learning Hidden Unit Contributions module and
the Squeeze-and-Excitation module. In addition, we strive to resolve the
problem of response Bias by applying a survey-submit model; The A/B testing
results indicate a reduction in survey sexual rate and survey inappropriate
rate, ranging from -1.44\% to -3.9\%. Additionally, we compared our methods
against an online baseline that does not incorporate our approach. The results
indicate that our approach significantly reduces the report rate and dislike
rate by 1\% to 2.27\% compared to the baseline, confirming the effectiveness of
our methods in enhancing user experience. After we launched the survey model
based our approach on our platform, the model is able to bring reductions of
1.75\%, 2.57\%, 2.06\% on reports, dislikes, survey inappropriate rate,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging <span class="highlight-title">Multimodal</span> LLM for Inspirational User Interface Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspirational search, the process of exploring designs to inform and inspire
new creative work, is pivotal in mobile user interface (UI) design. However,
exploring the vast space of UI references remains a challenge. Existing
AI-based UI search methods often miss crucial semantics like target users or
the mood of apps. Additionally, these models typically require metadata like
view hierarchies, limiting their practical use. We used a multimodal large
language model (MLLM) to extract and interpret semantics from mobile UI images.
We identified key UI semantics through a formative study and developed a
semantic-based UI search system. Through computational and human evaluations,
we demonstrate that our approach significantly outperforms existing UI
retrieval methods, offering UI designers a more enriched and contextually
relevant search experience. We enhance the understanding of mobile UI design
semantics and highlight MLLMs' potential in inspirational search, providing a
rich dataset of UI semantics for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems (CHI '25)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REAL: Realism Evaluation of Text-to-Image Generation Models for
  Effective Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Li, Xiaomeng Jin, Heng ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image (T2I) generation models have transformed
the field. However, challenges persist in generating images that reflect
demanding textual descriptions, especially for fine-grained details and unusual
relationships. Existing evaluation metrics focus on text-image alignment but
overlook the realism of the generated image, which can be crucial for
downstream applications like data augmentation in machine learning. To address
this gap, we propose REAL, an automatic evaluation framework that assesses
realism of T2I outputs along three dimensions: fine-grained visual attributes,
unusual visual relationships, and visual styles. REAL achieves a Spearman's rho
score of up to 0.62 in alignment with human judgement and demonstrates utility
in ranking and filtering augmented data for tasks like im- age captioning,
classification, and visual relationship detection. Empirical results show that
high-scoring images evaluated by our metrics improve F1 scores of image
classification by up to 11.3%, while low-scoring ones degrade that by up to
4.95%. We benchmark four major T2I models across the realism dimensions,
providing insights for future improvements in T2I output realism.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-14T00:00:00Z">2025-02-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">81</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-RLHF: The Next Step Forward in <span class="highlight-title">Multimodal</span> LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite notable advancements in Multimodal Large Language Models (MLLMs),
most state-of-the-art models have not undergone thorough alignment with human
preferences. This gap exists because current alignment research has primarily
achieved progress in specific areas (e.g., hallucination reduction), while the
broader question of whether aligning models with human preferences can
systematically enhance MLLM capability remains largely unexplored. To this end,
we introduce MM-RLHF, a dataset containing $\mathbf{120k}$ fine-grained,
human-annotated preference comparison pairs. This dataset represents a
substantial advancement over existing resources, offering superior size,
diversity, annotation granularity, and quality. Leveraging this dataset, we
propose several key innovations to improve both the quality of reward models
and the efficiency of alignment algorithms. Notably, we introduce a
Critique-Based Reward Model, which generates critiques of model outputs before
assigning scores, offering enhanced interpretability and more informative
feedback compared to traditional scalar reward mechanisms. Additionally, we
propose Dynamic Reward Scaling, a method that adjusts the loss weight of each
sample according to the reward signal, thereby optimizing the use of
high-quality comparison pairs. Our approach is rigorously evaluated across
$\mathbf{10}$ distinct dimensions and $\mathbf{27}$ benchmarks, with results
demonstrating significant and consistent improvements in model performance.
Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm
leads to a $\mathbf{19.5}$% increase in conversational abilities and a
$\mathbf{60}$% improvement in safety.
  We have open-sourced the preference dataset, reward model, training and
evaluation code, as well as reward modeling and safety benchmarks. For more
details, please visit our project page: https://mm-rlhf.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mm-rlhf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aspect-Oriented Summarization for Psychiatric Short-Term Readmission
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WonJin Yoon, Boyu Ren, Spencer Thomas, Chanwhi Kim, Guergana Savova, Mei-Hua Hall, Timothy Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models (LLMs) has enabled the automated
processing of lengthy documents even without supervised training on a
task-specific dataset. Yet, their zero-shot performance in complex tasks as
opposed to straightforward information extraction tasks remains suboptimal. One
feasible approach for tasks with lengthy, complex input is to first summarize
the document and then apply supervised fine-tuning to the summary. However, the
summarization process inevitably results in some loss of information. In this
study we present a method for processing the summaries of long documents aimed
to capture different important aspects of the original document. We hypothesize
that LLM summaries generated with different aspect-oriented prompts contain
different \textit{information signals}, and we propose methods to measure these
differences. We introduce approaches to effectively integrate signals from
these different summaries for supervised training of transformer models. We
validate our hypotheses on a high-impact task -- 30-day readmission prediction
from a psychiatric discharge -- using real-world data from four hospitals, and
show that our proposed method increases the prediction performance for the
complex task of predicting patient outcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unknown Word Detection for English as a Second Language (ESL) Learners
  Using Gaze and Pre-trained Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiexin Ding, Bowen Zhao, Yuntao Wang, Xinyun Liu, Rui Hao, Ishan Chatterjee, Yuanchun Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English as a Second Language (ESL) learners often encounter unknown words
that hinder their text comprehension. Automatically detecting these words as
users read can enable computing systems to provide just-in-time definitions,
synonyms, or contextual explanations, thereby helping users learn vocabulary in
a natural and seamless manner. This paper presents EyeLingo, a
transformer-based machine learning method that predicts the probability of
unknown words based on text content and eye gaze trajectory in real time with
high accuracy. A 20-participant user study revealed that our method can achieve
an accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time
reading assistance prototype to show the effectiveness of EyeLingo. The user
study shows improvement in willingness to use and usefulness compared to
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OWLS: Scaling Laws for Multilingual Speech Recognition and Translation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Huck Yang, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural scaling laws offer valuable insights for designing robust sequence
processing architectures. While these laws have been extensively characterized
in other modalities, their behavior in speech remains comparatively
underexplored. In this work, we introduce OWLS, an open-access, reproducible
suite of multilingual speech recognition and translation models spanning 0.25B
to 18B parameters, with the 18B version being the largest speech model, to the
best of our knowledge. OWLS leverages up to 360K hours of public speech data
across 150 languages, enabling a systematic investigation into how data, model,
and compute scaling each influence performance in multilingual speech tasks. We
use OWLS to derive neural scaling laws, showing how final performance can be
reliably predicted when scaling. One of our key findings is that scaling
enhances performance on low-resource languages/dialects, helping to mitigate
bias and improve the accessibility of speech technologies. Finally, we show how
OWLS can be used to power new research directions by discovering emergent
abilities in large-scale speech models. Model checkpoints will be released on
https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d
for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multilingual LLM Pretraining with Model-Based Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bettina Messmer, Vinko Sabolčec, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset curation has become a basis for strong large language model (LLM)
performance. While various rule-based filtering heuristics exist for English
and multilingual datasets, model-based filtering techniques have primarily
focused on English. To address the disparity stemming from limited research on
non-English languages, we propose a model-based filtering framework for
multilingual datasets that aims to identify a diverse set of structured and
knowledge-rich samples. Our approach emphasizes transparency, simplicity, and
efficiency, leveraging Transformer- and FastText-based classifiers to ensure
the broad accessibility of our technique and data. We conduct comprehensive
ablation studies on the FineWeb-2 web crawl dataset across diverse language
families, scripts, and resource availability to demonstrate the effectiveness
of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our
approach can match the baseline MMLU score with as little as 15% of the
training tokens, while also improving across other benchmarks. These findings
provide strong evidence for the generalizability of our approach to other
languages. As a result, we extend our framework to 20 languages for which we
release the refined pretraining datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Verification for Ambiguous Query Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngwon Lee, Seung-won Hwang, Ruofan Wu, Feng Yan, Danmei Xu, Moutasem Akkad, Zhewei Yao, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenge of disambiguating queries in
retrieval-augmented generation (RAG) to diverse yet answerable interpretations.
State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse
interpretations are generated by an LLM, later used as search queries to
retrieve supporting passages. Such a process may introduce noise in either
interpretations or retrieval, particularly in enterprise settings, where LLMs
-- trained on static data -- may struggle with domain-specific disambiguations.
Thus, a post-hoc verification phase is introduced to prune noises. Our
distinction is to unify diversification with verification by incorporating
feedback from retriever and generator early on. This joint approach improves
both efficiency and robustness by reducing reliance on multiple retrieval and
inference steps, which are susceptible to cascading errors. We validate the
efficiency and effectiveness of our method, Verified-Diversification with
Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve
diverse yet verifiable interpretations. Empirical results show that VERDICT
improves grounding-aware F1 score by an average of 23% over the strongest
baseline across different backbone LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Organize the Web: Constructing Domains Enhances Pre-Training Data
  Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models are trained on large, unstructured datasets consisting
of trillions of tokens and obtained by crawling the web. The unstructured
nature makes it difficult to reason about their contents and develop systematic
approaches to data curation. In this paper, we unpack monolithic web corpora by
developing taxonomies of their contents and organizing them into domains. We
introduce WebOrganizer, a framework for organizing web pages in terms of both
their topic and format. Using these two complementary notions of domains, we
automatically annotate pre-training data by distilling annotations from a large
language model into efficient classifiers. This allows us to study how data
from different domains should be mixed to improve models on downstream tasks,
and we show that we can combine insights about effective topics and formats to
further boost performance. We demonstrate that our domain mixing also improves
existing methods that select data based on quality. Furthermore, we study and
compare how quality-based methods will implicitly change the domain mixture.
Overall, our work demonstrates that constructing and mixing domains provides a
valuable complement to quality-based data curation methods, opening new avenues
for effective and insightful pre-training data curation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://weborganizer.allen.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAR: Spectral Truncation and Rescale for Model Merging <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Ang Lee, Ching-Yun Ko, Tejaswini Pedapati, I-Hsin Chung, Mi-Yen Yeh, Pin-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is an efficient way of obtaining a multi-task model from
several pretrained models without further fine-tuning, and it has gained
attention in various domains, including natural language processing (NLP).
Despite the efficiency, a key challenge in model merging is the seemingly
inevitable decrease in task performance as the number of models increases. In
this paper, we propose $\mathbf{S}$pectral $\mathbf{T}$runcation $\mathbf{A}$nd
$\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by
truncating small components in the respective spectral spaces, which is
followed by an automatic parameter rescaling scheme to retain the nuclear norm
of the original matrix. STAR requires no additional inference on original
training data and is robust to hyperparamater choice. We demonstrate the
effectiveness of STAR through extensive model merging cases on diverse NLP
tasks. Specifically, STAR works robustly across varying model sizes, and can
outperform baselines by 4.2$\%$ when merging 12 models on Flan-T5. Our code is
publicly available at https://github.com/IBM/STAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Meta- and Object-Level Reasoning of Large Language Models
  for Question Answering <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Ferguson, Liane Guillou, Alan Bundy, Kwabena Nuamah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in natural language tasks but still face
challenges in Question Answering (QA) tasks requiring complex, multi-step
reasoning. We outline the types of reasoning required in some of these tasks,
and reframe them in terms of meta-level reasoning (akin to high-level strategic
reasoning or planning) and object-level reasoning (embodied in lower-level
tasks such as mathematical reasoning). Franklin, a novel dataset with
requirements of meta- and object-level reasoning, is introduced and used along
with three other datasets to evaluate four LLMs at question answering tasks
requiring multiple steps of reasoning. Results from human annotation studies
suggest LLMs demonstrate meta-level reasoning with high frequency, but struggle
with object-level reasoning tasks in some of the datasets used. Additionally,
evidence suggests that LLMs find the object-level reasoning required for the
questions in the Franklin dataset challenging, yet they do exhibit strong
performance with respect to the meta-level reasoning requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted to the Workshop on Planning in the Era of LLMs
  (LM4Plan @ AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeltaProduct: Increasing the Expressivity of DeltaNet Through Products
  of Householders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive
alternatives to Transformers for sequence modeling, offering efficient training
and linear-time inference. However, existing architectures face a fundamental
trade-off between expressivity and efficiency, dictated by the structure of
their state-transition matrices. While diagonal matrices used in architectures
like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited
expressivity. To address this, recent architectures such as (Gated) DeltaNet
and RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous
token-channel mixing, which overcomes some expressivity limitations with only a
slight decrease in training efficiency. Building on the interpretation of
DeltaNet's recurrence as performing one step of online gradient descent per
token on an associative recall loss, we introduce DeltaProduct, which instead
takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus
rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized
Householder transformations, providing a tunable mechanism to balance
expressivity and efficiency and a stable recurrence. Through extensive
experiments, we demonstrate that DeltaProduct achieves superior state-tracking
and language modeling capabilities while exhibiting significantly improved
length extrapolation compared to DeltaNet. Additionally, we also strengthen the
theoretical foundation of DeltaNet's expressivity by proving that it can solve
dihedral group word problems in just two layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Large Language Models the future crowd workers of Linguistics? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iris Ferrazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data elicitation from human participants is one of the core data collection
strategies used in empirical linguistic research. The amount of participants in
such studies may vary considerably, ranging from a handful to crowdsourcing
dimensions. Even if they provide resourceful extensive data, both of these
settings come alongside many disadvantages, such as low control of
participants' attention during task completion, precarious working conditions
in crowdsourcing environments, and time-consuming experimental designs. For
these reasons, this research aims to answer the question of whether Large
Language Models (LLMs) may overcome those obstacles if included in empirical
linguistic pipelines. Two reproduction case studies are conducted to gain
clarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced
elicitation tasks, originally designed for human participants, are reproduced
in the proposed framework with the help of OpenAI's GPT-4o-mini model. Its
performance with our zero-shot prompting baseline shows the effectiveness and
high versatility of LLMs, that tend to outperform human informants in
linguistic tasks. The findings of the second replication further highlight the
need to explore additional prompting techniques, such as Chain-of-Thought (CoT)
prompting, which, in a second follow-up experiment, demonstrates higher
alignment to human performance on both critical and filler items. Given the
limited scale of this study, it is worthwhile to further explore the
performance of LLMs in empirical Linguistics and in other future applications
in the humanities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Synthetic Data for Monitoring <span class="highlight-title">Dataset</span> Mentions
  in Research Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aivin V. Solatorio, Rafael Macalaba, James Liounis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking how data is mentioned and used in research papers provides critical
insights for improving data discoverability, quality, and production. However,
manually identifying and classifying dataset mentions across vast academic
literature is resource-intensive and not scalable. This paper presents a
machine learning framework that automates dataset mention detection across
research domains by leveraging large language models (LLMs), synthetic data,
and a two-stage fine-tuning process. We employ zero-shot extraction from
research papers, an LLM-as-a-Judge for quality assessment, and a reasoning
agent for refinement to generate a weakly supervised synthetic dataset. The
Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by
fine-tuning on a manually annotated subset. At inference, a ModernBERT-based
classifier efficiently filters dataset mentions, reducing computational
overhead while maintaining high recall. Evaluated on a held-out manually
annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and
GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how
LLM-generated synthetic data can effectively address training data scarcity,
improving generalization in low-resource settings. This framework offers a
pathway toward scalable monitoring of dataset usage, enhancing transparency,
and supporting researchers, funders, and policymakers in identifying data gaps
and strengthening data accessibility for informed decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project GitHub repository at https://github.com/worldbank/ai4data-use</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision
  Language Models <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Karthik Kumar, Iheb Chaabane, Kebin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) excel in various visual benchmarks but are
often constrained by the lack of high-quality visual fine-tuning data. To
address this challenge, we introduce VisCon-100K, a novel dataset derived from
interleaved image-text web documents. Our approach transforms 45K web documents
from the OBELICS dataset into 100K image conversation samples. We utilize
GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert
these captions into diverse free-form and multiple-choice question-answer
pairs. Integrating this dataset for fine-tuning considerably enhances VLM
performance across multiple benchmarks. Unlike methods that focus solely on
fine-grained visual content, our approach leverages accompanying web context,
yielding superior results. We also discover that a `leaky modality mix,' where
conversation samples contain questions answerable from both the image and its
contextual caption, outperforms non-leaky combinations of captions and Q\&A
pairs. VisCon-100k dataset shows strong performance with two popular VLM
approaches: text-only large language model (LLM) aligned with a vision encoder
using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM
(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the
VisCon-100K dataset, we provide a contextual captioner trained on this dataset,
facilitating scalable fine-tuning data generation for future research and
open-source applications. Using the same pipeline, but substituting our trained
contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PAKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step-Video-T2V Technical Report: The Practice, Challenges, and Future of
  Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Post-Training Quantization Benefit from an Additional QLoRA
  Integration? <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiliang Zhu, Elena Khasanova, Cheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed natural language processing but
pose significant challenges for real-world deployment. These models necessitate
considerable computing resources, which can be costly and frequently
unavailable. Model compression techniques such as quantization are often
leveraged to alleviate resource demand, but they may have a negative impact on
the generation quality. In this study, we explore the integration of 4-bit
Post-training Quantization (PTQ) with QLoRA to address these issues. We
demonstrate through extensive experiments that this integration outperforms
standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,
validated across proprietary and public datasets with different quantization
algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,
offering a viable solution for deploying powerful LLMs in resource-constrained
environments without compromising on performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction hubs are context-informed frequent tokens in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrix M. G. Nielsen, Iuri Macocco, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hubness, the tendency for few points to be among the nearest neighbours of a
disproportionate number of other points, commonly arises when applying standard
distance measures to high-dimensional data, often negatively impacting
distance-based analysis. As autoregressive large language models (LLMs) operate
on high-dimensional representations, we ask whether they are also affected by
hubness. We first show, theoretically, that the only representation comparison
operation performed by LLMs, namely that between context and unembedding
vectors to determine continuation probabilities, is not characterized by the
concentration of distances phenomenon that typically causes the appeareance of
nuisance hubness. We then empirically show that this comparison still leads to
a high degree of hubness, but the hubs in this case do not constitute a
disturbance. They are rather the result of context-modulated frequent tokens
often appearing in the pool of likely candidates for next token prediction. On
the other hand, when other distance computations involving LLM representations
are performed, we do not have the same theoretical guarantees, and, indeed, we
see nuisance hubs appear. In summary, our work highlights, on the one hand, how
hubness, while omnipresent in high-dimensional spaces, is not always a negative
property that needs to be mitigated, and, on the other hand, it shows that
various widely-used LLMs have developed a guessing strategy that consists in
constantly assigning a high probability to frequent tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Generalization Power of a DNN in Terms of Symbolic
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Cheng, Junpeng Zhang, Qihan Ren, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to analyze the generalization power of deep neural networks
(DNNs) from the perspective of interactions. Unlike previous analysis of a
DNN's generalization power in a highdimensional feature space, we find that the
generalization power of a DNN can be explained as the generalization power of
the interactions. We found that the generalizable interactions follow a
decay-shaped distribution, while non-generalizable interactions follow a
spindle-shaped distribution. Furthermore, our theory can effectively
disentangle these two types of interactions from a DNN. We have verified that
our theory can well match real interactions in a DNN in experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.19198</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Models, Big Impact: Efficient Corpus and <span class="highlight-title">Graph</span>-Based Adaptation of
  Small Multilingual Language Models for Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Gurgurov, Ivan Vykopal, Josef van Genabith, Simon Ostermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource languages (LRLs) face significant challenges in natural language
processing (NLP) due to limited data. While current state-of-the-art large
language models (LLMs) still struggle with LRLs, smaller multilingual models
(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of
their capacity to low training data sizes. This study systematically
investigates parameter-efficient adapter-based methods for adapting mLMs to
LRLs, evaluating three architectures: Sequential Bottleneck, Invertible
Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and
structured knowledge from ConceptNet, we show that small adaptation datasets
(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains
in intrinsic (masked language modeling) and extrinsic tasks (topic
classification, sentiment analysis, and named entity recognition). We find that
Sequential Bottleneck adapters excel in language modeling, while Invertible
Bottleneck adapters slightly outperform other methods on downstream tasks due
to better embedding alignment and larger parameter counts. Adapter-based
methods match or outperform full fine-tuning while using far fewer parameters,
and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,
GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves
performance, pre-training data size remains the dominant factor, especially for
languages with extensive pre-training coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hands-off Image Editing: Language-guided Editing without any
  Task-specific Labeling, Masking or even Training <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Santos, António Branco, João Silva, João Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-guided image editing consists in taking an image and an
instruction and deliverring that image altered according to that instruction.
State-of-the-art approaches to this task suffer from the typical scaling up and
domain adaptation hindrances related to supervision as they eventually resort
to some kind of task-specific labelling, masking or training. We propose a
novel approach that does without any such task-specific supervision and offers
thus a better potential for improvement. Its assessment demonstrates that it is
highly effective, achieving very competitive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotating Compositionality Scores for Irish Noun Compounds is Hard Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abigail Walsh, Teresa Clifford, Emma Daly, Jane Dunne, Brian Davis, Gearóid Ó Cleircín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noun compounds constitute a challenging construction for NLP applications,
given their variability in idiomaticity and interpretation. In this paper, we
present an analysis of compound nouns identified in Irish text of varied
domains by expert annotators, focusing on compositionality as a key feature,
but also domain specificity, as well as familiarity and confidence of the
annotator giving the ratings. Our findings and the discussion that ensued
contributes towards a greater understanding of how these constructions appear
in Irish language, and how they might be treated separately from English noun
compounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTLM: an Innovative Language Model Training Paradigm for ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training Transformer-based language models (LMs) on a large amount of
text has proven crucial for improving automatic speech recognition (ASR)
performance. Generally, traditional LMs are unidirectional and unable to access
the context on the right. This paper proposes a method for training LMs that
enable traditional unidirectional LMs to fully utilize left and right contexts.
Compared with the unidirectional LMs, our LM facilitates ASR to transcribe
hypotheses more consistently and in a more semantically unambiguous way, as it
incorporates richer contextual representations. Finally, our experimental
results on the LibriSpeech corpus demonstrate that our model outperforms
traditional unidirectional LMs, whether n-best rescoring or shallow fusion is
used as the decoding algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORI: O Routing Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Shadid, Rahul Kumar, Mohit Mayank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single large language models (LLMs) often fall short when faced with the
ever-growing range of tasks, making a single-model approach insufficient. We
address this challenge by proposing ORI (O Routing Intelligence), a dynamic
framework that leverages a set of LLMs. By intelligently routing incoming
queries to the most suitable model, ORI not only improves task-specific
accuracy, but also maintains efficiency. Comprehensive evaluations across
diverse benchmarks demonstrate consistent accuracy gains while controlling
computational overhead. By intelligently routing queries, ORI outperforms the
strongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,
ties the top performance on ARC, and on BBH. These results underscore the
benefits of a multi-model strategy and demonstrate how ORI's adaptive
architecture can more effectively handle diverse tasks, offering a scalable,
high-performance solution for a system of multiple large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Lexical Manifold Construction in Large Language Models via
  Hierarchical Vector Field Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clive Pendleton, Ewan Harrington, Giles Fairbrother, Jasper Arkwright, Nigel Fenwick, Richard Katrix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical vector field interpolation introduces a structured probabilistic
framework for lexical representation, ensuring that word embeddings transition
smoothly across a continuous manifold rather than being constrained to discrete
token mappings. The proposed methodology constructs a probabilistic function
space where word representations adhere to topological consistency, mitigating
representational discontinuities commonly observed in transformer-based
embeddings. Empirical evaluations reveal that probabilistic constraints enhance
lexical coherence by refining contextual relationships, leading to improvements
in semantic stability across multiple linguistic distributions. The application
of divergence minimization techniques ensures that interpolated embeddings
maintain probabilistic consistency while preserving computational feasibility
for large-scale implementations. Experimental findings demonstrate that
interpolated lexical manifolds improve representation density alignment,
reducing anisotropic distortions in contextual embedding distributions.
Comparative analyses with standard transformer-based models highlight that
structured interpolation yields more stable representations, particularly in
tasks requiring fine-grained semantic differentiation. The statistical
evaluation of embedding divergence confirms that probabilistic lexical
manifolds reduce representational inconsistencies while maintaining coherence
across varying scales of contextual abstraction. An assessment of computational
efficiency reveals that while interpolation introduces minor processing
overhead, the structured representation learning approach remains scalable for
practical deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciClaimHunt: A Large <span class="highlight-title">Dataset</span> for Evidence-based Scientific Claim
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujit Kumar, Anshul Sharma, Siddharth Hemant Khincha, Gargi Shroff, Sanasam Ranbir Singh, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifying scientific claims presents a significantly greater challenge than
verifying political or news-related claims. Unlike the relatively broad
audience for political claims, the users of scientific claim verification
systems can vary widely, ranging from researchers testing specific hypotheses
to everyday users seeking information on a medication. Additionally, the
evidence for scientific claims is often highly complex, involving technical
terminology and intricate domain-specific concepts that require specialized
models for accurate verification. Despite considerable interest from the
research community, there is a noticeable lack of large-scale scientific claim
verification datasets to benchmark and train effective models. To bridge this
gap, we introduce two large-scale datasets, SciClaimHunt and SciClaimHunt_Num,
derived from scientific research papers. We propose several baseline models
tailored for scientific claim verification to assess the effectiveness of these
datasets. Additionally, we evaluate models trained on SciClaimHunt and
SciClaimHunt_Num against existing scientific claim verification datasets to
gauge their quality and reliability. Furthermore, we conduct human evaluations
of the claims in proposed datasets and perform error analysis to assess the
effectiveness of the proposed baseline models. Our findings indicate that
SciClaimHunt and SciClaimHunt_Num serve as highly reliable resources for
training models in scientific claim verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbBERT-Q: Breaking Memory Barriers in Embedded NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing,
setting new standards across a wide range of applications. However, their
relevant memory and computational demands make them impractical for deployment
on technologically-constrained tiny devices such as wearable devices and
Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a
novel tiny language model specifically designed for tiny devices with stringent
memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in
Natural Language Processing tasks in this scenario, with a total memory
footprint (weights and activations) of just 781 kB, representing a 25x
reduction in size with respect to SotA models. By combining architectural
innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently
outperforms several baseline models scaled down to a 2 MB memory budget (i.e.,
the maximum memory typically available in tiny devices), including heavily
compressed versions of BERT and MAMBA. Extensive experimental evaluations on
both a selected benchmark dataset, TinyNLP, specifically curated to evaluate
Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE
benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with
respect to existing approaches, achieving an unmatched balance between memory
and performance. To ensure the complete and immediate reproducibility of all
our results, we release all code, scripts, and model checkpoints at
https://github.com/RiccardoBravin/tiny-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 4 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models (ARMs) are widely regarded as the cornerstone of large
language models (LLMs). We challenge this notion by introducing LLaDA, a
diffusion model trained from scratch under the pre-training and supervised
fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data
masking process and a reverse process, parameterized by a vanilla Transformer
to predict masked tokens. By optimizing a likelihood bound, it provides a
principled generative approach for probabilistic inference. Across extensive
benchmarks, LLaDA demonstrates strong scalability, outperforming our
self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong
LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive
instruction-following abilities in case studies such as multi-turn dialogue.
Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal
poem completion task. Our findings establish diffusion models as a viable and
promising alternative to ARMs, challenging the assumption that key LLM
capabilities discussed above are inherently tied to ARMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from
  Multi-Turn Jailbreaks without Compromising Usability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of safety alignment techniques for LLMs,
defending against multi-turn jailbreaks is still a challenging task. In this
paper, we conduct a comprehensive comparison, revealing that some existing
defense methods can improve the robustness of LLMs against multi-turn
jailbreaks but compromise usability, i.e., reducing general capabilities or
causing the over-refusal problem. From the perspective of mechanism
interpretability of LLMs, we discover that these methods fail to establish a
boundary that exactly distinguishes safe and harmful feature representations.
Therefore, boundary-safe representations close to harmful representations are
inevitably disrupted, leading to a decline in usability. To address this issue,
we propose X-Boundary to push harmful representations away from boundary-safe
representations and obtain an exact distinction boundary. In this way, harmful
representations can be precisely erased without disrupting safe ones.
Experimental results show that X-Boundary achieves state-of-the-art defense
performance against multi-turn jailbreaks, while reducing the over-refusal rate
by about 20% and maintaining nearly complete general capability. Furthermore,
we theoretically prove and empirically verify that X-Boundary can accelerate
the convergence process during training. Please see our code at:
https://github.com/AI45Lab/X-Boundary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs
  - No Silver Bullet for LC or RAG Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, Minhao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively incorporating external knowledge into Large Language Models
(LLMs) is crucial for enhancing their capabilities and addressing real-world
needs. Retrieval-Augmented Generation (RAG) offers an effective method for
achieving this by retrieving the most relevant fragments into LLMs. However,
the advancements in context window size for LLMs offer an alternative approach,
raising the question of whether RAG remains necessary for effectively handling
external knowledge. Several existing studies provide inconclusive comparisons
between RAG and long-context (LC) LLMs, largely due to limitations in the
benchmark designs. In this paper, we present LaRA, a novel benchmark
specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses
2,326 test cases across four practical QA task categories and three types of
naturally occurring long texts. Through systematic evaluation of seven
open-source and four proprietary LLMs, we find that the optimal choice between
RAG and LC depends on a complex interplay of factors, including the model's
parameter size, long-text capabilities, context length, task type, and the
characteristics of the retrieved chunks. Our findings provide actionable
guidelines for practitioners to effectively leverage both RAG and LC approaches
in developing and deploying LLM applications. Our code and dataset is provided
at:
\href{https://github.com/likuanppd/LaRA}{\textbf{https://github.com/likuanppd/LaRA}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Valuation using Neural Networks for Efficient Instruction
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Dilek Hakkani-Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide crucial insights into model training, but
existing methods suffer from large computational costs and limited
generalization. Particularly, recent works have proposed various metrics and
algorithms to calculate the influence of data using language models, which do
not scale well with large models and datasets. This is because of the expensive
forward and backward passes required for computation, substantial memory
requirements to store large models, and poor generalization of influence
estimates to new data. In this paper, we explore the use of small neural
networks -- which we refer to as the InfluenceNetwork -- to estimate influence
values, achieving up to 99% cost reduction. Our evaluation demonstrates that
influence values can be estimated with models just 0.0027% the size of full
language models (we use 7B and 8B versions). We apply our algorithm of
estimating influence values (called NN-CIFT: Neural Networks for effiCient
Instruction Fine-Tuning) to the downstream task of subset selection for general
instruction fine-tuning. In our study, we include four state-of-the-art
influence functions and show no compromise in performance, despite large
speedups, between NN-CIFT and the original influence functions. We provide an
in-depth hyperparameter analyses of NN-CIFT. The code for our method can be
found here: https://github.com/agarwalishika/NN-CIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KGGen: Extracting Knowledge <span class="highlight-title">Graph</span>s from Plain Text with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent interest in building foundation models for KGs has highlighted a
fundamental challenge: knowledge-graph data is relatively scarce. The
best-known KGs are primarily human-labeled, created by pattern-matching, or
extracted using early NLP techniques. While human-generated KGs are in short
supply, automatically extracted KGs are of questionable quality. We present a
solution to this data scarcity problem in the form of a text-to-KG generator
(KGGen), a package that uses language models to create high-quality graphs from
plaintext. Unlike other KG extractors, KGGen clusters related entities to
reduce sparsity in extracted KGs. KGGen is available as a Python library
(\texttt{pip install kg-gen}), making it accessible to everyone. Along with
KGGen, we release the first benchmark, Measure of of Information in Nodes and
Edges (MINE), that tests an extractor's ability to produce a useful KG from
plain text. We benchmark our new tool against existing extractors and
demonstrate far superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Learning for Neural Topic Models with
  Variance-Invariance-Covariance Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiran Xu, Kengo Hirami, Koji Eguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our study, we propose a self-supervised neural topic model (NTM) that
combines the power of NTMs and regularized self-supervised learning methods to
improve performance. NTMs use neural networks to learn latent topics hidden
behind the words in documents, enabling greater flexibility and the ability to
estimate more coherent topics compared to traditional topic models. On the
other hand, some self-supervised learning methods use a joint embedding
architecture with two identical networks that produce similar representations
for two augmented versions of the same input. Regularizations are applied to
these representations to prevent collapse, which would otherwise result in the
networks outputting constant or redundant representations for all inputs. Our
model enhances topic quality by explicitly regularizing latent topic
representations of anchor and positive samples. We also introduced an
adversarial data augmentation method to replace the heuristic sampling method.
We further developed several variation models including those on the basis of
an NTM that incorporates contrastive learning with both positive and negative
samples. Experimental results on three datasets showed that our models
outperformed baselines and state-of-the-art models both quantitatively and
qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted in Springer Knowledge and Information Systems
  (KAIS), in press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Preliminary Exploration with GPT-4o Voice Mode 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Xiang Lin, Chih-Kai Yang, Wei-Chih Chen, Chen-An Li, Chien-yu Huang, Xuanjun Chen, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of multimodal large language models, GPT-4o stands out as a
pioneering model, driving us to evaluate its capabilities. This report assesses
GPT-4o across various tasks to analyze its audio processing and reasoning
abilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and
music understanding, performing well in tasks like intent classification,
spoken command classification, semantic and grammatical reasoning.,
multilingual speech recognition, and singing analysis. It also shows greater
robustness against hallucinations than other large audio-language models
(LALMs). However, it struggles with tasks such as audio duration prediction and
instrument classification. Additionally, GPT-4o's safety mechanisms cause it to
decline tasks like speaker identification, age classification, MOS prediction,
and audio deepfake detection. Notably, the model exhibits a significantly
different refusal rate when responding to speaker verification tasks on
different datasets. This is likely due to variations in the accompanying
instructions or the quality of the input audio, suggesting the sensitivity of
its built-in safeguards. Finally, we acknowledge that model performance varies
with evaluation protocols. This report only serves as a preliminary exploration
of the current state of LALMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot
  In-Context Inductive Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive Reasoning (IR), the ability to summarize rules from examples and
apply on new ones, has long been viewed as a primal ability for general
intelligence and widely studied by cognitive science and AI researchers. Many
benchmarks have been proposed to measure such ability for Large Language Models
(LLMs); however, they focus on few-shot (usually $<$10) setting and lack
evaluation for aggregating many pieces of information from long contexts. On
the other hand, the ever-growing context length of LLMs have brought forth the
novel paradigm of many-shot In-Context Learning (ICL), which addresses new
tasks with hundreds to thousands of examples without expensive and inefficient
fine-tuning. However, many-shot evaluations are mostly focused on
classification (a very limited aspect of IR), and popular long-context LLM
tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated
intelligence for integrating many pieces of information. To fix the issues from
both worlds, we propose MIR-Bench, the first many-shot in-context inductive
reasoning benchmark that asks LLM to induce output via input-output examples
from underlying functions with diverse data format. Based on MIR-Bench, we
study many novel problems for inductive reasoning and many-shot ICL, including
robustness against erroneous shots and the effect of Chain-of-Thought (CoT),
and acquired insightful findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism
  of Language Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia DeVrio, Myra Cheng, Lisa Egede, Alexandra Olteanu, Su Lin Blodgett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent attention to anthropomorphism -- the attribution of human-like
qualities to non-human objects or entities -- of language technologies like
LLMs has sparked renewed discussions about potential negative impacts of
anthropomorphism. To productively discuss the impacts of this anthropomorphism
and in what contexts it is appropriate, we need a shared vocabulary for the
vast variety of ways that language can be anthropomorphic. In this work, we
draw on existing literature and analyze empirical cases of user interactions
with language technologies to develop a taxonomy of textual expressions that
can contribute to anthropomorphism. We highlight challenges and tensions
involved in understanding linguistic anthropomorphism, such as how all language
is fundamentally human and how efforts to characterize and shift perceptions of
humanness in machines can also dehumanize certain humans. We discuss ways that
our taxonomy supports more precise and effective discussions of and decisions
about anthropomorphism of language technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 1 figure, to appear at CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence
  of Analogical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruva Karkada, James B. Simon, Yasaman Bahri, Michael R. DeWeese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable success of large language models relies on their ability to
implicitly learn structured latent representations from the pretraining corpus.
As a simpler surrogate for representation learning in language modeling, we
study a class of solvable contrastive self-supervised algorithms which we term
quadratic word embedding models. These models resemble the word2vec algorithm
and perform similarly on downstream tasks. Our main contributions are
analytical solutions for both the training dynamics (under certain
hyperparameter choices) and the final word embeddings, given in terms of only
the corpus statistics. Our solutions reveal that these models learn orthogonal
linear subspaces one at a time, each one incrementing the effective rank of the
embeddings until model capacity is saturated. Training on WikiText, we find
that the top subspaces represent interpretable concepts. Finally, we use our
dynamical theory to predict how and when models acquire the ability to complete
analogies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Hypothesis Validation with Agentic Sequential Falsifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Huang, Ying Jin, Ryan Li, Michael Y. Li, Emmanuel Candès, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypotheses are central to information acquisition, decision-making, and
discovery. However, many real-world hypotheses are abstract, high-level
statements that are difficult to validate directly. This challenge is further
intensified by the rise of hypothesis generation from Large Language Models
(LLMs), which are prone to hallucination and produce hypotheses in volumes that
make manual validation impractical. Here we propose Popper, an agentic
framework for rigorous automated validation of free-form hypotheses. Guided by
Karl Popper's principle of falsification, Popper validates a hypothesis using
LLM agents that design and execute falsification experiments targeting its
measurable implications. A novel sequential testing framework ensures strict
Type-I error control while actively gathering evidence from diverse
observations, whether drawn from existing data or newly conducted procedures.
We demonstrate Popper on six domains including biology, economics, and
sociology. Popper delivers robust error control, high power, and scalability.
Furthermore, compared to human scientists, Popper achieved comparable
performance in validating complex biological hypotheses while reducing time by
10 folds, providing a scalable, rigorous solution for hypothesis validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multitask Learning in Small Language Models Through
  Upside-Down Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Chen Lin, Sanat Sharma, Hari Manikandan, Jayant Kumar, Tracy Holloway King, Jing Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we demonstrate that small language models (SLMs), specifically
a 100M parameter GPT-2 model, can achieve competitive performance in multitask
prompt generation tasks while requiring only a fraction of the computational
resources needed by large language models (LLMs). Through a novel combination
of upside-down reinforcement learning and synthetic data distillation from a
powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5%
of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite
being up to 80 times smaller, making it highly suitable for
resource-constrained and real-time applications. This study highlights the
potential of SLMs as efficient multitask learners in multimodal settings,
providing a promising alternative to LLMs for scalable, low-latency
deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The <span class="highlight-title">Graph</span>'s Apprentice: Teaching an LLM Low Level Knowledge for Circuit
  Quality Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Moravej, Saurabh Bodhe, Zhanguang Zhang, Didier Chetelat, Dimitrios Tsaras, Yingxue Zhang, Hui-Ling Zhen, Jianye Hao, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logic synthesis is a crucial phase in the circuit design process, responsible
for transforming hardware description language (HDL) designs into optimized
netlists. However, traditional logic synthesis methods are computationally
intensive, restricting their iterative use in refining chip designs. Recent
advancements in large language models (LLMs), particularly those fine-tuned on
programming languages, present a promising alternative. This work proposes
augmenting LLMs with predictor networks trained to estimate circuit quality
directly from HDL code. To enhance performance, the model is regularized using
embeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)
graphs, thereby incorporating lower-level circuit insights. The proposed method
demonstrates superior performance compared to existing graph-based RTL-level
estimation techniques on the established benchmark OpenABCD, while providing
instant feedback on HDL code quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily
  Complex Proofs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve arithmetic word problems with high
accuracy, but little is known about how well they generalize to more complex
problems. This is difficult to study, as (i) much of the available evaluation
data has already been seen by the most capable models during training, and (ii)
existing benchmarks do not capture how problem proofs may be arbitrarily
complex in various ways. In this paper, we present a data-generation framework
for evaluating LLMs on problems with arbitrarily complex arithmetic proofs,
called MathGAP. MathGAP generates problem statements and chain-of-thought
reasoning traces according to specifications about their arithmetic proof
structure, enabling systematic studies on easy-to-hard generalization with
respect to complexity of proof trees. Using MathGAP, we find that LLMs show a
significant decrease in performance as proofs get deeper and wider. This effect
is more pronounced in complex, nonlinear proof structures, which are
challenging even for the most capable models. The models are also sensitive to
simple changes in sentence ordering. However, they remain capable of solving
some complex problems, suggesting that reasoning generalization is noisy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperMerge: An Approach For Gradient-Based Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Yang, Zheng Zhang, Saket Sathe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic,
monolithic, and possess the superpower to simultaneously support thousands of
tasks. However, high-throughput applications often prefer smaller task-specific
models because of their lower latency and cost. One challenge of using
task-specific models is the incremental need for solving newer tasks after the
model is already deployed for existing tasks. A straightforward solution
requires fine-tuning the model again for both existing and new tasks, which is
computationally expensive and time-consuming. To address this issue, we propose
a model merging based approach called SUPERMERGE. SUPERMERGE is a
gradient-based method to systematically merge several fine-tuned models trained
on existing and new tasks. SUPERMERGE is designed to be lightweight and fast,
and the merged model achieves similar performance to fully fine-tuned models on
all tasks. Furthermore, we proposed a hierarchical model merging strategy to
reduce the peak space requirement without sacrificing the performance of the
merged model. We experimentally demonstrate that SUPERMERGE outperforms
existing model merging methods on common natural language processing and
computer vision tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResearchArena: Benchmarking Large Language Models' Ability to Collect
  and Organize Information as Research Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Kang, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel across many natural language processing
tasks but face challenges in domain-specific, analytical tasks such as
conducting research surveys. This study introduces ResearchArena, a benchmark
designed to evaluate LLMs' capabilities in conducting academic
surveys$\unicode{x2013}$a foundational step in academic research. ResearchArena
models the process in three stages: (1) information discovery, identifying
relevant literature; (2) information selection, evaluating papers' relevance
and impact; and (3) information organization, structuring knowledge into
hierarchical frameworks such as mind-maps. Notably, mind-map construction is
treated as a bonus task, reflecting its supplementary role in survey-writing.
To support these evaluations, we construct an offline environment of 12M
full-text academic papers and 7.9K survey papers. To ensure ethical compliance,
we do not redistribute copyrighted materials; instead, we provide code to
construct the environment from the Semantic Scholar Open Research Corpus
(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform
compared to simpler keyword-based retrieval methods, underscoring significant
opportunities for advancing LLMs in autonomous research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Programming Every Example: Lifting Pre-training Data Quality Like
  Experts at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model pre-training has traditionally relied on human experts
to craft heuristics for improving the corpora quality, resulting in numerous
rules developed to date. However, these rules lack the flexibility to address
the unique characteristics of individual example effectively. Meanwhile,
applying tailored rules to every example is impractical for human experts. In
this paper, we demonstrate that even small language models, with as few as 0.3B
parameters, can exhibit substantial data refining capabilities comparable to
those of human experts. We introduce Programming Every Example (ProX), a novel
framework that treats data refinement as a programming task, enabling models to
refine corpora by generating and executing fine-grained operations, such as
string normalization, for each individual example at scale. Experimental
results show that models pre-trained on ProX-curated data outperform either
original data or data filtered by other selection methods by more than 2%
across various downstream benchmarks. Its effectiveness spans various model
sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,
FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in
domain-specific continual pre-training: without domain specific design, models
trained on OpenWebMath refined by ProX outperform human-crafted rule-based
methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for
Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable
to models like Llemma-7B trained on 200B tokens. Further analysis highlights
that ProX significantly saves training FLOPs, offering a promising path for
efficient LLM pre-training. We are open-sourcing ProX with >500B corpus,
models, and sharing all training and implementation details for reproducible
research and future innovation. Code: https://github.com/GAIR-NLP/ProX
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 13 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnigmaEval: A Benchmark of Long <span class="highlight-title">Multimodal</span> Reasoning Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models master existing reasoning benchmarks, we need new
challenges to evaluate their cognitive frontiers. Puzzle-solving events are
rich repositories of challenging multimodal problems that test a wide range of
advanced reasoning and knowledge capabilities, making them a unique testbed for
evaluating frontier language models. We introduce EnigmaEval, a dataset of
problems and solutions derived from puzzle competitions and events that probes
models' ability to perform implicit knowledge synthesis and multi-step
deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle
solving challenges models to discover hidden connections between seemingly
unrelated pieces of information to uncover solution paths. The benchmark
comprises 1184 puzzles of varying complexity -- each typically requiring teams
of skilled solvers hours to days to complete -- with unambiguous, verifiable
solutions that enable efficient evaluation. State-of-the-art language models
achieve extremely low accuracy on these puzzles, even lower than other
difficult benchmarks such as Humanity's Last Exam, unveiling models'
shortcomings when challenged with problems requiring unstructured and lateral
reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SequentialBreak: Large Language Models Can be Fooled by Embedding
  Jailbreak Prompts into Sequential Prompt Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijoy Ahmed Saiem, MD Sadik Hossain Shanto, Rakib Ahsan, Md Rafi ur Rashid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the integration of the Large Language Models (LLMs) into various
applications increases, so does their susceptibility to misuse, raising
significant security concerns. Numerous jailbreak attacks have been proposed to
assess the security defense of LLMs. Current jailbreak attacks mainly rely on
scenario camouflage, prompt obfuscation, prompt optimization, and prompt
iterative optimization to conceal malicious prompts. In particular, sequential
prompt chains in a single query can lead LLMs to focus on certain prompts while
ignoring others, facilitating context manipulation. This paper introduces
SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We
discuss several scenarios, not limited to examples like Question Bank, Dialog
Completion, and Game Environment, where the harmful prompt is embedded within
benign ones that can fool LLMs into generating harmful responses. The distinct
narrative structures of these scenarios show that SequentialBreak is flexible
enough to adapt to various prompt formats beyond those discussed. Extensive
experiments demonstrate that SequentialBreak uses only a single query to
achieve a substantial gain of attack success rate over existing baselines
against both open-source and closed-source models. Through our research, we
highlight the urgent need for more robust and resilient safeguards to enhance
LLM security and prevent potential misuse. All the result files and website
associated with this research are available in this GitHub repository:
https://anonymous.4open.science/r/JailBreakAttack-4F3B/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool
  Calling <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating tools into Large Language Models (LLMs) has facilitated the
widespread application. Despite this, in specialized downstream task contexts,
reliance solely on tools is insufficient to fully address the complexities of
the real world. This particularly restricts the effective deployment of LLMs in
fields such as medicine. In this paper, we focus on the downstream tasks of
medical calculators, which use standardized tests to assess an individual's
health status. We introduce MeNTi, a universal agent architecture for LLMs.
MeNTi integrates a specialized medical toolkit and employs meta-tool and nested
calling mechanisms to enhance LLM tool utilization. Specifically, it achieves
flexible tool selection and nested tool calling to address practical issues
faced in intricate medical scenarios, including calculator selection, slot
filling, and unit conversion. To assess the capabilities of LLMs for
quantitative assessment throughout the clinical process of calculator
scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical
calculators to perform calculations and assess patient health status. CalcQA is
constructed by professional physicians and includes 100 case-calculator pairs,
complemented by a toolkit of 281 medical tools. The experimental results
demonstrate significant performance improvements with our framework. This
research paves new directions for applying LLMs in demanding scenarios of
medicine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Critical Look At Tokenwise Reward-Guided Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Ruotian Wu, Julia Grosse, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can be improved by aligning with human
preferences through fine-tuning -- the so-called reinforcement learning from
human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive
for many users. Due to their ability to bypass LLM fine-tuning, prediction-time
tokenwise reward-guided text generation (RGTG) methods have recently been
proposed. They use a reward model trained on full sequences to score partial
sequences during decoding in a bid to steer the generation towards sequences
with high rewards. However, these methods have so far been only heuristically
motivated and poorly analyzed. In this work, we show that reward models trained
on full sequences are not compatible with scoring partial sequences. To
alleviate this issue, we propose to train a Bradley-Terry reward model on
partial sequences explicitly, and autoregressively sample from the implied
tokenwise policy during decoding time. We study the properties of this reward
model and the resulting policy: we show that this policy is proportional to the
ratio of two distinct RLHF policies. Our simple approach outperforms previous
RGTG methods and performs similarly to strong offline baselines without
large-scale LLM finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual
  LLMs: An Extensive Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14050v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14050v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vera Neplenbroek, Arianna Bisazza, Raquel Fernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative large language models (LLMs) show remarkable performance in
non-English languages, but when prompted in those languages they tend to
express higher harmful social biases and toxicity levels. Prior work has shown
that finetuning on specialized datasets can mitigate this behavior, and doing
so in English can transfer to other languages. In this work, we investigate the
impact of different finetuning methods on the model's bias and toxicity, but
also on its ability to produce fluent and diverse text. We reduce biases by
finetuning on curated non-harmful text, but find only direct preference
optimization to be effective for mitigating toxicity. The mitigation caused by
applying these methods in English also transfers to non-English languages. We
find evidence that the extent to which transfer takes place can be predicted by
the amount of data in a given language present in the model's pretraining data.
However, this transfer of bias and toxicity mitigation often comes at the
expense of decreased language generation ability in non-English languages,
highlighting the importance of developing language-specific bias and toxicity
mitigation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span>-based Retrieval Augmented Generation for Dynamic Few-shot Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Haoyang Li, Fei Teng, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A distributional simplicity bias in the learning dynamics of
  <span class="highlight-title">transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Rende, Federica Gerace, Alessandro Laio, Sebastian Goldt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capability of over-parameterised neural networks to generalise
effectively has been explained by invoking a ``simplicity bias'': neural
networks prevent overfitting by initially learning simple classifiers before
progressing to more complex, non-linear functions. While simplicity biases have
been described theoretically and experimentally in feed-forward networks for
supervised learning, the extent to which they also explain the remarkable
success of transformers trained with self-supervised techniques remains
unclear. In our study, we demonstrate that transformers, trained on natural
language data, also display a simplicity bias. Specifically, they sequentially
learn many-body interactions among input tokens, reaching a saturation point in
the prediction error for low-degree interactions while continuing to learn
high-degree interactions. To conduct this analysis, we develop a procedure to
generate \textit{clones} of a given natural language data set, which rigorously
capture the interactions between tokens up to a specified order. This approach
opens up the possibilities of studying how interactions of different orders in
the data affect learning, in natural language processing and beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Refinement Strategies for LLM-based Product Attribute Value
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Brinkmann, Christian Bizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured product data, in the form of attribute-value pairs, is essential
for e-commerce platforms to support features such as faceted product search and
attribute-based product comparison. However, vendors often provide unstructured
product descriptions, making attribute value extraction necessary to ensure
data consistency and usability. Large language models (LLMs) have demonstrated
their potential for product attribute value extraction in few-shot scenarios.
Recent research has shown that self-refinement techniques can improve the
performance of LLMs on tasks such as code generation and text-to-SQL
translation. For other tasks, the application of these techniques has resulted
in increased costs due to processing additional tokens, without achieving any
improvement in performance. This paper investigates applying two
self-refinement techniques (error-based prompt rewriting and self-correction)
to the product attribute value extraction task. The self-refinement techniques
are evaluated across zero-shot, few-shot in-context learning, and fine-tuning
scenarios using GPT-4o. The experiments show that both self-refinement
techniques fail to significantly improve the extraction performance while
substantially increasing processing costs. For scenarios with development data,
fine-tuning yields the highest performance, while the ramp-up costs of
fine-tuning are balanced out as the amount of product descriptions increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity
  Extraction in Chinese Hate Speech Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewen Bai, Yuanyuan Sun, Shengdi Yin, Junyu Lu, Jingjie Zeng, Haohao Zhu, Liang Yang, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of hate speech has caused significant harm to society. The
intensity and directionality of hate are closely tied to the target and
argument it is associated with. However, research on hate speech detection in
Chinese has lagged behind, and existing datasets lack span-level fine-grained
annotations. Furthermore, the lack of research on Chinese hateful slang poses a
significant challenge. In this paper, we provide a solution for fine-grained
detection of Chinese hate speech. First, we construct a dataset containing
Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first
span-level Chinese hate speech dataset. Secondly, we evaluate the span-level
hate speech detection performance of existing models using STATE ToxiCN.
Finally, we conduct the first study on Chinese hateful slang and evaluate the
ability of LLMs to detect such expressions. Our work contributes valuable
resources and insights to advance span-level hate speech detection in Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable abilities across
various language tasks, but solving complex reasoning problems remains a
significant challenge. While existing methods, such as Chain-of-Thought (CoT)
and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or
structuring prompts, they typically perform a single pass of reasoning and may
fail to revisit flawed paths, compromising accuracy. To address this
limitation, we propose a novel reasoning framework called Forest-of-Thought
(FoT), which integrates multiple reasoning trees to leverage collective
decision-making for solving complex logical problems. FoT employs sparse
activation strategies to select the most relevant reasoning paths, improving
both efficiency and accuracy. Additionally, we introduce a dynamic
self-correction strategy that enables real-time error correction, along with
consensus-guided decision-making strategies to optimize both correctness and
computational resources. Experimental results demonstrate that the FoT
framework, combined with these strategies, significantly enhances the reasoning
capabilities of LLMs, enabling them to solve complex tasks with greater
precision and efficiency.Code will be available at
https://github.com/iamhankai/Forest-of-Thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at
  https://github.com/iamhankai/Forest-of-Thought</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware or Context-Insensitive? Assessing LLMs' Performance in
  Document-Level Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wafaa Mohammed, Vlad Niculae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly strong contenders in machine
translation. In this work, we focus on document-level translation, where some
words cannot be translated without context from outside the sentence.
Specifically, we investigate the ability of prominent LLMs to utilize the
document context during translation through a perturbation analysis (analyzing
models' robustness to perturbed and randomized document context) and an
attribution analysis (examining the contribution of relevant context to the
translation). We conduct an extensive evaluation across nine LLMs from diverse
model families and training paradigms, including translation-specialized LLMs,
alongside two encoder-decoder transformer baselines. We find that LLMs'
improved document-translation performance compared to encoder-decoder models is
not reflected in pronoun translation performance. Our analysis highlight the
need for context-aware finetuning of LLMs with a focus on relevant parts of the
context to improve their reliability for document-level translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Approach to Routing and Cascading for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasper Dekoninck, Maximilian Baader, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of a wide range of large language models (LLMs) embedded in
various agentic systems has significantly increased the potential of model
selection strategies to improve the cost-performance tradeoff. Existing
strategies involve either routing, where a single model is chosen per query, or
cascading, which sequentially runs increasingly larger models until a
satisfactory answer is found. However, current approaches face three key
limitations: they (1) lack formal proofs of optimality, (2) fail to identify
the conditions under which these strategies are most effective to improve the
cost-performance tradeoff, and (3) are unable to combine both paradigms for
further improvements. To address these issues, we first derive a novel optimal
strategy for cascading and prove the optimality of an existing routing
strategy. Further, we propose cascade routing, a unified framework that
integrates routing and cascading into a theoretically optimal strategy. Through
our analysis, we identify good quality estimators as the critical factor for
the success of model selection paradigms. Finally, in our experiments, we show
that cascade routing consistently outperforms the individual approaches by a
large margin and we analyze quality estimators to determine when routing and/or
cascading are useful paradigms for model selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mechanism and Emergence of Stacked Attention Heads in Multi-Layer
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiberiu Musat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, I introduce the retrieval problem, a simple yet common
reasoning task that can be solved only by transformers with a minimum number of
layers, which grows logarithmically with the input size. I empirically show
that large language models can solve the task under different prompting
formulations without any fine-tuning. To understand how transformers solve the
retrieval problem, I train several transformers on a minimal formulation.
Successful learning occurs only under the presence of an implicit curriculum. I
uncover the learned mechanisms by studying the attention maps in the trained
transformers. I also study the training process, uncovering that attention
heads always emerge in a specific sequence guided by the implicit curriculum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating and Improving <span class="highlight-title">Graph</span> to Text Generation with Large Language
  Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie He, Yijun Yang, Wanqiu Long, Deyi Xiong, Victor Gutierrez-Basulto, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated immense potential across
various tasks. However, research for exploring and improving the capabilities
of LLMs in interpreting graph structures remains limited. To address this gap,
we conduct a comprehensive evaluation of prompting current open-source LLMs on
graph-to-text generation tasks. Although we explored the optimal prompting
strategies and proposed a novel and effective diversity-difficulty-based
few-shot sample selection method, we found that the improvements from
tuning-free approaches were incremental, as LLMs struggle with planning on
complex graphs, particularly those with a larger number of triplets. To further
improve LLMs in planning with graph sequences and grounding in truth, we
introduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:
reordering and attribution. Through extensive automatic and human evaluations,
we demonstrate significant improvements in the quality of generated text from
both few-shot learning and fine-tuning perspectives using the PlanGTG dataset.
Our study paves the way for new research directions in graph-to-text
generation. PlanGTG datasets can be found in https://github.com/probe2/kg_text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-based Depth Pruning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyun Wee, Minjae Park, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth pruning aims to reduce the inference cost of a large language model
without any hardware-specific complications, by simply removing several less
important transformer blocks. However, our empirical findings suggest that the
importance of a transformer block may be highly task-dependent -- a block that
is crucial for a task can be removed without degrading the accuracy on another
task. Based on this observation, we develop a dynamic depth pruning algorithm,
coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which
blocks to omit from the model based on the input prompt. PuDDing operates by
training a lightweight router to predict the best omission set among a set of
options, where this option set has also been constructed in a data-driven
manner. Empirical results on commonsense reasoning benchmarks demonstrate that
PuDDing effectively accelerates the inference language models, and achieves
better on-task performance than static depth pruning baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delving into LLM-assisted writing in biomedical publications through
  excess vocabulary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Kobak, Rita González-Márquez, Emőke-Ágnes Horvát, Jan Lause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like ChatGPT can generate and revise text with
human-level performance. These models come with clear limitations: they can
produce inaccurate information, reinforce existing biases, and be easily
misused. Yet, many scientists use them for their scholarly writing. But how
wide-spread is such LLM usage in the academic literature? To answer this
question for the field of biomedical research, we present an unbiased,
large-scale approach: we study vocabulary changes in over 15 million biomedical
abstracts from 2010--2024 indexed by PubMed, and show how the appearance of
LLMs led to an abrupt increase in the frequency of certain style words. This
excess word analysis suggests that at least 13.5% of 2024 abstracts were
processed with LLMs. This lower bound differed across disciplines, countries,
and journals, reaching 40% for some subcorpora. We show that LLMs have had an
unprecedented impact on scientific writing in biomedical research, surpassing
the effect of major world events such as the Covid pandemic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: Updating the manuscript to include all PubMed abstracts until the
  end of 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Possess Sensitive to Sentiment? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02370v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02370v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Xichou Zhu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Zhi Li, Zhiyang Xu, Wei Luo, Junhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently displayed their extraordinary
capabilities in language understanding. However, how to comprehensively assess
the sentiment capabilities of LLMs continues to be a challenge. This paper
investigates the ability of LLMs to detect and react to sentiment in text
modal. As the integration of LLMs into diverse applications is on the rise, it
becomes highly critical to comprehend their sensitivity to emotional tone, as
it can influence the user experience and the efficacy of sentiment-driven
tasks. We conduct a series of experiments to evaluate the performance of
several prominent LLMs in identifying and responding appropriately to
sentiments like positive, negative, and neutral emotions. The models' outputs
are analyzed across various sentiment benchmarks, and their responses are
compared with human evaluations. Our discoveries indicate that although LLMs
show a basic sensitivity to sentiment, there are substantial variations in
their accuracy and consistency, emphasizing the requirement for further
enhancements in their training processes to better capture subtle emotional
cues. Take an example in our findings, in some cases, the models might wrongly
classify a strongly positive sentiment as neutral, or fail to recognize sarcasm
or irony in the text. Such misclassifications highlight the complexity of
sentiment analysis and the areas where the models need to be refined. Another
aspect is that different LLMs might perform differently on the same set of
data, depending on their architecture and training datasets. This variance
calls for a more in-depth study of the factors that contribute to the
performance differences and how they can be optimized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Privacy-Savvy Are Large Language Models? A Case Study on Compliance
  and Privacy Technical <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02375v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02375v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Xichou Zhu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Zhi Li, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in large language models (LLMs) have significantly
expanded their applications across various fields such as language generation,
summarization, and complex question answering. However, their application to
privacy compliance and technical privacy reviews remains under-explored,
raising critical concerns about their ability to adhere to global privacy
standards and protect sensitive user data. This paper seeks to address this gap
by providing a comprehensive case study evaluating LLMs' performance in
privacy-related tasks such as privacy information extraction (PIE), legal and
regulatory key point detection (KPD), and question answering (QA) with respect
to privacy policies and data protection regulations. We introduce a Privacy
Technical Review (PTR) framework, highlighting its role in mitigating privacy
risks during the software development life-cycle. Through an empirical
assessment, we investigate the capacity of several prominent LLMs, including
BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks
and technical privacy reviews. Our experiments benchmark the models across
multiple dimensions, focusing on their precision, recall, and F1-scores in
extracting privacy-sensitive information and detecting key regulatory
compliance points. While LLMs show promise in automating privacy reviews and
identifying regulatory discrepancies, significant gaps persist in their ability
to fully comply with evolving legal standards. We provide actionable
recommendations for enhancing LLMs' capabilities in privacy compliance,
emphasizing the need for robust model improvements and better integration with
legal and regulatory requirements. This study underscores the growing
importance of developing privacy-aware LLMs that can both support businesses in
compliance efforts and safeguard user privacy rights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Cross-Lingual Explanation of Artwork in Large-scale Vision
  Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shintaro Ozaki, Kazuki Hayashi, Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the performance of Large-scale Vision Language Models (LVLMs) improves,
they are increasingly capable of responding in multiple languages, and there is
an expectation that the demand for explanations generated by LVLMs will grow.
However, pre-training of Vision Encoder and the integrated training of LLMs
with Vision Encoder are mainly conducted using English training data, leaving
it uncertain whether LVLMs can completely handle their potential when
generating explanations in languages other than English. In addition,
multilingual QA benchmarks that create datasets using machine translation have
cultural differences and biases, remaining issues for use as evaluation tasks.
To address these challenges, this study created an extended dataset in multiple
languages without relying on machine translation. This dataset that takes into
account nuances and country-specific phrases was then used to evaluate the
generation explanation abilities of LVLMs. Furthermore, this study examined
whether Instruction-Tuning in resource-rich English improves performance in
other languages. Our findings indicate that LVLMs perform worse in languages
other than English compared to English. In addition, it was observed that LVLMs
struggle to effectively manage the knowledge learned from English data. Our
dataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verbalized Machine Learning: Revisiting Machine Learning with Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Z. Xiao, Robert Bamler, Bernhard Schölkopf, Weiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the progress made by large language models (LLMs), we introduce
the framework of verbalized machine learning (VML). In contrast to conventional
machine learning (ML) models that are typically optimized over a continuous
parameter space, VML constrains the parameter space to be human-interpretable
natural language. Such a constraint leads to a new perspective of function
approximation, where an LLM with a text prompt can be viewed as a function
parameterized by the text prompt. Guided by this perspective, we revisit
classical ML problems, such as regression and classification, and find that
these problems can be solved by an LLM-parameterized learner and optimizer. The
major advantages of VML include (1) easy encoding of inductive bias: prior
knowledge about the problem and hypothesis class can be encoded in natural
language and fed into the LLM-parameterized learner; (2) automatic model class
selection: the optimizer can automatically select a model class based on data
and verbalized prior knowledge, and it can update the model class during
training; and (3) interpretable learner updates: the LLM-parameterized
optimizer can provide explanations for why an update is performed. We
empirically verify the effectiveness of VML, and hope that VML can serve as a
stepping stone to stronger interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (116 pages, 32
  figures, v3: refined the paper structure and added more empirical results)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiTAR: Diffusion <span class="highlight-title">Transformer</span> Autoregressive Modeling for Speech
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent studies have attempted to autoregressively generate continuous
speech representations without discrete speech tokens by combining diffusion
and autoregressive models, yet they often face challenges with excessive
computational loads or suboptimal outcomes. In this work, we propose Diffusion
Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive
framework combining a language model with a diffusion transformer. This
approach significantly enhances the efficacy of autoregressive models for
continuous tokens and reduces computational demands. DiTAR utilizes a
divide-and-conquer strategy for patch generation, where the language model
processes aggregated patch embeddings and the diffusion transformer
subsequently generates the next patch based on the output of the language
model. For inference, we propose defining temperature as the time point of
introducing noise during the reverse diffusion ODE to balance diversity and
determinism. We also show in the extensive scaling analysis that DiTAR has
superb scalability. In zero-shot speech generation, DiTAR achieves
state-of-the-art performance in robustness, speaker similarity, and
naturalness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Optimization for Reasoning with Pseudo Feedback <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy F. Chen, Shafiq Joty, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference optimization techniques, such as Direct Preference Optimization
(DPO), are frequently employed to enhance the reasoning capabilities of large
language models (LLMs) in domains like mathematical reasoning and coding,
typically following supervised fine-tuning. These methods rely on high-quality
labels for reasoning tasks to generate preference pairs; however, the
availability of reasoning datasets with human-verified labels is limited. In
this study, we introduce a novel approach to generate pseudo feedback for
reasoning tasks by framing the labeling of solutions to reason problems as an
evaluation against associated test cases. We explore two forms of pseudo
feedback based on test cases: one generated by frontier LLMs and the other by
extending self-consistency to multi-test-case. We conduct experiments on both
mathematical reasoning and coding tasks using pseudo feedback for preference
optimization, and observe improvements across both tasks. Specifically, using
Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,
surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and
College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,
respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on
LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 11 figures. ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Adoption and Efficacy of Large Language Models: Evidence From
  Consumer Complaints in the Financial Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16466v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16466v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyu Shin, Jin Kim, Jiwoong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are reshaping consumer decision-making,
particularly in communication with firms, yet our understanding of their impact
remains limited. This research explores the effect of LLMs on consumer
complaints submitted to the Consumer Financial Protection Bureau from 2015 to
2024, documenting the adoption of LLMs for drafting complaints and evaluating
the likelihood of obtaining relief from financial firms. We analyzed over 1
million complaints and identified a significant increase in LLM usage following
the release of ChatGPT. We find that LLM usage is associated with an increased
likelihood of obtaining relief from financial firms. To investigate this
relationship, we employ an instrumental variable approach to mitigate
endogeneity concerns around LLM adoption. Although instrumental variables
suggest a potential causal link, they cannot fully capture all unobserved
heterogeneity. To further establish this causal relationship, we conducted
controlled experiments, which support that LLMs can enhance the clarity and
persuasiveness of consumer narratives, thereby increasing the likelihood of
obtaining relief. Our findings suggest that facilitating access to LLMs can
help firms better understand consumer concerns and level the playing field
among consumers. This underscores the importance of policies promoting
technological accessibility, enabling all consumers to effectively voice their
concerns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RareAgents: Advancing Rare Disease Care through LLM-Empowered
  Multi-disciplinary Team 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanzhong Chen, Ye Jin, Xiaohao Mao, Lun Wang, Shuyang Zhang, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rare diseases, despite their low individual incidence, collectively impact
around 300 million people worldwide due to the vast number of diseases. The
involvement of multiple organs and systems, and the shortage of specialized
doctors with relevant experience make diagnosing and treating rare diseases
more challenging than common diseases. Recently, agents powered by large
language models (LLMs) have demonstrated notable applications across various
domains. In the medical field, some agent methods have outperformed direct
prompts in question-answering tasks from medical examinations. However, current
agent frameworks are not well-adapted to real-world clinical scenarios,
especially those involving the complex demands of rare diseases. To bridge this
gap, we introduce RareAgents, the first LLM-driven multi-disciplinary team
framework designed specifically for the complex clinical context of rare
diseases. RareAgents integrates advanced Multidisciplinary Team (MDT)
coordination, memory mechanisms, and medical tools utilization, leveraging
Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents
outperforms state-of-the-art domain-specific models, GPT-4o, and current agent
frameworks in differential diagnosis and medication recommendation for rare
diseases. Furthermore, we contribute a novel rare disease dataset,
MIMIC-IV-Ext-Rare, to support further advancements in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction,
  and Column Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems have unlocked easier access to critical data insights by
enabling natural language queries over structured databases. However, deploying
such systems in enterprise environments remains challenging due to factors such
as large, complex schemas (> 3000 columns), diverse SQL dialects (e.g.,
BigQuery, Snowflake) and sophisticated query requirements (e.g.,
transformation, analytics). Current state-of-the-art performance on the Spider
2.0 dataset -- a benchmark built to mimic such complex environments -- remains
limited at 20%. Key limitations include inadequate instruction-following, poor
long-context comprehension, weak self-refinement, and insufficient
dialect-specific knowledge. To address these gaps, we propose ReFoRCE
(Self-Refinement Agent with Format Restriction and Column Exploration) which
introduces (1) table compression to mitigate long-context limitations (2)
format restriction to ensure accurate answer format, and (3) iterative column
exploration for enhanced schema understanding. Additionally, it employs
self-refinement pipeline consisting of (1) parallelized workflows with voting
mechanisms and (2) a Common Table Expression (CTE) based refinement approach to
handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring
31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Guided Speculative Decoding for Efficient LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Reward-Guided Speculative Decoding (RSD), a novel framework
aimed at improving the efficiency of inference in large language models (LLMs).
RSD synergistically combines a lightweight draft model with a more powerful
target model, incorporating a controlled bias to prioritize high-reward
outputs, in contrast to existing speculative decoding methods that enforce
strict unbiasedness. RSD employs a process reward model to evaluate
intermediate decoding steps and dynamically decide whether to invoke the target
model, optimizing the trade-off between computational cost and output quality.
We theoretically demonstrate that a threshold-based mixture strategy achieves
an optimal balance between resource utilization and performance. Extensive
evaluations on challenging reasoning benchmarks, including Olympiad-level
tasks, show that RSD delivers significant efficiency gains against decoding
with the target model only (up to 4.4x fewer FLOPs), while achieving
significant better accuracy than parallel decoding method on average (up to
+3.5). These results highlight RSD as a robust and cost-effective approach for
deploying LLMs in resource-intensive scenarios. The code is available at
https://github.com/BaohaoLiao/RSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond the Singular: The Essential Role of Multiple Generations in
  Effective Benchmark Evaluation and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Zhang, Hengrui Cai, Wenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant utilities in
real-world applications, exhibiting impressive capabilities in natural language
processing and understanding. Benchmark evaluations are crucial for assessing
the capabilities of LLMs as they can provide a comprehensive assessment of
their strengths and weaknesses. However, current evaluation methods often
overlook the inherent randomness of LLMs by employing deterministic generation
strategies or relying on a single random sample, resulting in unaccounted
sampling variance and unreliable benchmark score estimates. In this paper, we
propose a hierarchical statistical model that provides a more comprehensive
representation of the benchmarking process by incorporating both benchmark
characteristics and LLM randomness. We show that leveraging multiple
generations improves the accuracy of estimating the benchmark score and reduces
variance. We also introduce $\mathbb P\left(\text{correct}\right)$, a
prompt-level difficulty score based on correct ratios, providing fine-grained
insights into individual prompts. Additionally, we create a data map that
visualizes difficulty and semantic prompts, enabling error detection and
quality control in benchmark construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 table, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model-<span class="highlight-title">Brain</span>ed GUI Agents: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18279v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18279v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GUIs have long been central to human-computer interaction, providing an
intuitive and visually-driven way to access and interact with digital systems.
The advent of LLMs, particularly multimodal models, has ushered in a new era of
GUI automation. They have demonstrated exceptional capabilities in natural
language understanding, code generation, and visual processing. This has paved
the way for a new generation of LLM-brained GUI agents capable of interpreting
complex GUI elements and autonomously executing actions based on natural
language instructions. These agents represent a paradigm shift, enabling users
to perform intricate, multi-step tasks through simple conversational commands.
Their applications span across web navigation, mobile app interactions, and
desktop automation, offering a transformative user experience that
revolutionizes how individuals interact with software. This emerging field is
rapidly advancing, with significant progress in both research and industry.
  To provide a structured understanding of this trend, this paper presents a
comprehensive survey of LLM-brained GUI agents, exploring their historical
evolution, core components, and advanced techniques. We address research
questions such as existing GUI agent frameworks, the collection and utilization
of data for training specialized GUI agents, the development of large action
models tailored for GUI tasks, and the evaluation metrics and benchmarks
necessary to assess their effectiveness. Additionally, we examine emerging
applications powered by these agents. Through a detailed analysis, this survey
identifies key research gaps and outlines a roadmap for future advancements in
the field. By consolidating foundational knowledge and state-of-the-art
developments, this work aims to guide both researchers and practitioners in
overcoming challenges and unlocking the full potential of LLM-brained GUI
agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The collection of papers reviewed in this survey will be hosted and
  regularly updated on the GitHub repository:
  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a
  searchable webpage is available at https://aka.ms/gui-agent for easier access
  and exploration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detect, Investigate, Judge and Determine: A Knowledge-guided Framework
  for Few-shot Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Liu, Jiajun Zhu, Xukai Liu, Haoyu Tang, Yanghai Zhang, Kai Zhang, Xiaofang Zhou, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news
from real ones in extremely low-resource scenarios. This task has garnered
increased attention due to the widespread dissemination and harmful impact of
fake news on social media. Large Language Models (LLMs) have demonstrated
competitive performance with the help of their rich prior knowledge and
excellent in-context learning abilities. However, existing methods face
significant limitations, such as the Understanding Ambiguity and Information
Scarcity, which significantly undermine the potential of LLMs. To address these
shortcomings, we propose a Dual-perspective Knowledge-guided Fake News
Detection (DKFND) model, designed to enhance LLMs from both inside and outside
perspectives. Specifically, DKFND first identifies the knowledge concepts of
each news article through a Detection Module. Subsequently, DKFND creatively
designs an Investigation Module to retrieve inside and outside valuable
information concerning to the current news, followed by another Judge Module to
evaluate the relevance and confidence of them. Finally, a Determination Module
further derives two respective predictions and obtain the final result.
Extensive experiments on two public datasets show the efficacy of our proposed
method, particularly in low-resource settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arithmetic in <span class="highlight-title">Transformer</span>s Explained 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02619v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02619v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Quirke, Clement Neo, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent work has shown transformers can learn addition, previous models
exhibit poor prediction accuracy and are limited to small numbers. Furthermore,
the relationship between single-task and multitask arithmetic capabilities
remains unexplored. In this work, we analyze 44 autoregressive transformer
models trained on addition, subtraction, or both. These include 16
addition-only models, 2 subtraction-only models, 8 "mixed" models trained to
perform addition and subtraction, and 14 mixed models initialized with
parameters from an addition-only model. The models span 5- to 15-digit
questions, 2 to 4 attention heads, and 2 to 3 layers. We show that the addition
models converge on a common logical algorithm, with most models achieving
>99.999% prediction accuracy. We provide a detailed mechanistic explanation of
how this algorithm is implemented within the network architecture.
Subtraction-only models have lower accuracy. With the initialized mixed models,
through parameter transfer experiments, we explore how multitask learning
dynamics evolve, revealing that some features originally specialized for
addition become polysemantic, serving both operations, and boosting subtraction
accuracy. We explain the mixed algorithm mechanically. Finally, we introduce a
reusable library of mechanistic interpretability tools to define, locate, and
visualize these algorithmic circuits across multiple models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological
  and Multilingual Knowledge Base <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  URIEL is a knowledge base offering geographical, phylogenetic, and
typological vector representations for 7970 languages. It includes distance
measures between these vectors for 4005 languages, which are accessible via the
lang2vec tool. Despite being frequently cited, URIEL is limited in terms of
linguistic inclusion and overall usability. To tackle these challenges, we
introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses
these limitations. In addition to expanding typological feature coverage for
2898 languages, URIEL+ improves the user experience with robust, customizable
distance calculations to better suit the needs of users. These upgrades also
offer competitive performance on downstream tasks and provide distances that
better align with linguistic distance studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accuracy and Political Bias of News Source Credibility Ratings by Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00228v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00228v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai-Cheng Yang, Filippo Menczer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engines increasingly leverage large language models (LLMs) to generate
direct answers, and AI chatbots now access the Internet for fresh data. As
information curators for billions of users, LLMs must assess the accuracy and
reliability of different sources. This paper audits nine widely used LLMs from
three leading providers -- OpenAI, Google, and Meta -- to evaluate their
ability to discern credible and high-quality information sources from
low-credibility ones. We find that while LLMs can rate most tested news
outlets, larger models more frequently refuse to provide ratings due to
insufficient information, whereas smaller models are more prone to making
errors in their ratings. For sources where ratings are provided, LLMs exhibit a
high level of agreement among themselves (average Spearman's $\rho = 0.79$),
but their ratings align only moderately with human expert evaluations (average
$\rho = 0.50$). Analyzing news sources with different political leanings in the
US, we observe a liberal bias in credibility ratings yielded by all LLMs in
default configurations. Additionally, assigning partisan roles to LLMs
consistently induces strong politically congruent bias in their ratings. These
findings have important implications for the use of LLMs in curating news and
political information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we tackle industry challenges in video content classification
by exploring and optimizing GPT-based models for zero-shot classification
across seven critical categories of video quality. We contribute a novel
approach to improving GPT's performance through prompt optimization and policy
refinement, demonstrating that simplifying complex policies significantly
reduces false negatives. Additionally, we introduce a new
decomposition-aggregation-based prompt engineering technique, which outperforms
traditional single-prompt methods. These experiments, conducted on real
industry problems, show that thoughtful prompt design can substantially enhance
GPT's performance without additional finetuning, offering an effective and
scalable solution for improving video classification systems across various
domains in industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing and Boosting the Power of Fine-Grained Visual Recognition for
  <span class="highlight-title">Multi-modal</span> Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, Yuxin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) have shown remarkable abilities in
various visual understanding tasks. However, MLLMs still struggle with
fine-grained visual recognition (FGVR), which aims to identify
subordinate-level categories from images. This can negatively impact more
advanced capabilities of MLLMs, such as object-centric visual question
answering and reasoning. In our study, we revisit three quintessential
capabilities of MLLMs for FGVR, including object information extraction,
category knowledge reserve, object-category alignment, and position of the root
cause as a misalignment problem. To address this issue, we present Finedefics,
an MLLM that enhances the model's FGVR capability by incorporating informative
attribute descriptions of objects into the training phase. We employ
contrastive learning on object-attribute pairs and attribute-category pairs
simultaneously and use examples from similar but incorrect categories as hard
negatives, naturally bringing representations of visual objects and category
names closer. Extensive evaluations across multiple popular FGVR datasets
demonstrate that Finedefics outperforms existing MLLMs of comparable parameter
sizes, showcasing its remarkable efficacy. The code is available at
https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SELP: Generating Safe and Efficient Task Plans for Robot Agents with
  Large Language Models <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wu, Zikang Xiong, Yiran Hu, Shreyash S. Iyengar, Nan Jiang, Aniket Bera, Lin Tan, Suresh Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in large language models (LLMs) that enhance
robot agents' understanding and execution of natural language (NL) commands,
ensuring the agents adhere to user-specified constraints remains challenging,
particularly for complex commands and long-horizon tasks. To address this
challenge, we present three key insights, equivalence voting, constrained
decoding, and domain-specific fine-tuning, which significantly enhance LLM
planners' capability in handling complex tasks. Equivalence voting ensures
consistency by generating and sampling multiple Linear Temporal Logic (LTL)
formulas from NL commands, grouping equivalent LTL formulas, and selecting the
majority group of formulas as the final LTL formula. Constrained decoding then
uses the generated LTL formula to enforce the autoregressive inference of
plans, ensuring the generated plans conform to the LTL. Domain-specific
fine-tuning customizes LLMs to produce safe and efficient plans within specific
task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these
insights to create LLM planners to generate plans adhering to user commands
with high confidence. We demonstrate the effectiveness and generalizability of
SELP across different robot agents and tasks, including drone navigation and
robot manipulation. For drone navigation tasks, SELP outperforms
state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks
conforming to NL commands) and by 19.8% in plan efficiency. For robot
manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our
datasets for evaluating NL-to-LTL and robot task planning will be released in
github.com/lt-asset/selp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the 2025 IEEE
  International Conference on Robotics and Automation (ICRA), May 19-23, 2025,
  Atlanta, USA, and for inclusion in the conference proceeding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree? <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05584v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05584v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, Xing Yu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward Models (RMs) are crucial for aligning language models with human
preferences. Currently, the evaluation of RMs depends on measuring accuracy
against a validation set of manually annotated preference data. Although this
method is straightforward and widely adopted, the relationship between RM
accuracy and downstream policy performance remains under-explored. In this
work, we conduct experiments in a synthetic setting to investigate how
differences in RM measured by accuracy translate into gaps in optimized policy
performance. Our findings reveal that while there is a weak positive
correlation between accuracy and downstream performance, policies optimized
towards RMs with similar accuracy can exhibit quite different performance.
Moreover, we discover that the way of measuring accuracy significantly impacts
its ability to predict the final policy performance. Through the lens of the
Regressional Goodhart effect, we recognize that accuracy, when used for
measuring RM quality, can fail to fully capture the potential RM
overoptimization. This underscores the inadequacy of relying solely on accuracy
to reflect their impact on policy optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Syntriever: How to Train Your Retriever with Synthetic Data from LLMs <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsang Kim, Seungjun Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have boosted progress in many AI applications. Recently, there were
attempts to distill the vast knowledge of LLMs into information retrieval
systems. Those distillation methods mostly use output probabilities of LLMs
which are unavailable in the latest black-box LLMs. We propose Syntriever, a
training framework for retrievers using synthetic data from black-box LLMs.
Syntriever consists of two stages. Firstly in the distillation stage, we
synthesize relevant and plausibly irrelevant passages and augmented queries
using chain-of-thoughts for the given queries. LLM is asked to self-verify the
synthetic data for possible hallucinations, after which retrievers are trained
with a loss designed to cluster the embeddings of relevant passages. Secondly
in the alignment stage, we align the retriever with the preferences of LLMs. We
propose a preference modeling called partial Plackett-Luce ranking to learn LLM
preferences with regularization which prevents the model from deviating
excessively from that trained in the distillation stage. Experiments show that
Syntriever achieves state-of-the-art performances on benchmark datasets from
various domains in nDCG@$K$. The code is available at
\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the Nations of the Americas Chapter of the Association for
  Computational Linguistics (NAACL), Findings, Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small
  Language Models for Biomedical Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12746v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12746v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Jian Wan, Siliang Tang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When addressing professional questions in the biomedical domain, humans
typically acquire multiple pieces of information as evidence and engage in
multifaceted analysis to provide high-quality answers. Current LLM-based
question answering methods lack a detailed definition and learning process for
evidence analysis, leading to the risk of error propagation and hallucinations
while using evidence. Although increasing the parameter size of LLMs can
alleviate these issues, it also presents challenges in training and deployment
with limited resources. In this study, we propose EvidenceMap, which aims to
enable a tiny pre-trained language model to explicitly learn multiple aspects
of biomedical evidence, including supportive evaluation, logical correlation
and content summarization, thereby latently guiding a small generative model
(around 3B parameters) to provide textual responses. Experimental results
demonstrate that our method, learning evidence analysis by fine-tuning a model
with only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and
5.7% in reference-based quality and accuracy, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAGNET: Augmenting Generative Decoders with Representation Learning and
  Infilling Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savya Khosla, Aditi Tiwari, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, Jing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While originally designed for unidirectional generative modeling,
decoder-only large language models (LLMs) are increasingly being adapted for
bidirectional modeling. However, unidirectional and bidirectional models are
typically trained separately with distinct objectives (generation and
representation learning). This separation overlooks the opportunity for
developing a more versatile language model and for these objectives to
complement each other. In this work, we propose MAGNET, a method for adapting
decoder-only LLMs to generate robust representations and infill missing text
spans. MAGNET employs three self-supervised training objectives and introduces
an attention mechanism that combines bidirectional and causal attention,
enabling unified training across all objectives. Our results demonstrate that
LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and
sentence-level representation learning tasks, (2) generate contextually
appropriate text infills by leveraging past and future contexts, (3) perform
open-ended text generation without excessive repetition of words or phrases,
and (4) preserve the knowledge and reasoning capability gained by the LLM
during pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">87</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an efficient multi-level convolution architecture
for 3D visual grounding. Conventional methods are difficult to meet the
requirements of real-time inference due to the two-stage or point-based
architecture. Inspired by the success of multi-level fully sparse convolutional
architecture in 3D object detection, we aim to build a new 3D visual grounding
framework following this technical route. However, as in 3D visual grounding
task the 3D scene representation should be deeply interacted with text
features, sparse convolution-based architecture is inefficient for this
interaction due to the large amount of voxel features. To this end, we propose
text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D
scene representation and text features in an efficient way by gradual region
pruning and target completion. Specifically, TGP iteratively sparsifies the 3D
scene representation and thus efficiently interacts the voxel features with
text features by cross-attention. To mitigate the affect of pruning on delicate
geometric information, CBA adaptively fixes the over-pruned region by voxel
completion with negligible computational overhead. Compared with previous
single-stage methods, our method achieves top inference speed and surpasses
previous fastest method by 100\% FPS. Our method also achieves state-of-the-art
accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on
ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code
is available at
\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-RLHF: The Next Step Forward in <span class="highlight-title">Multimodal</span> LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite notable advancements in Multimodal Large Language Models (MLLMs),
most state-of-the-art models have not undergone thorough alignment with human
preferences. This gap exists because current alignment research has primarily
achieved progress in specific areas (e.g., hallucination reduction), while the
broader question of whether aligning models with human preferences can
systematically enhance MLLM capability remains largely unexplored. To this end,
we introduce MM-RLHF, a dataset containing $\mathbf{120k}$ fine-grained,
human-annotated preference comparison pairs. This dataset represents a
substantial advancement over existing resources, offering superior size,
diversity, annotation granularity, and quality. Leveraging this dataset, we
propose several key innovations to improve both the quality of reward models
and the efficiency of alignment algorithms. Notably, we introduce a
Critique-Based Reward Model, which generates critiques of model outputs before
assigning scores, offering enhanced interpretability and more informative
feedback compared to traditional scalar reward mechanisms. Additionally, we
propose Dynamic Reward Scaling, a method that adjusts the loss weight of each
sample according to the reward signal, thereby optimizing the use of
high-quality comparison pairs. Our approach is rigorously evaluated across
$\mathbf{10}$ distinct dimensions and $\mathbf{27}$ benchmarks, with results
demonstrating significant and consistent improvements in model performance.
Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm
leads to a $\mathbf{19.5}$% increase in conversational abilities and a
$\mathbf{60}$% improvement in safety.
  We have open-sourced the preference dataset, reward model, training and
evaluation code, as well as reward modeling and safety benchmarks. For more
details, please visit our project page: https://mm-rlhf.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mm-rlhf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-Adaptive Sampling for Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have become the leading choice for generative tasks
across diverse domains. However, their reliance on multiple sequential forward
passes significantly limits real-time performance. Previous acceleration
methods have primarily focused on reducing the number of sampling steps or
reusing intermediate results, failing to leverage variations across spatial
regions within the image due to the constraints of convolutional U-Net
structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in
handling variable number of tokens, we introduce RAS, a novel, training-free
sampling strategy that dynamically assigns different sampling ratios to regions
within an image based on the focus of the DiT model. Our key observation is
that during each sampling step, the model concentrates on semantically
meaningful regions, and these areas of focus exhibit strong continuity across
consecutive steps. Leveraging this insight, RAS updates only the regions
currently in focus, while other regions are updated using cached noise from the
previous step. The model's focus is determined based on the output from the
preceding step, capitalizing on the temporal consistency we observed. We
evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up
to 2.36x and 2.51x, respectively, with minimal degradation in generation
quality. Additionally, a user study reveals that RAS delivers comparable
qualities under human evaluation while achieving a 1.6x speedup. Our approach
makes a significant step towards more efficient diffusion transformers,
enhancing their potential for real-time applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplifying DINO via Coding Rate Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Wu, Jingyuan Zhang, Druv Pai, XuDong Wang, Chandan Singh, Jianwei Yang, Jianfeng Gao, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DINO and DINOv2 are two model families being widely used to learn
representations from unlabeled imagery data at large scales. Their learned
representations often enable state-of-the-art performance for downstream tasks,
such as image classification and segmentation. However, they employ many
empirically motivated design choices and their training pipelines are highly
complex and unstable -- many hyperparameters need to be carefully tuned to
ensure that the representations do not collapse -- which poses considerable
difficulty to improving them or adapting them to new domains. In this work, we
posit that we can remove most such-motivated idiosyncrasies in the pre-training
pipelines, and only need to add an explicit coding rate term in the loss
function to avoid collapse of the representations. As a result, we obtain
highly simplified variants of the DINO and DINOv2 which we call SimDINO and
SimDINOv2, respectively. Remarkably, these simplified models are more robust to
different design choices, such as network architecture and hyperparameters, and
they learn even higher-quality representations, measured by performance on
downstream tasks, offering a Pareto improvement over the corresponding DINO and
DINOv2 models. This work highlights the potential of using simplifying design
principles to improve the empirical practice of deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ReStyle3D, a novel framework for scene-level appearance transfer
from a single style image to a real-world scene represented by multiple views.
The method combines explicit semantic correspondences with multi-view
consistency to achieve precise and coherent stylization. Unlike conventional
stylization methods that apply a reference style globally, ReStyle3D uses
open-vocabulary segmentation to establish dense, instance-level correspondences
between the style and real-world images. This ensures that each object is
stylized with semantically matched textures. It first transfers the style to a
single view using a training-free semantic-attention mechanism in a diffusion
model. It then lifts the stylization to additional views via a learned
warp-and-refine network guided by monocular depth and pixel-wise
correspondences. Experiments show that ReStyle3D consistently outperforms prior
methods in structure preservation, perceptual style similarity, and multi-view
coherence. User studies further validate its ability to produce
photo-realistic, semantically faithful results. Our code, pretrained models,
and dataset will be publicly released, to support new applications in interior
design, virtual staging, and 3D-consistent stylization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://restyle3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ocular Disease Classification Using CNN with Deep Convolutional
  Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Kunwar, Dibakar Raj Pant, Jukka Heikkonen, Rajeev Kanth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Convolutional Neural Network (CNN) has shown impressive performance in
image classification because of its strong learning capabilities. However, it
demands a substantial and balanced dataset for effective training. Otherwise,
networks frequently exhibit over fitting and struggle to generalize to new
examples. Publicly available dataset of fundus images of ocular disease is
insufficient to train any classification model to achieve satisfactory
accuracy. So, we propose Generative Adversarial Network(GAN) based data
generation technique to synthesize dataset for training CNN based
classification model and later use original disease containing ocular images to
test the model. During testing the model classification accuracy with the
original ocular image, the model achieves an accuracy rate of 78.6% for myopia,
88.6% for glaucoma, and 84.6% for cataract, with an overall classification
accuracy of 84.6%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Pranto, Omar Faruk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate object detection is an important topic in the
development of computer vision systems. With the advent of deep learning
techniques, the accuracy of object detection has increased significantly. The
project aims to integrate a modern technique for object detection with the aim
of achieving high accuracy with real-time performance. The reliance on other
computer vision algorithms in many object identification systems, which results
in poor and ineffective performance, is a significant obstacle. In this
research, we solve the end-to-end object detection problem entirely using deep
learning techniques. The network is trained using the most difficult publicly
available dataset, which is used for an annual item detection challenge.
Applications that need object detection can benefit the system's quick and
precise finding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer
  learning using Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Mishra, Ravindra T, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional solar forecasting models are based on several years of
site-specific historical irradiance data, often spanning five or more years,
which are unavailable for newer photovoltaic farms. As renewable energy is
highly intermittent, building accurate solar irradiance forecasting systems is
essential for efficient grid management and enabling the ongoing proliferation
of solar energy, which is crucial to achieve the United Nations' net zero
goals. In this work, we propose SPIRIT, a novel approach leveraging foundation
models for solar irradiance forecasting, making it applicable to newer solar
installations. Our approach outperforms state-of-the-art models in zero-shot
transfer learning by about 70%, enabling effective performance at new locations
without relying on any historical data. Further improvements in performance are
achieved through fine-tuning, as more location-specific data becomes available.
These findings are supported by statistical significance, further validating
our approach. SPIRIT represents a pivotal step towards rapid, scalable, and
adaptable solar forecasting solutions, advancing the integration of renewable
energy into global power systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for
  Scribble-Supervised Segmentation of Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien B. Nguyen-Tat, Hoang-An Vo, Phuoc-Sang Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of advanced deep learning models for medical image
segmentation is often constrained by the requirement for extensively annotated
datasets. Weakly-supervised learning, which allows less precise labels, has
become a promising solution to this challenge. Building on this approach, we
propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image
segmentation. This framework is built on the U-Net architecture, with the
encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks.
These blocks enhance the model's ability to learn local and global features
efficiently. Additionally, our approach integrates a query-based Transformer
decoder to refine features and an edge enhancement module to compensate for the
limited boundary information in the scribble label. We evaluate the proposed
QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal
polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation
metrics include the Dice similarity coefficient (DSC) and the 95th percentile
of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+
achieves 89.1\% DSC and 1.316mm HD95 on ACDC, 88.4\% DSC and 2.226mm HD95 on
MS-CMRSeg, 71.4\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\% DSC and 50.122mm
HD95 on BUSI. These results demonstrate that our method outperforms existing
approaches in terms of accuracy, robustness, and efficiency while remaining
competitive with fully-supervised learning approaches. This makes it ideal for
medical image analysis, where high-quality annotations are often scarce and
require significant effort and expense. The code is available at:
https://github.com/anpc849/QMaxViT-Unet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence to Assess Dental Findings from Panoramic
  Radio<span class="highlight-title">graph</span>s -- A Multinational Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin-Chih Chelsea Wang, Tsao-Lun Chen, Shankeeth Vinayahalingam, Tai-Hsien Wu, Chu Wei Chang, Hsuan Hao Chang, Hung-Jen Wei, Mu-Hsiung Chen, Ching-Chang Ko, David Anssari Moin, Bram van Ginneken, Tong Xi, Hsiao-Cheng Tsai, Min-Huey Chen, Tzu-Ming Harry Hsu, Hye Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dental panoramic radiographs (DPRs) are widely used in clinical practice for
comprehensive oral assessment but present challenges due to overlapping
structures and time constraints in interpretation.
  This study aimed to establish a solid baseline for the AI-automated
assessment of findings in DPRs by developing, evaluating an AI system, and
comparing its performance with that of human readers across multinational data
sets.
  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and
Taiwan), focusing on 8 types of dental findings. The AI system combined object
detection and semantic segmentation techniques for per-tooth finding
identification. Performance metrics included sensitivity, specificity, and area
under the receiver operating characteristic curve (AUC-ROC). AI
generalizability was tested across data sets, and performance was compared with
human dental practitioners.
  The AI system demonstrated comparable or superior performance to human
readers, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for
identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008)
sensitivity for identifying missing teeth. The AI achieved a macro-averaged
AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with
the reference were comparable to inter-human agreements in 7 of 8 findings
except for caries (p = .024). The AI system demonstrated robust generalization
across diverse imaging and demographic settings and processed images 79 times
faster (95% CI: 75-82) than human readers.
  The AI system effectively assessed findings in DPRs, achieving performance on
par with or better than human experts while significantly reducing
interpretation time. These results highlight the potential for integrating AI
into clinical workflows to improve diagnostic efficiency and accuracy, and
patient management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Perceptual Constancy in Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Sun, Suyang Yu, Yijiang Li, Qingying Gao, Haiyun Lyu, Hokin Deng, Dezhi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual constancy is the ability to maintain stable perceptions of objects
despite changes in sensory input, such as variations in distance, angle, or
lighting. This ability is crucial for recognizing visual information in a
dynamic world, making it essential for Vision-Language Models (VLMs). However,
whether VLMs are currently and theoretically capable of mastering this ability
remains underexplored. In this study, we evaluated 33 VLMs using 253
experiments across three domains: color, size, and shape constancy. The
experiments included single-image and video adaptations of classic cognitive
tasks, along with novel tasks in in-the-wild conditions, to evaluate the
models' recognition of object properties under varying conditions. We found
significant variability in VLM performance, with models performance in shape
constancy clearly dissociated from that of color and size constancy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MITO: Enabling Non-Line-of-Sight Perception using Millimeter-waves
  through Real-World <span class="highlight-title">Dataset</span>s and Simulation Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Dodds, Tara Boroushaki, Fadel Adib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MITO, the first dataset of multi-spectral millimeter-wave (mmWave)
images of everyday objects. Unlike visible light, mmWave signals can image
through everyday occlusions (e.g., cardboard boxes, fabric, plastic). However,
due to the dearth of publicly-available mmWave images and the interdisciplinary
challenges in collecting and processing mmWave signals, it remains difficult
today for computer vision researchers to develop mmWave-based non-line-of-sight
perception algorithms and models.
  To overcome these challenges, we introduce a real-world dataset and
open-source simulation tool for mmWave imaging. The dataset is acquired using a
UR5 robotic arm with two mmWave radars operating at different frequencies and
an RGB-D camera. Through a signal processing pipeline, we capture and create
over 580 real-world 3D mmWave images from over 76 different objects in the YCB
dataset, a standard dataset for robotics manipulation. We provide real-world
mmWave images in line-of-sight and non-line-of-sight, as well as RGB-D images
and ground truth segmentation masks. We also develop an open-source simulation
tool that can be used to generate synthetic mmWave images for any 3D triangle
mesh, which achieves a median F-Score of 94% when compared to real-world mmWave
images.
  We show the usefulness of this dataset and simulation tool in multiple CV
tasks in non-line-of-sight. First, we perform object segmentation for mmWave
images using the segment anything model (SAM), and achieve a median precision
and recall of 92.6% and 64%. Second, we train a classifier that can recognize
objects in non-line-of-sight. It is trained on synthetic images and can
classify real-world images with 85% accuracy.
  We believe MITO will be a valuable resource for computer vision researchers
in developing non-line-of-sight perception, similar to how early camera-based
datasets shaped the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PromptArtisan: Multi-instruction Image Editing in Single Pass with
  Complete Attention Control <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Swami, Raghu Chittersu, Pranav Adlinge, Rajeev Irny, Shashavali Doodekula, Alok Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PromptArtisan, a groundbreaking approach to multi-instruction
image editing that achieves remarkable results in a single pass, eliminating
the need for time-consuming iterative refinement. Our method empowers users to
provide multiple editing instructions, each associated with a specific mask
within the image. This flexibility allows for complex edits involving mask
intersections or overlaps, enabling the realization of intricate and nuanced
image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix
model in conjunction with a novel Complete Attention Control Mechanism (CACM).
This mechanism ensures precise adherence to user instructions, granting
fine-grained control over the editing process. Furthermore, our approach is
zero-shot, requiring no additional training, and boasts improved processing
complexity compared to traditional iterative methods. By seamlessly integrating
multi-instruction capabilities, single-pass efficiency, and complete attention
control, PromptArtisan unlocks new possibilities for creative and efficient
image editing workflows, catering to both novice and expert users alike.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision
  Language Models <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Karthik Kumar, Iheb Chaabane, Kebin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) excel in various visual benchmarks but are
often constrained by the lack of high-quality visual fine-tuning data. To
address this challenge, we introduce VisCon-100K, a novel dataset derived from
interleaved image-text web documents. Our approach transforms 45K web documents
from the OBELICS dataset into 100K image conversation samples. We utilize
GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert
these captions into diverse free-form and multiple-choice question-answer
pairs. Integrating this dataset for fine-tuning considerably enhances VLM
performance across multiple benchmarks. Unlike methods that focus solely on
fine-grained visual content, our approach leverages accompanying web context,
yielding superior results. We also discover that a `leaky modality mix,' where
conversation samples contain questions answerable from both the image and its
contextual caption, outperforms non-leaky combinations of captions and Q\&A
pairs. VisCon-100k dataset shows strong performance with two popular VLM
approaches: text-only large language model (LLM) aligned with a vision encoder
using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM
(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the
VisCon-100K dataset, we provide a contextual captioner trained on this dataset,
facilitating scalable fine-tuning data generation for future research and
open-source applications. Using the same pipeline, but substituting our trained
contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PAKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step-Video-T2V Technical Report: The Practice, Challenges, and Future of
  Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping bathymetry of inland water bodies on the North Slope of Alaska
  with Landsat using Random Forest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark L. Carroll, Margaret R. Wooten, Claire E. Simpson, Caleb S. Spradlin, Melanie J. Frost, Mariana Blanco-Rojas, Zachary W. Williams, Jordan A. Caraballo-Vega, Christopher S. R. Neigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The North Slope of Alaska is dominated by small waterbodies that provide
critical ecosystem services for local population and wildlife. Detailed
information on the depth of the waterbodies is scarce due to the challenges
with collecting such information. In this work we have trained a machine
learning (Random Forest Regressor) model to predict depth from multispectral
Landsat data in waterbodies across the North Slope of Alaska. The greatest
challenge is the scarcity of in situ data, which is expensive and difficult to
obtain, to train the model. We overcame this challenge by using modeled depth
predictions from a prior study as synthetic training data to provide a more
diverse training data pool for the Random Forest. The final Random Forest model
was more robust than models trained directly on the in situ data and when
applied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall
$r^{2}$ value of 0.76 on validation. The final map has been made available
through the Oak Ridge National Laboratory Distribute Active Archive Center
(ORNL-DAAC). This map represents a first of its kind regional assessment of
waterbody depth with per pixel estimates of depth for the entire North Slope of
Alaska.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 Pages, 6 Figures, 1 Table. This article is a US Government work.
  Landsat data from the US Geological Survey Earth Explorer system:
  https://earthexplorer.usgs.gov. Sonar training measurements:
  https://doi.org/10.18739/A2JD4PP1H. Output maps from the Oak Ridge National
  Laboratory Distribute Active Archive Center (ORNL-DAAC):
  https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=2243</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Camera Bias of Person Re-identification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myungseo Song, Jin-Woo Park, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically investigate the camera bias of person re-identification (ReID)
models. Previously, camera-aware methods have been proposed to address this
issue, but they are largely confined to training domains of the models. We
measure the camera bias of ReID models on unseen domains and reveal that camera
bias becomes more pronounced under data distribution shifts. As a debiasing
method for unseen domain data, we revisit feature normalization on embedding
vectors. While the normalization has been used as a straightforward solution,
its underlying causes and broader applicability remain unexplored. We analyze
why this simple method is effective at reducing bias and show that it can be
applied to detailed bias factors such as low-level image properties and body
angle. Furthermore, we validate its generalizability across various models and
benchmarks, highlighting its potential as a simple yet effective test-time
postprocessing method for ReID. In addition, we explore the inherent risk of
camera bias in unsupervised learning of ReID models. The unsupervised models
remain highly biased towards camera labels even for seen domain data,
indicating substantial room for improvement. Based on observations of the
negative impact of camera-biased pseudo labels on training, we suggest simple
training strategies to mitigate the bias. By applying these strategies to
existing unsupervised learning algorithms, we show that significant performance
improvements can be achieved with minor modifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Generalization Power of a DNN in Terms of Symbolic
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Cheng, Junpeng Zhang, Qihan Ren, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to analyze the generalization power of deep neural networks
(DNNs) from the perspective of interactions. Unlike previous analysis of a
DNN's generalization power in a highdimensional feature space, we find that the
generalization power of a DNN can be explained as the generalization power of
the interactions. We found that the generalizable interactions follow a
decay-shaped distribution, while non-generalizable interactions follow a
spindle-shaped distribution. Furthermore, our theory can effectively
disentangle these two types of interactions from a DNN. We have verified that
our theory can well match real interactions in a DNN in experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.19198</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoForce: Learnable Image-conditioned Physics Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Agishev, Karel Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel model for the prediction of robot trajectories on rough
offroad terrain from the onboard camera images. This model enforces the laws of
classical mechanics through a physics-aware neural symbolic layer while
preserving the ability to learn from large-scale data as it is end-to-end
differentiable. The proposed hybrid model integrates a black-box component that
predicts robot-terrain interaction forces with a neural-symbolic layer. This
layer includes a differentiable physics engine that computes the robot's
trajectory by querying these forces at the points of contact with the terrain.
As the proposed architecture comprises substantial geometrical and physics
priors, the resulting model can also be seen as a learnable physics engine
conditioned on real images that delivers $10^4$ trajectories per second. We
argue and empirically demonstrate that this architecture reduces the
sim-to-real gap and mitigates out-of-distribution sensitivity. The
differentiability, in conjunction with the rapid simulation speed, makes the
model well-suited for various applications including model predictive control,
trajectory shooting, supervised and reinforcement learning or SLAM. The codes
and data are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Robotics (T-RO), 2025. Code:
  https://github.com/ctu-vras/monoforce</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Concept-based Deep Learning Framework for <span class="highlight-title">Multimodal</span> Human
  Behavior Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Li, Marwa Mahmoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the contemporary era of intelligent connectivity, Affective Computing
(AC), which enables systems to recognize, interpret, and respond to human
behavior states, has become an integrated part of many AI systems. As one of
the most critical components of responsible AI and trustworthiness in all
human-centered systems, explainability has been a major concern in AC.
Particularly, the recently released EU General Data Protection Regulation
requires any high-risk AI systems to be sufficiently interpretable, including
biometric-based systems and emotion recognition systems widely used in the
affective computing field. Existing explainable methods often compromise
between interpretability and performance. Most of them focus only on
highlighting key network parameters without offering meaningful,
domain-specific explanations to the stakeholders. Additionally, they also face
challenges in effectively co-learning and explaining insights from multimodal
data sources. To address these limitations, we propose a novel and
generalizable framework, namely the Attention-Guided Concept Model (AGCM),
which provides learnable conceptual explanations by identifying what concepts
that lead to the predictions and where they are observed. AGCM is extendable to
any spatial and temporal signals through multimodal concept alignment and
co-learning, empowering stakeholders with deeper insights into the model's
decision-making process. We validate the efficiency of AGCM on well-established
Facial Expression Recognition benchmark datasets while also demonstrating its
generalizability on more complex real-world human behavior understanding
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging V2X for Collaborative HD Maps Construction Using Scene <span class="highlight-title">Graph</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gamal Elghazaly, Raphael Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-Definition (HD) maps play a crucial role in autonomous vehicle
navigation, complementing onboard perception sensors for improved accuracy and
safety. Traditional HD map generation relies on dedicated mapping vehicles,
which are costly and fail to capture real-time infrastructure changes. This
paper presents HDMapLaneNet, a novel framework leveraging V2X communication and
Scene Graph Generation to collaboratively construct a localized geometric layer
of HD maps. The approach extracts lane centerlines from front-facing camera
images, represents them as graphs, and transmits the data for global
aggregation to the cloud via V2X. Preliminary results on the nuScenes dataset
demonstrate superior association prediction performance compared to a
state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compress image to patches for Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Zhao, Yaoru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Vision Transformer (ViT) has made significant strides in the field of
computer vision. However, as the depth of the model and the resolution of the
input images increase, the computational cost associated with training and
running ViT models has surged dramatically.This paper proposes a hybrid model
based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a
module called CI2P, which utilizes the CompressAI encoder to compress images
and subsequently generates a sequence of patches through a series of
convolutions. CI2P can replace the Patch Embedding component in the ViT model,
enabling seamless integration into existing ViT models.Compared to ViT-B/16,
CI2P-ViT has the number of patches input to the self-attention layer reduced to
a quarter of the original.This design not only significantly reduces the
computational cost of the ViT model but also effectively enhances the model's
accuracy by introducing the inductive bias properties of CNN.The ViT model's
precision is markedly enhanced.When trained from the ground up on the
Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing
a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's
computational operations, measured in floating-point operations per second
(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in
training velocity on identical hardware configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Embedding Sampling Method for Diverse Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Waheed, Na Min An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning for state-of-the-art VLMs has significantly improved over
time; however, this comes at the cost of increased computational complexity,
making them less accessible for resource-constrained applications such as
mobile devices and assistive technologies. Alternatively, smaller VLMs
prioritize high-level scene descriptions, overlooking finer details that
contribute to a richer understanding of an image. In this paper, we introduce a
training-free framework that enhances caption diversity and informativeness by
explicitly attending to distinct image regions using a comparably small VLM,
BLIP, as the backbone. Our approach leverages structured segmentation to
produce hierarchical representations that capture both global and localized
semantics. Without requiring additional model training, we demonstrate that our
method allows smaller VLMs to achieve performance comparable to larger models
in terms of image-caption alignment, semantic integrity, and diversity. We
evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,
achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset
respectively, while maintaining strong image-caption relevancy and semantic
integrity with the human-annotated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hands-off Image Editing: Language-guided Editing without any
  Task-specific Labeling, Masking or even Training <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Santos, António Branco, João Silva, João Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-guided image editing consists in taking an image and an
instruction and deliverring that image altered according to that instruction.
State-of-the-art approaches to this task suffer from the typical scaling up and
domain adaptation hindrances related to supervision as they eventually resort
to some kind of task-specific labelling, masking or training. We propose a
novel approach that does without any such task-specific supervision and offers
thus a better potential for improvement. Its assessment demonstrates that it is
highly effective, achieving very competitive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiSciPLE: Learning Interpretable Programs for Scientific Visual
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Mall, Cheng Perng Phoo, Mia Chiquier, Bharath Hariharan, Kavita Bala, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual data is used in numerous different scientific workflows ranging from
remote sensing to ecology. As the amount of observation data increases, the
challenge is not just to make accurate predictions but also to understand the
underlying mechanisms for those predictions. Good interpretation is important
in scientific workflows, as it allows for better decision-making by providing
insights into the data. This paper introduces an automatic way of obtaining
such interpretable-by-design models, by learning programs that interleave
neural networks. We propose DiSciPLE (Discovering Scientific Programs using
LLMs and Evolution) an evolutionary algorithm that leverages common sense and
prior knowledge of large language models (LLMs) to create Python programs
explaining visual data. Additionally, we propose two improvements: a program
critic and a program simplifier to improve our method further to synthesize
good programs. On three different real-world problems, DiSciPLE learns
state-of-the-art programs on novel tasks with no prior literature. For example,
we can learn programs with 35% lower error than the closest non-interpretable
baseline for population density estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RealCam-I2V: Real-World Image-to-Video Generation with Interactive
  Complex Camera Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Li, Guangcong Zheng, Rui Jiang,  Shuigenzhan, Tao Wu, Yehao Lu, Yining Lin, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in camera-trajectory-guided image-to-video generation
offer higher precision and better support for complex camera control compared
to text-based approaches. However, they also introduce significant usability
challenges, as users often struggle to provide precise camera parameters when
working with arbitrary real-world images without knowledge of their depth nor
scene scale. To address these real-world application issues, we propose
RealCam-I2V, a novel diffusion-based video generation framework that integrates
monocular metric depth estimation to establish 3D scene reconstruction in a
preprocessing step. During training, the reconstructed 3D scene enables scaling
camera parameters from relative to absolute values, ensuring compatibility and
scale consistency across diverse real-world images. In inference, RealCam-I2V
offers an intuitive interface where users can precisely draw camera
trajectories by dragging within the 3D scene. To further enhance precise camera
control and scene consistency, we propose scene-constrained noise shaping,
which shapes high-level noise and also allows the framework to maintain
dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves
significant improvements in controllability and video quality on the
RealEstate10K and out-of-domain images. We further enables applications like
camera-controlled looping video generation and generative frame interpolation.
We will release our absolute-scale annotation, codes, and all checkpoints.
Please see dynamic results in https://zgctroy.github.io/RealCam-I2V.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Polyp Counting In Full-Procedure Colonoscopy Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Parolari, Andrea Cherubini, Lamberto Ballan, Carlo Biffi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated colonoscopy reporting holds great potential for enhancing quality
control and improving cost-effectiveness of colonoscopy procedures. A major
challenge lies in the automated identification, tracking, and re-association
(ReID) of polyps tracklets across full-procedure colonoscopy videos. This is
essential for precise polyp counting and enables automated computation of key
quality metrics, such as Adenoma Detection Rate (ADR) and Polyps Per
Colonoscopy (PPC). However, polyp ReID is challenging due to variations in
polyp appearance, frequent disappearance from the field of view, and
occlusions. In this work, we leverage the REAL-Colon dataset, the first
open-access dataset providing full-procedure videos, to define tasks, data
splits and metrics for the problem of automatically count polyps in
full-procedure videos, establishing an open-access framework. We re-implement
previously proposed SimCLR-based methods for learning representations of polyp
tracklets, both single-frame and multi-view, and adapt them to the polyp
counting task. We then propose an Affinity Propagation-based clustering method
to further improve ReID based on these learned representations, ultimately
enhancing polyp counting. Our approach achieves state-of-the-art performance,
with a polyp fragmentation rate of 6.30 and a false positive rate (FPR) below
5% on the REAL-Colon dataset. We release code at
https://github.com/lparolari/towards-polyp-counting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in
  Arbitrary Virtual Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeong Hwang, Seong-Eun Hong, Hyeongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating lifelike virtual agents capable of interacting with their
environments is a longstanding goal in computer graphics. This paper addresses
the challenge of generating natural head rotations, a critical aspect of
believable agent behavior for visual information gathering and dynamic
responses to environmental cues. Although earlier methods have made significant
strides, many rely on data-driven or saliency-based approaches, which often
underperform in diverse settings and fail to capture deeper cognitive factors
such as risk assessment, information seeking, and contextual prioritization.
Consequently, generated behaviors can appear rigid or overlook critical scene
elements, thereby diminishing the sense of realism. In this paper, we propose
\textbf{ViRAC}, a \textbf{Vi}sion-\textbf{R}easoning \textbf{A}gent Head
Movement \textbf{C}ontrol framework, which exploits the common-sense knowledge
and reasoning capabilities of large-scale models, including Vision-Language
Models (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling
every cognitive mechanism, ViRAC leverages the biases and patterns internalized
by these models from extensive training, thus emulating human-like perceptual
processes without hand-tuned heuristics. Experimental results in multiple
scenarios reveal that ViRAC produces more natural and context-aware head
rotations than recent state-of-the-art techniques. Quantitative evaluations
show a closer alignment with real human head-movement data, while user studies
confirm improved realism and cognitive plausibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow
  for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin He, Qiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-conditioned manipulation is a vital but challenging robotic task due
to the high-level abstraction of language. To address this, researchers have
sought improved goal representations derived from natural language. In this
paper, we highlight 3D flow - representing the motion trend of 3D particles
within a scene - as an effective bridge between language-based future image
generation and fine-grained action prediction. To this end, we develop
ManiTrend, a unified framework that models the dynamics of 3D particles, vision
observations and manipulation actions with a causal transformer. Within this
framework, features for 3D flow prediction serve as additional conditions for
future image generation and action prediction, alleviating the complexity of
pixel-wise spatiotemporal modeling and providing seamless action guidance.
Furthermore, 3D flow can substitute missing or heterogeneous action labels
during large-scale pretraining on cross-embodiment demonstrations. Experiments
on two comprehensive benchmarks demonstrate that our method achieves
state-of-the-art performance with high efficiency. Our code and model
checkpoints will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Label Ambiguity for Facial Expression Recognition in the Wild <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JunGyu Lee, Yeji Choi, Haksub Kim, Ig-Jae Kim, Gi Pyo Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) remains a challenging task due to label
ambiguity caused by the subjective nature of facial expressions and noisy
samples. Additionally, class imbalance, which is common in real-world datasets,
further complicates FER. Although many studies have shown impressive
improvements, they typically address only one of these issues, leading to
suboptimal results. To tackle both challenges simultaneously, we propose a
novel framework called Navigating Label Ambiguity (NLA), which is robust under
real-world conditions. The motivation behind NLA is that dynamically estimating
and emphasizing ambiguous samples at each iteration helps mitigate noise and
class imbalance by reducing the model's bias toward majority classes. To
achieve this, NLA consists of two main components: Noise-aware Adaptive
Weighting (NAW) and consistency regularization. Specifically, NAW adaptively
assigns higher importance to ambiguous samples and lower importance to noisy
ones, based on the correlation between the intermediate prediction scores for
the ground truth and the nearest negative. Moreover, we incorporate a
regularization term to ensure consistent latent distributions. Consequently,
NLA enables the model to progressively focus on more challenging ambiguous
samples, which primarily belong to the minority class, in the later stages of
training. Extensive experiments demonstrate that NLA outperforms existing
methods in both overall and mean accuracy, confirming its robustness against
noise and class imbalance. To the best of our knowledge, this is the first
framework to address both problems simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from
  Multi-Turn Jailbreaks without Compromising Usability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of safety alignment techniques for LLMs,
defending against multi-turn jailbreaks is still a challenging task. In this
paper, we conduct a comprehensive comparison, revealing that some existing
defense methods can improve the robustness of LLMs against multi-turn
jailbreaks but compromise usability, i.e., reducing general capabilities or
causing the over-refusal problem. From the perspective of mechanism
interpretability of LLMs, we discover that these methods fail to establish a
boundary that exactly distinguishes safe and harmful feature representations.
Therefore, boundary-safe representations close to harmful representations are
inevitably disrupted, leading to a decline in usability. To address this issue,
we propose X-Boundary to push harmful representations away from boundary-safe
representations and obtain an exact distinction boundary. In this way, harmful
representations can be precisely erased without disrupting safe ones.
Experimental results show that X-Boundary achieves state-of-the-art defense
performance against multi-turn jailbreaks, while reducing the over-refusal rate
by about 20% and maintaining nearly complete general capability. Furthermore,
we theoretically prove and empirically verify that X-Boundary can accelerate
the convergence process during training. Please see our code at:
https://github.com/AI45Lab/X-Boundary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with
  <span class="highlight-title">Multi-Modal</span> Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Stephen F. Smith, Yu-Chiang Frank Wang, Min-Hung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current autonomous driving vehicles rely mainly on their individual sensors
to understand surrounding scenes and plan for future trajectories, which can be
unreliable when the sensors are malfunctioning or occluded. To address this
problem, cooperative perception methods via vehicle-to-vehicle (V2V)
communication have been proposed, but they have tended to focus on detection
and tracking. How those approaches contribute to overall cooperative planning
performance is still under-explored. Inspired by recent progress using Large
Language Models (LLMs) to build autonomous driving systems, we propose a novel
problem setting that integrates an LLM into cooperative autonomous driving,
with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and
benchmark. We also propose our baseline method Vehicle-to-Vehicle Large
Language Model (V2V-LLM), which uses an LLM to fuse perception information from
multiple connected autonomous vehicles (CAVs) and answer driving-related
questions: grounding, notable object identification, and planning. Experimental
results show that our proposed V2V-LLM can be a promising unified model
architecture for performing various tasks in cooperative autonomous driving,
and outperforms other baseline methods that use different fusion approaches.
Our work also creates a new research direction that can improve the safety of
future autonomous driving systems. Our project website:
https://eddyhkchiu.github.io/v2vllm.github.io/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Latent Coding with Learnable Synthesized Reference for Deep
  Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Wu, Yinda Chen, Dong Liu, Zhihai He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study how to synthesize a dynamic reference from an
external dictionary to perform conditional coding of the input image in the
latent domain and how to learn the conditional latent synthesis and coding
modules in an end-to-end manner. Our approach begins by constructing a
universal image feature dictionary using a multi-stage approach involving
modified spatial pyramid pooling, dimension reduction, and multi-scale feature
clustering. For each input image, we learn to synthesize a conditioning latent
by selecting and synthesizing relevant features from the dictionary, which
significantly enhances the model's capability in capturing and exploring image
source correlation. This conditional latent synthesis involves a
correlation-based feature matching and alignment strategy, comprising a
Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis
(CLS) module. The synthesized latent is then used to guide the encoding
process, allowing for more efficient compression by exploiting the correlation
between the input image and the reference dictionary. According to our
theoretical analysis, the proposed conditional latent coding (CLC) method is
robust to perturbations in the external dictionary samples and the selected
conditioning latent, with an error bound that scales logarithmically with the
dictionary size, ensuring stability even with large and diverse dictionaries.
Experimental results on benchmark datasets show that our new method improves
the coding performance by a large margin (up to 1.2 dB) with a very small
overhead of approximately 0.5\% bits per pixel. Our code is publicly available
at https://github.com/ydchen0806/CLC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly
  Supervised Group Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuming Wang, Yihao Zheng, Jiarui Li, Yaofei Wu, Yan Huang, Zun Li, Lifang Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing weakly supervised group activity recognition methods rely on object
detectors or attention mechanisms to capture key areas automatically. However,
they overlook the semantic information associated with captured areas, which
may adversely affect the recognition performance. In this paper, we propose a
novel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM)
which effectively captures the locations of individual actions and integrates
them with action semantics for weakly supervised group activity recognition.It
generates individual action prototypes from training set as visual conceptual
knowledge to bridge action semantics and visual representations. Guided by this
knowledge, VicKAM produces action maps that indicate the likelihood of each
action occurring at various locations, based on image correlation theorem. It
further augments individual action maps using group activity related
statistical information, representing individual action distribution under
different group activities, to establish connections between action maps and
specific group activities. The augmented action map is incorporated with action
semantic representations for group activity recognition.Extensive experiments
on two public benchmarks, the Volleyball and the NBA datasets, demonstrate the
effectiveness of our proposed method, even in cases of limited training data.
The code will be released later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating on Generated: An Approach Towards Self-Evolving Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xulu Zhang, Xiaoyong Wei, Jinlin Wu, Jiaxin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recursive Self-Improvement (RSI) enables intelligence systems to autonomously
refine their capabilities. This paper explores the application of RSI in
text-to-image diffusion models, addressing the challenge of training collapse
caused by synthetic data. We identify two key factors contributing to this
collapse: the lack of perceptual alignment and the accumulation of generative
hallucinations. To mitigate these issues, we propose three strategies: (1) a
prompt construction and filtering pipeline designed to facilitate the
generation of perceptual aligned data, (2) a preference sampling method to
identify human-preferred samples and filter out generative hallucinations, and
(3) a distribution-based weighting scheme to penalize selected samples with
hallucinatory errors. Our extensive experiments validate the effectiveness of
these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Cui, Yifei Zou, Siyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  China's Chang'e 5 mission has been a remarkable success, with the chang'e 5
lander traveling on the Oceanus Procellarum to collect images of the lunar
surface. Over the past half century, people have brought back some lunar rock
samples, but its quantity does not meet the need for research. Under current
circumstances, people still mainly rely on the analysis of rocks on the lunar
surface through the detection of lunar rover. The Oceanus Procellarum, chosen
by Chang'e 5 mission, contains various kind of rock species. Therefore, we
first applied to the National Astronomical Observatories of the China under the
Chinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of
the lunar surface image, and established a lunar surface rock image data set
CE5ROCK. The data set contains 100 images, which randomly divided into
training, validation and test set. Experimental results show that the
identification accuracy testing on convolutional neural network (CNN) models
like AlexNet or MobileNet is about to 40.0%. In order to make full use of the
global information in Moon images, this paper proposes the MRNet (MoonRockNet)
network architecture. The encoding structure of the network uses VGG16 for
feature extraction, and the decoding part adds dilated convolution and commonly
used U-Net structure on the original VGG16 decoding structure, which is more
conducive to identify more refined but more sparsely distributed types of lunar
rocks. We have conducted extensive experiments on the established CE5ROCK data
set, and the experimental results show that MRNet can achieve more accurate
rock type identification, and outperform other existing mainstream algorithms
in the identification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 8th International Conference on Advances in
  Machinery, Material Science and Engineering Application (MMSE 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight and Effective Image Tampering Localization Network with
  Vision Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Guo, Gang Cao, Zijie Lou, Xianglin Huang, Jiaoyun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current image tampering localization methods primarily rely on Convolutional
Neural Networks (CNNs) and Transformers. While CNNs suffer from limited local
receptive fields, Transformers offer global context modeling at the expense of
quadratic computational complexity. Recently, the state space model Mamba has
emerged as a competitive alternative, enabling linear-complexity global
dependency modeling. Inspired by it, we propose a lightweight and effective
FORensic network based on vision MAmba (ForMa) for blind image tampering
localization. Firstly, ForMa captures multi-scale global features that achieves
efficient global dependency modeling through linear complexity. Then the
pixel-wise localization map is generated by a lightweight decoder, which
employs a parameter-free pixel shuffle layer for upsampling. Additionally, a
noise-assisted decoding strategy is proposed to integrate complementary
manipulation traces from tampered images, boosting decoder sensitivity to
forgery cues. Experimental results on 10 standard datasets demonstrate that
ForMa achieves state-of-the-art generalization ability and robustness, while
maintaining the lowest computational complexity. Code is available at
https://github.com/multimediaFor/ForMa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Scale and Shift Invariant Automatic Event Recognition using the
  Mellin Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Shen, Julian Gamboa, Tabassom Hamidfar, Shamima A. Mitu, Selim M. Shahriar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Spatio-temporal holographic correlator combines the traditional 2D
optical image correlation techniques with inhomogeneously broadened arrays of
cold atoms to achieve 3D time-space correlation to realize automatic event
recognition at an ultra-high speed. Here we propose a method to realize such
event recognition for videos running at different speeds. With this method, we
can highly improve recognition accuracy and filter almost all the unwanted
events in the video database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Parameter Localization for Textual Generation in Diffusion
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Staniszewski, Bartosz Cywiński, Franziska Boenisch, Kamil Deja, Adam Dziedzic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel diffusion models can synthesize photo-realistic images with integrated
high-quality text. Surprisingly, we demonstrate through attention activation
patching that only less than 1% of diffusion models' parameters, all contained
in attention layers, influence the generation of textual content within the
images. Building on this observation, we improve textual generation efficiency
and performance by targeting cross and joint attention layers of diffusion
models. We introduce several applications that benefit from localizing the
layers responsible for textual content generation. We first show that a
LoRA-based fine-tuning solely of the localized layers enhances, even more, the
general text-generation capabilities of large diffusion models while preserving
the quality and diversity of the diffusion models' generations. Then, we
demonstrate how we can use the localized layers to edit textual content in
generated images. Finally, we extend this idea to the practical use case of
preventing the generation of toxic text in a cost-free manner. In contrast to
prior work, our localization approach is broadly applicable across various
diffusion model architectures, including U-Net (e.g., LDM and SDXL) and
transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing
diverse text encoders (e.g., from CLIP to the large language models like T5).
Project page available at https://t2i-text-loc.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AffectSRNet : Facial Emotion-Aware Super-Resolution Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Sameen Ahmad Rizvi, Soham Kumar, Aryan Seth, Pratik Narang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) systems in low-resolution settings face
significant challenges in accurately identifying expressions due to the loss of
fine-grained facial details. This limitation is especially problematic for
applications like surveillance and mobile communications, where low image
resolution is common and can compromise recognition accuracy. Traditional
single-image face super-resolution (FSR) techniques, however, often fail to
preserve the emotional intent of expressions, introducing distortions that
obscure the original affective content. Given the inherently ill-posed nature
of single-image super-resolution, a targeted approach is required to balance
image quality enhancement with emotion retention. In this paper, we propose
AffectSRNet, a novel emotion-aware super-resolution framework that reconstructs
high-quality facial images from low-resolution inputs while maintaining the
intensity and fidelity of facial expressions. Our method effectively bridges
the gap between image resolution and expression accuracy by employing an
expression-preserving loss function, specifically tailored for FER
applications. Additionally, we introduce a new metric to assess emotion
preservation in super-resolved images, providing a more nuanced evaluation of
FER system performance in low-resolution scenarios. Experimental results on
standard datasets, including CelebA, FFHQ, and Helen, demonstrate that
AffectSRNet outperforms existing FSR approaches in both visual quality and
emotion fidelity, highlighting its potential for integration into practical FER
applications. This work not only improves image clarity but also ensures that
emotion-driven applications retain their core functionality in suboptimal
resolution environments, paving the way for broader adoption in FER systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransGUNet: <span class="highlight-title">Transformer</span> Meets <span class="highlight-title">Graph</span>-based Skip Connection for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Hyeon Nam, Nur Suriza Syazwany, Sang-Chul Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skip connection engineering is primarily employed to address the semantic gap
between the encoder and decoder, while also integrating global dependencies to
understand the relationships among complex anatomical structures in medical
image segmentation. Although several models have proposed transformer-based
approaches to incorporate global dependencies within skip connections, they
often face limitations in capturing detailed local features with high
computational complexity. In contrast, graph neural networks (GNNs) exploit
graph structures to effectively capture local and global features. Leveraging
these properties, we introduce an attentional cross-scale graph neural network
(ACS-GNN), which enhances the skip connection framework by converting
cross-scale feature maps into a graph structure and capturing complex
anatomical structures through node attention. Additionally, we observed that
deep learning models often produce uninformative feature maps, which degrades
the quality of spatial attention maps. To address this problem, we integrated
entropy-driven feature selection (EFS) with spatial attention, calculating an
entropy score for each channel and filtering out high-entropy feature maps. Our
innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial
attentio} to effectively enhance domain generalizability across various
modalities by leveraging GNNs alongside a reliable spatial attention map,
ensuring more robust features within the skip connection. Through comprehensive
experiments and analysis, TransGUNet achieved superior segmentation performance
on six seen and eight unseen datasets, demonstrating significantly higher
efficiency compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Tree Tensor Networks for Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Nie, Junfang Chen, Yajie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Originating in quantum physics, tensor networks (TNs) have been widely
adopted as exponential machines and parameter decomposers for recognition
tasks. Typical TN models, such as Matrix Product States (MPS), have not yet
achieved successful application in natural image processing. When employed,
they primarily serve to compress parameters within off-the-shelf networks, thus
losing their distinctive capability to enhance exponential-order feature
interactions. This paper introduces a novel architecture named
\textit{\textbf{D}eep \textbf{T}ree \textbf{T}ensor \textbf{N}etwork} (DTTN),
which captures $2^L$-order multiplicative interactions across features through
multilinear operations, while essentially unfolding into a \emph{tree}-like TN
topology with the parameter-sharing property. DTTN is stacked with multiple
antisymmetric interacting modules (AIMs), and this design facilitates efficient
implementation. Moreover, we theoretically reveal the equivalency among
quantum-inspired TN models and polynomial and multilinear networks under
certain conditions, and we believe that DTTN can inspire more interpretable
studies in this field. We evaluate the proposed model against a series of
benchmarks and achieve excellent performance compared to its peers and
cutting-edge architectures. Our code will soon be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Granite Vision: a lightweight, open-source <span class="highlight-title">multimodal</span> model for
  enterprise Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Granite Vision Team, Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Nikolaos Livathinos, Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira de Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Shila Ofek-koifman, Sriram Raghavan, Tanveer Syeda-Mahmood, Peter Staar, Tal Drory, Rogerio Feris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Granite Vision, a lightweight large language model with vision
capabilities, specifically designed to excel in enterprise use cases,
particularly in visual document understanding. Our model is trained on a
comprehensive instruction-following dataset, including document-related tasks,
such as content extraction from tables, charts, diagrams, sketches, and
infographics, as well as general image tasks. The architecture of Granite
Vision is centered around visual modality alignment with a decoder-only, 2
billion parameter Granite large language model. Additionally, we introduce a
dedicated safety classification approach in test-time that leverages a sparse
set of attention vectors to identify potential harmful inputs. Despite its
lightweight architecture, Granite Vision achieves strong results in standard
benchmarks related to visual document understanding, as well as on the LiveXiv
benchmark, which is designed to avoid test set contamination by using a
constantly updated corpus of recently published Arxiv papers. We are releasing
the model under the Apache-2 license, allowing for both research and commercial
use, while offering complete visibility into the training data and other
relevant details. See https://huggingface.co/ibm-granite/ for model weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TaskGalaxy: Scaling <span class="highlight-title">Multi-modal</span> Instruction Fine-tuning with Tens of
  Thousands Vision Task Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankang Chen, Tianke Zhang, Changyi Liu, Haojie Ding, Yaya Shi, Feng Cheng, Huihui Xiao, Bin Wen, Fan Yang, Tingting Gao, Di Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal visual language models are gaining prominence in open-world
applications, driven by advancements in model architectures, training
techniques, and high-quality data. However, their performance is often limited
by insufficient task-specific data, leading to poor generalization and biased
outputs. Existing efforts to increase task diversity in fine-tuning datasets
are hindered by the labor-intensive process of manual task labeling, which
typically produces only a few hundred task types. To address this, we propose
TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising
19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o
to enrich task diversity by expanding from a small set of manually defined
tasks, with CLIP and GPT-4o filtering those that best match open-source images,
and generating relevant question-answer pairs. Multiple models are employed to
ensure sample quality. This automated process enhances both task diversity and
data quality, reducing manual intervention. Incorporating TaskGalaxy into
LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance
improvements across 16 benchmarks, demonstrating the critical importance of
task diversity. TaskGalaxy is publicly released at
https://github.com/Kwai-YuanQi/TaskGalaxy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistent Model-based Adaptation for Visual Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinning Zhou, Chengyang Ying, Yao Feng, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reinforcement learning agents typically face serious performance
declines in real-world applications caused by visual distractions. Existing
methods rely on fine-tuning the policy's representations with hand-crafted
augmentations. In this work, we propose Self-Consistent Model-based Adaptation
(SCMA), a novel method that fosters robust adaptation without modifying the
policy. By transferring cluttered observations to clean ones with a denoising
model, SCMA can mitigate distractions for various policies as a plug-and-play
enhancement. To optimize the denoising model in an unsupervised manner, we
derive an unsupervised distribution matching objective with a theoretical
analysis of its optimality. We further present a practical algorithm to
optimize the objective by estimating the distribution of clean observations
with a pre-trained world model. Extensive experiments on multiple visual
generalization benchmarks and real robot data demonstrate that SCMA effectively
boosts performance across various distractions and exhibits better sample
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insect-Foundation: A Foundation Model and Large <span class="highlight-title">Multimodal</span> <span class="highlight-title">Dataset</span> for
  Vision-Language Insect Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Dat Truong, Hoang-Quan Nguyen, Xuan-Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal conversational generative AI has shown impressive capabilities in
various vision and language understanding through learning massive text-image
data. However, current conversational models still lack knowledge about visual
insects since they are often trained on the general knowledge of
vision-language data. Meanwhile, understanding insects is a fundamental problem
in precision agriculture, helping to promote sustainable development in
agriculture. Therefore, this paper proposes a novel multimodal conversational
model, Insect-LLaVA, to promote visual understanding in insect-domain
knowledge. In particular, we first introduce a new large-scale Multimodal
Insect Dataset with Visual Insect Instruction Data that enables the capability
of learning the multimodal foundation models. Our proposed dataset enables
conversational models to comprehend the visual and semantic features of the
insects. Second, we propose a new Insect-LLaVA model, a new general Large
Language and Vision Assistant in Visual Insect Understanding. Then, to enhance
the capability of learning insect features, we develop an Insect Foundation
Model by introducing a new micro-feature self-supervised learning with a
Patch-wise Relevant Attention mechanism to capture the subtle differences among
insect images. We also present Description Consistency loss to improve
micro-feature learning via text descriptions. The experimental results
evaluated on our new Visual Insect Question Answering benchmarks illustrate the
effective performance of our proposed approach in visual insect understanding
and achieve State-of-the-Art performance on standard benchmarks of
insect-related tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic-Computed Tomo<span class="highlight-title">graph</span>y Angio<span class="highlight-title">graph</span>y for Cerebral Vessel Templates
  and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrikanth Yadav, Jisoo Kim, Geoffrey Young, Lei Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Computed Tomography Angiography (CTA) is crucial for
cerebrovascular disease diagnosis. Dynamic CTA is a type of imaging that
captures temporal information about the We aim to develop and evaluate two
segmentation techniques to segment vessels directly on CTA images: (1) creating
and registering population-averaged vessel atlases and (2) using deep learning
(DL). Methods: We retrieved 4D-CT of the head from our institutional research
database, with bone and soft tissue subtracted from post-contrast images. An
Advanced Normalization Tools pipeline was used to create angiographic atlases
from 25 patients. Then, atlas-driven ROIs were identified by a CT attenuation
threshold to generate segmentation of the arteries and veins using non-linear
registration. To create DL vessel segmentations, arterial and venous structures
were segmented using the MRA vessel segmentation tool, iCafe, in 29 patients.
These were then used to train a DL model, with bone-in CT images as input.
Multiple phase images in the 4D-CT were used to increase the training and
validation dataset. Both segmentation approaches were evaluated on a test 4D-CT
dataset of 11 patients which were also processed by iCafe and validated by a
neuroradiologist. Specifically, branch-wise segmentation accuracy was
quantified with 20 labels for arteries and one for veins. DL outperformed the
atlas-based segmentation models for arteries (average modified dice coefficient
(amDC) 0.856 vs. 0.324) and veins (amDC 0.743 vs. 0.495) overall. For ICAs,
vertebral and basilar arteries, DL and atlas -based segmentation had an amDC of
0.913 and 0.402, respectively. The amDC for MCA-M1, PCA-P1, and ACA-A1 segments
were 0.932 and 0.474, respectively. Conclusion: Angiographic CT templates are
developed for the first time in literature. Using 4D-CTA enables the use of
tools like iCafe, lessening the burden of manual annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FrGNet: A fourier-guided weakly-supervised framework for nuclear
  instance segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclear instance segmentation has played a critical role in pathology image
analysis. The main challenges arise from the difficulty in accurately
segmenting instances and the high cost of precise mask-level annotations for
fully-supervised training.In this work, we propose a fourier guidance framework
for solving the weakly-supervised nuclear instance segmentation problem. In
this framework, we construct a fourier guidance module to fuse the priori
information into the training process of the model, which facilitates the model
to capture the relevant features of the nuclear.Meanwhile, in order to further
improve the model's ability to represent the features of nuclear, we propose
the guide-based instance level contrastive module. This module makes full use
of the framework's own properties and guide information to effectively enhance
the representation features of nuclear. We show on two public datasets that our
model can outperform current SOTA methods under fully-supervised design, and in
weakly-supervised experiments, with only a small amount of labeling our model
still maintains close to the performance under full supervision.In addition, we
also perform generalization experiments on a private dataset, and without any
labeling, our model is able to segment nuclear images that have not been seen
during training quite effectively. As open science, all codes and pre-trained
models are available at https://github.com/LQY404/FrGNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpei Guo, Zheng Chen, Wenbo Li, Yong Guo, Yulun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable success in image restoration
tasks. However, their multi-step denoising process introduces significant
computational overhead, limiting their practical deployment. Furthermore,
existing methods struggle to effectively remove severe JPEG artifact,
especially in highly compressed images. To address these challenges, we propose
CODiff, a compression-aware one-step diffusion model for JPEG artifact removal.
The core of CODiff is the compression-aware visual embedder (CaVE), which
extracts and leverages JPEG compression priors to guide the diffusion model. We
propose a dual learning strategy that combines explicit and implicit learning.
Specifically, explicit learning enforces a quality prediction objective to
differentiate low-quality images with different compression levels. Implicit
learning employs a reconstruction objective that enhances the model's
generalization. This dual learning allows for a deeper and more comprehensive
understanding of JPEG compression. Experimental results demonstrate that CODiff
surpasses recent leading methods in both quantitative and visual quality
metrics. The code and models will be released at
https://github.com/jp-guo/CODiff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Calibrate for Reliable Visual Fire Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Xiuzhuang Zhou, Xiangyang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fire is characterized by its sudden onset and destructive power, making early
fire detection crucial for ensuring human safety and protecting property. With
the advancement of deep learning, the application of computer vision in fire
detection has significantly improved. However, deep learning models often
exhibit a tendency toward overconfidence, and most existing works focus
primarily on enhancing classification performance, with limited attention given
to uncertainty modeling. To address this issue, we propose transforming the
Expected Calibration Error (ECE), a metric for measuring uncertainty, into a
differentiable ECE loss function. This loss is then combined with the
cross-entropy loss to guide the training process of multi-class fire detection
models. Additionally, to achieve a good balance between classification accuracy
and reliable decision, we introduce a curriculum learning-based approach that
dynamically adjusts the weight of the ECE loss during training. Extensive
experiments are conducted on two widely used multi-class fire detection
datasets, DFAN and EdgeFireSmoke, validating the effectiveness of our
uncertainty modeling method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HealthGPT: A Medical Large Vision-Language Model for Unifying
  Comprehension and Generation via Heterogeneous Knowledge Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HealthGPT, a powerful Medical Large Vision-Language Model
(Med-LVLM) that integrates medical visual comprehension and generation
capabilities within a unified autoregressive paradigm. Our bootstrapping
philosophy is to progressively adapt heterogeneous comprehension and generation
knowledge to pre-trained large language models (LLMs). This is achieved through
a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is
complemented by a tailored hierarchical visual perception approach and a
three-stage learning strategy. To effectively learn the HealthGPT, we devise a
comprehensive medical domain-specific comprehension and generation dataset
called VL-Health. Experimental results demonstrate exceptional performance and
scalability of HealthGPT in medical visual unified tasks. Our project can be
accessed at https://github.com/DCDmllm/HealthGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2CFormer: Reorienting Learned Image Compression from Spatial
  Interaction to Channel Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunuo Chen, Qian Li, Bing He, Donghui Feng, Ronghua Wu, Qi Wang, Li Song, Guo Lu, Wenjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved significant success in learned image compression
(LIC), with Swin Transformers emerging as the mainstream choice for nonlinear
transforms. A common belief is that their sophisticated spatial operations
contribute most to their efficacy. However, the crucial role of the
feed-forward network (FFN) based Channel Aggregation module within the
transformer architecture has been largely overlooked, and the over-design of
spatial operations leads to a suboptimal trade-off between decoding latency and
R-D performance. In this paper, we reevaluate the key factors behind the
competence of transformers in LIC. By replacing spatial operations with
identity mapping, we are surprised to find that channel operations alone can
approach the R-D performance of the leading methods. This solid lower bound of
performance emphasizes that the presence of channel aggregation is more
essential for the LIC model to achieve competitive performance, while the
previously complex spatial interactions are partly redundant. Based on this
insight, we initiate the "S2CFormer" paradigm, a general architecture that
reorients the focus of LIC from Spatial Interaction to Channel Aggregation. We
present two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each
one incorporates a simple operator for spatial interaction and serves as
nonlinear transform blocks for our LIC models. Both models demonstrate
state-of-the-art (SOTA) R-D performance and significantly faster decoding
speed. These results also motivate further exploration of advanced FFN
structures to enhance the R-D performance while maintaining model efficiency.
With these foundations, we introduce S2C-Hybrid, an enhanced LIC model that
combines the strengths of different S2CFormer instantiations. This model
outperforms all the existing methods on several datasets, setting a new
benchmark for efficient and high-performance LIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for
  Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17331v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17331v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqing Wang, Wentao Wan, Qiqing Lao, Runmeng Chen, Minjie Lang, Xiao Wang, Keze Wang, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, to comprehensively improve Vision Language Models (VLMs) for Visual
Question Answering (VQA), several methods have been proposed to further
reinforce the inference capabilities of VLMs to independently tackle VQA tasks
rather than some methods that only utilize VLMs as aids to Large Language
Models (LLMs). However, these methods ignore the rich common-sense knowledge
inside the given VQA image sampled from the real world. Thus, they cannot fully
use the powerful VLM for the given VQA question to achieve optimal performance.
Attempt to overcome this limitation and inspired by the human top-down
reasoning process, i.e., systematically exploring relevant issues to derive a
comprehensive answer, this work introduces a novel, explainable multi-agent
collaboration framework by leveraging the expansive knowledge of Large Language
Models (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our
framework comprises three agents, i.e., Responder, Seeker, and Integrator, to
collaboratively answer the given VQA question by seeking its relevant issues
and generating the final answer in such a top-down reasoning process. The
VLM-based Responder agent generates the answer candidates for the question and
responds to other relevant issues. The Seeker agent, primarily based on LLM,
identifies relevant issues related to the question to inform the Responder
agent and constructs a Multi-View Knowledge Base (MVKB) for the given visual
scene by leveraging the build-in world knowledge of LLM. The Integrator agent
combines knowledge from the Seeker agent and the Responder agent to produce the
final VQA answer. Extensive and comprehensive evaluations on diverse VQA
datasets with a variety of VLMs demonstrate the superior performance and
interpretability of our framework over the baseline method in the zero-shot
setting without extra training cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magic 1-For-1: Generating One Minute Video Clips within One Minute 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, Daquan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we present Magic 1-For-1 (Magic141), an efficient
video generation model with optimized memory consumption and inference latency.
The key idea is simple: factorize the text-to-video generation task into two
separate easier tasks for diffusion step distillation, namely text-to-image
generation and image-to-video generation. We verify that with the same
optimization algorithm, the image-to-video task is indeed easier to converge
over the text-to-video task. We also explore a bag of optimization tricks to
reduce the computational cost of training the image-to-video (I2V) models from
three aspects: 1) model convergence speedup by using a multi-modal prior
condition injection; 2) inference latency speed up by applying an adversarial
step distillation, and 3) inference memory cost optimization with parameter
sparsification. With those techniques, we are able to generate 5-second video
clips within 3 seconds. By applying a test time sliding window, we are able to
generate a minute-long video within one minute with significantly improved
visual quality and motion dynamics, spending less than 1 second for generating
1 second video clips on average. We conduct a series of preliminary
explorations to find out the optimal tradeoff between computational cost and
video quality during diffusion step distillation and hope this could be a good
foundation model for open-source explorations. The code and the model weights
are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Serious modification needed.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the enigma: Enhancing faithfulness and comprehensibility in
  explanations of deep networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Mamalakis, Antonios Mamalakis, Ingrid Agartz, Lynn Egeland Mørch-Johnsen, Graham Murray, John Suckling, Pietro Lio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerated progress of artificial intelligence (AI) has popularized deep
learning models across various domains, yet their inherent opacity poses
challenges, particularly in critical fields like healthcare, medicine, and the
geosciences. Explainable AI (XAI) has emerged to shed light on these 'black
box' models, aiding in deciphering their decision-making processes. However,
different XAI methods often produce significantly different explanations,
leading to high inter-method variability that increases uncertainty and
undermines trust in deep networks' predictions. In this study, we address this
challenge by introducing a novel framework designed to enhance the
explainability of deep networks through a dual focus on maximizing both
accuracy and comprehensibility in the explanations. Our framework integrates
outputs from multiple established XAI methods and leverages a non-linear neural
network model, termed the 'explanation optimizer,' to construct a unified,
optimal explanation. The optimizer evaluates explanations using two key
metrics: faithfulness (accuracy in reflecting the network's decisions) and
complexity (comprehensibility). By balancing these, it provides accurate and
accessible explanations, addressing a key XAI limitation. Experiments on
multi-class and binary classification in 2D object and 3D neuroscience imaging
confirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63%
higher than the best XAI methods in 3D and 2D tasks, respectively, while also
reducing complexity for better understanding. These results demonstrate that
optimal explanations based on specific quality criteria are achievable,
offering a solution to the issue of inter-method variability in the current XAI
literature and supporting more trustworthy deep network predictions
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>keywords: XAI, neuroscience, brain, 3D, 2D, computer vision,
  classification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Devil is in the Prompts: De-Identification Traces Enhance
  Memorization Risks in Synthetic Chest X-Ray Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, particularly text-to-image (T2I) diffusion models, play a
crucial role in medical image analysis. However, these models are prone to
training data memorization, posing significant risks to patient privacy.
Synthetic chest X-ray generation is one of the most common applications in
medical image analysis with the MIMIC-CXR dataset serving as the primary data
repository for this task. This study presents the first systematic attempt to
identify prompts and text tokens in MIMIC-CXR that contribute the most to
training data memorization. Our analysis reveals two unexpected findings: (1)
prompts containing traces of de-identification procedures (markers introduced
to hide Protected Health Information) are the most memorized, and (2) among all
tokens, de-identification markers contribute the most towards memorization.
This highlights a broader issue with the standard anonymization practices and
T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time
memorization mitigation strategies are ineffective and fail to sufficiently
reduce the model's reliance on memorized text tokens. On this front, we propose
actionable strategies for different stakeholders to enhance privacy and improve
the reliability of generative models in medical imaging. Finally, our results
provide a foundation for future work on developing and benchmarking
memorization mitigation techniques for synthetic chest X-ray generation using
the MIMIC-CXR dataset. The anonymized code is available at
https://anonymous.4open.science/r/diffusion_memorization-8011/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution
  Detection <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanhu Zeng, Zhen Cheng, Fei Zhu, Hongxin Wei, Xu-Yao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-Distribution (OOD) detection, aiming to distinguish outliers from
known categories, has gained prominence in practical scenarios. Recently, the
advent of vision-language models (VLM) has heightened interest in enhancing OOD
detection for VLM through few-shot tuning. However, existing methods mainly
focus on optimizing global prompts, ignoring refined utilization of local
information with regard to outliers. Motivated by this, we freeze global
prompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to
emphasize regional enhancement with local prompts. Our method comprises two
integral components: global prompt guided negative augmentation and local
prompt enhanced regional regularization. The former utilizes frozen, coarse
global prompts as guiding cues to incorporate negative augmentation, thereby
leveraging local outlier knowledge. The latter employs trainable local prompts
and a regional regularization to capture local information effectively, aiding
in outlier identification. We also propose regional-related metric to empower
the enrichment of OOD detection. Moreover, since our approach explores
enhancing local prompts only, it can be seamlessly integrated with trained
global prompts during inference to boost the performance. Comprehensive
experiments demonstrate the effectiveness and potential of our method. Notably,
our method reduces average FPR95 by 5.17% against state-of-the-art method in
4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot
results of previous methods. Code is released at
https://github.com/AuroraZengfh/Local-Prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Thirteenth International Conference on Learning
  Representations (ICLR 2025). Code is available at
  https://github.com/AuroraZengfh/Local-Prompt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surface Vision Mamba: Leveraging Bidirectional State Space Model for
  Efficient Spherical Manifold Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14679v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14679v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhao He, Weihao Zheng, Leilei Zhao, Ying Wang, Dalin Zhu, Dan Wu, Bin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based methods have demonstrated exceptional performance in
modelling long-range dependencies on spherical cortical surfaces, surpassing
traditional Geometric Deep Learning (GDL) models. However, their extensive
inference time and high memory demands pose challenges for application to large
datasets with limited computing resources. Inspired by the state space model in
computer vision, we introduce the attention-free Vision Mamba (Vim) to
spherical surfaces, presenting a domain-agnostic architecture for analyzing
data on spherical manifolds. Our method achieves surface patching by
representing spherical data as a sequence of triangular patches derived from a
subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on
multiple neurodevelopmental phenotype regression tasks using cortical surface
metrics from neonatal brains. Experimental results demonstrate that SiM
outperforms both attention- and GDL-based methods, delivering 4.8 times faster
inference and achieving 91.7% lower memory consumption compared to the Surface
Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity
analysis further underscores the potential of SiM to identify subtle cognitive
developmental patterns. The code is available at
https://github.com/Rongzhao-He/surface-vision-mamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is What You Ask For What You Get? Investigating Concept Associations in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salma Abdel Magid, Weiwei Pan, Simon Warchol, Grace Guo, Junsik Kim, Mahia Rahman, Hanspeter Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models are increasingly used in impactful real-life
applications. As such, there is a growing need to audit these models to ensure
that they generate desirable, task-appropriate images. However, systematically
inspecting the associations between prompts and generated content in a
human-understandable way remains challenging. To address this, we propose
Concept2Concept, a framework where we characterize conditional distributions of
vision language models using interpretable concepts and metrics that can be
defined in terms of these concepts. This characterization allows us to use our
framework to audit models and prompt-datasets. To demonstrate, we investigate
several case studies of conditional distributions of prompts, such as
user-defined distributions or empirical, real-world distributions. Lastly, we
implement Concept2Concept as an open-source interactive visualization tool to
facilitate use by non-technical end-users. A demo is available at
https://tinyurl.com/Concept2ConceptDemo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TractShapeNet: Efficient Multi-Shape Learning with 3D Tracto<span class="highlight-title">graph</span>y Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yui Lo, Yuqian Chen, Dongnan Liu, Jon Haitz Legarreta, Leo Zekelman, Fan Zhang, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J. O'Donnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain imaging studies have demonstrated that diffusion MRI tractography
geometric shape descriptors can inform the study of the brain's white matter
pathways and their relationship to brain function. In this work, we investigate
the possibility of utilizing a deep learning model to compute shape measures of
the brain's white matter connections. We introduce a novel framework,
TractShapeNet, that leverages a point cloud representation of tractography to
compute five shape measures: length, span, volume, total surface area, and
irregularity. We assess the performance of the method on a large dataset
including 1065 healthy young adults. Experiments for shape measure computation
demonstrate that our proposed TractShapeNet outperforms other point cloud-based
neural network models in both the Pearson correlation coefficient and
normalized error metrics. We compare the inference runtime results with the
conventional shape computation tool DSI-Studio. Our results demonstrate that a
deep learning approach enables faster and more efficient shape measure
computation. We also conduct experiments on two downstream language cognition
prediction tasks, showing that shape measures from TractShapeNet perform
similarly to those computed by DSI-Studio. Our code will be available at:
https://github.com/SlicerDMRI/TractShapeNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 4 tables. This work has been accepted to 2025
  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreeBlend: Advancing Concept Blending with Staged Feedback-Driven
  Interpolation Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Zhou, Haoyu Shen, Huan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept blending is a promising yet underexplored area in generative models.
While recent approaches, such as embedding mixing and latent modification based
on structural sketches, have been proposed, they often suffer from incompatible
semantic information and discrepancies in shape and appearance. In this work,
we introduce FreeBlend, an effective, training-free framework designed to
address these challenges. To mitigate cross-modal loss and enhance feature
detail, we leverage transferred image embeddings as conditional inputs. The
framework employs a stepwise increasing interpolation strategy between latents,
progressively adjusting the blending ratio to seamlessly integrate auxiliary
features. Additionally, we introduce a feedback-driven mechanism that updates
the auxiliary latents in reverse order, facilitating global blending and
preventing rigid or unnatural outputs. Extensive experiments demonstrate that
our method significantly improves both the semantic coherence and visual
quality of blended images, yielding compelling and coherent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Webpage: https://petershen-csworld.github.io/FreeBlend. -- updated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEW: Self-calibration Enhanced Whole Slide Pathology Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoming Luo, Xiaotian Yu, Shengxuming Zhang, Jiabin Xia, Yang Jian, Yuning Sun, Liang Xue, Mingli Song, Jing Zhang, Xiuming Zhang, Zunlei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology images are considered the ``gold standard" for cancer diagnosis and
treatment, with gigapixel images providing extensive tissue and cellular
information. Existing methods fail to simultaneously extract global structural
and local detail features for comprehensive pathology image analysis
efficiently. To address these limitations, we propose a self-calibration
enhanced framework for whole slide pathology image analysis, comprising three
components: a global branch, a focus predictor, and a detailed branch. The
global branch initially classifies using the pathological thumbnail, while the
focus predictor identifies relevant regions for classification based on the
last layer features of the global branch. The detailed extraction branch then
assesses whether the magnified regions correspond to the lesion area. Finally,
a feature consistency constraint between the global and detail branches ensures
that the global branch focuses on the appropriate region and extracts
sufficient discriminative features for final identification. These focused
discriminative features prove invaluable for uncovering novel prognostic tumor
markers from the perspective of feature cluster uniqueness and tissue spatial
distribution. Extensive experiment results demonstrate that the proposed
framework can rapidly deliver accurate and explainable results for pathological
grading and prognosis tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anti-Forgetting Adaptation for Unsupervised Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Francois Bremond, Nicu Sebe, Shiliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular unsupervised domain adaptive person re-identification (ReID) focuses
on adapting a model from a source domain to a fixed target domain. However, an
adapted ReID model can hardly retain previously-acquired knowledge and
generalize to unseen data. In this paper, we propose a Dual-level Joint
Adaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a
model to new domains without forgetting source domain and each adapted target
domain. We explore the possibility of using prototype and instance-level
consistency to mitigate the forgetting during the adaptation. Specifically, we
store a small number of representative image samples and corresponding cluster
prototypes in a memory buffer, which is updated at each adaptation step. With
the buffered images and prototypes, we regularize the image-to-image similarity
and image-to-prototype similarity to rehearse old knowledge. After the
multi-step adaptation, the model is tested on all seen domains and several
unseen domains to validate the generalization ability of our method. Extensive
experiments demonstrate that our proposed method significantly improves the
anti-forgetting, generalization and backward-compatible ability of an
unsupervised person ReID model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation
  Learning with Visual Observations <span class="chip">ICML
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minung Kim, Kawon Lee, Jungmo Kim, Sungho Choi, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning (IL) enables agents to mimic expert behavior without
reward signals but faces challenges in cross-domain scenarios with
high-dimensional, noisy, and incomplete visual observations. To address this,
we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning
(DIFF-IL), a novel IL method that extracts domain-invariant features from
individual frames and adapts them into sequences to isolate and replicate
expert behaviors. We also introduce a frame-wise time labeling technique to
segment expert behaviors by timesteps and assign rewards aligned with temporal
contexts, enhancing task performance. Experiments across diverse visual
environments demonstrate the effectiveness of DIFF-IL in addressing complex
visual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main, 19 pages appendix with reference. Submitted to ICML
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ City-Scale Multi-Camera Vehicle Tracking System with Improved
  Self-Supervised Camera Link Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqiang Lin, Sam Lockyer, Nic Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms
the basis for numerous future city-wide systems (e.g. traffic management, crash
detection, etc.). However, the challenge of matching vehicle trajectories
across different cameras based solely on feature extraction poses significant
difficulties. This article introduces an innovative multi-camera vehicle
tracking system that utilizes a self-supervised camera link model. In contrast
to related works that rely on manual spatial-temporal annotations, our model
automatically extracts crucial multi-camera relationships for vehicle matching.
The camera link is established through a pre-matching process that evaluates
feature similarities, pair numbers, and time variance for high-quality tracks.
This process calculates the probability of spatial linkage for all camera
combinations, selecting the highest scoring pairs to create camera links. Our
approach significantly improves deployment times by eliminating the need for
human annotation, offering substantial improvements in efficiency and
cost-effectiveness when it comes to real-world application. This pairing
process supports cross camera matching by setting spatial-temporal constraints,
reducing the searching space for potential vehicle matches. According to our
experimental results, the proposed method achieves a new state-of-the-art among
automatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1
Score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Upload the revised manuscript with the publisher's requirement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10919v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10919v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhao, Tingwei Chen, Zhijie Cai, Xiaoyang Li, Hang Li, Qimei Chen, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Wi-Fi sensing has garnered significant attention due to its
numerous benefits, such as privacy protection, low cost, and penetration
ability. Extensive research has been conducted in this field, focusing on areas
such as gesture recognition, people identification, and fall detection.
However, many data-driven methods encounter challenges related to domain shift,
where the model fails to perform well in environments different from the
training data. One major factor contributing to this issue is the limited
availability of Wi-Fi sensing datasets, which makes models learn excessive
irrelevant information and over-fit to the training set. Unfortunately,
collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a
challenging task. To address this problem, we propose CrossFi, a siamese
network-based approach that excels in both in-domain scenario and cross-domain
scenario, including few-shot, zero-shot scenarios, and even works in few-shot
new-class scenario where testing set contains new categories. The core
component of CrossFi is a sample-similarity calculation network called CSi-Net,
which improves the structure of the siamese network by using an attention
mechanism to capture similarity information, instead of simply calculating the
distance or cosine similarity. Based on it, we develop an extra Weight-Net that
can generate a template for each class, so that our CrossFi can work in
different scenarios. Experimental results demonstrate that our CrossFi achieves
state-of-the-art performance across various scenarios. In gesture recognition
task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%
in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,
and 84.75% in one-shot new-class scenario. The code for our model is publicly
available at https://github.com/RS2002/CrossFi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HaSPeR: An Image Repository for Hand Shadow Puppet Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10360v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10360v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Rifat Raiyan, Zibran Zarif Amio, Sabbir Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of
theatrical art and storytelling where hand shadows are projected onto flat
surfaces to create illusions of living creatures. The skilled performers create
these silhouettes by hand positioning, finger movements, and dexterous gestures
to resemble shadows of animals and objects. Due to the lack of practitioners
and a seismic shift in people's entertainment standards, this art form is on
the verge of extinction. To facilitate its preservation and proliferate it to a
wider audience, we introduce ${\rm H{\small A}SP{\small E}R}$, a novel dataset
consisting of 15,000 images of hand shadow puppets across 15 classes extracted
from both professional and amateur hand shadow puppeteer clips. We provide a
detailed statistical analysis of the dataset and employ a range of pretrained
image classification models to establish baselines. Our findings show a
substantial performance superiority of skip-connected convolutional models over
attention-based transformer architectures. We also find that lightweight
models, such as MobileNetV2, suited for mobile applications and embedded
devices, perform comparatively well. We surmise that such low-latency
architectures can be useful in developing ombromanie teaching tools, and we
create a prototype application to explore this surmission. Keeping the
best-performing model ResNet34 under the limelight, we conduct comprehensive
feature-spatial, explainability, and error analyses to gain insights into its
decision-making process. To the best of our knowledge, this is the first
documented dataset and research endeavor to preserve this dying art for future
generations, with computer vision approaches. Our code and data will be
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Machine Vision and Applications, 13 pages, 105 figures,
  2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why does my medical AI look at pictures of birds? Exploring the efficacy
  of transfer learning across domain boundaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Jonske, Moon Kim, Enrico Nasca, Janis Evers, Johannes Haubold, René Hosch, Felix Nensa, Michael Kamp, Constantin Seibold, Jan Egger, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is an open secret that ImageNet is treated as the panacea of pretraining.
Particularly in medical machine learning, models not trained from scratch are
often finetuned based on ImageNet-pretrained models. We posit that pretraining
on data from the domain of the downstream task should almost always be
preferred instead. We leverage RadNet-12M, a dataset containing more than 12
million computed tomography (CT) image slices, to explore the efficacy of
self-supervised pretraining on medical and natural images. Our experiments
cover intra- and cross-domain transfer scenarios, varying data scales,
finetuning vs. linear evaluation, and feature space analysis. We observe that
intra-domain transfer compares favorably to cross-domain transfer, achieving
comparable or improved performance (0.44% - 2.07% performance increase using
RadNet pretraining, depending on the experiment) and demonstrate the existence
of a domain boundary-related generalization gap and domain-specific learned
features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available from
  https://github.com/TIO-IKIM/Transfer-learning-across-domain-boundaries/ -
  Paper, code, and contents are subject to the CC-BY-NC 4.0 license</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verbalized Machine Learning: Revisiting Machine Learning with Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Z. Xiao, Robert Bamler, Bernhard Schölkopf, Weiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the progress made by large language models (LLMs), we introduce
the framework of verbalized machine learning (VML). In contrast to conventional
machine learning (ML) models that are typically optimized over a continuous
parameter space, VML constrains the parameter space to be human-interpretable
natural language. Such a constraint leads to a new perspective of function
approximation, where an LLM with a text prompt can be viewed as a function
parameterized by the text prompt. Guided by this perspective, we revisit
classical ML problems, such as regression and classification, and find that
these problems can be solved by an LLM-parameterized learner and optimizer. The
major advantages of VML include (1) easy encoding of inductive bias: prior
knowledge about the problem and hypothesis class can be encoded in natural
language and fed into the LLM-parameterized learner; (2) automatic model class
selection: the optimizer can automatically select a model class based on data
and verbalized prior knowledge, and it can update the model class during
training; and (3) interpretable learner updates: the LLM-parameterized
optimizer can provide explanations for why an update is performed. We
empirically verify the effectiveness of VML, and hope that VML can serve as a
stepping stone to stronger interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (116 pages, 32
  figures, v3: refined the paper structure and added more empirical results)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised contrastive learning for cell stage classification of animal
  embryos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmine Hachani, Patrick Bouthemy, Elisa Fromont, Sylvie Ruffini, Ludivine Laffont, Alline de Paula Reis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video microscopy, when combined with machine learning, offers a promising
approach for studying the early development of in vitro produced (IVP) embryos.
However, manually annotating developmental events, and more specifically cell
divisions, is time-consuming for a biologist and cannot scale up for practical
applications. We aim to automatically classify the cell stages of embryos from
2D time-lapse microscopy videos with a deep learning approach. We focus on the
analysis of bovine embryonic development using video microscopy, as we are
primarily interested in the application of cattle breeding, and we have created
a Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1)
low-quality images and bovine dark cells that make the identification of cell
stages difficult, (2) class ambiguity at the boundaries of developmental
stages, and (3) imbalanced data distribution. To address these challenges, we
introduce CLEmbryo, a novel method that leverages supervised contrastive
learning combined with focal loss for training, and the lightweight 3D neural
network CSN-50 as an encoder. We also show that our method generalizes well.
CLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset
and the publicly available NYU Mouse Embryos dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Predictive Coding Networks -- Made Simple 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Pinchetti, Chang Qi, Oleh Lokshyn, Gaspard Olivers, Cornelius Emde, Mufeng Tang, Amine M'Charrak, Simon Frieder, Bayar Menzat, Rafal Bogacz, Thomas Lukasiewicz, Tommaso Salvatori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the problems of efficiency and scalability for
predictive coding networks (PCNs) in machine learning. To do so, we propose a
library, called PCX, that focuses on performance and simplicity, and use it to
implement a large set of standard benchmarks for the community to use for their
experiments. As most works in the field propose their own tasks and
architectures, do not compare one against each other, and focus on small-scale
tasks, a simple and fast open-source library and a comprehensive set of
benchmarks would address all these concerns. Then, we perform extensive tests
on such benchmarks using both existing algorithms for PCNs, as well as
adaptations of other methods popular in the bio-plausible deep learning
community. All this has allowed us to (i) test architectures much larger than
commonly used in the literature, on more complex datasets; (ii)~reach new
state-of-the-art results in all of the tasks and datasets provided;
(iii)~clearly highlight what the current limitations of PCNs are, allowing us
to state important future research directions. With the hope of galvanizing
community efforts towards one of the main open problems in the field,
scalability, we release code, tests, and benchmarks. Link to the library:
https://github.com/liukidar/pcx
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00625v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00625v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Peng, Xiao Lin, Nachuan Ma, Jiayuan Du, Chuangwei Liu, Chengju Liu, Qijun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual anomaly detection is vital in real-world applications, such as
industrial defect detection and medical diagnosis. However, most existing
methods focus on local structural anomalies and fail to detect higher-level
functional anomalies under logical conditions. Although recent studies have
explored logical anomaly detection, they can only address simple anomalies like
missing or addition and show poor generalizability due to being heavily
data-driven. To fill this gap, we propose SAM-LAD, a zero-shot, plug-and-play
framework for logical anomaly detection in any scene. First, we obtain a query
image's feature map using a pre-trained backbone. Simultaneously, we retrieve
the reference images and their corresponding feature maps via the nearest
neighbor search of the query image. Then, we introduce the Segment Anything
Model (SAM) to obtain object masks of the query and reference images. Each
object mask is multiplied with the entire image's feature map to obtain object
feature maps. Next, an Object Matching Model (OMM) is proposed to match objects
in the query and reference images. To facilitate object matching, we further
propose a Dynamic Channel Graph Attention (DCGA) module, treating each object
as a keypoint and converting its feature maps into feature vectors. Finally,
based on the object matching relations, an Anomaly Measurement Model (AMM) is
proposed to detect objects with logical anomalies. Structural anomalies in the
objects can also be detected. We validate our proposed SAM-LAD using various
benchmarks, including industrial datasets (MVTec Loco AD, MVTec AD), and the
logical dataset (DigitAnatomy). Extensive experimental results demonstrate that
SAM-LAD outperforms existing SoTA methods, particularly in detecting logical
anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.05768 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Framework for Automated Segmentation of Perivascular
  Spaces in <span class="highlight-title">Brain</span> <span class="highlight-title">MRI</span> with the nnU-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Pham, Alexander Jarema, Donggyu Rim, Zhibin Chen, Mohamed S. H. Khlif, Vaughan G. Macefield, Luke A. Henderson, Amy Brodtmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Enlargement of perivascular spaces (PVS) is common in
neurodegenerative disorders including cerebral small vessel disease,
Alzheimer's disease, and Parkinson's disease. PVS enlargement may indicate
impaired clearance pathways and there is a need for reliable PVS detection
methods which are currently lacking. Aim: To optimise a widely used deep
learning model, the no-new-UNet (nnU-Net), for PVS segmentation. Methods: In 30
healthy participants (mean$\pm$SD age: 50$\pm$18.9 years; 13 females),
T1-weighted MRI images were acquired using three different protocols on three
MRI scanners (3T Siemens Tim Trio, 3T Philips Achieva, and 7T Siemens
Magnetom). PVS were manually segmented across ten axial slices in each
participant. Segmentations were completed using a sparse annotation strategy.
In total, 11 models were compared using various strategies for image handling,
preprocessing and semi-supervised learning with pseudo-labels. Model
performance was evaluated using 5-fold cross validation (5FCV). The main
performance metric was the Dice Similarity Coefficient (DSC). Results: The
voxel-spacing agnostic model (mean$\pm$SD DSC=64.3$\pm$3.3%) outperformed
models which resampled images to a common resolution (DSC=40.5-55%). Model
performance improved substantially following iterative label cleaning
(DSC=85.7$\pm$1.2%). Semi-supervised learning with pseudo-labels (n=12,740)
from 18 additional datasets improved the agreement between raw and predicted
PVS cluster counts (Lin's concordance correlation coefficient=0.89,
95%CI=0.82-0.94). We extended the model to enable PVS segmentation in the
midbrain (DSC=64.3$\pm$6.5%) and hippocampus (DSC=67.8$\pm$5%). Conclusions:
Our deep learning models provide a robust and holistic framework for the
automated quantification of PVS in brain MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with
  Semantic-Aware Views for Efficient Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Yang, Lijian Xu, Hongsheng Li, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a scalable and straightforward pre-training paradigm for
efficient visual conceptual representation called occluded image contrastive
learning (OCL). Our OCL approach is simple: we randomly mask patches to
generate different views within an image and contrast them among a mini-batch
of images. The core idea behind OCL consists of two designs. First, masked
tokens have the potential to significantly diminish the conceptual redundancy
inherent in images, and create distinct views with substantial fine-grained
differences on the semantic concept level instead of the instance level.
Second, contrastive learning is adept at extracting high-level semantic
conceptual features during the pre-training, circumventing the high-frequency
interference and additional costs associated with image reconstruction.
Importantly, OCL learns highly semantic conceptual representations efficiently
without relying on hand-crafted data augmentations or additional auxiliary
modules. Empirically, OCL demonstrates high scalability with Vision
Transformers, as the ViT-L/16 can complete pre-training in 133 hours using only
4 A100 GPUs, achieving 85.8\% accuracy in downstream fine-tuning tasks. Code is
available at https://anonymous.4open.science/r/OLRS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Forgery Localization with State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Lou, Gang Cao, Kun Guo, Shaowei Weng, Lifang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pixel dependency modeling from tampered images is pivotal for image forgery
localization. Current approaches predominantly rely on Convolutional Neural
Networks (CNNs) or Transformer-based models, which often either lack sufficient
receptive fields or entail significant computational overheads. Recently, State
Space Models (SSMs), exemplified by Mamba, have emerged as a promising
approach. They not only excel in modeling long-range interactions but also
maintain a linear computational complexity. In this paper, we propose LoMa, a
novel image forgery localization method that leverages the selective SSMs.
Specifically, LoMa initially employs atrous selective scan to traverse the
spatial domain and convert the tampered image into ordered patch sequences, and
subsequently applies multi-directional state space modeling. In addition, an
auxiliary convolutional branch is introduced to enhance local feature
extraction. Extensive experimental results validate the superiority of LoMa
over CNN-based and Transformer-based state-of-the-arts. To our best knowledge,
this is the first image forgery localization model constructed based on the
SSM-based model. We aim to establish a baseline and provide valuable insights
for the future development of more efficient and effective SSM-based forgery
localization models. Code is available at
https://github.com/multimediaFor/LoMa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRISHUL: Towards Region Identification and Screen Hierarchy
  Understanding for Large VLM based GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Singh, Shreyas Singh, Mukund Khanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Vision Language Models (LVLMs) have enabled the
development of LVLM-based Graphical User Interface (GUI) agents under various
paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle
with cross-dataset and cross-platform generalization due to their reliance on
dataset-specific training. Generalist LVLMs, such as GPT-4V, employ
Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires
metadata like HTML source, which is not consistently available across
platforms. Moreover, existing methods often specialize in singular GUI tasks
rather than achieving comprehensive GUI understanding. To address these
limitations, we introduce TRISHUL, a novel, training-free agentic framework
that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior
works that focus on either action grounding (mapping instructions to GUI
elements) or GUI referring (describing GUI elements given a location), TRISHUL
seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen
Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module,
which work synergistically to provide multi-granular, spatially, and
semantically enriched representations of GUI elements. Our results demonstrate
TRISHUL's superior performance in action grounding across the ScreenSpot,
VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,
TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new
standard for robust and adaptable GUI comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Personalized Content Synthesis with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xulu Zhang, Xiaoyong Wei, Wengyu Zhang, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative models have significantly impacted content
creation, leading to the emergence of Personalized Content Synthesis (PCS).
With a small set of user-provided examples, PCS aims to customize the subject
of interest to specific user-defined prompts. Over the past two years, more
than 150 methods have been proposed. However, existing surveys mainly focus on
text-to-image generation, with few providing up-to-date summaries on PCS. This
paper offers a comprehensive survey of PCS, with a particular focus on the
diffusion models. Specifically, we introduce the generic frameworks of PCS
research, which can be broadly classified into optimization-based and
learning-based approaches. We further categorize and analyze these
methodologies, discussing their strengths, limitations, and key techniques.
Additionally, we delve into specialized tasks within the field, such as
personalized object generation, face synthesis, and style personalization,
highlighting their unique challenges and innovations. Despite encouraging
progress, we also present an analysis of the challenges such as overfitting and
the trade-off between subject fidelity and text alignment. Through this
detailed overview and analysis, we propose future directions to advance the
development of PCS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Video Coding Meets <span class="highlight-title">Multimodal</span> Large Language Models: A Unified
  Paradigm for Video Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Zhang, Jinlong Li, Kecheng Chen, Meng Wang, Long Xu, Haoliang Li, Nicu Sebe, Sam Kwong, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing codecs are designed to eliminate intrinsic redundancies to create a
compact representation for compression. However, strong external priors from
Multimodal Large Language Models (MLLMs) have not been explicitly explored in
video compression. Herein, we introduce a unified paradigm for Cross-Modality
Video Coding (CMVC), which is a pioneering approach to explore multimodality
representation and video generative models in video coding. Specifically, on
the encoder side, we disentangle a video into spatial content and motion
components, which are subsequently transformed into distinct modalities to
achieve very compact representation by leveraging MLLMs. During decoding,
previously encoded components and video generation models are leveraged to
create multiple encoding-decoding modes that optimize video reconstruction
quality for specific decoding requirements, including Text-Text-to-Video (TT2V)
mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)
mode to achieve superb perceptual consistency. In addition, we propose an
efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)
tuning to guarantee perceptual quality, which allows the generated motion cues
to behave smoothly. Experiments on benchmarks indicate that TT2V achieves
effective semantic reconstruction, while IT2V exhibits competitive perceptual
consistency. These results highlight potential directions for future research
in video coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level
  Feature Distribution for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong-Nam Tran, Nhat Truong Pham, Duc Ngoc Minh Dang, Eui-Nam Huh, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation plays a crucial role in assisting healthcare
professionals with accurate diagnoses and enabling automated diagnostic
processes. Traditional convolutional neural networks (CNNs) often struggle with
capturing long-range dependencies, while transformer-based architectures,
despite their effectiveness, come with increased computational complexity.
Recent efforts have focused on combining CNNs and transformers to balance
performance and efficiency, but existing approaches still face challenges in
achieving high segmentation accuracy while maintaining low computational costs.
Furthermore, many methods underutilize the CNN encoder's capability to capture
local spatial information, concentrating primarily on mitigating long-range
dependency issues. To address these limitations, we propose QTSeg, a novel
architecture for medical image segmentation that effectively integrates local
and global information. QTSeg features a dual-mix attention decoder designed to
enhance segmentation performance through: (1) a cross-attention mechanism for
improved feature alignment, (2) a spatial attention module to capture
long-range dependencies, and (3) a channel attention block to learn
inter-channel relationships. Additionally, we introduce a multi-level feature
distribution module, which adaptively balances feature propagation between the
encoder and decoder, further boosting performance. Extensive experiments on
five publicly available datasets covering diverse segmentation tasks, including
lesion, polyp, breast cancer, cell, and retinal vessel segmentation,
demonstrate that QTSeg outperforms state-of-the-art methods across multiple
evaluation metrics while maintaining lower computational costs. Our
implementation can be found at: https://github.com/tpnam0901/QTSeg (v1.0.0)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature
  Learning in Chest CT Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexing Ding, Jun Wang, Hongbing Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of medical image segmentation, challenges such as indistinct
lesion features, ambiguous boundaries,and multi-scale characteristics have long
revailed. This paper proposes an improved method named Intensity-Spatial Dual
Masked AutoEncoder (ISD-MAE). Based on the tissue-contrast semi-masked
autoencoder, a Masked AutoEncoder (MAE) branch is introduced to perform
intensity masking and spatial masking operations on chest CT images for
multi-scale feature learning and segmentation tasks. The model utilizes a
dual-branch structure and contrastive learning to enhance the ability to learn
tissue features and boundary details. Experiments are conducted on multiple 2D
and 3D datasets. The results show that ISD-MAE significantly outperforms other
methods in 2D pneumonia and mediastinal tumor segmentation tasks. For example,
the Dice score reaches 90.10% on the COVID19 LESION dataset, and the
performance is relatively stable. However, there is still room for improvement
on 3D datasets. In response to this, improvement directions are proposed,
including optimizing the loss function, using enhanced 3D convolution blocks,
and processing datasets from multiple perspectives.Our code is available
at:https://github.com/prowontheus/ISD-MAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>During further verification, we found that due to operational errors,
  a small number of images in the dataset used for training appeared in the
  validation set, which led to inaccurate main conclusions. We are correcting
  these problems and plan to withdraw this paper.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Scene Understanding through Object-Centric Voxelization and
  Neural Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Zhao, Yiwei Hao, Siyu Gao, Yunbo Wang, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning object-centric representations from unsupervised videos is
challenging. Unlike most previous approaches that focus on decomposing 2D
images, we present a 3D generative model named DynaVol-S for dynamic scenes
that enables object-centric learning within a differentiable volume rendering
framework. The key idea is to perform object-centric voxelization to capture
the 3D nature of the scene, which infers per-object occupancy probabilities at
individual spatial locations. These voxel features evolve through a
canonical-space deformation function and are optimized in an inverse rendering
pipeline with a compositional NeRF. Additionally, our approach integrates 2D
semantic features to create 3D semantic grids, representing the scene through
multiple disentangled voxel grids. DynaVol-S significantly outperforms existing
models in both novel view synthesis and unsupervised decomposition tasks for
dynamic scenes. By jointly considering geometric structures and semantic
features, it effectively addresses challenging real-world scenarios involving
complex object interactions. Furthermore, once trained, the explicitly
meaningful voxel features enable additional capabilities that 2D scene
decomposition methods cannot achieve, such as novel scene generation through
editing geometric shapes or manipulating the motion trajectories of objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we tackle industry challenges in video content classification
by exploring and optimizing GPT-based models for zero-shot classification
across seven critical categories of video quality. We contribute a novel
approach to improving GPT's performance through prompt optimization and policy
refinement, demonstrating that simplifying complex policies significantly
reduces false negatives. Additionally, we introduce a new
decomposition-aggregation-based prompt engineering technique, which outperforms
traditional single-prompt methods. These experiments, conducted on real
industry problems, show that thoughtful prompt design can substantially enhance
GPT's performance without additional finetuning, offering an effective and
scalable solution for improving video classification systems across various
domains in industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textrm{A}^{\textrm{2}}$RNet: Adversarial Attack Resilient Network for
  Robust Infrared and Visible Image Fusion <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Li, Hongwei Yu, Jiansheng Chen, Xinlong Ding, Jinlong Wang, Jinyuan Liu, Bochao Zou, Huimin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing
visual performance by integrating unique information from different modalities
into one fused image. Exiting methods pay more attention to conducting fusion
with undisturbed data, while overlooking the impact of deliberate interference
on the effectiveness of fusion results. To investigate the robustness of fusion
models, in this paper, we propose a novel adversarial attack resilient network,
called $\textrm{A}^{\textrm{2}}$RNet. Specifically, we develop an adversarial
paradigm with an anti-attack loss function to implement adversarial attacks and
training. It is constructed based on the intrinsic nature of IVIF and provide a
robust foundation for future research advancements. We adopt a Unet as the
pipeline with a transformer-based defensive refinement module (DRM) under this
paradigm, which guarantees fused image quality in a robust coarse-to-fine
manner. Compared to previous works, our method mitigates the adverse effects of
adversarial perturbations, consistently maintaining high-fidelity fusion
results. Furthermore, the performance of downstream tasks can also be well
maintained under adversarial attacks. Code is available at
https://github.com/lok-18/A2RNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing and Boosting the Power of Fine-Grained Visual Recognition for
  <span class="highlight-title">Multi-modal</span> Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, Yuxin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) have shown remarkable abilities in
various visual understanding tasks. However, MLLMs still struggle with
fine-grained visual recognition (FGVR), which aims to identify
subordinate-level categories from images. This can negatively impact more
advanced capabilities of MLLMs, such as object-centric visual question
answering and reasoning. In our study, we revisit three quintessential
capabilities of MLLMs for FGVR, including object information extraction,
category knowledge reserve, object-category alignment, and position of the root
cause as a misalignment problem. To address this issue, we present Finedefics,
an MLLM that enhances the model's FGVR capability by incorporating informative
attribute descriptions of objects into the training phase. We employ
contrastive learning on object-attribute pairs and attribute-category pairs
simultaneously and use examples from similar but incorrect categories as hard
negatives, naturally bringing representations of visual objects and category
names closer. Extensive evaluations across multiple popular FGVR datasets
demonstrate that Finedefics outperforms existing MLLMs of comparable parameter
sizes, showcasing its remarkable efficacy. The code is available at
https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedMimic: Physician-Inspired <span class="highlight-title">Multimodal</span> Fusion for Early Diagnosis of
  Fever of Unknown Origin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minrui Chen, Yi Zhou, Huidong Jiang, Yuhan Zhu, Guanjie Zou, Minqi Chen, Rong Tian, Hiroto Saigo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is
introduced as a multimodal framework inspired by real-world diagnostic
processes. It uses pretrained models such as DINOv2, Vision Transformer, and
ResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into
low-dimensional, semantically meaningful features. A learnable
self-attention-based fusion network then integrates these imaging features with
clinical data for classification. Using 416 FUO patient cases from Sichuan
University West China Hospital from 2017 to 2023, the multimodal fusion
classification network MFCN achieved macro-AUROC scores ranging from 0.8654 to
0.9291 across seven tasks, outperforming conventional machine learning and
single-modality deep learning methods. Ablation studies and five-fold
cross-validation further validated its effectiveness. By combining the
strengths of pretrained large models and deep learning, MedMimic offers a
promising solution for disease classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coevolution of Camouflage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11793v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11793v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig Reynolds
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflage in nature seems to arise from competition between predator and
prey. To survive, predators must find prey, and prey must avoid being found.
This work simulates an abstract model of that adversarial relationship. It
looks at crypsis through evolving prey camouflage patterns (as color textures)
in competition with evolving predator vision. During their "lifetime" predators
learn to better locate camouflaged prey. The environment for this 2D simulation
is provided by a set of photographs, typically of natural scenes. This model is
based on two evolving populations, one of prey and another of predators. Mutual
conflict between these populations can produce both effective prey camouflage
and predators skilled at "breaking" camouflage. The result is an open source
artificial life model to help study camouflage in nature, and the perceptual
phenomenon of camouflage more generally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification
  with Snoring Usecase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06110v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06110v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rakibul Hasan, Shreya Ghosh, Pradyumna Agrawal, Zhixi Cai, Abhinav Dhall, Tom Gedeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an atomic behaviour intervention strategy using Pavlok
device. Pavlok utilises beeps, vibration and shocks as a mode of aversion
technique to help individuals with behaviour modification. While the device can
be useful in certain periodic daily life situations, like alarms and exercise
notifications, the device relies on manual operations that limit its usage. To
automate behaviour modification, we propose a framework that first detects
targeted behaviours through a lightweight deep learning model and subsequently
nudges the user through Pavlok. Our proposed solution is implemented and
verified in the context of snoring, which captures audio from the environment
following a prediction of whether the audio content is a snore or not using a
1D convolutional neural network. Based on the prediction, we use Pavlok to
nudge users for preventive measures, such as a change in sleeping posture. We
believe that this simple solution can help people change their atomic habits,
which may lead to long-term health benefits. Our proposed real-time,
lightweight model (99.8% fewer parameters over SOTA; 1,278,049 --> 1337)
achieves SOTA performance (test accuracy of 0.99) on a public benchmark. The
code and model are publicly available at
https://github.com/hasan-rakibul/pavlok-nudge-snore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Md Rakibul Hasan and Shreya Ghosh are co-first authors</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Cross-Stage Coordination Pre-ranking Model for Online
  Recommendation Systems <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binglei Zhao, Houying Qi, Guang Xu, Mian Ma, Xiwei Zhao, Feng Mei, Sulong Xu, Jinghe Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale recommendation systems often adopt cascading architecture
consisting of retrieval, pre-ranking, ranking, and re-ranking stages. With
strict latency requirements, pre-ranking utilizes lightweight models to perform
a preliminary selection from massive retrieved candidates. However, recent
works focus solely on improving consistency with ranking, relying exclusively
on downstream stages. Since downstream input is derived from the pre-ranking
output, they will exacerbate the sample selection bias (SSB) issue and Matthew
effect, leading to sub-optimal results. To address the limitation, we propose a
novel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate
information from upstream (retrieval) and downstream (ranking, re-ranking)
stages. Specifically, cross-stage coordination refers to the pre-ranking's
adaptability to the entire stream and the role of serving as a more effective
bridge between upstream and downstream. HCCP consists of Hybrid Sample
Construction and Hybrid Objective Optimization. Hybrid sample construction
captures multi-level unexposed data from the entire stream and rearranges them
to become the optimal guiding "ground truth" for pre-ranking learning. Hybrid
objective optimization contains the joint optimization of consistency and
long-tail precision through our proposed Margin InfoNCE loss. It is
specifically designed to learn from such hybrid unexposed samples, improving
the overall performance and mitigating the SSB issue. The appendix describes a
proof of the efficacy of the proposed loss in selecting potential positives.
Extensive offline and online experiments indicate that HCCP outperforms SOTA
methods by improving cross-stage coordination. It contributes up to 14.9% UCVR
and 1.3% UCTR in the JD E-commerce recommendation system. Concerning code
privacy, we provide a pseudocode for reference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProReco: A Process Discovery Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsung-Hao Huang, Tarek Junied, Marco Pegoraro, Wil M. P. van der Aalst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process discovery aims to automatically derive process models from historical
execution data (event logs). While various process discovery algorithms have
been proposed in the last 25 years, there is no consensus on a dominating
discovery algorithm. Selecting the most suitable discovery algorithm remains a
challenge due to competing quality measures and diverse user requirements.
Manually selecting the most suitable process discovery algorithm from a range
of options for a given event log is a time-consuming and error-prone task. This
paper introduces ProReco, a Process discovery Recommender system designed to
recommend the most appropriate algorithm based on user preferences and event
log characteristics. ProReco incorporates state-of-the-art discovery
algorithms, extends the feature pools from previous work, and utilizes
eXplainable AI (XAI) techniques to provide explanations for its
recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 9 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SessionRec: Next Session Prediction Paradigm For Generative Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Hao Guo, Linzhi Peng, Long Zhang, Xiaoteng Wang, Daoyuan Wang, Shichao Wang, Jinpeng Wang, Lei Wang, Sheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for
generative sequential recommendation, addressing the fundamental misalignment
between conventional next-item prediction paradigm (NIPP) and real-world
recommendation scenarios. Unlike NIPP's item-level autoregressive generation
that contradicts actual session-based user interactions, our framework
introduces a session-aware representation learning through hierarchical
sequence aggregation (intra/inter-session), reducing attention computation
complexity while enabling implicit modeling of massive negative interactions,
and a session-based prediction objective that better captures users' diverse
interests through multi-item recommendation in next sessions. Moreover, we
found that incorporating a rank loss for items within the session under the
next session prediction paradigm can significantly improve the ranking
effectiveness of generative sequence recommendation models. We also verified
that SessionRec exhibits clear power-law scaling laws similar to those observed
in LLMs. Extensive experiments conducted on public datasets and online A/B test
in Meituan App demonstrate the effectiveness of SessionRec. The proposed
paradigm establishes new foundations for developing industrial-scale generative
recommendation systems through its model-agnostic architecture and
computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petru Neague, Quinten Stokkink, Naman Goel, Johan Pouwelse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Centralized search engines are key for the Internet, but lead to undesirable
concentration of power. Decentralized alternatives fail to offer equal document
retrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come
close to the performance of centralized solutions when the semantics of
documents are properly captured. This work uses embeddings from Large Language
Models to capture semantics and fulfill the promise of Semantic Overlay
Networks. Our proposed algorithm, called Semantica, constructs a prefix tree
(trie) utilizing document embeddings calculated by a language model. Users
connect to each other based on the embeddings of their documents, ensuring that
semantically similar users are directly linked. Thereby, this construction
makes it more likely for user searches to be answered by the users that they
are directly connected to, or by the users they are close to in the network
connection graph. The implementation of our algorithm also accommodates the
semantic diversity of individual users by spawning "clone" user identifiers in
the tree. Our experiments use emulation with a real-world workload to show
Semantica's ability to identify and connect to similar users quickly. Semantica
finds up to ten times more semantically similar users than current
state-of-the-art approaches. At the same time, Semantica can retrieve more than
two times the number of relevant documents given the same network load. We also
make our code publicly available to facilitate further research in the area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on LLM-powered Agents for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyao Peng, Hongtao Liu, Hua Huang, Qing Yang, Minglai Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are essential components of many online platforms, yet
traditional approaches still struggle with understanding complex user
preferences and providing explainable recommendations. The emergence of Large
Language Model (LLM)-powered agents offers a promising approach by enabling
natural language interactions and interpretable reasoning, potentially
transforming research in recommender systems. This survey provides a systematic
review of the emerging applications of LLM-powered agents in recommender
systems. We identify and analyze three key paradigms in current research: (1)
Recommender-oriented approaches, which leverage intelligent agents to enhance
the fundamental recommendation mechanisms; (2) Interaction-oriented approaches,
which facilitate dynamic user engagement through natural dialogue and
interpretable suggestions; and (3) Simulation-oriented approaches, which employ
multi-agent frameworks to model complex user-item interactions and system
dynamics. Beyond paradigm categorization, we analyze the architectural
foundations of LLM-powered recommendation agents, examining their essential
components: profile construction, memory management, strategic planning, and
action execution. Our investigation extends to a comprehensive analysis of
benchmark datasets and evaluation frameworks in this domain. This systematic
examination not only illuminates the current state of LLM-powered agent
recommender systems but also charts critical challenges and promising research
directions in this transformative field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KGGen: Extracting Knowledge <span class="highlight-title">Graph</span>s from Plain Text with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent interest in building foundation models for KGs has highlighted a
fundamental challenge: knowledge-graph data is relatively scarce. The
best-known KGs are primarily human-labeled, created by pattern-matching, or
extracted using early NLP techniques. While human-generated KGs are in short
supply, automatically extracted KGs are of questionable quality. We present a
solution to this data scarcity problem in the form of a text-to-KG generator
(KGGen), a package that uses language models to create high-quality graphs from
plaintext. Unlike other KG extractors, KGGen clusters related entities to
reduce sparsity in extracted KGs. KGGen is available as a Python library
(\texttt{pip install kg-gen}), making it accessible to everyone. Along with
KGGen, we release the first benchmark, Measure of of Information in Nodes and
Edges (MINE), that tests an extractor's ability to produce a useful KG from
plain text. We benchmark our new tool against existing extractors and
demonstrate far superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven effective in integrating
external knowledge into large language models (LLMs) for question-answer (QA)
tasks. The state-of-the-art RAG approaches often use the graph data as the
external data since they capture the rich semantic information and link
relationships between entities. However, existing graph-based RAG approaches
cannot accurately identify the relevant information from the graph and also
consume large numbers of tokens in the online retrieval process. To address
these issues, we introduce a novel graph-based RAG approach, called Attributed
Community-based Hierarchical RAG (ArchRAG), by augmenting the question using
attributed communities, and also introducing a novel LLM-based hierarchical
clustering method. To retrieve the most relevant information from the graph for
the question, we build a novel hierarchical index structure for the attributed
communities and develop an effective online retrieval method. Experimental
results demonstrate that ArchRAG outperforms existing methods in terms of both
accuracy and token cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Large Recommendation Model: Towards a Resource-Optimal
  Scaling Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songpei Xu, Shijia Wang, Da Guo, Xianwen Guo, Qiang Xiao, Fangjian Li, Chuanjiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of scaling up recommendation models confronts intrinsic tensions
between expanding model capacity and preserving computational tractability.
While prior studies have explored scaling laws for recommendation systems,
their resource-intensive paradigms -- often requiring tens of thousands of A100
GPU hours -- remain impractical for most industrial applications. This work
addresses a critical gap: achieving sustainable model scaling under strict
computational budgets. We propose Climber, a resource-efficient recommendation
framework comprising two synergistic components: the ASTRO model architecture
for algorithmic innovation and the TURBO acceleration framework for engineering
optimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts
two core innovations: (1) multi-scale sequence partitioning that reduces
attention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,
enabling more efficient scaling with sequence length; (2) dynamic temperature
modulation that adaptively adjusts attention scores for multimodal
distributions arising from inherent multi-scenario and multi-behavior
interactions. Complemented by TURBO (Two-stage Unified Ranking with Batched
Output), a co-designed acceleration framework integrating gradient-aware
feature compression and memory-efficient Key-Value caching, Climber achieves
5.15x throughput gains without performance degradation. Comprehensive offline
experiments on multiple datasets validate that Climber exhibits a more ideal
scaling curve. To our knowledge, this is the first publicly documented
framework where controlled model scaling drives continuous online metric growth
(12.19% overall lift) without prohibitive resource costs. Climber has been
successfully deployed on Netease Cloud Music, one of China's largest music
streaming platforms, serving tens of millions of users daily.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResearchArena: Benchmarking Large Language Models' Ability to Collect
  and Organize Information as Research Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Kang, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel across many natural language processing
tasks but face challenges in domain-specific, analytical tasks such as
conducting research surveys. This study introduces ResearchArena, a benchmark
designed to evaluate LLMs' capabilities in conducting academic
surveys$\unicode{x2013}$a foundational step in academic research. ResearchArena
models the process in three stages: (1) information discovery, identifying
relevant literature; (2) information selection, evaluating papers' relevance
and impact; and (3) information organization, structuring knowledge into
hierarchical frameworks such as mind-maps. Notably, mind-map construction is
treated as a bonus task, reflecting its supplementary role in survey-writing.
To support these evaluations, we construct an offline environment of 12M
full-text academic papers and 7.9K survey papers. To ensure ethical compliance,
we do not redistribute copyrighted materials; instead, we provide code to
construct the environment from the Semantic Scholar Open Research Corpus
(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform
compared to simpler keyword-based retrieval methods, underscoring significant
opportunities for advancing LLMs in autonomous research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion
  Rate Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Cheng, Yucheng Lu, Boyang Xia, Jiangxia Cao, Kuan Xu, Mingxing Wen, Wei Jiang, Jiaming Zhang, Zhaojie Liu, Liyin Hong, Kun Gai, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-click conversion rate (CVR) estimation is a vital task in many
recommender systems of revenue businesses, e.g., e-commerce and advertising. In
a perspective of sample, a typical CVR positive sample usually goes through a
funnel of exposure to click to conversion. For lack of post-event labels for
un-clicked samples, CVR learning task commonly only utilizes clicked samples,
rather than all exposed samples as for click-through rate (CTR) learning task.
However, during online inference, CVR and CTR are estimated on the same assumed
exposure space, which leads to a inconsistency of sample space between training
and inference, i.e., sample selection bias (SSB). To alleviate SSB, previous
wisdom proposes to design novel auxiliary tasks to enable the CVR learning on
un-click training samples, such as CTCVR and counterfactual CVR, etc. Although
alleviating SSB to some extent, none of them pay attention to the
discrimination between ambiguous negative samples (un-clicked) and factual
negative samples (clicked but un-converted) during modelling, which makes CVR
model lacks robustness. To full this gap, we propose a novel ChorusCVR model to
realize debiased CVR learning in entire-space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span>-based Retrieval Augmented Generation for Dynamic Few-shot Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Haoyang Li, Fei Teng, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span> Foundation Models for Recommendation: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) serve as a fundamental tool for navigating the vast
expanse of online information, with deep learning advancements playing an
increasingly important role in improving ranking accuracy. Among these, graph
neural networks (GNNs) excel at extracting higher-order structural information,
while large language models (LLMs) are designed to process and comprehend
natural language, making both approaches highly effective and widely adopted.
Recent research has focused on graph foundation models (GFMs), which integrate
the strengths of GNNs and LLMs to model complex RS problems more efficiently by
leveraging the graph-based structure of user-item relationships alongside
textual understanding. In this survey, we provide a comprehensive overview of
GFM-based RS technologies by introducing a clear taxonomy of current
approaches, diving into methodological details, and highlighting key challenges
and future directions. By synthesizing recent advancements, we aim to offer
valuable insights into the evolving landscape of GFM-based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal <span class="highlight-title">Dataset</span> Size for Recommender Systems: Evaluating Algorithms'
  Performance via Downsampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ardalan Arabzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis investigates dataset downsampling as a strategy to optimize
energy efficiency in recommender systems while maintaining competitive
performance. With increasing dataset sizes posing computational and
environmental challenges, this study explores the trade-offs between energy
efficiency and recommendation quality in Green Recommender Systems, which aim
to reduce environmental impact. By applying two downsampling approaches to
seven datasets, 12 algorithms, and two levels of core pruning, the research
demonstrates significant reductions in runtime and carbon emissions. For
example, a 30% downsampling portion can reduce runtime by 52% compared to the
full dataset, leading to a carbon emission reduction of up to 51.02 KgCO2e
during the training of a single algorithm on a single dataset. The analysis
reveals that algorithm performance under different downsampling portions
depends on factors like dataset characteristics, algorithm complexity, and the
specific downsampling configuration (scenario dependent). Some algorithms,
which showed lower nDCG@10 scores compared to higher-performing ones, exhibited
lower sensitivity to the amount of training data, offering greater potential
for efficiency in lower downsampling portions. On average, these algorithms
retained 81% of full-size performance using only 50% of the training set. In
certain downsampling configurations, where more users were progressively
included while keeping the test set size fixed, they even showed higher nDCG@10
scores than when using the full dataset. These findings highlight the
feasibility of balancing sustainability and effectiveness, providing insights
for designing energy-efficient recommender systems and promoting sustainable AI
practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Middleman Bias in Advertising: Aligning Relevance of Keyphrase
  Recommendations with Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumik Dey, Wei Zhang, Hansi Wu, Bingfeng Dong, Binbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce sellers are recommended keyphrases based on their inventory on
which they advertise to increase buyer engagement (clicks/sales). Keyphrases
must be pertinent to items; otherwise, it can result in seller dissatisfaction
and poor targeting -- towards that end relevance filters are employed. In this
work, we describe the shortcomings of training relevance filter models on
biased click/sales signals. We re-conceptualize advertiser keyphrase relevance
as interaction between two dynamical systems -- Advertising which produces the
keyphrases and Search which acts as a middleman to reach buyers. We discuss the
bias of search relevance systems (middleman bias) and the need to align
advertiser keyphrases with search relevance signals. We also compare the
performance of cross encoders and bi-encoders in modeling this alignment and
the scalability of such a solution for sellers at eBay.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accuracy and Political Bias of News Source Credibility Ratings by Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00228v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00228v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai-Cheng Yang, Filippo Menczer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engines increasingly leverage large language models (LLMs) to generate
direct answers, and AI chatbots now access the Internet for fresh data. As
information curators for billions of users, LLMs must assess the accuracy and
reliability of different sources. This paper audits nine widely used LLMs from
three leading providers -- OpenAI, Google, and Meta -- to evaluate their
ability to discern credible and high-quality information sources from
low-credibility ones. We find that while LLMs can rate most tested news
outlets, larger models more frequently refuse to provide ratings due to
insufficient information, whereas smaller models are more prone to making
errors in their ratings. For sources where ratings are provided, LLMs exhibit a
high level of agreement among themselves (average Spearman's $\rho = 0.79$),
but their ratings align only moderately with human expert evaluations (average
$\rho = 0.50$). Analyzing news sources with different political leanings in the
US, we observe a liberal bias in credibility ratings yielded by all LLMs in
default configurations. Additionally, assigning partisan roles to LLMs
consistently induces strong politically congruent bias in their ratings. These
findings have important implications for the use of LLMs in curating news and
political information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG
  Capabilities <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14482v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14482v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K
context window, designed to bridge the gap between open-source LLMs and leading
proprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context understanding
and retrieval-augmented generation (RAG) capabilities. These two capabilities
are complementary to each other and essential for LLMs to process large volumes
of information that cannot fit into a single prompt. We present a detailed
continued training recipe to extend the context window of Llama3-70B-base from
8K to 128K tokens, along with a three-stage instruction tuning process to
enhance the model's instruction-following, RAG performance, and long-context
understanding capabilities. Our results demonstrate that the
Llama3-ChatQA-2-70B model outperforms most existing state-of-the-art models,
including GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and
Llama3.1-70B-Instruct, on ultra-long tasks beyond 100K tokens, as well as on
the RAG benchmark using only a 4K context window, showing the strong long
context capability across varying sequence lengths. We further provide
extensive comparisons between direct long-context and RAG solutions using the
same state-of-the-art long-context LLMs. Interestingly, we find that the
performance of strong long-context LLMs using RAG improves when retrieving a
larger number of chunks. With a large set of top-k chunks, RAG consistently
outperforms direct long-context solution using the same state-of-the-art
long-context models (e.g., Llama3-ChatQA-2-70B and Qwen2-72B-Instruct) on both
32K and 128K benchmarks. We open-source the model weights, training data, and
the evaluation setup for the for the community:
https://chatqa2-project.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an efficient multi-level convolution architecture
for 3D visual grounding. Conventional methods are difficult to meet the
requirements of real-time inference due to the two-stage or point-based
architecture. Inspired by the success of multi-level fully sparse convolutional
architecture in 3D object detection, we aim to build a new 3D visual grounding
framework following this technical route. However, as in 3D visual grounding
task the 3D scene representation should be deeply interacted with text
features, sparse convolution-based architecture is inefficient for this
interaction due to the large amount of voxel features. To this end, we propose
text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D
scene representation and text features in an efficient way by gradual region
pruning and target completion. Specifically, TGP iteratively sparsifies the 3D
scene representation and thus efficiently interacts the voxel features with
text features by cross-attention. To mitigate the affect of pruning on delicate
geometric information, CBA adaptively fixes the over-pruned region by voxel
completion with negligible computational overhead. Compared with previous
single-stage methods, our method achieves top inference speed and surpasses
previous fastest method by 100\% FPS. Our method also achieves state-of-the-art
accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on
ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code
is available at
\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (How) Can <span class="highlight-title">Transformer</span>s Predict Pseudo-Random Numbers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tao, Darshil Doshi, Dayal Singh Kalra, Tianyu He, Maissam Barkeshli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers excel at discovering patterns in sequential data, yet their
fundamental limitations and learning mechanisms remain crucial topics of
investigation. In this paper, we study the ability of Transformers to learn
pseudo-random number sequences from linear congruential generators (LCGs),
defined by the recurrence relation $x_{t+1} = a x_t + c \;\mathrm{mod}\; m$.
Our analysis reveals that with sufficient architectural capacity and training
data variety, Transformers can perform in-context prediction of LCG sequences
with unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding
layers and attention patterns, we uncover how Transformers develop algorithmic
structures to learn these sequences in two scenarios of increasing complexity.
First, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but
fixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our
analysis reveals that models learn to factorize the modulus and utilize
digit-wise number representations to make sequential predictions. In the
second, more challenging scenario of unseen moduli, we show that Transformers
can generalize to unseen moduli up to $m_{\text{test}} = 2^{16}$. In this case,
the model employs a two-step strategy: first estimating the unknown modulus
from the context, then utilizing prime factorizations to generate predictions.
For this task, we observe a sharp transition in the accuracy at a critical
depth $=3$. We also find that the number of in-context sequence elements needed
to reach high accuracy scales sublinearly with the modulus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10+16 pages, 12+20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing the Scales: A Theoretical and Algorithmic Framework for
  Learning from Imbalanced Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Corinna Cortes, Anqi Mao, Mehryar Mohri, Yutao Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance remains a major challenge in machine learning, especially in
multi-class problems with long-tailed distributions. Existing methods, such as
data resampling, cost-sensitive techniques, and logistic loss modifications,
though popular and often effective, lack solid theoretical foundations. As an
example, we demonstrate that cost-sensitive methods are not Bayes consistent.
This paper introduces a novel theoretical framework for analyzing
generalization in imbalanced classification. We propose a new class-imbalanced
margin loss function for both binary and multi-class settings, prove its strong
$H$-consistency, and derive corresponding learning guarantees based on
empirical loss and a new notion of class-sensitive Rademacher complexity.
Leveraging these theoretical results, we devise novel and general learning
algorithms, IMMAX (Imbalanced Margin Maximization), which incorporate
confidence margins and are applicable to various hypothesis sets. While our
focus is theoretical, we also present extensive empirical results demonstrating
the effectiveness of our algorithms compared to existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OWLS: Scaling Laws for Multilingual Speech Recognition and Translation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Huck Yang, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural scaling laws offer valuable insights for designing robust sequence
processing architectures. While these laws have been extensively characterized
in other modalities, their behavior in speech remains comparatively
underexplored. In this work, we introduce OWLS, an open-access, reproducible
suite of multilingual speech recognition and translation models spanning 0.25B
to 18B parameters, with the 18B version being the largest speech model, to the
best of our knowledge. OWLS leverages up to 360K hours of public speech data
across 150 languages, enabling a systematic investigation into how data, model,
and compute scaling each influence performance in multilingual speech tasks. We
use OWLS to derive neural scaling laws, showing how final performance can be
reliably predicted when scaling. One of our key findings is that scaling
enhances performance on low-resource languages/dialects, helping to mitigate
bias and improve the accessibility of speech technologies. Finally, we show how
OWLS can be used to power new research directions by discovering emergent
abilities in large-scale speech models. Model checkpoints will be released on
https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d
for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AffinityFlow: Guided Flows for Antibody Affinity Maturation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Chen, Karla-Luise Herpoldt, Chenchao Zhao, Zichen Wang, Marcus Collins, Shang Shang, Ron Benson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are widely used as therapeutics, but their development requires
costly affinity maturation, involving iterative mutations to enhance binding
affinity.This paper explores a sequence-only scenario for affinity maturation,
using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold
within flow matching to generate diverse protein structures, enabling a
sequence-conditioned generative model of structure. Building on this, we
propose an alternating optimization framework that (1) fixes the sequence to
guide structure generation toward high binding affinity using a structure-based
affinity predictor, then (2) applies inverse folding to create sequence
mutations, refined by a sequence-based affinity predictor for post selection.
To address this, we develop a co-teaching module that incorporates valuable
information from noisy biophysical energies into predictor refinement. The
sequence-based predictor selects consensus samples to teach the structure-based
predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art
performance in affinity maturation experiments. We plan to open-source our code
after acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Wang, Zirui Wang, Junli Ren, Qingwei Ben, Tao Huang, Weinan Zhang, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traversing risky terrains with sparse footholds poses a significant challenge
for humanoid robots, requiring precise foot placements and stable locomotion.
Existing approaches designed for quadrupedal robots often fail to generalize to
humanoid robots due to differences in foot geometry and unstable morphology,
while learning-based approaches for humanoid locomotion still face great
challenges on complex terrains due to sparse foothold reward signals and
inefficient learning processes. To address these challenges, we introduce
BeamDojo, a reinforcement learning (RL) framework designed for enabling agile
humanoid locomotion on sparse footholds. BeamDojo begins by introducing a
sampling-based foothold reward tailored for polygonal feet, along with a double
critic to balancing the learning process between dense locomotion rewards and
sparse foothold rewards. To encourage sufficient trail-and-error exploration,
BeamDojo incorporates a two-stage RL approach: the first stage relaxes the
terrain dynamics by training the humanoid on flat terrain while providing it
with task terrain perceptive observations, and the second stage fine-tunes the
policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based
elevation map to enable real-world deployment. Extensive simulation and
real-world experiments demonstrate that BeamDojo achieves efficient learning in
simulation and enables agile locomotion with precise foot placement on sparse
footholds in the real world, maintaining a high success rate even under
significant external disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://why618188.github.io/beamdojo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multilingual LLM Pretraining with Model-Based Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bettina Messmer, Vinko Sabolčec, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset curation has become a basis for strong large language model (LLM)
performance. While various rule-based filtering heuristics exist for English
and multilingual datasets, model-based filtering techniques have primarily
focused on English. To address the disparity stemming from limited research on
non-English languages, we propose a model-based filtering framework for
multilingual datasets that aims to identify a diverse set of structured and
knowledge-rich samples. Our approach emphasizes transparency, simplicity, and
efficiency, leveraging Transformer- and FastText-based classifiers to ensure
the broad accessibility of our technique and data. We conduct comprehensive
ablation studies on the FineWeb-2 web crawl dataset across diverse language
families, scripts, and resource availability to demonstrate the effectiveness
of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our
approach can match the baseline MMLU score with as little as 15% of the
training tokens, while also improving across other benchmarks. These findings
provide strong evidence for the generalizability of our approach to other
languages. As a result, we extend our framework to 20 languages for which we
release the refined pretraining datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proper Learnability and the Role of Unlabeled Data <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proper learning refers to the setting in which learners must emit predictors
in the underlying hypothesis class $H$, and often leads to learners with simple
algorithmic forms (e.g. empirical risk minimization (ERM), structural risk
minimization (SRM)). The limitation of proper learning, however, is that there
exist problems which can only be learned improperly, e.g. in multiclass
classification. Thus, we ask: Under what assumptions on the hypothesis class or
the information provided to the learner is a problem properly learnable? We
first demonstrate that when the unlabeled data distribution is given, there
always exists an optimal proper learner governed by distributional
regularization, a randomized generalization of regularization. We refer to this
setting as the distribution-fixed PAC model, and continue to evaluate the
learner on its worst-case performance over all distributions. Our result holds
for all metric loss functions and any finite learning problem (with no
dependence on its size). Further, we demonstrate that sample complexities in
the distribution-fixed PAC model can shrink by only a logarithmic factor from
the classic PAC model, strongly refuting the role of unlabeled data in PAC
learning (from a worst-case perspective).
  We complement this with impossibility results which obstruct any
characterization of proper learnability in the realizable PAC model. First, we
observe that there are problems whose proper learnability is logically
undecidable, i.e., independent of the ZFC axioms. We then show that proper
learnability is not a monotone property of the underlying hypothesis class, and
that it is not a local property (in a precise sense). Our impossibility results
all hold even for the fundamental setting of multiclass classification, and go
through a reduction of EMX learning (Ben-David et al., 2019) to proper
classification which may be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALT 2025, 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Euler Factors of Elliptic Curves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelica Babei, François Charton, Edgar Costa, Xiaoyu Huang, Kyu-Hwan Lee, David Lowry-Duda, Ashvni Narayanan, Alexey Pozdnyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply transformer models and feedforward neural networks to predict
Frobenius traces $a_p$ from elliptic curves given other traces $a_q$. We train
further models to predict $a_p \bmod 2$ from $a_q \bmod 2$, and cross-analysis
such as $a_p \bmod 2$ from $a_q$. Our experiments reveal that these models
achieve high accuracy, even in the absence of explicit number-theoretic tools
like functional equations of $L$-functions. We also present partial
interpretability findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimension-free Score Matching and Time Bootstrapping for Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syamantak Kumar, Dheeraj Nagaraj, Purnamrita Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models generate samples by estimating the score function of the
target distribution at various noise levels. The model is trained using samples
drawn from the target distribution, progressively adding noise. In this work,
we establish the first (nearly) dimension-free sample complexity bounds for
learning these score functions, achieving a double exponential improvement in
dimension over prior results. A key aspect of our analysis is the use of a
single function approximator to jointly estimate scores across noise levels, a
critical feature of diffusion models in practice which enables generalization
across timesteps. Our analysis introduces a novel martingale-based error
decomposition and sharp variance bounds, enabling efficient learning from
dependent data generated by Markov processes, which may be of independent
interest. Building on these insights, we propose Bootstrapped Score Matching
(BSM), a variance reduction technique that utilizes previously learned scores
to improve accuracy at higher noise levels. These results provide crucial
insights into the efficiency and effectiveness of diffusion models for
generative modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assortment Optimization for Patient-Provider Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naveen Raman, Holly Wiberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rising provider turnover forces healthcare administrators to frequently
rematch patients to available providers, which can be cumbersome and
labor-intensive. To reduce the burden of rematching, we study algorithms for
matching patients and providers through assortment optimization. We develop a
patient-provider matching model in which we simultaneously offer each patient a
menu of providers, and patients subsequently respond and select providers. By
offering assortments upfront, administrators can balance logistical ease and
patient autonomy. We study policies for assortment optimization and
characterize their performance under different problem settings. We demonstrate
that the selection of assortment policy is highly dependent on problem
specifics and, in particular, on a patient's willingness to match and the ratio
between patients and providers. On real-world data, we show that our best
policy can improve match quality by 13% over a greedy solution by tailoring
assortment sizes based on patient characteristics. We conclude with
recommendations for running a real-world patient-provider matching system
inspired by our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAR: Spectral Truncation and Rescale for Model Merging <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Ang Lee, Ching-Yun Ko, Tejaswini Pedapati, I-Hsin Chung, Mi-Yen Yeh, Pin-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is an efficient way of obtaining a multi-task model from
several pretrained models without further fine-tuning, and it has gained
attention in various domains, including natural language processing (NLP).
Despite the efficiency, a key challenge in model merging is the seemingly
inevitable decrease in task performance as the number of models increases. In
this paper, we propose $\mathbf{S}$pectral $\mathbf{T}$runcation $\mathbf{A}$nd
$\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by
truncating small components in the respective spectral spaces, which is
followed by an automatic parameter rescaling scheme to retain the nuclear norm
of the original matrix. STAR requires no additional inference on original
training data and is robust to hyperparamater choice. We demonstrate the
effectiveness of STAR through extensive model merging cases on diverse NLP
tasks. Specifically, STAR works robustly across varying model sizes, and can
outperform baselines by 4.2$\%$ when merging 12 models on Flan-T5. Our code is
publicly available at https://github.com/IBM/STAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Studying number theory with deep learning: a case study with the
  Möbius and squarefree indicator functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Lowry-Duda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on work of Charton, we train small transformer models to calculate
the M\"obius function $\mu(n)$ and the squarefree indicator function
$\mu^2(n)$. The models attain nontrivial predictive power. We then iteratively
train additional models to understand how the model functions, ultimately
finding a theoretical explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfoPos: A ML-Assisted Solution Design Support Framework for Industrial
  Cyber-Physical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uraz Odyurt, Richard Loendersloot, Tiedo Tinga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variety of building blocks and algorithms incorporated in data-centric
and ML-assisted solutions is high, contributing to two challenges: selection of
most effective set and order of building blocks, as well as achieving such a
selection with minimum cost. Considering that ML-assisted solution design is
influenced by the extent of available data, as well as available knowledge of
the target system, it is advantageous to be able to select matching building
blocks. We introduce the first iteration of our InfoPos framework, allowing the
placement of use-cases considering the available positions (levels), i.e., from
poor to rich, of knowledge and data dimensions. With that input, designers and
developers can reveal the most effective corresponding choice(s), streamlining
the solution design process. The results from our demonstrator, an anomaly
identification use-case for industrial Cyber-Physical Systems, reflects
achieved effects upon the use of different building blocks throughout knowledge
and data positions. The achieved ML model performance is considered as the
indicator. Our data processing code and the composed data sets are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiOpt: Self-supervised Diffusion for Constrained Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Ding, Yimiao Zhou, Ke Hu, Xi Yao, Junchi Yan, Xiaoying Tang, Ye Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models show promising potential for
learning-based optimization by leveraging their multimodal sampling capability
to escape local optima. However, existing diffusion-based optimization
approaches, often reliant on supervised training, lacks a mechanism to ensure
strict constraint satisfaction which is often required in real-world
applications. One resulting observation is the distributional misalignment,
i.e. the generated solution distribution often exhibits small overlap with the
feasible domain. In this paper, we propose DiOpt, a novel diffusion paradigm
that systematically learns near-optimal feasible solution distributions through
iterative self-training. Our framework introduces several key innovations: a
target distribution specifically designed to maximize overlap with the
constrained solution manifold; a bootstrapped self-training mechanism that
adaptively weights candidate solutions based on the severity of constraint
violations and optimality gaps; and a dynamic memory buffer that accelerates
convergence by retaining high-quality solutions over training iterations. To
our knowledge, DiOpt represents the first successful integration of
self-supervised diffusion with hard constraint satisfaction. Evaluations on
diverse tasks, including power grid control, motion retargeting, wireless
allocation demonstrate its superiority in terms of both optimality and
constraint satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalised Parallel Tempering: Flexible Replica Exchange via Flows and
  Diffusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Zhang, Peter Potaptchik, Arnaud Doucet, Hai-Dang Dau, Saifuddin Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging
parallel computation to sample efficiently from high-dimensional, multimodal or
otherwise complex distributions via annealing. One limitation of the standard
formulation of PT is the growth of computational resources required to generate
high-quality samples, as measured by effective sample size or round trip rate,
for increasingly challenging distributions. To address this issue, we propose
the framework: Generalised Parallel Tempering (GePT) which allows for the
incorporation of recent advances in modern generative modelling, such as
normalising flows and diffusion models, within Parallel Tempering, while
maintaining the same theoretical guarantees as MCMC-based methods. For
instance, we show that this allows us to utilise diffusion models in a
parallelised manner, bypassing the usual computational cost of a large number
of steps to generate quality samples. Further, we empirically demonstrate that
GePT can improve sample quality and reduce the growth of computational
resources required to handle complex distributions over the classical
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process Reward Models for LLM Agents: Practical Framework and Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjiban Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Agent Process Reward Models (AgentPRM), a simple and scalable
framework for training LLM agents to continually improve through interactions.
AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo
rollouts to compute reward targets and optimize policies. It requires minimal
modifications to existing RLHF pipelines, making it easy to integrate at scale.
Beyond AgentPRM, we propose InversePRM, which learns process rewards directly
from demonstrations without explicit outcome supervision. We also explore key
challenges and opportunities, including exploration, process reward shaping,
and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that
small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o
baselines, and analyze test-time scaling, reward hacking, and more. Our code is
available at: https://github.com/sanjibanc/agent_prm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExplainReduce: Summarising local explanations via proxies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauri Seppäläinen, Mudong Guo, Kai Puolamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most commonly used non-linear machine learning methods are closed-box models,
uninterpretable to humans. The field of explainable artificial intelligence
(XAI) aims to develop tools to examine the inner workings of these closed
boxes. An often-used model-agnostic approach to XAI involves using simple
models as local approximations to produce so-called local explanations;
examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows
how a large set of local explanations can be reduced to a small "proxy set" of
simple models, which can act as a generative global explanation. This reduction
procedure, ExplainReduce, can be formulated as an optimisation problem and
approximated efficiently using greedy heuristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages with a 7 page appendix, 7 + 5 figures, 2 tables. The
  datasets and source code used in the paper are available at
  https://github.com/edahelsinki/explainreduce</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Powered Preference Elicitation in Combinatorial Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ermis Soumalias, Yanchen Jiang, Kehang Zhu, Michael Curry, Sven Seuken, David C. Parkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the potential of large language models (LLMs) as proxies for humans
to simplify preference elicitation (PE) in combinatorial assignment. While
traditional PE methods rely on iterative queries to capture preferences, LLMs
offer a one-shot alternative with reduced human effort. We propose a framework
for LLM proxies that can work in tandem with SOTA ML-powered preference
elicitation schemes. Our framework handles the novel challenges introduced by
LLMs, such as response variability and increased computational costs. We
experimentally evaluate the efficiency of LLM proxies against human queries in
the well-studied course allocation domain, and we investigate the model
capabilities required for success. We find that our approach improves
allocative efficiency by up to 20%, and these results are robust across
different LLMs and to differences in quality and accuracy of reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer
  learning using Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Mishra, Ravindra T, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional solar forecasting models are based on several years of
site-specific historical irradiance data, often spanning five or more years,
which are unavailable for newer photovoltaic farms. As renewable energy is
highly intermittent, building accurate solar irradiance forecasting systems is
essential for efficient grid management and enabling the ongoing proliferation
of solar energy, which is crucial to achieve the United Nations' net zero
goals. In this work, we propose SPIRIT, a novel approach leveraging foundation
models for solar irradiance forecasting, making it applicable to newer solar
installations. Our approach outperforms state-of-the-art models in zero-shot
transfer learning by about 70%, enabling effective performance at new locations
without relying on any historical data. Further improvements in performance are
achieved through fine-tuning, as more location-specific data becomes available.
These findings are supported by statistical significance, further validating
our approach. SPIRIT represents a pivotal step towards rapid, scalable, and
adaptable solar forecasting solutions, advancing the integration of renewable
energy into global power systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeltaProduct: Increasing the Expressivity of DeltaNet Through Products
  of Householders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive
alternatives to Transformers for sequence modeling, offering efficient training
and linear-time inference. However, existing architectures face a fundamental
trade-off between expressivity and efficiency, dictated by the structure of
their state-transition matrices. While diagonal matrices used in architectures
like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited
expressivity. To address this, recent architectures such as (Gated) DeltaNet
and RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous
token-channel mixing, which overcomes some expressivity limitations with only a
slight decrease in training efficiency. Building on the interpretation of
DeltaNet's recurrence as performing one step of online gradient descent per
token on an associative recall loss, we introduce DeltaProduct, which instead
takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus
rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized
Householder transformations, providing a tunable mechanism to balance
expressivity and efficiency and a stable recurrence. Through extensive
experiments, we demonstrate that DeltaProduct achieves superior state-tracking
and language modeling capabilities while exhibiting significantly improved
length extrapolation compared to DeltaNet. Additionally, we also strengthen the
theoretical foundation of DeltaNet's expressivity by proving that it can solve
dihedral group word problems in just two layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fenchel-Young Variational Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Sklaviadis, Sweta Agrawal, Antonio Farinhas, Andre Martins, Mario Figueiredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From a variational perspective, many statistical learning criteria involve
seeking a distribution that balances empirical risk and regularization. In this
paper, we broaden this perspective by introducing a new general class of
variational methods based on Fenchel-Young (FY) losses, treated as divergences
that generalize (and encompass) the familiar Kullback-Leibler divergence at the
core of classical variational learning. Our proposed formulation -- FY
variational learning -- includes as key ingredients new notions of FY free
energy, FY evidence, FY evidence lower bound, and FY posterior. We derive
alternating minimization and gradient backpropagation algorithms to compute (or
lower bound) the FY evidence, which enables learning a wider class of models
than previous variational formulations. This leads to generalized FY variants
of classical algorithms, such as an FY expectation-maximization (FYEM)
algorithm, and latent-variable models, such as an FY variational autoencoder
(FYVAE). Our new methods are shown to be empirically competitive, often
outperforming their classical counterparts, and most importantly, to have
qualitatively novel features. For example, FYEM has an adaptively sparse
E-step, while the FYVAE can support models with sparse observations and sparse
posteriors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Loss Bounds for Online Learning Separated Function Classes: A
  Gaussian Process Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Block, Abhishek Shetty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to develop practical and efficient algorithms while circumventing
overly pessimistic computational lower bounds, recent work has been interested
in developing oracle-efficient algorithms in a variety of learning settings.
Two such settings of particular interest are online and differentially private
learning. While seemingly different, these two fields are fundamentally
connected by the requirement that successful algorithms in each case satisfy
stability guarantees; in particular, recent work has demonstrated that
algorithms for online learning whose performance adapts to beneficial problem
instances, attaining the so-called small-loss bounds, require a form of
stability similar to that of differential privacy. In this work, we identify
the crucial role that separation plays in allowing oracle-efficient algorithms
to achieve this strong stability. Our notion, which we term $\rho$-separation,
generalizes and unifies several previous approaches to enforcing this strong
stability, including the existence of small-separator sets and the recent
notion of $\gamma$-approximability. We present an oracle-efficient algorithm
that is capable of achieving small-loss bounds with improved rates in greater
generality than previous work, as well as a variant for differentially private
learning that attains optimal rates, again under our separation condition. In
so doing, we prove a new stability result for minimizers of a Gaussian process
that strengthens and generalizes previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Mixup Unlearning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Peng, Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is a critical area of research aimed at safeguarding data
privacy by enabling the removal of sensitive information from machine learning
models. One unique challenge in this field is catastrophic unlearning, where
erasing specific data from a well-trained model unintentionally removes
essential knowledge, causing the model to deviate significantly from a
retrained one. To address this, we introduce a novel approach that regularizes
the unlearning process by utilizing synthesized mixup samples, which simulate
the data susceptible to catastrophic effects. At the core of our approach is a
generator-unlearner framework, MixUnlearn, where a generator adversarially
produces challenging mixup examples, and the unlearner effectively forgets
target information based on these synthesized data. Specifically, we first
introduce a novel contrastive objective to train the generator in an
adversarial direction: generating examples that prompt the unlearner to reveal
information that should be forgotten, while losing essential knowledge. Then
the unlearner, guided by two other contrastive loss terms, processes the
synthesized and real data jointly to ensure accurate unlearning without losing
critical knowledge, overcoming catastrophic effects. Extensive evaluations
across benchmark datasets demonstrate that our method significantly outperforms
state-of-the-art approaches, offering a robust solution to machine unlearning.
This work not only deepens understanding of unlearning mechanisms but also lays
the foundation for effective machine unlearning with mixup augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Super-Resolution for High-Fidelity Physical System
  Simulations with Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Zhang, Connor Duffin, Alex Glyn-Davies, Arnaud Vadeboncoeur, Mark Girolami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Super-resolution (SR) is a promising tool for generating high-fidelity
simulations of physical systems from low-resolution data, enabling fast and
accurate predictions in engineering applications. However, existing
deep-learning based SR methods, require large labeled datasets and lack
reliable uncertainty quantification (UQ), limiting their applicability in
real-world scenarios. To overcome these challenges, we propose a probabilistic
SR framework that leverages the Statistical Finite Element Method and
energy-based generative modeling. Our method enables efficient high-resolution
predictions with inherent UQ, while eliminating the need for extensive labeled
datasets. The method is validated on a 2D Poisson example and compared with
bicubic interpolation upscaling. Results demonstrate a computational speed-up
over high-resolution numerical solvers while providing reliable uncertainty
estimates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Synthetic Data for Monitoring <span class="highlight-title">Dataset</span> Mentions
  in Research Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aivin V. Solatorio, Rafael Macalaba, James Liounis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking how data is mentioned and used in research papers provides critical
insights for improving data discoverability, quality, and production. However,
manually identifying and classifying dataset mentions across vast academic
literature is resource-intensive and not scalable. This paper presents a
machine learning framework that automates dataset mention detection across
research domains by leveraging large language models (LLMs), synthetic data,
and a two-stage fine-tuning process. We employ zero-shot extraction from
research papers, an LLM-as-a-Judge for quality assessment, and a reasoning
agent for refinement to generate a weakly supervised synthetic dataset. The
Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by
fine-tuning on a manually annotated subset. At inference, a ModernBERT-based
classifier efficiently filters dataset mentions, reducing computational
overhead while maintaining high recall. Evaluated on a held-out manually
annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and
GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how
LLM-generated synthetic data can effectively address training data scarcity,
improving generalization in low-resource settings. This framework offers a
pathway toward scalable monitoring of dataset usage, enhancing transparency,
and supporting researchers, funders, and policymakers in identifying data gaps
and strengthening data accessibility for informed decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project GitHub repository at https://github.com/worldbank/ai4data-use</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Zero-Order Federated Finetuning of Language Models for
  Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Aboelenien Ahmed, Kilian Pfeiffer, Ramin Khalili, Heba Khdr, Jörg Henkel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated fine-tuning offers a promising approach for tuning Large Language
Models (LLMs) on edge devices while preserving data privacy. However,
fine-tuning these models on edge devices remains challenging due to high
memory, communication, and computational demands. Zero-order optimization with
task alignment provides a potential solution, enabling fine-tuning with
inference-level memory requirements but requires a longer convergence time. In
this paper, we propose Federated Split-Perturbation Zero-order Optimization
(FedSPZO) that divides the network into two blocks, applying a different number
of perturbations per block in a computationally effective way, achieving faster
convergence. Our evaluation shows a $2.5 - 7\times $ reduction in computation
overhead compared to zero-order state of the art techniques in federated
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, <span class="highlight-author">Yoshua Bengio</span>, Luca Scimeca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models (DPMs) are powerful generative models that
have achieved unparalleled success in a number of generative tasks. In this
work, we aim to build inductive biases into the training and sampling of
diffusion models to better accommodate the target distribution of the data to
model. For topologically structured data, we devise a frequency-based noising
operator to purposefully manipulate, and set, these inductive biases. We first
show that appropriate manipulations of the noising forward process can lead
DPMs to focus on particular aspects of the distribution to learn. We show that
different datasets necessitate different inductive biases, and that appropriate
frequency-based noise control induces increased generative performance compared
to standard diffusion. Finally, we demonstrate the possibility of ignoring
information at particular frequencies while learning. We show this in an image
corruption and recovery task, where we train a DPM to recover the original
target distribution after severe noise corruption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaPTS: Adapting Univariate Foundation Models to Probabilistic
  Multivariate Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelhakim Benechehab, Vasilii Feofanov, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Balázs Kégl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained foundation models (FMs) have shown exceptional performance in
univariate time series forecasting tasks. However, several practical challenges
persist, including managing intricate dependencies among features and
quantifying uncertainty in predictions. This study aims to tackle these
critical limitations by introducing adapters; feature-space transformations
that facilitate the effective use of pre-trained univariate time series FMs for
multivariate tasks. Adapters operate by projecting multivariate inputs into a
suitable latent space and applying the FM independently to each dimension.
Inspired by the literature on representation learning and partially stochastic
Bayesian neural networks, we present a range of adapters and
optimization/inference strategies. Experiments conducted on both synthetic and
real-world datasets confirm the efficacy of adapters, demonstrating substantial
enhancements in forecasting accuracy and uncertainty quantification compared to
baseline methods. Our framework, AdaPTS, positions adapters as a modular,
scalable, and effective solution for leveraging time series FMs in multivariate
contexts, thereby promoting their wider adoption in real-world applications. We
release the code at https://github.com/abenechehab/AdaPTS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via
  Hierarchical and Parallel Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurin Luttmann, Lin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge
in warehouse logistics, where pickers must navigate a mixed-shelves environment
to retrieve SKUs efficiently. Traditional heuristics and optimization-based
approaches struggle with scalability, while recent machine learning methods
often rely on sequential decision-making, leading to high solution latency and
suboptimal agent coordination. In this work, we propose a novel hierarchical
and parallel decoding approach for solving the min-max variant of the MSPRP via
multi-agent reinforcement learning. While our approach generates a joint
distribution over agent actions, allowing for fast decoding and effective
picker coordination, our method introduces a sequential action selection to
avoid conflicts in the multi-dimensional action space. Experiments show
state-of-the-art performance in both solution quality and inference speed,
particularly for large-scale and out-of-distribution instances. Our code is
publicly available at http://github.com/LTluttmann/marl4msprp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProReco: A Process Discovery Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsung-Hao Huang, Tarek Junied, Marco Pegoraro, Wil M. P. van der Aalst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process discovery aims to automatically derive process models from historical
execution data (event logs). While various process discovery algorithms have
been proposed in the last 25 years, there is no consensus on a dominating
discovery algorithm. Selecting the most suitable discovery algorithm remains a
challenge due to competing quality measures and diverse user requirements.
Manually selecting the most suitable process discovery algorithm from a range
of options for a given event log is a time-consuming and error-prone task. This
paper introduces ProReco, a Process discovery Recommender system designed to
recommend the most appropriate algorithm based on user preferences and event
log characteristics. ProReco incorporates state-of-the-art discovery
algorithms, extends the feature pools from previous work, and utilizes
eXplainable AI (XAI) techniques to provide explanations for its
recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 9 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forget the Data and Fine-Tuning! Just Fold the Network to Compress <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Wang, Haris Šikić, Lothar Thiele, Olga Saukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce model folding, a novel data-free model compression technique
that merges structurally similar neurons across layers, significantly reducing
the model size without the need for fine-tuning or access to training data.
Unlike existing methods, model folding preserves data statistics during
compression by leveraging k-means clustering, and using novel data-free
techniques to prevent variance collapse or explosion. Our theoretical framework
and experiments across standard benchmarks, including ResNet18 and LLaMA-7B,
demonstrate that model folding achieves comparable performance to data-driven
compression techniques and outperforms recently proposed data-free methods,
especially at high sparsity levels. This approach is particularly effective for
compressing large-scale models, making it suitable for deployment in
resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by The Thirteenth International
  Conference on Learning Representations(ICLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Reason Causally Like Us? Even Better? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna M. Dettki, Brenden M. Lake, Charley M. Wu, Bob Rehder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning is a core component of intelligence. Large language models
(LLMs) have shown impressive capabilities in generating human-like text,
raising questions about whether their responses reflect true understanding or
statistical patterns. We compared causal reasoning in humans and four LLMs
using tasks based on collider graphs, rating the likelihood of a query variable
occurring given evidence from other variables. We find that LLMs reason
causally along a spectrum from human-like to normative inference, with
alignment shifting based on model, context, and task. Overall, GPT-4o and
Claude showed the most normative behavior, including "explaining away", whereas
Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected
independence of causes - Claude the least - they exhibited strong associative
reasoning and predictive inference when assessing the likelihood of the effect
given its causes. These findings underscore the need to assess AI biases as
they increasingly assist human decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping bathymetry of inland water bodies on the North Slope of Alaska
  with Landsat using Random Forest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark L. Carroll, Margaret R. Wooten, Claire E. Simpson, Caleb S. Spradlin, Melanie J. Frost, Mariana Blanco-Rojas, Zachary W. Williams, Jordan A. Caraballo-Vega, Christopher S. R. Neigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The North Slope of Alaska is dominated by small waterbodies that provide
critical ecosystem services for local population and wildlife. Detailed
information on the depth of the waterbodies is scarce due to the challenges
with collecting such information. In this work we have trained a machine
learning (Random Forest Regressor) model to predict depth from multispectral
Landsat data in waterbodies across the North Slope of Alaska. The greatest
challenge is the scarcity of in situ data, which is expensive and difficult to
obtain, to train the model. We overcame this challenge by using modeled depth
predictions from a prior study as synthetic training data to provide a more
diverse training data pool for the Random Forest. The final Random Forest model
was more robust than models trained directly on the in situ data and when
applied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall
$r^{2}$ value of 0.76 on validation. The final map has been made available
through the Oak Ridge National Laboratory Distribute Active Archive Center
(ORNL-DAAC). This map represents a first of its kind regional assessment of
waterbody depth with per pixel estimates of depth for the entire North Slope of
Alaska.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 Pages, 6 Figures, 1 Table. This article is a US Government work.
  Landsat data from the US Geological Survey Earth Explorer system:
  https://earthexplorer.usgs.gov. Sonar training measurements:
  https://doi.org/10.18739/A2JD4PP1H. Output maps from the Oak Ridge National
  Laboratory Distribute Active Archive Center (ORNL-DAAC):
  https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=2243</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-flow anomaly detection by process mining-based feature
  extraction and dimensionality reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Vitale, Marco Pegoraro, Wil M. P. van der Aalst, Nicola Mazzocca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The business processes of organizations may deviate from normal control flow
due to disruptive anomalies, including unknown, skipped, and wrongly-ordered
activities. To identify these control-flow anomalies, process mining can check
control-flow correctness against a reference process model through conformance
checking, an explainable set of algorithms that allows linking any deviations
with model elements. However, the effectiveness of conformance checking-based
techniques is negatively affected by noisy event data and low-quality process
models. To address these shortcomings and support the development of
competitive and explainable conformance checking-based techniques for
control-flow anomaly detection, we propose a novel process mining-based feature
extraction approach with alignment-based conformance checking. This variant
aligns the deviating control flow with a reference process model; the resulting
alignment can be inspected to extract additional statistics such as the number
of times a given activity caused mismatches. We integrate this approach into a
flexible and explainable framework for developing techniques for control-flow
anomaly detection. The framework combines process mining-based feature
extraction and dimensionality reduction to handle high-dimensional feature
sets, achieve detection effectiveness, and support explainability. The results
show that the framework techniques implementing our approach outperform the
baseline conformance checking-based techniques while maintaining the
explainable nature of conformance checking. We also provide an explanation of
why existing conformance checking-based techniques may be ineffective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 7 tables, 56 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGS-<span class="highlight-title">GNN</span>: A Supervised <span class="highlight-title">Graph</span> Sparsification method for <span class="highlight-title">Graph</span> Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha Shankar Das, Naheed Anjum Arafat, Muftiqur Rahman, S M Ferdous, Alex Pothen, Mahantesh M Halappanavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SGS-GNN, a novel supervised graph sparsifier that learns the
sampling probability distribution of edges and samples sparse subgraphs of a
user-specified size to reduce the computational costs required by GNNs for
inference tasks on large graphs. SGS-GNN employs regularizers in the loss
function to enhance homophily in sparse subgraphs, boosting the accuracy of
GNNs on heterophilic graphs, where a significant number of the neighbors of a
node have dissimilar labels. SGS-GNN also supports conditional updates of the
probability distribution learning module based on a prior, which helps narrow
the search space for sparse graphs. SGS-GNN requires fewer epochs to obtain
high accuracies since it learns the search space of subgraphs more effectively
than methods using fixed distributions such as random sampling. Extensive
experiments using 33 homophilic and heterophilic graphs demonstrate the
following: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN
improves the F1-scores by a geometric mean of 4% relative to the original
graph; on heterophilic graphs, the prediction accuracy is better up to 30%.
(ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores
of 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and
(iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires
about half the number of epochs to converge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking around you: external information enhances representations for
  event sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Kovaleva, Petr Sokerin, Sofia Krehova, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning produces models in different domains, such as store
purchases, client transactions, and general people's behaviour. However, such
models for sequential data usually process a single sequence, ignoring context
from other relevant ones, even in domains with rapidly changing external
environments like finance or misguiding the prediction for a user with no
recent events.
  We are the first to propose a method that aggregates information from
multiple user representations augmenting a specific user one for a scenario of
multiple co-occurring event sequences. Our study considers diverse aggregation
approaches, ranging from simple pooling techniques to trainable attention-based
approaches, especially Kernel attention aggregation, that can highlight more
complex information flow from other users. The proposed method operates atop an
existing encoder and supports its efficient fine-tuning. Across considered
datasets of financial transactions and downstream tasks, Kernel attention
improves ROC AUC scores, both with and without fine-tuning, while mean pooling
yields a smaller but still significant gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-in-the-Loop Sensing and Communication Joint Design for Edge
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Cai, Xiaowen Cao, Xu Chen, Yuanhao Cui, Guangxu Zhu, Kaibin Huang, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in artificial intelligence (AI), wireless
communications, and sensing technologies have accelerated the evolution of edge
intelligence. However, conventional systems still grapple with issues such as
low communication efficiency, redundant data acquisition, and poor model
generalization. To overcome these challenges, we propose an innovative
framework that enhances edge intelligence through AI-in-the-loop joint sensing
and communication (JSAC). This framework features an AI-driven closed-loop
control architecture that jointly optimizes system resources, thereby
delivering superior system-level performance. A key contribution of our work is
establishing an explicit relationship between validation loss and the system's
tunable parameters. This insight enables dynamic reduction of the
generalization error through AI-driven closed-loop control. Specifically, for
sensing control, we introduce an adaptive data collection strategy based on
gradient importance sampling, allowing edge devices to autonomously decide when
to terminate data acquisition and how to allocate sample weights based on
real-time model feedback. For communication control, drawing inspiration from
stochastic gradient Langevin dynamics (SGLD), our joint optimization of
transmission power and batch size converts channel and data noise into gradient
perturbations that help mitigate overfitting. Experimental evaluations
demonstrate that our framework reduces communication energy consumption by up
to 77 percent and sensing costs measured by the number of collected samples by
up to 52 percent while significantly improving model generalization -- with up
to 58 percent reductions of the final validation loss. It validates that the
proposed scheme can harvest the mutual benefit of AI and JSAC systems by
incorporating the model itself into the control loop of the system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Reinforcement Learning for Actors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katsunari Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly
controls system dynamics, instead of the actor (action-generating neural
network) outputs at each moment, bringing about a major qualitative shift in
reinforcement learning (RL) from static to dynamic. The actor is initially
designed to generate chaotic dynamics through the loop with its environment,
enabling the agent to perform flexible and deterministic exploration. Dynamic
RL controls global system dynamics using a local index called "sensitivity,"
which indicates how much the input neighborhood contracts or expands into the
corresponding output neighborhood through each neuron's processing. While
sensitivity adjustment learning (SAL) prevents excessive convergence of the
dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them --
to converge more to improve reproducibility around better state transitions
with positive TD error and to diverge more to enhance exploration around worse
transitions with negative TD error. Dynamic RL was applied only to the actor in
an Actor-Critic RL architecture while applying it to the critic remains a
challenge. It was tested on two dynamic tasks and functioned effectively
without external exploration noise or backward computation through time.
Moreover, it exhibited excellent adaptability to new environments, although
some problems remain. Drawing parallels between 'exploration' and 'thinking,'
the author hypothesizes that "exploration grows into thinking through learning"
and believes this RL could be a key technique for the emergence of thinking,
including inspiration that cannot be reconstructed from massive existing text
data. Finally, despite being presumptuous, the author presents the argument
that this research should not proceed due to its potentially fatal risks,
aiming to encourage discussion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Camera Bias of Person Re-identification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myungseo Song, Jin-Woo Park, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically investigate the camera bias of person re-identification (ReID)
models. Previously, camera-aware methods have been proposed to address this
issue, but they are largely confined to training domains of the models. We
measure the camera bias of ReID models on unseen domains and reveal that camera
bias becomes more pronounced under data distribution shifts. As a debiasing
method for unseen domain data, we revisit feature normalization on embedding
vectors. While the normalization has been used as a straightforward solution,
its underlying causes and broader applicability remain unexplored. We analyze
why this simple method is effective at reducing bias and show that it can be
applied to detailed bias factors such as low-level image properties and body
angle. Furthermore, we validate its generalizability across various models and
benchmarks, highlighting its potential as a simple yet effective test-time
postprocessing method for ReID. In addition, we explore the inherent risk of
camera bias in unsupervised learning of ReID models. The unsupervised models
remain highly biased towards camera labels even for seen domain data,
indicating substantial room for improvement. Based on observations of the
negative impact of camera-biased pseudo labels on training, we suggest simple
training strategies to mitigate the bias. By applying these strategies to
existing unsupervised learning algorithms, we show that significant performance
improvements can be achieved with minor modifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Powerful Random Forest Featuring Linear Extensions (RaFFLE) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Raymaekers, Peter J. Rousseeuw, Thomas Servotte, Tim Verdonck, Ruicong Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random forests are widely used in regression. However, the decision trees
used as base learners are poor approximators of linear relationships. To
address this limitation we propose RaFFLE (Random Forest Featuring Linear
Extensions), a novel framework that integrates the recently developed PILOT
trees (Piecewise Linear Organic Trees) as base learners within a random forest
ensemble. PILOT trees combine the computational efficiency of traditional
decision trees with the flexibility of linear model trees. To ensure sufficient
diversity of the individual trees, we introduce an adjustable regularization
parameter and use node-level feature sampling. These modifications improve the
accuracy of the forest. We establish theoretical guarantees for the consistency
of RaFFLE under weak conditions, and its faster convergence when the data are
generated by a linear model. Empirical evaluations on 136 regression datasets
demonstrate that RaFFLE outperforms the classical CART and random forest
methods, the regularized linear methods Lasso and Ridge, and the
state-of-the-art XGBoost algorithm, across both linear and nonlinear datasets.
By balancing predictive accuracy and computational efficiency, RaFFLE proves to
be a versatile tool for tackling a wide variety of regression problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realistic Evaluation of Deep Partial-Label Learning Algorithms <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wang, Dong-Dong Wu, Jindong Wang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is a weakly supervised learning problem in which
each example is associated with multiple candidate labels and only one is the
true label. In recent years, many deep PLL algorithms have been developed to
improve model performance. However, we find that some early developed
algorithms are often underestimated and can outperform many later algorithms
with complicated designs. In this paper, we delve into the empirical
perspective of PLL and identify several critical but previously overlooked
issues. First, model selection for PLL is non-trivial, but has never been
systematically studied. Second, the experimental settings are highly
inconsistent, making it difficult to evaluate the effectiveness of the
algorithms. Third, there is a lack of real-world image datasets that can be
compatible with modern network architectures. Based on these findings, we
propose PLENCH, the first Partial-Label learning bENCHmark to systematically
compare state-of-the-art deep PLL algorithms. We investigate the model
selection problem for PLL for the first time, and propose novel model selection
criteria with theoretical guarantees. We also create Partial-Label CIFAR-10
(PLCIFAR10), an image dataset of human-annotated partial labels collected from
Amazon Mechanical Turk, to provide a testbed for evaluating the performance of
PLL algorithms in more realistic scenarios. Researchers can quickly and
conveniently perform a comprehensive and fair evaluation and verify the
effectiveness of newly developed algorithms based on PLENCH. We hope that
PLENCH will facilitate standardized, fair, and practical evaluation of PLL
algorithms in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Markov to Laplace: How Mamba In-Context Learns Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Bondaschi, Nived Rajaraman, Xiuying Wei, Kannan Ramchandran, Razvan Pascanu, Caglar Gulcehre, Michael Gastpar, Ashok Vardhan Makkuva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While transformer-based language models have driven the AI revolution thus
far, their computational complexity has spurred growing interest in viable
alternatives, such as structured state space sequence models (SSMs) and
Selective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown
remarkable inference speed ups over transformers while achieving comparable or
superior performance on complex language modeling tasks. However, despite these
architectural innovations and empirical successes, the fundamental learning
capabilities of Mamba remain poorly understood. In this paper, we address this
gap by studying in-context learning (ICL) on Markov chains and uncovering a
surprising phenomenon: unlike transformers, even a single-layer Mamba
efficiently learns the in-context Laplacian smoothing estimator, which is both
Bayes and minimax optimal, for all Markovian orders. To explain this, we
theoretically characterize the representation capacity of Mamba and reveal the
fundamental role of convolution in enabling it to represent the optimal
Laplacian smoothing. These theoretical insights align strongly with empirical
results and, to the best of our knowledge, represent the first formal
connection between Mamba and optimal statistical estimators. Finally, we
outline promising research directions inspired by these findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a
  Language Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Ni, Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proteins are dynamic molecular machines whose biological functions, spanning
enzymatic catalysis, signal transduction, and structural adaptation, are
intrinsically linked to their motions. Designing proteins with targeted dynamic
properties, however, remains a challenge due to the complex, degenerate
relationships between sequence, structure, and molecular motion. Here, we
introduce VibeGen, a generative AI framework that enables end-to-end de novo
protein design conditioned on normal mode vibrations. VibeGen employs an
agentic dual-model architecture, comprising a protein designer that generates
sequence candidates based on specified vibrational modes and a protein
predictor that evaluates their dynamic accuracy. This approach synergizes
diversity, accuracy, and novelty during the design process. Via full-atom
molecular simulations as direct validation, we demonstrate that the designed
proteins accurately reproduce the prescribed normal mode amplitudes across the
backbone while adopting various stable, functionally relevant structures.
Notably, generated sequences are de novo, exhibiting no significant similarity
to natural proteins, thereby expanding the accessible protein space beyond
evolutionary constraints. Our work integrates protein dynamics into generative
protein design, and establishes a direct, bidirectional link between sequence
and vibrational behavior, unlocking new pathways for engineering biomolecules
with tailored dynamical and functional properties. This framework holds broad
implications for the rational design of flexible enzymes, dynamic scaffolds,
and biomaterials, paving the way toward dynamics-informed AI-driven protein
engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing anomaly detection with topology-aware autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal S. Ngairangbam, Błażej Rozwoda, Kazuki Sakurai, Michael Spannowsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in high-energy physics is essential for identifying new
physics beyond the Standard Model. Autoencoders provide a signal-agnostic
approach but are limited by the topology of their latent space. This work
explores topology-aware autoencoders, embedding phase-space distributions onto
compact manifolds that reflect energy-momentum conservation. We construct
autoencoders with spherical ($S^n$), product ($S^2 \otimes S^2$), and
projective ($\mathbb{RP}^2$) latent spaces and compare their anomaly detection
performance against conventional Euclidean embeddings. Our results show that
autoencoders with topological priors significantly improve anomaly separation
by preserving the global structure of the data manifold and reducing spurious
reconstruction errors. Applying our approach to simulated hadronic top-quark
decays, we show that latent spaces with appropriate topological constraints
enhance sensitivity and robustness in detecting anomalous events. This study
establishes topology-aware autoencoders as a powerful tool for unsupervised
searches for new physics in particle-collision data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Generalization Power of a DNN in Terms of Symbolic
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Cheng, Junpeng Zhang, Qihan Ren, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to analyze the generalization power of deep neural networks
(DNNs) from the perspective of interactions. Unlike previous analysis of a
DNN's generalization power in a highdimensional feature space, we find that the
generalization power of a DNN can be explained as the generalization power of
the interactions. We found that the generalizable interactions follow a
decay-shaped distribution, while non-generalizable interactions follow a
spindle-shaped distribution. Furthermore, our theory can effectively
disentangle these two types of interactions from a DNN. We have verified that
our theory can well match real interactions in a DNN in experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.19198</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combinatorial Reinforcement Learning with Preference Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongkyu Lee, Min-hwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider combinatorial reinforcement learning with
preference feedback, where a learning agent sequentially offers an action--an
assortment of multiple items to--a user, whose preference feedback follows a
multinomial logistic (MNL) model. This framework allows us to model real-world
scenarios, particularly those involving long-term user engagement, such as in
recommender systems and online advertising. However, this framework faces two
main challenges: (1) the unknown value of each item, unlike traditional MNL
bandits that only address single-step preference feedback, and (2) the
difficulty of ensuring optimism while maintaining tractable assortment
selection in the combinatorial action space with unknown values. In this paper,
we assume a contextual MNL preference model, where the mean utilities are
linear, and the value of each item is approximated by a general function. We
propose an algorithm, MNL-VQL, that addresses these challenges, making it both
computationally and statistically efficient. As a special case, for linear MDPs
(with the MNL preference feedback), we establish the first regret lower bound
in this framework and show that MNL-VQL achieves nearly minimax-optimal regret.
To the best of our knowledge, this is the first work to provide statistical
guarantees in combinatorial RL with preference feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serkan Sulun, Paula Viana, Matthew E. P. Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMSYNC, a video-based symbolic music generation model that
aligns music with a video's emotional content and temporal boundaries. It
follows a two-stage framework, where a pretrained video emotion classifier
extracts emotional features, and a conditional music generator produces MIDI
sequences guided by both emotional and temporal cues. We introduce boundary
offsets, a novel temporal conditioning mechanism that enables the model to
anticipate and align musical chords with scene cuts. Unlike existing models,
our approach retains event-based encoding, ensuring fine-grained timing control
and expressive musical nuances. We also propose a mapping scheme to bridge the
video emotion classifier, which produces discrete emotion categories, with the
emotion-conditioned MIDI generator, which operates on continuous-valued
valence-arousal inputs. In subjective listening tests, EMSYNC outperforms
state-of-the-art models across all subjective metrics, for music theory-aware
participants as well as the general listeners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Joint Conference on Artificial
  Intelligence (IJCAI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Efficient RL under Episode-Wise Safety in Linear CMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshinori Kitamura, Arnob Ghosh, Tadashi Kozuno, Wataru Kumagai, Kazumi Kasaura, Kenta Hoshino, Yohei Hosoe, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the reinforcement learning (RL) problem in a constrained Markov
decision process (CMDP), where an agent explores the environment to maximize
the expected cumulative reward while satisfying a single constraint on the
expected total utility value in every episode. While this problem is well
understood in the tabular setting, theoretical results for function
approximation remain scarce. This paper closes the gap by proposing an RL
algorithm for linear CMDPs that achieves $\widetilde{\mathcal{O}}(\sqrt{K})$
regret with an episode-wise zero-violation guarantee. Furthermore, our method
is computationally efficient, scaling polynomially with problem-dependent
parameters while remaining independent of the state space size. Our results
significantly improve upon recent linear CMDP algorithms, which either violate
the constraint or incur exponential computational costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Relational Tabular Data without Shared Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaomin Wu, Shida Wang, Ziyang Wang, Bingsheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning relational tabular data has gained significant attention recently,
but most studies focus on single tables, overlooking the potential of
cross-table learning. Cross-table learning, especially in scenarios where
tables lack shared features and pre-aligned data, offers vast opportunities but
also introduces substantial challenges. The alignment space is immense, and
determining accurate alignments between tables is highly complex. We propose
Latent Entity Alignment Learning (Leal), a novel framework enabling effective
cross-table training without requiring shared features or pre-aligned data.
Leal operates on the principle that properly aligned data yield lower loss than
misaligned data, a concept embodied in its soft alignment mechanism. This
mechanism is coupled with a differentiable cluster sampler module, ensuring
efficient scaling to large relational tables. Furthermore, we provide a
theoretical proof of the cluster sampler's approximation capacity. Extensive
experiments on five real-world and five synthetic datasets show that Leal
achieves up to a 26.8% improvement in predictive performance compared to
state-of-the-art methods, demonstrating its effectiveness and scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modern Hopfield Networks with Continuous-Time Memories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saul Santos, António Farinhas, Daniel C. McNamee, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has established a connection between modern Hopfield networks
(HNs) and transformer attention heads, with guarantees of exponential storage
capacity. However, these models still face challenges scaling storage
efficiently. Inspired by psychological theories of continuous neural resource
allocation in working memory, we propose an approach that compresses large
discrete Hopfield memories into smaller, continuous-time memories. Leveraging
continuous attention, our new energy function modifies the update rule of HNs,
replacing the traditional softmax-based probability mass function with a
probability density, over the continuous memory. This formulation aligns with
modern perspectives on human executive function, offering a principled link
between attractor dynamics in working memory and resource-efficient memory
allocation. Our framework maintains competitive performance with HNs while
leveraging a compressed memory, reducing computational costs across synthetic
and video datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeWA: Selective Weight Average via Probabilistic Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Shengchao Hu, Zerui Tao, Guoxia Wang, Dianhai Yu, Li Shen, Quan Zheng, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weight averaging has become a standard technique for enhancing model
performance. However, methods such as Stochastic Weight Averaging (SWA) and
Latest Weight Averaging (LAWA) often require manually designed procedures to
sample from the training trajectory, and the results depend heavily on
hyperparameter tuning. To minimize human effort, this paper proposes a simple
yet efficient algorithm called Selective Weight Averaging (SeWA), which
adaptively selects checkpoints during the final stages of training for
averaging. Based on SeWA, we show that only a few points are needed to achieve
better generalization and faster convergence. Theoretically, solving the
discrete subset selection problem is inherently challenging. To address this,
we transform it into a continuous probabilistic optimization framework and
employ the Gumbel-Softmax estimator to learn the non-differentiable mask for
each checkpoint. Further, we theoretically derive the SeWA's stability-based
generalization bounds, which are sharper than that of SGD under both convex and
non-convex assumptions. Finally, solid extended experiments in various domains,
including behavior cloning, image classification, and text classification,
further validate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerometry-based Energy Expenditure Estimation During Activities of
  Daily Living: A Comparison Among Different Accelerometer Compositions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhao Que, Remco Poelarends, Peter Veltink, Miriam Vollenbroek-Hutten, Ying Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical activity energy expenditure (PAEE) can be measured from
breath-by-breath respiratory data, which can serve as a reference.
Alternatively, PAEE can be predicted from the body movements, which can be
measured and estimated with accelerometers. The body center of mass (COM)
acceleration reflects the movements of the whole body and thus serves as a good
predictor for PAEE. However, the wrist has also become a popular location due
to recent advancements in wrist-worn devices. Therefore, in this work, using
the respiratory data measured by COSMED K5 as the reference, we evaluated and
compared the performances of COM-based settings and wrist-based settings. The
COM-based settings include two different accelerometer compositions, using only
the pelvis accelerometer (pelvis-acc) and the pelvis accelerometer with two
accelerometers from two thighs (3-acc). The wrist-based settings include using
only the left wrist accelerometer (l-wrist-acc) and only the right wrist
accelerometer (r-wrist-acc). We implemented two existing PAEE estimation
methods on our collected dataset, where 9 participants performed activities of
daily living while wearing 5 accelerometers (i.e., pelvis, two thighs, and two
wrists). These two methods include a linear regression (LR) model and a
CNN-LSTM model. Both models yielded the best results with the COM-based 3-acc
setting (LR: $R^2$ = 0.41, CNN-LSTM: $R^2$ = 0.53). No significant difference
was found between the 3-acc and pelvis-acc settings (p-value = 0.278). For both
models, neither the l-wrist-acc nor the r-wrist-acc settings demonstrated
predictive power on PAEE with $R^2$ values close to 0, significantly
outperformed by the two COM-based settings (p-values $<$ 0.05). No significant
difference was found between the two wrists (p-value = 0.329).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMBINEX: A Unified Counterfactual Explainer for <span class="highlight-title">Graph</span> Neural Networks
  via Node Feature and Structural Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Giorgi, Fabrizio Silvestri, Gabriele Tolomei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations have emerged as a powerful tool to unveil the
opaque decision-making processes of graph neural networks (GNNs). However,
existing techniques primarily focus on edge modifications, often overlooking
the crucial role of node feature perturbations in shaping model predictions. To
address this limitation, we propose COMBINEX, a novel GNN explainer that
generates counterfactual explanations for both node and graph classification
tasks. Unlike prior methods, which treat structural and feature-based changes
independently, COMBINEX optimally balances modifications to edges and node
features by jointly optimizing these perturbations. This unified approach
ensures minimal yet effective changes required to flip a model's prediction,
resulting in realistic and interpretable counterfactuals. Additionally,
COMBINEX seamlessly handles both continuous and discrete node features,
enhancing its versatility across diverse datasets and GNN architectures.
Extensive experiments on real-world datasets and various GNN architectures
demonstrate the effectiveness and robustness of our approach over existing
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroXVocal: Detection and Explanation of Alzheimer's Disease through
  Non-invasive Analysis of Picture-prompted Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Magda Tsolaki, Vasileios Argyriou, Panagiotis Sarigianndis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The early diagnosis of Alzheimer's Disease (AD) through non invasive methods
remains a significant healthcare challenge. We present NeuroXVocal, a novel
dual-component system that not only classifies but also explains potential AD
cases through speech analysis. The classification component (Neuro) processes
three distinct data streams: acoustic features capturing speech patterns and
voice characteristics, textual features extracted from speech transcriptions,
and precomputed embeddings representing linguistic patterns. These streams are
fused through a custom transformer-based architecture that enables robust
cross-modal interactions. The explainability component (XVocal) implements a
Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models
combined with a domain-specific knowledge base of AD research literature. This
architecture enables XVocal to retrieve relevant clinical studies and research
findings to generate evidence-based context-sensitive explanations of the
acoustic and linguistic markers identified in patient speech. Using the IS2021
ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art
performance with 95.77% accuracy in AD classification, significantly
outperforming previous approaches. The explainability component was
qualitatively evaluated using a structured questionnaire completed by medical
professionals, validating its clinical relevance. NeuroXVocal's unique
combination of high-accuracy classification and interpretable,
literature-grounded explanations demonstrates its potential as a practical tool
for supporting clinical AD diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Adaptive Low-Rank Sparse Subspace Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivica Kopriva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank sparse subspace clustering (LRSSC) algorithms built on
self-expressive model effectively capture both the global and local structure
of the data. However, existing solutions, primarily based on proximal operators
associated with Sp/Lp , p e {0, 1/2, 2/3, 1}, norms are not data-adaptive. In
this work, we propose an LRSSC algorithm incorporating a data-adaptive
surrogate for the S0/L0 quasi-norm. We provide a numerical solution for the
corresponding proximal operator in cases where an analytical expression is
unavailable. The proposed LRSSC algorithm is formulated within the proximal
mapping framework, and we present theoretical proof of its global convergence
toward a stationary point. We evaluate the performance of the proposed method
on three well known datasets, comparing it against LRSSC algorithms constrained
by Sp/Lp, p e {0, 1/2, 2/3, 1}, norms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Information Prioritization for Efficient Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Cao, Fan Feng, Tianpei Yang, Jing Huo, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Reinforcement Learning (RL) methods often suffer from
sample-inefficiency, resulting from blind exploration strategies that neglect
causal relationships among states, actions, and rewards. Although recent causal
approaches aim to address this problem, they lack grounded modeling of
reward-guided causal understanding of states and actions for goal-orientation,
thus impairing learning efficiency. To tackle this issue, we propose a novel
method named Causal Information Prioritization (CIP) that improves sample
efficiency by leveraging factored MDPs to infer causal relationships between
different dimensions of states and actions with respect to rewards, enabling
the prioritization of causal information. Specifically, CIP identifies and
leverages causal relationships between states and rewards to execute
counterfactual data augmentation to prioritize high-impact state features under
the causal understanding of the environments. Moreover, CIP integrates a
causality-aware empowerment learning objective, which significantly enhances
the agent's execution of reward-guided actions for more efficient exploration
in complex environments. To fully assess the effectiveness of CIP, we conduct
extensive experiments across 39 tasks in 5 diverse continuous control
environments, encompassing both locomotion and manipulation skills learning
with pixel-based and sparse reward settings. Experimental results demonstrate
that CIP consistently outperforms existing RL methods across a wide range of
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Learning on Out of Distribution in Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The open-world assumption in model development suggests that a model might
lack sufficient information to adequately handle data that is entirely distinct
or out of distribution (OOD). While deep learning methods have shown promising
results in handling OOD data through generalization techniques, they often
require specialized hardware that may not be accessible to all users. We
present TCL, a lightweight yet effective solution that operates efficiently on
standard CPU hardware. Our approach adapts contrastive learning principles
specifically for tabular data structures, incorporating full matrix
augmentation and simplified loss calculation. Through comprehensive experiments
across 10 diverse datasets, we demonstrate that TCL outperforms existing
models, including FT-Transformer and ResNet, particularly in classification
tasks, while maintaining competitive performance in regression problems. TCL
achieves these results with significantly reduced computational requirements,
making it accessible to users with limited hardware capabilities. This study
also provides practical guidance for detecting and evaluating OOD data through
straightforward experiments and visualizations. Our findings show that TCL
offers a promising balance between performance and efficiency in handling OOD
prediction tasks, which is particularly beneficial for general machine learning
practitioners working with computational constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel approach to data generation in generative model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JaeHong Kim, Jaewon Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders (VAEs) and other generative models are widely
employed in artificial intelligence to synthesize new data. However, current
approaches rely on Euclidean geometric assumptions and statistical
approximations that fail to capture the structured and emergent nature of data
generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory,
a novel geometric framework that redefines data generation by integrating
dimensional expansion accompanied by qualitative transformation. By modifying
the latent space geometry to interact with emergent high-dimensional
structures, CFP theory addresses key challenges such as identifiability issues
and unintended artifacts like hallucinations in Large Language Models (LLMs).
CFP theory is based on two key conceptual hypotheses that redefine how
generative models structure relationships between data and algorithms. Through
the lens of CFP theory, we critically examine existing metric-learning
approaches. CFP theory advances this perspective by introducing time-reversed
metric embeddings and structural convergence mechanisms, leading to a novel
geometric approach that better accounts for data generation as a structured
epistemic process. Beyond its computational implications, CFP theory provides
philosophical insights into the ontological underpinnings of data generation.
By offering a systematic framework for high-dimensional learning dynamics, CFP
theory contributes to establishing a theoretical foundation for understanding
the data-relationship structures in AI. Finally, future research in CFP theory
will be led to its implications for fully realizing qualitative
transformations, introducing the potential of Hilbert space in generative
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 2 tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS
  ACAM for Energy-Efficient Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kieran Woodward, Eiman Kanjo, Georgios Papandroulidakis, Shady Agwa, Themis Prodromakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the development of smart edge computing systems to process
information locally is on the rise. Many near-sensor machine learning (ML)
approaches have been implemented to introduce accurate and energy efficient
template matching operations in resource-constrained edge sensing systems, such
as wearables. To introduce novel solutions that can be viable for extreme edge
cases, hybrid solutions combining conventional and emerging technologies have
started to be proposed. Deep Neural Networks (DNN) optimised for edge
application alongside new approaches of computing (both device and architecture
-wise) could be a strong candidate in implementing edge ML solutions that aim
at competitive accuracy classification while using a fraction of the power of
conventional ML solutions. In this work, we are proposing a hybrid
software-hardware edge classifier aimed at the extreme edge near-sensor
systems. The classifier consists of two parts: (i) an optimised digital tinyML
network, working as a front-end feature extractor, and (ii) a back-end
RRAM-CMOS analogue content addressable memory (ACAM), working as a final stage
template matching system. The combined hybrid system exhibits a competitive
trade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$
and $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with
78.06$\mu$J for the original teacher model, representing a 792-fold reduction,
making it a viable solution for extreme edge applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Empowerment Gain through Causal Structure Learning in
  Model-Based RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Cao, Fan Feng, Meng Fang, Shaokang Dong, Tianpei Yang, Jing Huo, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Model-Based Reinforcement Learning (MBRL), incorporating causal structures
into dynamics models provides agents with a structured understanding of the
environments, enabling efficient decision. Empowerment as an intrinsic
motivation enhances the ability of agents to actively control their
environments by maximizing the mutual information between future states and
actions. We posit that empowerment coupled with causal understanding can
improve controllability, while enhanced empowerment gain can further facilitate
causal reasoning in MBRL. To improve learning efficiency and controllability,
we propose a novel framework, Empowerment through Causal Learning (ECL), where
an agent with the awareness of causal dynamics models achieves
empowerment-driven exploration and optimizes its causal structure for task
learning. Specifically, ECL operates by first training a causal dynamics model
of the environment based on collected data. We then maximize empowerment under
the causal structure for exploration, simultaneously using data gathered
through exploration to update causal dynamics model to be more controllable
than dense dynamics model without causal structure. In downstream task
learning, an intrinsic curiosity reward is included to balance the causality,
mitigating overfitting. Importantly, ECL is method-agnostic and is capable of
integrating various causal discovery methods. We evaluate ECL combined with 3
causal discovery methods across 6 environments including pixel-based tasks,
demonstrating its superior performance compared to other causal MBRL methods,
in terms of causal discovery, sample efficiency, and asymptotic performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Temporal <span class="highlight-title">Graph</span>s using Persistent Homology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Pritam, Rohit Roy, Madhav Cherupilil Sajeev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graphs effectively model dynamic systems by representing
interactions as timestamped edges. However, analytical tools for temporal
graphs are limited compared to static graphs. We propose a novel method for
analyzing temporal graphs using Persistent Homology. Our approach leverages
$\delta$-temporal motifs (recurrent subgraphs) to capture temporal dynamics
%without aggregation
  . By evolving these motifs, we define the \textit{average filtration} and
compute PH on the associated clique complex. This method captures both local
and global temporal structures and is stable with respect to reference models.
We demonstrate the applicability of our approach to the temporal graph
classification task. Experiments verify the effectiveness of our approach,
achieving over 92\% accuracy, with some cases reaching 100\%. Unlike existing
methods that require node classes, our approach is node class free, offering
flexibility for a wide range of temporal graph analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological Neural Networks over the Air 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Fiorellino, Claudio Battiloro, Paolo Di Lorenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological neural networks (TNNs) are information processing architectures
that model representations from data lying over topological spaces (e.g.,
simplicial or cell complexes) and allow for decentralized implementation
through localized communications over different neighborhoods. Existing TNN
architectures have not yet been considered in realistic communication
scenarios, where channel effects typically introduce disturbances such as
fading and noise. This paper aims to propose a novel TNN design, operating on
regular cell complexes, that performs over-the-air computation, incorporating
the wireless communication model into its architecture. Specifically, during
training and inference, the proposed method considers channel impairments such
as fading and noise in the topological convolutional filtering operation, which
takes place over different signal orders and neighborhoods. Numerical results
illustrate the architecture's robustness to channel impairments during testing
and the superior performance with respect to existing architectures, which are
either communication-agnostic or graph-based.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiSciPLE: Learning Interpretable Programs for Scientific Visual
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Mall, Cheng Perng Phoo, Mia Chiquier, Bharath Hariharan, Kavita Bala, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual data is used in numerous different scientific workflows ranging from
remote sensing to ecology. As the amount of observation data increases, the
challenge is not just to make accurate predictions but also to understand the
underlying mechanisms for those predictions. Good interpretation is important
in scientific workflows, as it allows for better decision-making by providing
insights into the data. This paper introduces an automatic way of obtaining
such interpretable-by-design models, by learning programs that interleave
neural networks. We propose DiSciPLE (Discovering Scientific Programs using
LLMs and Evolution) an evolutionary algorithm that leverages common sense and
prior knowledge of large language models (LLMs) to create Python programs
explaining visual data. Additionally, we propose two improvements: a program
critic and a program simplifier to improve our method further to synthesize
good programs. On three different real-world problems, DiSciPLE learns
state-of-the-art programs on novel tasks with no prior literature. For example,
we can learn programs with 35% lower error than the closest non-interpretable
baseline for population density estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Resource Allocation with Multi-task Learning for Wireless
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos A. Mitsiou, Pavlos S. Bouzinis, Panagiotis G. Sarigiannidis, George K. Karagiannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimal solution to an optimization problem depends on the problem's
objective function, constraints, and size. While deep neural networks (DNNs)
have proven effective in solving optimization problems, changes in the
problem's size, objectives, or constraints often require adjustments to the DNN
architecture to maintain effectiveness, or even retraining a new DNN from
scratch. Given the dynamic nature of wireless networks, which involve multiple
and diverse objectives that can have conflicting requirements and constraints,
we propose a multi-task learning (MTL) framework to enable a single DNN to
jointly solve a range of diverse optimization problems. In this framework,
optimization problems with varying dimensionality values, objectives, and
constraints are treated as distinct tasks. To jointly address these tasks, we
propose a conditional computation-based MTL approach with routing. The
multi-task DNN consists of two components, the base DNN (bDNN), which is the
single DNN used to extract the solutions for all considered optimization
problems, and the routing DNN (rDNN), which manages which nodes and layers of
the bDNN to be used during the forward propagation of each task. The output of
the rDNN is a binary vector which is multiplied with all bDNN's weights during
the forward propagation, creating a unique computational path through the bDNN
for each task. This setup allows the tasks to either share parameters or use
independent ones, with the decision controlled by the rDNN. The proposed
framework supports both supervised and unsupervised learning scenarios.
Numerical results demonstrate the efficiency of the proposed MTL approach in
solving diverse optimization problems. In contrast, benchmark DNNs lacking the
rDNN mechanism were unable to achieve similar levels of performance,
highlighting the effectiveness of the proposed architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Online Confidence Bounds for Multinomial Logistic Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongkyu Lee, Min-hwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an improved online confidence bound for multinomial
logistic (MNL) models and apply this result to MNL bandits, achieving
variance-dependent optimal regret. Recently, Lee & Oh (2024) established an
online confidence bound for MNL models and achieved nearly minimax-optimal
regret in MNL bandits. However, their results still depend on the
norm-boundedness of the unknown parameter $B$ and the maximum size of possible
outcomes $K$. To address this, we first derive an online confidence bound of
$O\left(\sqrt{d \log t} + B \right)$, which is a significant improvement over
the previous bound of $O (B \sqrt{d} \log t \log K )$ (Lee & Oh, 2024). This is
mainly achieved by establishing tighter self-concordant properties of the MNL
loss and introducing a novel intermediary term to bound the estimation error.
Using this new online confidence bound, we propose a constant-time algorithm,
OFU-MNL++, which achieves a variance-dependent regret bound of $O \Big( d \log
T \sqrt{ \smash[b]{\sum_{t=1}^T} \sigma_t^2 } \Big) $ for sufficiently large
$T$, where $\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$
is the dimension of the contexts, and $T$ is the total number of rounds.
Furthermore, we introduce an Maximum Likelihood Estimation (MLE)-based
algorithm that achieves an anytime, OFU-MN$^2$L, poly($(B)$)-free regret of $O
\Big( d \log (BT) \sqrt{ \smash[b]{\sum_{t=1}^T} \sigma_t^2 } \Big) $.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterGridNet: An Electric Network Frequency Approach for Audio Source
  Location Classification Using Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Korgialas, Ioannis Tsingalis, Georgios Tzolopoulos, Constantine Kotropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel framework, called InterGridNet, is introduced, leveraging a shallow
RawNet model for geolocation classification of Electric Network Frequency (ENF)
signatures in the SP Cup 2016 dataset. During data preparation, recordings are
sorted into audio and power groups based on inherent characteristics, further
divided into 50 Hz and 60 Hz groups via spectrogram analysis. Residual blocks
within the classification model extract frame-level embeddings, aiding
decision-making through softmax activation. The topology and the
hyperparameters of the shallow RawNet are optimized using a Neural Architecture
Search. The overall accuracy of InterGridNet in the test recordings is 92%,
indicating its effectiveness against the state-of-the-art methods tested in the
SP Cup 2016. These findings underscore InterGridNet's effectiveness in
accurately classifying audio recordings from diverse power grids, advancing
state-of-the-art geolocation estimation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 10th International Conference on Advances in Signal, Image and
  Video Processing (SIGNAL 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbBERT-Q: Breaking Memory Barriers in Embedded NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing,
setting new standards across a wide range of applications. However, their
relevant memory and computational demands make them impractical for deployment
on technologically-constrained tiny devices such as wearable devices and
Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a
novel tiny language model specifically designed for tiny devices with stringent
memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in
Natural Language Processing tasks in this scenario, with a total memory
footprint (weights and activations) of just 781 kB, representing a 25x
reduction in size with respect to SotA models. By combining architectural
innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently
outperforms several baseline models scaled down to a 2 MB memory budget (i.e.,
the maximum memory typically available in tiny devices), including heavily
compressed versions of BERT and MAMBA. Extensive experimental evaluations on
both a selected benchmark dataset, TinyNLP, specifically curated to evaluate
Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE
benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with
respect to existing approaches, achieving an unmatched balance between memory
and performance. To ensure the complete and immediate reproducibility of all
our results, we release all code, scripts, and model checkpoints at
https://github.com/RiccardoBravin/tiny-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 4 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of the Learning Coefficient Using Empirical Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuyoshi Takio, Joe Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The learning coefficient plays a crucial role in analyzing the performance of
information criteria, such as the Widely Applicable Information Criterion
(WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC), which
Sumio Watanabe developed to assess model generalization ability. In regular
statistical models, the learning coefficient is given by d/2, where d is the
dimension of the parameter space. More generally, it is defined as the absolute
value of the pole order of a zeta function derived from the Kullback-Leibler
divergence and the prior distribution. However, except for specific cases such
as reduced-rank regression, the learning coefficient cannot be derived in a
closed form. Watanabe proposed a numerical method to estimate the learning
coefficient, which Imai further refined to enhance its convergence properties.
These methods utilize the asymptotic behavior of WBIC and have been shown to be
statistically consistent as the sample size grows. In this paper, we propose a
novel numerical estimation method that fundamentally differs from previous
approaches and leverages a new quantity, "Empirical Loss," which was introduced
by Watanabe. Through numerical experiments, we demonstrate that our proposed
method exhibits both lower bias and lower variance compared to those of
Watanabe and Imai. Additionally, we provide a theoretical analysis that
elucidates why our method outperforms existing techniques and present empirical
evidence that supports our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models (ARMs) are widely regarded as the cornerstone of large
language models (LLMs). We challenge this notion by introducing LLaDA, a
diffusion model trained from scratch under the pre-training and supervised
fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data
masking process and a reverse process, parameterized by a vanilla Transformer
to predict masked tokens. By optimizing a likelihood bound, it provides a
principled generative approach for probabilistic inference. Across extensive
benchmarks, LLaDA demonstrates strong scalability, outperforming our
self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong
LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive
instruction-following abilities in case studies such as multi-turn dialogue.
Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal
poem completion task. Our findings establish diffusion models as a viable and
promising alternative to ARMs, challenging the assumption that key LLM
capabilities discussed above are inherently tied to ARMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from
  Multi-Turn Jailbreaks without Compromising Usability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of safety alignment techniques for LLMs,
defending against multi-turn jailbreaks is still a challenging task. In this
paper, we conduct a comprehensive comparison, revealing that some existing
defense methods can improve the robustness of LLMs against multi-turn
jailbreaks but compromise usability, i.e., reducing general capabilities or
causing the over-refusal problem. From the perspective of mechanism
interpretability of LLMs, we discover that these methods fail to establish a
boundary that exactly distinguishes safe and harmful feature representations.
Therefore, boundary-safe representations close to harmful representations are
inevitably disrupted, leading to a decline in usability. To address this issue,
we propose X-Boundary to push harmful representations away from boundary-safe
representations and obtain an exact distinction boundary. In this way, harmful
representations can be precisely erased without disrupting safe ones.
Experimental results show that X-Boundary achieves state-of-the-art defense
performance against multi-turn jailbreaks, while reducing the over-refusal rate
by about 20% and maintaining nearly complete general capability. Furthermore,
we theoretically prove and empirically verify that X-Boundary can accelerate
the convergence process during training. Please see our code at:
https://github.com/AI45Lab/X-Boundary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Volume Minimization in Conformal Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batiste Le Bars, Pierre Humbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the question of volume optimality in split conformal regression, a
topic still poorly understood in comparison to coverage control. Using the fact
that the calibration step can be seen as an empirical volume minimization
problem, we first derive a finite-sample upper-bound on the excess volume loss
of the interval returned by the classical split method. This important quantity
measures the difference in length between the interval obtained with the split
method and the shortest oracle prediction interval. Then, we introduce EffOrt,
a methodology that modifies the learning step so that the base prediction
function is selected in order to minimize the length of the returned intervals.
In particular, our theoretical analysis of the excess volume loss of the
prediction sets produced by EffOrt reveals the links between the learning and
calibration steps, and notably the impact of the choice of the function class
of the base predictor. We also introduce Ad-EffOrt, an extension of the
previous method, which produces intervals whose size adapts to the value of the
covariate. Finally, we evaluate the empirical performance and the robustness of
our methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal
  Dependencies in Complex Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsh Poonia, Felix Divo, Kristian Kersting, Devendra Singh Dhami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causality in time series can be difficult to determine, especially in the
presence of non-linear dependencies. The concept of Granger causality helps
analyze potential relationships between variables, thereby offering a method to
determine whether one time series can predict-Granger cause-future values of
another. Although successful, Granger causal methods still struggle with
capturing long-range relations between variables. To this end, we leverage the
recently successful Extended Long Short-Term Memory (xLSTM) architecture and
propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between
the time series components by using a novel dynamic lass penalty on the initial
projection. Specifically, we adaptively improve the model and identify sparsity
candidates. Our joint optimization procedure then ensures that the Granger
causal relations are recovered in a robust fashion. Our experimental
evaluations on three datasets demonstrate the overall efficacy of our proposed
GC-xLSTM model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Machine Learning Interatomic Potentials are Ready for Solid
  Ion Conductors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Du, Jian Hui, Lanting Zhang, Hong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of energy storage technology, high-performance
solid-state electrolytes (SSEs) have become critical for next-generation
lithium-ion batteries. These materials require high ionic conductivity,
excellent electrochemical stability, and good mechanical properties to meet the
demands of electric vehicles and portable electronics. However, traditional
methods like density functional theory (DFT) and empirical force fields face
challenges such as high computational costs, poor scalability, and limited
accuracy across material systems. Universal machine learning interatomic
potentials (uMLIPs) offer a promising solution with their efficiency and
near-DFT-level accuracy.This study systematically evaluates six advanced uMLIP
models (MatterSim, MACE, SevenNet, CHGNet, M3GNet, and ORBFF) in terms of
energy, forces, thermodynamic properties, elastic moduli, and lithium-ion
diffusion behavior. The results show that MatterSim outperforms others in
nearly all metrics, particularly in complex material systems, demonstrating
superior accuracy and physical consistency. Other models exhibit significant
deviations due to issues like energy inconsistency or insufficient training
data coverage.Further analysis reveals that MatterSim achieves excellent
agreement with reference values in lithium-ion diffusivity calculations,
especially at room temperature. Studies on Li3YCl6 and Li6PS5Cl uncover how
crystal structure, anion disorder levels, and Na/Li arrangements influence
ionic conductivity. Appropriate S/Cl disorder levels and optimized Na/Li
arrangements enhance diffusion pathway connectivity, improving overall ionic
transport performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Valuation using Neural Networks for Efficient Instruction
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Dilek Hakkani-Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide crucial insights into model training, but
existing methods suffer from large computational costs and limited
generalization. Particularly, recent works have proposed various metrics and
algorithms to calculate the influence of data using language models, which do
not scale well with large models and datasets. This is because of the expensive
forward and backward passes required for computation, substantial memory
requirements to store large models, and poor generalization of influence
estimates to new data. In this paper, we explore the use of small neural
networks -- which we refer to as the InfluenceNetwork -- to estimate influence
values, achieving up to 99% cost reduction. Our evaluation demonstrates that
influence values can be estimated with models just 0.0027% the size of full
language models (we use 7B and 8B versions). We apply our algorithm of
estimating influence values (called NN-CIFT: Neural Networks for effiCient
Instruction Fine-Tuning) to the downstream task of subset selection for general
instruction fine-tuning. In our study, we include four state-of-the-art
influence functions and show no compromise in performance, despite large
speedups, between NN-CIFT and the original influence functions. We provide an
in-depth hyperparameter analyses of NN-CIFT. The code for our method can be
found here: https://github.com/agarwalishika/NN-CIFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KGGen: Extracting Knowledge <span class="highlight-title">Graph</span>s from Plain Text with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent interest in building foundation models for KGs has highlighted a
fundamental challenge: knowledge-graph data is relatively scarce. The
best-known KGs are primarily human-labeled, created by pattern-matching, or
extracted using early NLP techniques. While human-generated KGs are in short
supply, automatically extracted KGs are of questionable quality. We present a
solution to this data scarcity problem in the form of a text-to-KG generator
(KGGen), a package that uses language models to create high-quality graphs from
plaintext. Unlike other KG extractors, KGGen clusters related entities to
reduce sparsity in extracted KGs. KGGen is available as a Python library
(\texttt{pip install kg-gen}), making it accessible to everyone. Along with
KGGen, we release the first benchmark, Measure of of Information in Nodes and
Edges (MINE), that tests an extractor's ability to produce a useful KG from
plain text. We benchmark our new tool against existing extractors and
demonstrate far superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Space Folds of ReLU Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Lewandowski, Hamid Eghbalzadeh, Bernhard Heinzl, Raphael Pisoni, Bernhard A. Moser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent findings suggest that the consecutive layers of ReLU neural networks
can be understood geometrically as space folding transformations of the input
space, revealing patterns of self-similarity. In this paper, we present the
first quantitative analysis of this space folding phenomenon in ReLU neural
networks. Our approach focuses on examining how straight paths in the Euclidean
input space are mapped to their counterparts in the Hamming activation space.
In this process, the convexity of straight lines is generally lost, giving rise
to non-convex folding behavior. To quantify this effect, we introduce a novel
measure based on range metrics, similar to those used in the study of random
walks, and provide the proof for the equivalence of convexity notions between
the input and activation spaces. Furthermore, we provide empirical analysis on
a geometrical analysis benchmark (CantorNet) as well as an image classification
benchmark (MNIST). Our work advances the understanding of the activation space
in ReLU neural networks by leveraging the phenomena of geometric folding,
providing valuable insights on how these models process input information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transactions on Machine Learning Research (TMLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage
  Encoding Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Cui, Alexander Capstick, Payam Barnaghi, Gregory Scott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the analysis of remote healthcare monitoring data, time series
representation learning offers substantial value in uncovering deeper patterns
of patient behavior, especially given the fine temporal granularity of the
data. In this study, we focus on a dataset of home activity records from people
living with Dementia. We propose a two-stage self-supervised learning approach.
The first stage involves converting time-series activities into text strings,
which are then encoded by a fine-tuned language model. In the second stage,
these time-series vectors are bi-dimensionalized for applying PageRank method,
to analyze latent state transitions to quantitatively assess participants
behavioral patterns and identify activity biases. These insights, combined with
diagnostic data, aim to support personalized care interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 workshop Time Series in the Age of Large Models. arXiv
  admin note: substantial text overlap with arXiv:2502.09173</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Interactive Framework for Implementing Privacy-Preserving Federated
  Learning: Experiments on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasra Ahmadi, Rouzbeh Behnia, Reza Ebrahimi, Mehran Mozaffari Kermani, Jeremiah Birrell, Jason Pacheco, Attila A Yavuz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enhances privacy by keeping user data on local
devices. However, emerging attacks have demonstrated that the updates shared by
users during training can reveal significant information about their data. This
has greatly thwart the adoption of FL methods for training robust AI models in
sensitive applications. Differential Privacy (DP) is considered the gold
standard for safeguarding user data. However, DP guarantees are highly
conservative, providing worst-case privacy guarantees. This can result in
overestimating privacy needs, which may compromise the model's accuracy.
Additionally, interpretations of these privacy guarantees have proven to be
challenging in different contexts. This is further exacerbated when other
factors, such as the number of training iterations, data distribution, and
specific application requirements, can add further complexity to this problem.
In this work, we proposed a framework that integrates a human entity as a
privacy practitioner to determine an optimal trade-off between the model's
privacy and utility. Our framework is the first to address the variable memory
requirement of existing DP methods in FL settings, where resource-limited
devices (e.g., cell phones) can participate. To support such settings, we adopt
a recent DP method with fixed memory usage to ensure scalable private FL. We
evaluated our proposed framework by fine-tuning a BERT-based LLM model using
the GLUE dataset (a common approach in literature), leveraging the new
accountant, and employing diverse data partitioning strategies to mimic
real-world conditions. As a result, we achieved stable memory usage, with an
average accuracy reduction of 1.33% for $\epsilon = 10$ and 1.9% for $\epsilon
= 6$, when compared to the state-of-the-art DP accountant which does not
support fixed memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The <span class="highlight-title">Graph</span>'s Apprentice: Teaching an LLM Low Level Knowledge for Circuit
  Quality Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Moravej, Saurabh Bodhe, Zhanguang Zhang, Didier Chetelat, Dimitrios Tsaras, Yingxue Zhang, Hui-Ling Zhen, Jianye Hao, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logic synthesis is a crucial phase in the circuit design process, responsible
for transforming hardware description language (HDL) designs into optimized
netlists. However, traditional logic synthesis methods are computationally
intensive, restricting their iterative use in refining chip designs. Recent
advancements in large language models (LLMs), particularly those fine-tuned on
programming languages, present a promising alternative. This work proposes
augmenting LLMs with predictor networks trained to estimate circuit quality
directly from HDL code. To enhance performance, the model is regularized using
embeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)
graphs, thereby incorporating lower-level circuit insights. The proposed method
demonstrates superior performance compared to existing graph-based RTL-level
estimation techniques on the established benchmark OpenABCD, while providing
instant feedback on HDL code quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Clustered Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19272v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19272v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Malekmohammadi, Afaf Taik, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL), which is a decentralized machine learning (ML)
approach, often incorporates differential privacy (DP) to provide rigorous data
privacy guarantees. Previous works attempted to address high structured data
heterogeneity in vanilla FL settings through clustering clients (a.k.a
clustered FL), but these methods remain sensitive and prone to errors, further
exacerbated by the DP noise. This vulnerability makes the previous methods
inappropriate for differentially private FL (DPFL) settings with structured
data heterogeneity. To address this gap, we propose an algorithm for
differentially private clustered FL, which is robust to the DP noise in the
system and identifies the underlying clients' clusters correctly. To this end,
we propose to cluster clients based on both their model updates and training
loss values. Furthermore, for clustering clients' model updates at the end of
the first round, our proposed approach addresses the server's uncertainties by
employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce
the impact of DP and stochastic noise and avoid potential clustering errors.
This idea is efficient especially in privacy-sensitive scenarios with more DP
noise. We provide theoretical analysis to justify our approach and evaluate it
across diverse data distributions and privacy budgets. Our experimental results
show its effectiveness in addressing large structured data heterogeneity in
DPFL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New tools for comparing classical and neural ODE models for tumor growth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony D. Blaom, Samuel Okon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new computational tool TumorGrowth$.$jl for modeling tumor growth is
introduced. The tool allows the comparison of standard textbook models, such as
General Bertalanffy and Gompertz, with some newer models, including, for the
first time, neural ODE models. As an application, we revisit a human meta-study
of non-small cell lung cancer and bladder cancer lesions, in patients
undergoing two different treatment options, to determine if previously reported
performance differences are statistically significant, and if newer, more
complex models perform any better. In a population of examples with at least
four time-volume measurements available for calibration, and an average of
about 6.3, our main conclusion is that the General Bertalanffy model has
superior performance, on average. However, where more measurements are
available, we argue that more complex models, capable of capturing rebound and
relapse behavior, may be better choices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures. Related software is archived at
  https://github.com/ablaom/TumorGrowth.jl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise-Aware Algorithm for Heterogeneous Differentially Private Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03519v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03519v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Malekmohammadi, Yaoliang Yu, Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High utility and rigorous data privacy are of the main goals of a federated
learning (FL) system, which learns a model from the data distributed among some
clients. The latter has been tried to achieve by using differential privacy in
FL (DPFL). There is often heterogeneity in clients privacy requirements, and
existing DPFL works either assume uniform privacy requirements for clients or
are not applicable when server is not fully trusted (our setting). Furthermore,
there is often heterogeneity in batch and/or dataset size of clients, which as
shown, results in extra variation in the DP noise level across clients model
updates. With these sources of heterogeneity, straightforward aggregation
strategies, e.g., assigning clients aggregation weights proportional to their
privacy parameters will lead to lower utility. We propose Robust-HDP, which
efficiently estimates the true noise level in clients model updates and reduces
the noise-level in the aggregated model updates considerably. Robust-HDP
improves utility and convergence speed, while being safe to the clients that
may maliciously send falsified privacy parameter to server. Extensive
experimental results on multiple datasets and our theoretical analysis confirm
the effectiveness of Robust-HDP. Our code can be found here.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 41 st International Conference on Machine
  Learning, Vienna, Austria. PMLR 235, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily
  Complex Proofs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve arithmetic word problems with high
accuracy, but little is known about how well they generalize to more complex
problems. This is difficult to study, as (i) much of the available evaluation
data has already been seen by the most capable models during training, and (ii)
existing benchmarks do not capture how problem proofs may be arbitrarily
complex in various ways. In this paper, we present a data-generation framework
for evaluating LLMs on problems with arbitrarily complex arithmetic proofs,
called MathGAP. MathGAP generates problem statements and chain-of-thought
reasoning traces according to specifications about their arithmetic proof
structure, enabling systematic studies on easy-to-hard generalization with
respect to complexity of proof trees. Using MathGAP, we find that LLMs show a
significant decrease in performance as proofs get deeper and wider. This effect
is more pronounced in complex, nonlinear proof structures, which are
challenging even for the most capable models. The models are also sensitive to
simple changes in sentence ordering. However, they remain capable of solving
some complex problems, suggesting that reasoning generalization is noisy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RASPNet: A Benchmark <span class="highlight-title">Dataset</span> for Radar Adaptive Signal Processing
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Venkatasubramanian, Bosung Kang, Ali Pezeshki, Muralidhar Rangaswamy, Vahid Tarokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a large-scale dataset for radar adaptive signal processing (RASP)
applications to support the development of data-driven models within the
adaptive radar community. The dataset, RASPNet, exceeds 16 TB in size and
comprises 100 realistic scenarios compiled over a variety of topographies and
land types from across the contiguous United States. For each scenario, RASPNet
consists of 10,000 clutter realizations from an airborne radar setting, which
can be used to benchmark radar and complex-valued learning algorithms. RASPNet
intends to fill a prominent gap in the availability of a large-scale, realistic
dataset that standardizes the evaluation of adaptive radar processing
techniques and complex-valued neural networks. We outline its construction,
organization, and several applications, including a transfer learning example
to demonstrate how RASPNet can be used for realistic adaptive radar processing
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise
  Sufficient Reasons <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahaf Bassan, Ron Eliav, Shlomit Gur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  *Minimal sufficient reasons* represent a prevalent form of explanation - the
smallest subset of input features which, when held constant at their
corresponding values, ensure that the prediction remains unchanged. Previous
*post-hoc* methods attempt to obtain such explanations but face two main
limitations: (1) Obtaining these subsets poses a computational challenge,
leading most scalable methods to converge towards suboptimal, less meaningful
subsets; (2) These methods heavily rely on sampling out-of-distribution input
assignments, potentially resulting in counterintuitive behaviors. To tackle
these limitations, we propose in this work a self-supervised training approach,
which we term *sufficient subset training* (SST). Using SST, we train models to
generate concise sufficient reasons for their predictions as an integral part
of their output. Our results indicate that our framework produces succinct and
faithful subsets substantially more efficiently than competing post-hoc
methods, while maintaining comparable predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Neural Networks on Data Sources with Unknown Reliability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02895v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02895v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Capstick, Francesca Palermo, Tianyu Cui, Payam Barnaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When data is generated by multiple sources, conventional training methods
update models assuming equal reliability for each source and do not consider
their individual data quality. However, in many applications, sources have
varied levels of reliability that can have negative effects on the performance
of a neural network. A key issue is that often the quality of the data for
individual sources is not known during training. Previous methods for training
models in the presence of noisy data do not make use of the additional
information that the source label can provide. Focusing on supervised learning,
we aim to train neural networks on each data source for a number of steps
proportional to the source's estimated reliability by using a dynamic
re-weighting strategy motivated by likelihood tempering. This way, we allow
training on all sources during the warm-up and reduce learning on less reliable
sources during the final training stages, when it has been shown that models
overfit to noise. We show through diverse experiments that this can
significantly improve model performance when trained on mixtures of reliable
and unreliable data sources, and maintain performance when models are trained
on reliable sources only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Explanations Through Probabilistic Self-Explainable
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Vadillo, Roberto Santana, Jose A. Lozano, Marta Kwiatkowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of transparency of Deep Neural Networks continues to be a limitation
that severely undermines their reliability and usage in high-stakes
applications. Promising approaches to overcome such limitations are
Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions
rely on the similarity between the input at hand and a set of prototypical
representations of the output classes, offering therefore a deep, yet
transparent-by-design, architecture. In this paper, we introduce a
probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point
estimates for the prototypes with probability distributions over their values.
This provides not only a more flexible framework for an end-to-end learning of
prototypes, but can also capture the explanatory uncertainty of the model,
which is a missing feature in previous approaches. In addition, since the
prototypes determine both the explanation and the prediction, Prob-PSENNs allow
us to detect when the model is making uninformed or uncertain predictions, and
to obtain valid explanations for them. Our experiments demonstrate that
Prob-PSENNs provide more meaningful and robust explanations than their
non-probabilistic counterparts, while remaining competitive in terms of
predictive performance, thus enhancing the explainability and reliability of
the models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Devil is in the Prompts: De-Identification Traces Enhance
  Memorization Risks in Synthetic Chest X-Ray Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, particularly text-to-image (T2I) diffusion models, play a
crucial role in medical image analysis. However, these models are prone to
training data memorization, posing significant risks to patient privacy.
Synthetic chest X-ray generation is one of the most common applications in
medical image analysis with the MIMIC-CXR dataset serving as the primary data
repository for this task. This study presents the first systematic attempt to
identify prompts and text tokens in MIMIC-CXR that contribute the most to
training data memorization. Our analysis reveals two unexpected findings: (1)
prompts containing traces of de-identification procedures (markers introduced
to hide Protected Health Information) are the most memorized, and (2) among all
tokens, de-identification markers contribute the most towards memorization.
This highlights a broader issue with the standard anonymization practices and
T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time
memorization mitigation strategies are ineffective and fail to sufficiently
reduce the model's reliance on memorized text tokens. On this front, we propose
actionable strategies for different stakeholders to enhance privacy and improve
the reliability of generative models in medical imaging. Finally, our results
provide a foundation for future work on developing and benchmarking
memorization mitigation techniques for synthetic chest X-ray generation using
the MIMIC-CXR dataset. The anonymized code is available at
https://anonymous.4open.science/r/diffusion_memorization-8011/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilong Zhao, Han Wu, Aad Van Moorsel, Lydia Y. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the application of Vertical Federated Learning (VFL) to
generate synthetic tabular data using Generative Adversarial Networks (GANs).
VFL is a collaborative approach to train machine learning models among distinct
tabular data holders, such as financial institutions, who possess disjoint
features for the same group of customers. In this paper we introduce the VT-GAN
framework, Vertical federated Tabular GAN, and demonstrate that VFL can be
successfully used to implement GANs for distributed tabular data in
privacy-preserving manner, with performance close to centralized GANs that
assume shared data. We make design choices with respect to the distribution of
GAN generator and discriminator models and introduce a training-with-shuffling
technique so that no party can reconstruct training data from the GAN
conditional vector. The paper presents (1) an implementation of VT-GAN, (2) a
detailed quality evaluation of the VT-GAN-generated synthetic data, (3) an
overall scalability examination of VT-GAN framework, (4) a security analysis on
VT-GAN's robustness against Membership Inference Attack with different settings
of Differential Privacy, for a range of datasets with diverse distribution
characteristics. Our results demonstrate that VT-GAN can consistently generate
high-fidelity synthetic tabular data of comparable quality to that generated by
a centralized GAN algorithm. The difference in machine learning utility can be
as low as 2.7%, even under extremely imbalanced data distributions across
clients or with different numbers of clients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Regularized Newton Method for Nonconvex Optimization with Global and
  Local Complexity Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Jintao Xu, Chenglong Bao, Chao Ding, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding an $\epsilon$-stationary point of a
nonconvex function with a Lipschitz continuous Hessian and propose a quadratic
regularized Newton method incorporating a new class of regularizers constructed
from the current and previous gradients. The method leverages a recently
developed linear conjugate gradient approach with a negative curvature monitor
to solve the regularized Newton equation. Notably, our algorithm is adaptive,
requiring no prior knowledge of the Lipschitz constant of the Hessian, and
achieves a global complexity of $O(\epsilon^{-\frac{3}{2}}) + \tilde O(1)$ in
terms of the second-order oracle calls, and $\tilde O(\epsilon^{-\frac{7}{4}})$
for Hessian-vector products, respectively. Moreover, when the iterates converge
to a point where the Hessian is positive definite, the method exhibits
quadratic local convergence. Preliminary numerical results illustrate the
competitiveness of our algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Programming Every Example: Lifting Pre-training Data Quality Like
  Experts at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model pre-training has traditionally relied on human experts
to craft heuristics for improving the corpora quality, resulting in numerous
rules developed to date. However, these rules lack the flexibility to address
the unique characteristics of individual example effectively. Meanwhile,
applying tailored rules to every example is impractical for human experts. In
this paper, we demonstrate that even small language models, with as few as 0.3B
parameters, can exhibit substantial data refining capabilities comparable to
those of human experts. We introduce Programming Every Example (ProX), a novel
framework that treats data refinement as a programming task, enabling models to
refine corpora by generating and executing fine-grained operations, such as
string normalization, for each individual example at scale. Experimental
results show that models pre-trained on ProX-curated data outperform either
original data or data filtered by other selection methods by more than 2%
across various downstream benchmarks. Its effectiveness spans various model
sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,
FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in
domain-specific continual pre-training: without domain specific design, models
trained on OpenWebMath refined by ProX outperform human-crafted rule-based
methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for
Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable
to models like Llemma-7B trained on 200B tokens. Further analysis highlights
that ProX significantly saves training FLOPs, offering a promising path for
efficient LLM pre-training. We are open-sourcing ProX with >500B corpus,
models, and sharing all training and implementation details for reproducible
research and future innovation. Code: https://github.com/GAIR-NLP/ProX
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 13 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Anomaly and Out-of-Distribution Detection: A
  <span class="highlight-title">Survey</span> <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01980v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01980v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyao Xu, Kaize Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting anomalies or out-of-distribution (OOD) samples is critical for
maintaining the reliability and trustworthiness of machine learning systems.
Recently, Large Language Models (LLMs) have demonstrated their effectiveness
not only in natural language processing but also in broader applications due to
their advanced comprehension and generative capabilities. The integration of
LLMs into anomaly and OOD detection marks a significant shift from the
traditional paradigm in the field. This survey focuses on the problem of
anomaly and OOD detection under the context of LLMs. We propose a new taxonomy
to categorize existing approaches into two classes based on the role played by
LLMs. Following our proposed taxonomy, we further discuss the related work
under each of the categories and finally discuss potential challenges and
directions for future research in this field. We also provide an up-to-date
reading list of relevant papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SequentialBreak: Large Language Models Can be Fooled by Embedding
  Jailbreak Prompts into Sequential Prompt Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bijoy Ahmed Saiem, MD Sadik Hossain Shanto, Rakib Ahsan, Md Rafi ur Rashid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the integration of the Large Language Models (LLMs) into various
applications increases, so does their susceptibility to misuse, raising
significant security concerns. Numerous jailbreak attacks have been proposed to
assess the security defense of LLMs. Current jailbreak attacks mainly rely on
scenario camouflage, prompt obfuscation, prompt optimization, and prompt
iterative optimization to conceal malicious prompts. In particular, sequential
prompt chains in a single query can lead LLMs to focus on certain prompts while
ignoring others, facilitating context manipulation. This paper introduces
SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We
discuss several scenarios, not limited to examples like Question Bank, Dialog
Completion, and Game Environment, where the harmful prompt is embedded within
benign ones that can fool LLMs into generating harmful responses. The distinct
narrative structures of these scenarios show that SequentialBreak is flexible
enough to adapt to various prompt formats beyond those discussed. Extensive
experiments demonstrate that SequentialBreak uses only a single query to
achieve a substantial gain of attack success rate over existing baselines
against both open-source and closed-source models. Through our research, we
highlight the urgent need for more robust and resilient safeguards to enhance
LLM security and prevent potential misuse. All the result files and website
associated with this research are available in this GitHub repository:
https://anonymous.4open.science/r/JailBreakAttack-4F3B/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strada-LLM: <span class="highlight-title">Graph</span> LLM for traffic prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mohamad Moghadas, Yangxintong Lyu, Bruno Cornelis, Alexandre Alahi, Adrian Munteanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic prediction is a vital component of intelligent transportation
systems. By reasoning about traffic patterns in both the spatial and temporal
dimensions, accurate and interpretable predictions can be provided. A
considerable challenge in traffic prediction lies in handling the diverse data
distributions caused by vastly different traffic conditions occurring at
different locations. LLMs have been a dominant solution due to their remarkable
capacity to adapt to new datasets with very few labeled data samples, i.e.,
few-shot adaptability. However, existing forecasting techniques mainly focus on
extracting local graph information and forming a text-like prompt, leaving LLM-
based traffic prediction an open problem. This work presents a probabilistic
LLM for traffic forecasting with three highlights. We propose a graph-aware LLM
for traffic prediction that considers proximal traffic information.
Specifically, by considering the traffic of neighboring nodes as covariates,
our model outperforms the corresponding time-series LLM. Furthermore, we adopt
a lightweight approach for efficient domain adaptation when facing new data
distributions in few-shot fashion. The comparative experiment demonstrates the
proposed method outperforms the state-of-the-art LLM-based methods and the
traditional GNN- based supervised approaches. Furthermore, Strada-LLM can be
easily adapted to different LLM backbones without a noticeable performance
drop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The reviewers decided to reject it. After getting the reviews, we
  wanted to study more.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Critical Look At Tokenwise Reward-Guided Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Ruotian Wu, Julia Grosse, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can be improved by aligning with human
preferences through fine-tuning -- the so-called reinforcement learning from
human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive
for many users. Due to their ability to bypass LLM fine-tuning, prediction-time
tokenwise reward-guided text generation (RGTG) methods have recently been
proposed. They use a reward model trained on full sequences to score partial
sequences during decoding in a bid to steer the generation towards sequences
with high rewards. However, these methods have so far been only heuristically
motivated and poorly analyzed. In this work, we show that reward models trained
on full sequences are not compatible with scoring partial sequences. To
alleviate this issue, we propose to train a Bradley-Terry reward model on
partial sequences explicitly, and autoregressively sample from the implied
tokenwise policy during decoding time. We study the properties of this reward
model and the resulting policy: we show that this policy is proportional to the
ratio of two distinct RLHF policies. Our simple approach outperforms previous
RGTG methods and performs similarly to strong offline baselines without
large-scale LLM finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial-aware decision-making with ring attractors in reinforcement
  learning systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Negre Saura, Richard Allmendinger, Wei Pan, Theodore Papamarkou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of ring attractors, a mathematical model
inspired by neural circuit dynamics, into the Reinforcement Learning (RL)
action selection process. Serving as specialized brain-inspired structures that
encode spatial information and uncertainty, ring attractors offer a
biologically plausible mechanism to improve learning speed and accuracy in RL.
They do so by explicitly encoding the action space, facilitating the
organization of neural activity, and enabling the distribution of spatial
representations across the neural network in the context of Deep Reinforcement
Learning (DRL). For example, preserving the continuity between rotation angles
in robotic control or adjacency between tactical moves in game-like
environments. The application of ring attractors in the action selection
process involves mapping actions to specific locations on the ring and decoding
the selected action based on neural activity. We investigate the application of
ring attractors by both building an exogenous model and integrating them as
part of DRL agents. Our approach significantly improves state-of-the-art
performance on the Atari 100k benchmark, achieving a 53\% increase in
performance across selected state-of-the-art baselines. Codebase available at
https://anonymous.4open.science/r/RA_RL-8026.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary
  Directed Differential with Normalized Density and Self-Adaption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00677v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00677v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Density-based clustering is the most popular clustering algorithm since it
can identify clusters of arbitrary shape as long as they are separated by
low-density regions. However, a high-density region that is not separated by
low-density ones might also have different structures belonging to multiple
clusters. As far as we know, all previous density-based clustering algorithms
fail to detect such structures. In this paper, we provide a novel density-based
clustering scheme to address this problem. It is the rst clustering algorithm
that can detect meticulous structures in a high-density region that is not
separated by low-density ones and thus extends the range of applications of
clustering. The algorithm employs secondary directed differential, hierarchy,
normalized density, as well as the self-adaption coefficient, called Structure
Detecting Cluster by Hierarchical Secondary Directed Differential with
Normalized Density and Self-Adaption, dubbed SDC-HSDD-NDSA. Experiments on
synthetic and real datasets are implemented to verify the effectiveness,
robustness, and granularity independence of the algorithm, and the scheme is
compared to unsupervised schemes in the Python package Scikit-learn. Results
demonstrate that our algorithm outperforms previous ones in many situations,
especially significantly when clusters have regular internal structures. For
example, averaging over the eight noiseless synthetic datasets with structures
employing ARI and NMI criteria, previous algorithms obtain scores below 0.6 and
0.7, while the presented algorithm obtains scores higher than 0.9 and 0.95,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span>-based Retrieval Augmented Generation for Dynamic Few-shot Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Haoyang Li, Fei Teng, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span> Foundation Models for Recommendation: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) serve as a fundamental tool for navigating the vast
expanse of online information, with deep learning advancements playing an
increasingly important role in improving ranking accuracy. Among these, graph
neural networks (GNNs) excel at extracting higher-order structural information,
while large language models (LLMs) are designed to process and comprehend
natural language, making both approaches highly effective and widely adopted.
Recent research has focused on graph foundation models (GFMs), which integrate
the strengths of GNNs and LLMs to model complex RS problems more efficiently by
leveraging the graph-based structure of user-item relationships alongside
textual understanding. In this survey, we provide a comprehensive overview of
GFM-based RS technologies by introducing a clear taxonomy of current
approaches, diving into methodological details, and highlighting key challenges
and future directions. By synthesizing recent advancements, we aim to offer
valuable insights into the evolving landscape of GFM-based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stabilizing and Solving Inverse Problems using Data and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Burman, Mats G. Larson, Karl Larsson, Carl Lundholm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an inverse problem involving the reconstruction of the solution
to a nonlinear partial differential equation (PDE) with unknown boundary
conditions. Instead of direct boundary data, we are provided with a large
dataset of boundary observations for typical solutions (collective data) and a
bulk measurement of a specific realization. To leverage this collective data,
we first compress the boundary data using proper orthogonal decomposition (POD)
in a linear expansion. Next, we identify a possible nonlinear low-dimensional
structure in the expansion coefficients using an autoencoder, which provides a
parametrization of the dataset in a lower-dimensional latent space. We then
train an operator network to map the expansion coefficients representing the
boundary data to the finite element solution of the PDE. Finally, we connect
the autoencoder's decoder to the operator network which enables us to solve the
inverse problem by optimizing a data-fitting term over the latent space. We
analyze the underlying stabilized finite element method in the linear setting
and establish an optimal error estimate in the $H^1$-norm. The nonlinear
problem is then studied numerically, demonstrating the effectiveness of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shield Synthesis for LTL Modulo Theories <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andoni Rodriguez, Guy Amir, Davide Corsi, Cesar Sanchez, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Machine Learning (ML) models have achieved remarkable
success in various domains. However, these models also tend to demonstrate
unsafe behaviors, precluding their deployment in safety-critical systems. To
cope with this issue, ample research focuses on developing methods that
guarantee the safe behaviour of a given ML model. A prominent example is
shielding which incorporates an external component (a ``shield'') that blocks
unwanted behavior. Despite significant progress, shielding suffers from a main
setback: it is currently geared towards properties encoded solely in
propositional logics (e.g., LTL) and is unsuitable for richer logics. This, in
turn, limits the widespread applicability of shielding in many real-world
systems. In this work, we address this gap, and extend shielding to LTL modulo
theories, by building upon recent advances in reactive synthesis modulo
theories. This allowed us to develop a novel approach for generating shields
conforming to complex safety specifications in these more expressive, logics.
We evaluated our shields and demonstrate their ability to handle rich data with
temporal dynamics. To the best of our knowledge, this is the first approach for
synthesizing shields for such expressivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Mode Connectivity in Differentiable Tree Ensembles <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuichi Kanoh, Mahito Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Mode Connectivity (LMC) refers to the phenomenon that performance
remains consistent for linearly interpolated models in the parameter space. For
independently optimized model pairs from different random initializations,
achieving LMC is considered crucial for understanding the stable success of the
non-convex optimization in modern machine learning models and for facilitating
practical parameter-based operations such as model merging. While LMC has been
achieved for neural networks by considering the permutation invariance of
neurons in each hidden layer, its attainment for other models remains an open
question. In this paper, we first achieve LMC for soft tree ensembles, which
are tree-based differentiable models extensively used in practice. We show the
necessity of incorporating two invariances: subtree flip invariance and
splitting order invariance, which do not exist in neural networks but are
inherent to tree architectures, in addition to permutation invariance of trees.
Moreover, we demonstrate that it is even possible to exclude such additional
invariances while keeping LMC by designing decision list-based tree
architectures, where such invariances do not exist by definition. Our findings
indicate the significance of accounting for architecture-specific invariances
in achieving LMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multifidelity Simulation-based Inference for Computationally Expensive
  Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia N. Krouglova, Hayden R. Johnson, Basile Confavreux, Michael Deistler, Pedro J. Gonçalves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across many domains of science, stochastic models are an essential tool to
understand the mechanisms underlying empirically observed data. Models can be
of different levels of detail and accuracy, with models of high-fidelity (i.e.,
high accuracy) to the phenomena under study being often preferable. However,
inferring parameters of high-fidelity models via simulation-based inference is
challenging, especially when the simulator is computationally expensive. We
introduce MF-NPE, a multifidelity approach to neural posterior estimation that
leverages inexpensive low-fidelity simulations to infer parameters of
high-fidelity simulators within a limited simulation budget. MF-NPE performs
neural posterior estimation with limited high-fidelity resources by virtue of
transfer learning, with the ability to prioritize individual observations using
active learning. On one statistical task with analytical ground-truth and two
real-world tasks, MF-NPE shows comparable performance to current approaches
while requiring up to two orders of magnitude fewer high-fidelity simulations.
Overall, MF-NPE opens new opportunities to perform efficient Bayesian inference
on computationally expensive simulators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero Shot Time Series Forecasting Using Kolmogorov Arnold Networks <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhiroop Bhattacharya, Nandinee Haq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate energy price forecasting is crucial for participants in day-ahead
energy markets, as it significantly influences their decision-making processes.
While machine learning-based approaches have shown promise in enhancing these
forecasts, they often remain confined to the specific markets on which they are
trained, thereby limiting their adaptability to new or unseen markets. In this
paper, we introduce a cross-domain adaptation model designed to forecast energy
prices by learning market-invariant representations across different markets
during the training phase. We propose a doubly residual N-BEATS network with
Kolmogorov Arnold networks at its core for time series forecasting. These
networks, grounded in the Kolmogorov-Arnold representation theorem, offer a
powerful way to approximate multivariate continuous functions. The cross domain
adaptation model was generated with an adversarial framework. The model's
effectiveness was tested in predicting day-ahead electricity prices in a zero
shot fashion. In comparison with baseline models, our proposed framework shows
promising results. By leveraging the Kolmogorov-Arnold networks, our model can
potentially enhance its ability to capture complex patterns in energy price
data, thus improving forecast accuracy across diverse market conditions. This
addition not only enriches the model's representational capacity but also
contributes to a more robust and flexible forecasting tool adaptable to various
energy markets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published In: 2024 NeurIPS Workshop on Time Series in the Age of
  Large Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The shape of the <span class="highlight-title">brain</span>'s connections is predictive of cognitive
  performance: an explainable machine learning study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yui Lo, Yuqian Chen, Dongnan Liu, Wan Liu, Leo Zekelman, Jarrett Rushmore, Fan Zhang, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J. O'Donnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shape of the brain's white matter connections is relatively unexplored in
diffusion MRI tractography analysis. While it is known that tract shape varies
in populations and across the human lifespan, it is unknown if the variability
in dMRI tractography-derived shape may relate to the brain's functional
variability across individuals. This work explores the potential of leveraging
tractography fiber cluster shape measures to predict subject-specific cognitive
performance. We implement machine learning models to predict individual
cognitive performance scores. We study a large-scale database from the HCP-YA
study. We apply an atlas-based fiber cluster parcellation to the dMRI
tractography of each individual. We compute 15 shape, microstructure, and
connectivity features for each fiber cluster. Using these features as input, we
train a total of 210 models to predict 7 different NIH Toolbox cognitive
performance assessments. We apply an explainable AI technique, SHAP, to assess
the importance of each fiber cluster for prediction. Our results demonstrate
that shape measures are predictive of individual cognitive performance. The
studied shape measures, such as irregularity, diameter, total surface area,
volume, and branch volume, are as effective for prediction as microstructure
and connectivity measures. The overall best-performing feature is a shape
feature, irregularity, which describes how different a cluster's shape is from
an idealized cylinder. Further interpretation using SHAP values suggest that
fiber clusters with features highly predictive of cognitive ability are
widespread throughout the brain, including fiber clusters from the superficial
association, deep association, cerebellar, striatal, and projection pathways.
This study demonstrates the strong potential of shape descriptors to enhance
the study of the brain's white matter and its relationship to cognitive
function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by Human Brain Mapping for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goedel-Prover: A Frontier Model for Open-Source Automated Theorem
  Proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Goedel-Prover, an open-source large language model (LLM) that
achieves the state-of-the-art (SOTA) performance in automated formal proof
generation for mathematical problems. The key challenge in this field is the
scarcity of formalized math statements and proofs, which we tackle in the
following ways. We train statement formalizers to translate the natural
language math problems from Numina into formal language (Lean 4), creating a
dataset of 1.64 million formal statements. LLMs are used to check that the
formal statements accurately preserve the content of the original natural
language problems. We then iteratively build a large dataset of formal proofs
by training a series of provers. Each prover succeeds in proving many
statements that the previous ones could not, and these new proofs are added to
the training set for the next prover. Despite using only supervised
fine-tuning, our final prover significantly outperforms the previous best
open-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.
On the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),
surpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover
successfully solves 7 problems (Pass@512), ranking first on the leaderboard.
Furthermore, it generates 29.7K formal proofs for Lean Workbook problems,
nearly doubling the 15.7K produced by earlier works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eidetic Learning: an Efficient and Provable Solution to Catastrophic
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Dronen, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting -- the phenomenon of a neural network learning a task
t1 and losing the ability to perform it after being trained on some other task
t2 -- is a long-standing problem for neural networks [McCloskey and Cohen,
1989]. We present a method, Eidetic Learning, that provably solves catastrophic
forgetting. A network trained with Eidetic Learning -- here, an EideticNet --
requires no rehearsal or replay. We consider successive discrete tasks and show
how at inference time an EideticNet automatically routes new instances without
auxiliary task information. An EideticNet bears a family resemblance to the
sparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network
capacity is partitioned across tasks and the network itself performs
data-conditional routing. An EideticNet is easy to implement and train, is
efficient, and has time and space complexity linear in the number of
parameters. The guarantee of our method holds for normalization layers of
modern neural networks during both pre-training and fine-tuning. We show with a
variety of network architectures and sets of tasks that EideticNets are immune
to forgetting. While the practical benefits of EideticNets are substantial, we
believe they can be benefit practitioners and theorists alike. The code for
training EideticNets is available at
https://github.com/amazon-science/eideticnet-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures; code is available at
  https://github.com/amazon-science/eideticnet-training</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Deep Learning finally better than Decision Trees on Tabular Data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guri Zabërgja, Arlind Kadra, Christian M. M. Frey, Josif Grabocka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is a ubiquitous data modality due to its versatility and ease of
use in many real-world applications. The predominant heuristics for handling
classification tasks on tabular data rely on classical machine learning
techniques, as the superiority of deep learning models has not yet been
demonstrated. This raises the question of whether new deep learning paradigms
can surpass classical approaches. Recent studies on tabular data offer a unique
perspective on the limitations of neural networks in this domain and highlight
the superiority of gradient boosted decision trees (GBDTs) in terms of
scalability and robustness across various datasets. However, novel foundation
models have not been thoroughly assessed regarding quality or fairly compared
to existing methods for tabular classification. Our study categorizes ten
state-of-the-art neural models based on their underlying learning paradigm,
demonstrating specifically that meta-learned foundation models outperform GBDTs
in small data regimes. Although dataset-specific neural networks generally
outperform LLM-based tabular classifiers, they are surpassed by an AutoML
library which exhibits the best performance but at the cost of higher
computational demands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Channel Distinguishability in Local Neighborhoods of the Model
  Space in Quantum Neural Networks <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Herbst, Sandeep Suresh Cranganore, Vincenzo De Maio, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing interest in Quantum Machine Learning, Quantum Neural
Networks (QNNs) have emerged and gained significant attention. These models
have, however, been shown to be notoriously difficult to train, which we
hypothesize is partially due to the architectures, called ansatzes, that are
hardly studied at this point. Therefore, in this paper, we take a step back and
analyze ansatzes. We initially consider their expressivity, i.e., the space of
operations they are able to express, and show that the closeness to being a
2-design, the primarily used measure, fails at capturing this property. Hence,
we look for alternative ways to characterize ansatzes by considering the local
neighborhood of the model space, in particular, analyzing model
distinguishability upon small perturbation of parameters. We derive an upper
bound on their distinguishability, showcasing that QNNs with few parameters are
hardly discriminable upon update. Our numerical experiments support our bounds
and further indicate that there is a significant degree of variability, which
stresses the need for warm-starting or clever initialization. Altogether, our
work provides an ansatz-centric perspective on training dynamics and
difficulties in QNNs, ultimately suggesting that iterative training of small
quantum models may not be effective, which contrasts their initial motivation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025 (https://openreview.net/forum?id=gDcL7cgZBt)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EQ-VAE: Equivariance Regularized Latent Space for Improved Generative
  Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent generative models have emerged as a leading approach for high-quality
image synthesis. These models rely on an autoencoder to compress images into a
latent space, followed by a generative model to learn the latent distribution.
We identify that existing autoencoders lack equivariance to semantic-preserving
transformations like scaling and rotation, resulting in complex latent spaces
that hinder generative performance. To address this, we propose EQ-VAE, a
simple regularization approach that enforces equivariance in the latent space,
reducing its complexity without degrading reconstruction quality. By finetuning
pre-trained autoencoders with EQ-VAE, we enhance the performance of several
state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,
achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.
EQ-VAE is compatible with both continuous and discrete autoencoders, thus
offering a versatile enhancement for a wide range of latent generative models.
Project page and code: https://eq-vae.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement
  Learning <span class="chip">ICML
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional robust methods in multi-agent reinforcement learning (MARL) often
struggle against coordinated adversarial attacks in cooperative scenarios. To
address this limitation, we propose the Wolfpack Adversarial Attack framework,
inspired by wolf hunting strategies, which targets an initial agent and its
assisting agents to disrupt cooperation. Additionally, we introduce the
Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust
MARL policies to defend against the proposed Wolfpack attack by fostering
system-wide collaboration. Experimental results underscore the devastating
impact of the Wolfpack attack and the significant robustness improvements
achieved by WALL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main, 21 pages appendix with reference. Submitted to ICML
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Federated Learning with Tabular Data Silos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achmad Ginanjar, Xue Li, Wen Hua, Jiaming Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from vertical partitioned data silos is challenging due to the
segmented nature of data, sample misalignment, and strict privacy concerns.
Federated learning has been proposed as a solution. However, sample
misalignment across silos often hinders optimal model performance and suggests
data sharing within the model, which breaks privacy. Our proposed solution is
Contrastive Federated Learning with Tabular Data Silos (CFL), which offers a
solution for data silos with sample misalignment without the need for sharing
original or representative data to maintain privacy. CFL begins with local
acquisition of contrastive representations of the data within each silo and
aggregates knowledge from other silos through the federated learning algorithm.
Our experiments demonstrate that CFL solves the limitations of existing
algorithms for data silos and outperforms existing tabular contrastive
learning. CFL provides performance improvements without loosening privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 Pages. 1stversion was submitted on Artificial Intelligence
  Journal, Jan 29, 2024, ARTINT-D-24-00098</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Representations and Interventions in Time Series Foundation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Wiliński, Mononito Goswami, Nina Żukowska, Willa Potosnak, Artur Dubrawski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series foundation models (TSFMs) promise to be powerful tools for a wide
range of applications. However, their internal representations and learned
concepts are still not well understood. In this study, we investigate the
structure and redundancy of representations across various TSFMs, examining the
self-similarity of model layers within and across different model sizes. This
analysis reveals block-like redundancy in the representations, which can be
utilized for informed pruning to improve inference speed and efficiency.
Additionally, we explore the concepts learned by these models - such as
periodicity and trends - and how these can be manipulated through latent space
steering to influence model behavior. Our experiments show that steering
interventions can introduce new features, e.g., adding periodicity or trends to
signals that initially lacked them. These findings underscore the value of
representational analysis for optimizing models and demonstrate how conceptual
steering offers new possibilities for more controlled and efficient time series
analysis with TSFMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MassSpecGym: A benchmark for the discovery and identification of
  molecules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23326v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23326v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Bushuiev, Anton Bushuiev, Niek F. de Jonge, Adamo Young, Fleming Kretschmer, Raman Samusevich, Janne Heirman, Fei Wang, Luke Zhang, Kai Dührkop, Marcus Ludwig, Nils A. Haupt, Apurva Kalia, Corinna Brungs, Robin Schmid, Russell Greiner, Bo Wang, David S. Wishart, Li-Ping Liu, Juho Rousu, Wout Bittremieux, Hannes Rost, Tytus D. Mak, Soha Hassoun, Florian Huber, Justin J. J. van der Hooft, Michael A. Stravs, Sebastian Böcker, Josef Sivic, Tomáš Pluskal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery and identification of molecules in biological and environmental
samples is crucial for advancing biomedical and chemical sciences. Tandem mass
spectrometry (MS/MS) is the leading technique for high-throughput elucidation
of molecular structures. However, decoding a molecular structure from its mass
spectrum is exceptionally challenging, even when performed by human experts. As
a result, the vast majority of acquired MS/MS spectra remain uninterpreted,
thereby limiting our understanding of the underlying (bio)chemical processes.
Despite decades of progress in machine learning applications for predicting
molecular structures from MS/MS spectra, the development of new methods is
severely hindered by the lack of standard datasets and evaluation protocols. To
address this problem, we propose MassSpecGym -- the first comprehensive
benchmark for the discovery and identification of molecules from MS/MS data.
Our benchmark comprises the largest publicly available collection of
high-quality labeled MS/MS spectra and defines three MS/MS annotation
challenges: de novo molecular structure generation, molecule retrieval, and
spectrum simulation. It includes new evaluation metrics and a
generalization-demanding data split, therefore standardizing the MS/MS
annotation tasks and rendering the problem accessible to the broad machine
learning community. MassSpecGym is publicly available at
https://github.com/pluskal-lab/MassSpecGym.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CR-CTC: Consistency regularization on CTC for improved speech
  recognition <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05101v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05101v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectionist Temporal Classification (CTC) is a widely used method for
automatic speech recognition (ASR), renowned for its simplicity and
computational efficiency. However, it often falls short in recognition
performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC),
which enforces consistency between two CTC distributions obtained from
different augmented views of the input speech mel-spectrogram. We provide
in-depth insights into its essential behaviors from three perspectives: 1) it
conducts self-distillation between random pairs of sub-models that process
different augmented views; 2) it learns contextual representation through
masked prediction for positions within time-masked regions, especially when we
increase the amount of time masking; 3) it suppresses the extremely peaky CTC
distributions, thereby reducing overfitting and improving the generalization
ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech
datasets demonstrate the effectiveness of our CR-CTC. It significantly improves
the CTC performance, achieving state-of-the-art results comparable to those
attained by transducer or systems combining CTC and attention-based
encoder-decoder (CTC/AED). We release our code at
https://github.com/k2-fsa/icefall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mechanism and Emergence of Stacked Attention Heads in Multi-Layer
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiberiu Musat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, I introduce the retrieval problem, a simple yet common
reasoning task that can be solved only by transformers with a minimum number of
layers, which grows logarithmically with the input size. I empirically show
that large language models can solve the task under different prompting
formulations without any fine-tuning. To understand how transformers solve the
retrieval problem, I train several transformers on a minimal formulation.
Successful learning occurs only under the presence of an implicit curriculum. I
uncover the learned mechanisms by studying the attention maps in the trained
transformers. I also study the training process, uncovering that attention
heads always emerge in a specific sequence guided by the implicit curriculum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Liu, Chen Chen, Lingjuan Lyu, Yaochu Jin, Gang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is notorious for its vulnerability to Byzantine
attacks. Most current Byzantine defenses share a common inductive bias: among
all the gradients, the densely distributed ones are more likely to be honest.
However, such a bias is a poison to Byzantine robustness due to a newly
discovered phenomenon in this paper - gradient skew. We discover that a group
of densely distributed honest gradients skew away from the optimal gradient
(the average of honest gradients) due to heterogeneous data. This gradient skew
phenomenon allows Byzantine gradients to hide within the densely distributed
skewed gradients. As a result, Byzantine defenses are confused into believing
that Byzantine gradients are honest. Motivated by this observation, we propose
a novel skew-aware attack called STRIKE: first, we search for the skewed
gradients; then, we construct Byzantine gradients within the skewed gradients.
Experiments on three benchmark datasets validate the effectiveness of our
attack
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Consolidated Volatility Prediction with Back Propagation Neural
  Network and Genetic Algorithm <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07223v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07223v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zong Ke, Jingyu Xu, Zizhou Zhang, Yu Cheng, Wenjun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a unique approach with AI algorithms to predict emerging
stock markets volatility. Traditionally, stock volatility is derived from
historical volatility,Monte Carlo simulation and implied volatility as well. In
this paper, the writer designs a consolidated model with back-propagation
neural network and genetic algorithm to predict future volatility of emerging
stock markets and found that the results are quite accurate with low errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, 1 table, The paper will be published by IEEE on
  conference: 2024 3rd International Conference on Image Processing, Computer
  Vision and Machine Learning (ICICML 2024) (V2)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning with Strategic Selection and Forgetting for Network
  Intrusion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinchen Zhang, Running Zhao, Zhihan Jiang, Handi Chen, Yulong Ding, Edith C. H. Ngai, Shuang-Hua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrusion Detection Systems (IDS) are crucial for safeguarding digital
infrastructure. In dynamic network environments, both threat landscapes and
normal operational behaviors are constantly changing, resulting in concept
drift. While continuous learning mitigates the adverse effects of concept
drift, insufficient attention to drift patterns and excessive preservation of
outdated knowledge can still hinder the IDS's adaptability. In this paper, we
propose SSF (Strategic Selection and Forgetting), a novel continual learning
method for IDS, providing continuous model updates with a constantly refreshed
memory buffer. Our approach features a strategic sample selection algorithm to
select representative new samples and a strategic forgetting mechanism to drop
outdated samples. The proposed strategic sample selection algorithm prioritizes
new samples that cause the `drifted' pattern, enabling the model to better
understand the evolving landscape. Additionally, we introduce strategic
forgetting upon detecting significant drift by discarding outdated samples to
free up memory, allowing the incorporation of more recent data. SSF captures
evolving patterns effectively and ensures the model is aligned with the change
of data patterns, significantly enhancing the IDS's adaptability to concept
drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15
datasets demonstrates its superior adaptability to concept drift for network
intrusion detection. The code is released at
https://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE International Conference on Computer Communications
  (INFOCOM) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anti-Forgetting Adaptation for Unsupervised Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Francois Bremond, Nicu Sebe, Shiliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular unsupervised domain adaptive person re-identification (ReID) focuses
on adapting a model from a source domain to a fixed target domain. However, an
adapted ReID model can hardly retain previously-acquired knowledge and
generalize to unseen data. In this paper, we propose a Dual-level Joint
Adaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a
model to new domains without forgetting source domain and each adapted target
domain. We explore the possibility of using prototype and instance-level
consistency to mitigate the forgetting during the adaptation. Specifically, we
store a small number of representative image samples and corresponding cluster
prototypes in a memory buffer, which is updated at each adaptation step. With
the buffered images and prototypes, we regularize the image-to-image similarity
and image-to-prototype similarity to rehearse old knowledge. After the
multi-step adaptation, the model is tested on all seen domains and several
unseen domains to validate the generalization ability of our method. Extensive
experiments demonstrate that our proposed method significantly improves the
anti-forgetting, generalization and backward-compatible ability of an
unsupervised person ReID model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation
  Learning with Visual Observations <span class="chip">ICML
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minung Kim, Kawon Lee, Jungmo Kim, Sungho Choi, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning (IL) enables agents to mimic expert behavior without
reward signals but faces challenges in cross-domain scenarios with
high-dimensional, noisy, and incomplete visual observations. To address this,
we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning
(DIFF-IL), a novel IL method that extracts domain-invariant features from
individual frames and adapts them into sequences to isolate and replicate
expert behaviors. We also introduce a frame-wise time labeling technique to
segment expert behaviors by timesteps and assign rewards aligned with temporal
contexts, enhancing task performance. Experiments across diverse visual
environments demonstrate the effectiveness of DIFF-IL in addressing complex
visual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main, 19 pages appendix with reference. Submitted to ICML
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Diffusion Model-Enabled Low-Latency Semantic Communication in the
  Presence of Semantic Ambiguities and Wireless Channel Noises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06644v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06644v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Pei, Cheng Feng, Ping Wang, Hina Tabassum, Dongyuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL)-based Semantic Communications (SemCom) is becoming
critical to maximize overall efficiency of communication networks.
Nevertheless, SemCom is sensitive to wireless channel uncertainties, source
outliers, and suffer from poor generalization bottlenecks. To address the
mentioned challenges, this paper develops a latent diffusion model-enabled
SemCom system with three key contributions, i.e., i) to handle potential
outliers in the source data, semantic errors obtained by projected gradient
descent based on the vulnerabilities of DL models, are utilized to update the
parameters and obtain an outlier-robust encoder, ii) a lightweight single-layer
latent space transformation adapter completes one-shot learning at the
transmitter and is placed before the decoder at the receiver, enabling
adaptation for out-of-distribution data and enhancing human-perceptual quality,
and iii) an end-to-end consistency distillation (EECD) strategy is used to
distill the diffusion models trained in latent space, enabling deterministic
single or few-step low-latency denoising in various noisy channels while
maintaining high semantic quality. Extensive numerical experiments across
different datasets demonstrate the superiority of the proposed SemCom system,
consistently proving its robustness to outliers, the capability to transmit
data with unknown distributions, and the ability to perform real-time channel
denoising tasks while preserving high human perceptual quality, outperforming
the existing denoising approaches in semantic metrics such as multi-scale
structural similarity index measure (MS-SSIM) and learned perceptual image path
similarity (LPIPS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry-Aware Bayesian Flow Networks for Crystal Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Ruple, Luca Torresi, Henrik Schopmans, Pascal Friederich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of new crystalline materials is essential to scientific and
technological progress. However, traditional trial-and-error approaches are
inefficient due to the vast search space. Recent advancements in machine
learning have enabled generative models to predict new stable materials by
incorporating structural symmetries and to condition the generation on desired
properties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian
Flow Network (BFN) for crystalline material generation that accurately
reproduces the distribution of space groups found in experimentally observed
crystals. SymmBFN substantially improves efficiency, generating stable
structures at least 50 times faster than the next-best method. Furthermore, we
demonstrate its capability for property-conditioned generation, enabling the
design of materials with tailored properties. Our findings establish BFNs as an
effective tool for accelerating the discovery of crystalline materials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Aware Virtual Training: Enhancing Generalization in
  Meta-Reinforcement Learning for Out-of-Distribution Tasks <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongmo Kim, Yisak Park, Minung Kim, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning aims to develop policies that generalize to
unseen tasks sampled from a task distribution. While context-based meta-RL
methods improve task representation using task latents, they often struggle
with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware
Virtual Training (TAVT), a novel algorithm that accurately captures task
characteristics for both training and OOD scenarios using metric-based
representation learning. Our method successfully preserves task characteristics
in virtual tasks and employs a state regularization technique to mitigate
overestimation errors in state-varying environments. Numerical results
demonstrate that TAVT significantly enhances generalization to OOD tasks across
various MuJoCo and MetaWorld environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main paper, 19 pages appendices with reference, Submitted to
  ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bregman firmly nonexpansive proximal operator for baryconvex
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mastane Achab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a generalization of the proximal operator defined through a convex
combination of convex objectives, where the coefficients are updated in a
minimax fashion. We prove that this new operator is Bregman firmly nonexpansive
with respect to a Bregman divergence that combines Euclidean and information
geometries. Finally, we derive the associated continuous flows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10919v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10919v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhao, Tingwei Chen, Zhijie Cai, Xiaoyang Li, Hang Li, Qimei Chen, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Wi-Fi sensing has garnered significant attention due to its
numerous benefits, such as privacy protection, low cost, and penetration
ability. Extensive research has been conducted in this field, focusing on areas
such as gesture recognition, people identification, and fall detection.
However, many data-driven methods encounter challenges related to domain shift,
where the model fails to perform well in environments different from the
training data. One major factor contributing to this issue is the limited
availability of Wi-Fi sensing datasets, which makes models learn excessive
irrelevant information and over-fit to the training set. Unfortunately,
collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a
challenging task. To address this problem, we propose CrossFi, a siamese
network-based approach that excels in both in-domain scenario and cross-domain
scenario, including few-shot, zero-shot scenarios, and even works in few-shot
new-class scenario where testing set contains new categories. The core
component of CrossFi is a sample-similarity calculation network called CSi-Net,
which improves the structure of the siamese network by using an attention
mechanism to capture similarity information, instead of simply calculating the
distance or cosine similarity. Based on it, we develop an extra Weight-Net that
can generate a template for each class, so that our CrossFi can work in
different scenarios. Experimental results demonstrate that our CrossFi achieves
state-of-the-art performance across various scenarios. In gesture recognition
task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%
in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,
and 84.75% in one-shot new-class scenario. The code for our model is publicly
available at https://github.com/RS2002/CrossFi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning
  with Noisy Demonstrations <span class="chip">ICML
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyeon Lee, Sangjun Bae, Yisak Park, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen
tasks but faces challenges in long-horizon environments. Skill-based approaches
tackle this by decomposing state-action sequences into reusable skills and
employing hierarchical decision-making. However, these methods are highly
susceptible to noisy offline demonstrations, resulting in unstable skill
learning and degraded performance. To overcome this, we propose Prioritized
Refinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates
exploration near noisy data to generate online trajectories and combines them
with offline data. Through prioritization, PRISM extracts high-quality data to
learn task-relevant skills effectively. By addressing the impact of noise, our
method ensures stable skill learning and achieves superior performance in
long-horizon tasks, even with noisy and sub-optimal data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main, 19 pages appendix with reference. Submitted to ICML
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analog In-memory Training on General Non-ideal Resistive Elements: The
  Impact of Response Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxian Wu, Quan Xiao, Tayfun Gokmen, Omobayode Fagbohungbe, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the economic and environmental costs of training and deploying large
vision or language models increase dramatically, analog in-memory computing
(AIMC) emerges as a promising energy-efficient solution. However, the training
perspective, especially its training dynamic, is underexplored. In AIMC
hardware, the trainable weights are represented by the conductance of resistive
elements and updated using consecutive electrical pulses. Among all the
physical properties of resistive elements, the response to the pulses directly
affects the training dynamics. This paper first provides a theoretical
foundation for gradient-based training on AIMC hardware and studies the impact
of response functions. We demonstrate that noisy update and asymmetric response
functions negatively impact Analog SGD by imposing an implicit penalty term on
the objective. To overcome the issue, Tiki-Taka, a residual learning algorithm,
converges exactly to a critical point by optimizing a main array and a residual
array bilevelly. The conclusion is supported by simulations validating our
theoretical insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Relative Homology Theory of Representation in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosio Beshkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has proven that the set of maps implemented by neural
networks with a ReLU activation function is identical to the set of piecewise
linear continuous maps. Furthermore, such networks induce a hyperplane
arrangement splitting the input domain into convex polyhedra $G_J$ over which
the network $\Phi$ operates in an affine manner.
  In this work, we leverage these properties to define the equivalence class of
inputs $\sim_\Phi$, which can be split into two sets related to the local rank
of $\Phi_J$ and the intersections $\cap \text{Im}\Phi_{J_i}$. We refer to the
latter as the overlap decomposition $O_\Phi$ and prove that if the
intersections between each polyhedron and the input manifold are convex, the
homology groups of neural representations are isomorphic to relative homology
groups $H_k(\Phi(M)) \simeq H_k(M,O_\Phi)$. This lets us compute Betti numbers
without the choice of an external metric. We develop methods to numerically
compute the overlap decomposition through linear programming and a union-find
algorithm.
  Using this framework, we perform several experiments on toy datasets showing
that, compared to standard persistent homology, our relative homology-based
computation of Betti numbers tracks purely topological rather than geometric
features. Finally, we study the evolution of the overlap decomposition during
training on various classification problems while varying network width and
depth and discuss some shortcomings of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Language Models are RNNs: Balancing Parallelization and
  Expressivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, Jannes Gladrow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models (SSMs) and transformers dominate the language modeling
landscape. However, they are constrained to a lower computational complexity
than classical recurrent neural networks (RNNs), limiting their expressivity.
In contrast, RNNs lack parallelization during training, raising fundamental
questions about the trade off between parallelization and expressivity. We
propose implicit SSMs, which iterate a transformation until convergence to a
fixed point. Theoretically, we show that implicit SSMs implement the non-linear
state-transitions of RNNs. Empirically, we find that only approximate
fixed-point convergence suffices, enabling the design of a scalable training
curriculum that largely retains parallelization, with full convergence required
only for a small subset of tokens. Our approach demonstrates superior
state-tracking capabilities on regular languages, surpassing transformers and
SSMs. We further scale implicit SSMs to natural language reasoning tasks and
pretraining of large-scale language models up to 1.3B parameters on 207B tokens
- representing, to our knowledge, the largest implicit model trained to date.
Notably, our implicit models outperform their explicit counterparts on standard
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Temporal <span class="highlight-title">Graph</span> Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Zihao Zhou, Xianghong Xu, Qian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graph clustering is a complex task that involves discovering
meaningful structures in dynamic graphs where relationships and entities change
over time. Existing methods typically require centralized data collection,
which poses significant privacy and communication challenges. In this work, we
introduce a novel Federated Temporal Graph Clustering (FTGC) framework that
enables decentralized training of graph neural networks (GNNs) across multiple
clients, ensuring data privacy throughout the process. Our approach
incorporates a temporal aggregation mechanism to effectively capture the
evolution of graph structures over time and a federated optimization strategy
to collaboratively learn high-quality clustering representations. By preserving
data privacy and reducing communication overhead, our framework achieves
competitive performance on temporal graph datasets, making it a promising
solution for privacy-sensitive, real-world applications involving dynamic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Training: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12040v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12040v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Lies Hadjadj, Emilie Devijver, Yury Maximov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised algorithms aim to learn prediction functions from a small set
of labeled observations and a large set of unlabeled observations. Because this
framework is relevant in many applications, they have received a lot of
interest in both academia and industry. Among the existing techniques,
self-training methods have undoubtedly attracted greater attention in recent
years. These models are designed to find the decision boundary on low density
regions without making additional assumptions about the data distribution, and
use the unsigned output score of a learned classifier, or its margin, as an
indicator of confidence. The working principle of self-training algorithms is
to learn a classifier iteratively by assigning pseudo-labels to the set of
unlabeled training samples with a margin greater than a certain threshold. The
pseudo-labeled examples are then used to enrich the labeled training data and
to train a new classifier in conjunction with the labeled training set. In this
paper, we present self-training methods for binary and multi-class
classification; as well as their variants and two related approaches, namely
consistency-based approaches and transductive learning. We examine the impact
of significant self-training features on various methods, using different
general and image classification benchmarks, and we discuss our ideas for
future research in self-training. To the best of our knowledge, this is the
first thorough and complete survey on this subject.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verbalized Machine Learning: Revisiting Machine Learning with Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Z. Xiao, Robert Bamler, Bernhard Schölkopf, Weiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the progress made by large language models (LLMs), we introduce
the framework of verbalized machine learning (VML). In contrast to conventional
machine learning (ML) models that are typically optimized over a continuous
parameter space, VML constrains the parameter space to be human-interpretable
natural language. Such a constraint leads to a new perspective of function
approximation, where an LLM with a text prompt can be viewed as a function
parameterized by the text prompt. Guided by this perspective, we revisit
classical ML problems, such as regression and classification, and find that
these problems can be solved by an LLM-parameterized learner and optimizer. The
major advantages of VML include (1) easy encoding of inductive bias: prior
knowledge about the problem and hypothesis class can be encoded in natural
language and fed into the LLM-parameterized learner; (2) automatic model class
selection: the optimizer can automatically select a model class based on data
and verbalized prior knowledge, and it can update the model class during
training; and (3) interpretable learner updates: the LLM-parameterized
optimizer can provide explanations for why an update is performed. We
empirically verify the effectiveness of VML, and hope that VML can serve as a
stepping stone to stronger interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (116 pages, 32
  figures, v3: refined the paper structure and added more empirical results)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiTAR: Diffusion <span class="highlight-title">Transformer</span> Autoregressive Modeling for Speech
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent studies have attempted to autoregressively generate continuous
speech representations without discrete speech tokens by combining diffusion
and autoregressive models, yet they often face challenges with excessive
computational loads or suboptimal outcomes. In this work, we propose Diffusion
Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive
framework combining a language model with a diffusion transformer. This
approach significantly enhances the efficacy of autoregressive models for
continuous tokens and reduces computational demands. DiTAR utilizes a
divide-and-conquer strategy for patch generation, where the language model
processes aggregated patch embeddings and the diffusion transformer
subsequently generates the next patch based on the output of the language
model. For inference, we propose defining temperature as the time point of
introducing noise during the reverse diffusion ODE to balance diversity and
determinism. We also show in the extensive scaling analysis that DiTAR has
superb scalability. In zero-shot speech generation, DiTAR achieves
state-of-the-art performance in robustness, speaker similarity, and
naturalness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Based Privacy-Preserving Knowledge Transfer for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaomin Wu, Jizhou Guo, Junyi Hou, Bingsheng He, Lixin Fan, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more prevalent, effectively utilizing
domain-specific knowledge while ensuring privacy has become critical. Existing
methods often struggle to balance utility and privacy. For instance,
retrieval-augmented generation (RAG) enables LLMs to access domain-specific
knowledge but compromises the privacy of sensitive data. On the other hand,
differentially private data synthesis techniques offer strong privacy
guarantees but often result in poor utility. To address this challenge, we
propose Llamdex, a novel framework that enhances LLMs using only models trained
on domain-specific data, integrated into LLMs through carefully designed
connection modules. Our approach significantly enhances the accuracy of
domain-specific tasks, achieving up to a 26% accuracy improvement compared to
state-of-the-art data synthesis methods under the same differential privacy
constraints. Experimental results show that Llamdex not only improves the
accuracy of LLM responses but also maintains comparable inference efficiency to
the original LLM, highlighting its potential for real applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MQFL-FHE: <span class="highlight-title">Multimodal</span> Quantum Federated Learning Framework with Fully
  Homomorphic Encryption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01858v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01858v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Dutta, Nouhaila Innan, Sadok Ben Yahia, Muhammad Shafique, David Esteban Bernal Neira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of fully homomorphic encryption (FHE) in federated learning
(FL) has led to significant advances in data privacy. However, during the
aggregation phase, it often results in performance degradation of the
aggregated model, hindering the development of robust representational
generalization. In this work, we propose a novel multimodal quantum federated
learning framework that utilizes quantum computing to counteract the
performance drop resulting from FHE. For the first time in FL, our framework
combines a multimodal quantum mixture of experts (MQMoE) model with FHE,
incorporating multimodal datasets for enriched representation and task-specific
learning. Our MQMoE framework enhances performance on multimodal datasets and
combined genomics and brain MRI scans, especially for underrepresented
categories. Our results also demonstrate that the quantum-enhanced approach
mitigates the performance degradation associated with FHE and improves
classification accuracy across diverse datasets, validating the potential of
quantum interventions in enhancing privacy in FL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 6 Tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and
  learning in neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoony Kang, Wolfgang Losert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The brain can rapidly adapt to new contexts and learn from limited data, a
coveted characteristic that artificial intelligence algorithms have struggled
to mimic. Inspired by oscillatory rhythms of the mechanical structures of
neural cells, we developed a learning paradigm that is based on oscillations in
link strengths and associates learning with the coordination of these
oscillations. We find that this paradigm yields rapid adaptation and learning
in artificial neural networks. Link oscillations can rapidly change
coordination, endowing the network with the ability to sense subtle context
changes in an unsupervised manner. In other words, the network generates the
missing contextual tokens required to perform as a generalist AI architecture
capable of predicting dynamics in multiple contexts. Oscillations also allow
the network to extrapolate dynamics to never-seen-before contexts. These
capabilities make our learning paradigm a powerful starting point for novel
models of learning and cognition. Furthermore, learning through link
coordination is agnostic to the specifics of the neural network architecture,
hence our study opens the door for introducing rapid adaptation and learning
capabilities into leading AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures. v.2 comments: Updated email, updated typo on
  p.11: h -> h^2 for RMSE. v.3 comments: Updated reference style, added
  reference to Github repository</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Predict Global Atrial Fibrillation Dynamics from Sparse
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Jenkins, Andrea Cini, Joseph Barker, Alexander Sharp, Arunashis Sau, Varun Valentine, Srushti Valasang, Xinyang Li, Tom Wong, Timothy Betts, Danilo Mandic, Cesare Alippi, Fu Siong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all
treatment with limited success in persistent AF. This may be due to our
inability to map the dynamics of AF with the limited resolution and coverage
provided by sequential contact mapping catheters, preventing effective patient
phenotyping for personalised, targeted ablation. Here we introduce FibMap, a
graph recurrent neural network model that reconstructs global AF dynamics from
sparse measurements. Trained and validated on 51 non-contact whole atria
recordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,
achieving a 210% lower mean absolute error and an order of magnitude higher
performance in tracking phase singularities compared to baseline methods.
Clinical utility of FibMap is demonstrated on real-world contact mapping
recordings, achieving reconstruction fidelity comparable to non-contact
mapping. FibMap's state-spaces and patient-specific parameters offer insights
for electrophenotyping AF. Integrating FibMap into clinical practice could
enable personalised AF care and improve outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space-aware Socioeconomic Indicator Inference with Heterogeneous <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Zou, Jiani Huang, Xixuan Hao, Yuhao Yang, Haomin Wen, Yibo Yan, Chao Huang, Chen Chao, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regional socioeconomic indicators are critical across various domains, yet
their acquisition can be costly. Inferring global socioeconomic indicators from
a limited number of regional samples is essential for enhancing management and
sustainability in urban areas and human settlements. Current inference methods
typically rely on spatial interpolation based on the assumption of spatial
continuity, which does not adequately address the complex variations present
within regional spaces. In this paper, we present GeoHG, the first space-aware
socioeconomic indicator inference method that utilizes a heterogeneous
graph-based structure to represent geospace for non-continuous inference.
Extensive experiments demonstrate the effectiveness of GeoHG in comparison to
existing methods, achieving an $R^2$ score exceeding 0.8 under extreme data
scarcity with a masked ratio of 95\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Decouple Complex Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhou, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A complex system with cluttered observations may be a coupled mixture of
multiple simple sub-systems corresponding to latent entities. Such sub-systems
may hold distinct dynamics in the continuous-time domain; therein, complicated
interactions between sub-systems also evolve over time. This setting is fairly
common in the real world but has been less considered. In this paper, we
propose a sequential learning approach under this setting by decoupling a
complex system for handling irregularly sampled and cluttered sequential
observations. Such decoupling brings about not only subsystems describing the
dynamics of each latent entity but also a meta-system capturing the interaction
between entities over time. Specifically, we argue that the meta-system
evolving within a simplex is governed by projected differential equations
(ProjDEs). We further analyze and provide neural-friendly projection operators
in the context of Bregman divergence. Experimental results on synthetic and
real-world datasets show the advantages of our approach when facing complex and
cluttered sequential data compared to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Predictive Coding Networks -- Made Simple 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Pinchetti, Chang Qi, Oleh Lokshyn, Gaspard Olivers, Cornelius Emde, Mufeng Tang, Amine M'Charrak, Simon Frieder, Bayar Menzat, Rafal Bogacz, Thomas Lukasiewicz, Tommaso Salvatori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the problems of efficiency and scalability for
predictive coding networks (PCNs) in machine learning. To do so, we propose a
library, called PCX, that focuses on performance and simplicity, and use it to
implement a large set of standard benchmarks for the community to use for their
experiments. As most works in the field propose their own tasks and
architectures, do not compare one against each other, and focus on small-scale
tasks, a simple and fast open-source library and a comprehensive set of
benchmarks would address all these concerns. Then, we perform extensive tests
on such benchmarks using both existing algorithms for PCNs, as well as
adaptations of other methods popular in the bio-plausible deep learning
community. All this has allowed us to (i) test architectures much larger than
commonly used in the literature, on more complex datasets; (ii)~reach new
state-of-the-art results in all of the tasks and datasets provided;
(iii)~clearly highlight what the current limitations of PCNs are, allowing us
to state important future research directions. With the hope of galvanizing
community efforts towards one of the main open problems in the field,
scalability, we release code, tests, and benchmarks. Link to the library:
https://github.com/liukidar/pcx
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning with Reservoir State Analysis for Time Series Anomaly
  Detection <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keigo Nogami, Hiroto Tamura, Gouhei Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a growing data privacy concern, federated learning has emerged as a
promising framework to train machine learning models without sharing locally
distributed data. In federated learning, local model training by multiple
clients and model integration by a server are repeated only through model
parameter sharing. Most existing federated learning methods assume training
deep learning models, which are often computationally demanding. To deal with
this issue, we propose federated learning methods with reservoir state analysis
to seek computational efficiency and data privacy protection simultaneously.
Specifically, our method relies on Mahalanobis Distance of Reservoir States
(MD-RS) method targeting time series anomaly detection, which learns a
distribution of reservoir states for normal inputs and detects anomalies based
on a deviation from the learned distribution. Iterative updating of statistical
parameters in the MD-RS enables incremental federated learning (IncFed MD-RS).
We evaluate the performance of IncFed MD-RS using benchmark datasets for time
series anomaly detection. The results show that IncFed MD-RS outperforms other
federated learning methods with deep learning and reservoir computing models
particularly when clients' data are relatively short and heterogeneous. We
demonstrate that IncFed MD-RS is robust against reduced sample data compared to
other methods. We also show that the computational cost of IncFed MD-RS can be
reduced by subsampling from the reservoir states without performance
degradation. The proposed method is beneficial especially in anomaly detection
applications where computational efficiency, algorithm simplicity, and low
communication cost are required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 16 figures, submitted to IJCNN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Growth strategies for arbitrary DAG neural architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stella Douka, Manon Verbockhaven, Théo Rudkiewicz, Stéphane Rivaud, François P. Landes, Sylvain Chevallier, Guillaume Charpiat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown impressive results obtained at the cost of training
huge neural networks. However, the larger the architecture, the higher the
computational, financial, and environmental costs during training and
inference. We aim at reducing both training and inference durations. We focus
on Neural Architecture Growth, which can increase the size of a small model
when needed, directly during training using information from the
backpropagation. We expand existing work and freely grow neural networks in the
form of any Directed Acyclic Graph by reducing expressivity bottlenecks in the
architecture. We explore strategies to reduce excessive computations and steer
network growth toward more parameter-efficient architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Song, Yunbo Wang, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of computational physics, an enduring topic is the numerical
solutions to partial differential equations (PDEs). Recently, the attention of
researchers has shifted towards Neural Operator methods, renowned for their
capability to approximate ``operators'' -- mappings from functions to
functions. Despite the universal approximation theorem within neural operators,
ensuring error bounds often requires employing numerous Fourier layers.
However, what about lightweight models? In response to this question, we
introduce DimOL (Dimension-aware Operator Learning), drawing insights from
dimensional analysis. To implement DimOL, we propose the ProdLayer, which can
be seamlessly integrated into FNO-based and Transformer-based PDE solvers,
enhancing their ability to handle sum-of-products structures inherent in many
physical systems. Empirically, DimOL models achieve up to 48% performance gain
within the PDE datasets. Furthermore, by analyzing Fourier components' weights,
we can symbolically discern the physical significance of each term. This sheds
light on the opaque nature of neural networks, unveiling underlying physical
principles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Inference on the Boolean Hypercube with the Quantum Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliot Beyler, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we derive variational inference upper-bounds on the
log-partition function of pairwise Markov random fields on the Boolean
hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We
then propose an efficient algorithm to compute these bounds based on
primal-dual optimization. An improvement of these bounds through the use of
''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and
we present a greedy algorithm to select among these relaxations. We carry
extensive numerical experiments and compare with state-of-the-art methods for
this inference problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEMS and ECM Sensor Technologies for Cardiorespiratory Sound Monitoring
  - A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasaman Torabi, Shahram Shirani, James P. Reilly, Gail M Gauvreau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive review of cardiorespiratory auscultation
sensing devices (i.e., stethoscopes), which is useful for understanding the
theoretical aspects and practical design notes. In this paper, we first
introduce the acoustic properties of the heart and lungs, as well as a brief
history of stethoscope evolution. Then, we discuss the basic concept of
electret condenser microphones (ECMs) and a stethoscope based on them. Then, we
discuss the microelectromechanical systems (MEMSs) technology, particularly
focusing on piezoelectric transducer sensors. This paper comprehensively
reviews sensing technologies for cardiorespiratory auscultation, emphasizing
MEMS-based wearable designs in the past decade. To our knowledge, this is the
first paper to summarize ECM and MEMS applications for heart and lung sound
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Channel Access and Transmission for NR Sidelink and Wi-Fi
  Coexistence over Unlicensed Spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuangzhuang Yan, Xinyu Gu, Zhenyu Liu, Liyang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of various internet of things (IoT) applications,
including industrial IoT (IIoT) and visual IoT (VIoT), the demand for direct
device-to-device communication to support high data rates continues to grow. To
address this demand, 5G-Advanced has introduced sidelink communication over the
unlicensed spectrum (SL-U) to increase data rates. However, the primary
challenge of SL-U in the unlicensed spectrum is ensuring fair coexistence with
other incumbent systems, such as Wi-Fi. In this paper, we address the challenge
by designing channel access mechanisms and power control strategies to mitigate
interference and ensure fair coexistence. First, we propose a novel
collaborative channel access (CCHA) mechanism that integrates channel access
with resource allocation through collaborative interactions between base
stations (BS) and SL-U users. This mechanism ensures fair coexistence with
incumbent systems while improving resource utilization. Second, to further
enhance the performance of the coexistence system, we develop a cooperative
subgoal-based hierarchical deep reinforcement learning (C-GHDRL) algorithm
framework. The framework enables SL-U users to make globally optimal decisions
by leveraging cooperative operations between the BS and SL-U users, effectively
overcoming the limitations of traditional optimization methods in solving joint
optimization problems with nonlinear constraints. Finally, we mathematically
model the joint channel access and power control problem and balance the
trade-off between fairness and transmission rate in the coexistence system by
defining a suitable reward function in the C-GHDRL algorithm. Simulation
results demonstrate that the proposed scheme significantly enhances the
performance of the coexistence system while ensuring fair coexistence between
SL-U and Wi-Fi users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CATCH: Channel-Aware multivariate Time Series Anomaly Detection via
  Frequency Patching <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo, Hui Xiong, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in multivariate time series is challenging as heterogeneous
subsequence anomalies may occur. Reconstruction-based methods, which focus on
learning normal patterns in the frequency domain to detect diverse abnormal
subsequences, achieve promising results, while still falling short on capturing
fine-grained frequency characteristics and channel correlations. To contend
with the limitations, we introduce CATCH, a framework based on frequency
patching. We propose to patchify the frequency domain into frequency bands,
which enhances its ability to capture fine-grained frequency characteristics.
To perceive appropriate channel correlations, we propose a Channel Fusion
Module (CFM), which features a patch-wise mask generator and a masked-attention
mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM
is encouraged to iteratively discover appropriate patch-wise channel
correlations, and to cluster relevant channels while isolating adverse effects
from irrelevant channels. Extensive experiments on 10 real-world datasets and
12 synthetic datasets demonstrate that CATCH achieves state-of-the-art
performance. We make our code and datasets available at
https://github.com/decisionintelligence/CATCH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Center Cooling System Optimization Using Offline Reinforcement
  Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianyuan Zhan, Xiangyu Zhu, Peng Cheng, Xiao Hu, Ziteng He, Hanfei Geng, Jichao Leng, Huiwen Zheng, Chenhui Liu, Tianshun Hong, Yan Liang, Yunxin Liu, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in information technology and artificial intelligence
have fueled a rapid expansion of the data center (DC) industry worldwide,
accompanied by an immense appetite for electricity to power the DCs. In a
typical DC, around 30~40% of the energy is spent on the cooling system rather
than on computer servers, posing a pressing need for developing new
energy-saving optimization technologies for DC cooling systems. However,
optimizing such real-world industrial systems faces numerous challenges,
including but not limited to a lack of reliable simulation environments,
limited historical data, and stringent safety and control robustness
requirements. In this work, we present a novel physics-informed offline
reinforcement learning (RL) framework for energy efficiency optimization of DC
cooling systems. The proposed framework models the complex dynamical patterns
and physical dependencies inside a server room using a purposely designed graph
neural network architecture that is compliant with the fundamental
time-reversal symmetry. Because of its well-behaved and generalizable
state-action representations, the model enables sample-efficient and robust
latent space offline policy learning using limited real-world operational data.
Our framework has been successfully deployed and verified in a large-scale
production DC for closed-loop control of its air-cooling units (ACUs). We
conducted a total of 2000 hours of short and long-term experiments in the
production DC environment. The results show that our method achieves 14~21%
energy savings in the DC cooling system, without any violation of the safety or
operational constraints. Our results have demonstrated the significant
potential of offline RL in solving a broad range of data-limited,
safety-critical real-world industrial control problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking
  Effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Fei, Wenjie Hou, Xuan Hai, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in AI voice cloning, fueled by machine learning, have
significantly impacted text-to-speech (TTS) and voice conversion (VC) fields.
While these developments have led to notable progress, they have also raised
concerns about the misuse of AI VC technology, causing economic losses and
negative public perceptions. To address this challenge, this study focuses on
creating active defense mechanisms against AI VC systems.
  We propose a novel active defense method, VocalCrypt, which embeds
pseudo-timbre (jamming information) based on SFS into audio segments that are
imperceptible to the human ear, thereby forming systematic fragments to prevent
voice cloning. This approach protects the voice without compromising its
quality. In comparison to existing methods, such as adversarial noise
incorporation, VocalCrypt significantly enhances robustness and real-time
performance, achieving a 500\% increase in generation speed while maintaining
interference effectiveness.
  Unlike audio watermarking techniques, which focus on post-detection, our
method offers preemptive defense, reducing implementation costs and enhancing
feasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets
show that our AI-cloned speech defense system performs excellently in automatic
speaker verification (ASV) tests while preserving the integrity of the
protected audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, four figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serkan Sulun, Paula Viana, Matthew E. P. Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EMSYNC, a video-based symbolic music generation model that
aligns music with a video's emotional content and temporal boundaries. It
follows a two-stage framework, where a pretrained video emotion classifier
extracts emotional features, and a conditional music generator produces MIDI
sequences guided by both emotional and temporal cues. We introduce boundary
offsets, a novel temporal conditioning mechanism that enables the model to
anticipate and align musical chords with scene cuts. Unlike existing models,
our approach retains event-based encoding, ensuring fine-grained timing control
and expressive musical nuances. We also propose a mapping scheme to bridge the
video emotion classifier, which produces discrete emotion categories, with the
emotion-conditioned MIDI generator, which operates on continuous-valued
valence-arousal inputs. In subjective listening tests, EMSYNC outperforms
state-of-the-art models across all subjective metrics, for music theory-aware
participants as well as the general listeners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Joint Conference on Artificial
  Intelligence (IJCAI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Concept-based Deep Learning Framework for <span class="highlight-title">Multimodal</span> Human
  Behavior Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Li, Marwa Mahmoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the contemporary era of intelligent connectivity, Affective Computing
(AC), which enables systems to recognize, interpret, and respond to human
behavior states, has become an integrated part of many AI systems. As one of
the most critical components of responsible AI and trustworthiness in all
human-centered systems, explainability has been a major concern in AC.
Particularly, the recently released EU General Data Protection Regulation
requires any high-risk AI systems to be sufficiently interpretable, including
biometric-based systems and emotion recognition systems widely used in the
affective computing field. Existing explainable methods often compromise
between interpretability and performance. Most of them focus only on
highlighting key network parameters without offering meaningful,
domain-specific explanations to the stakeholders. Additionally, they also face
challenges in effectively co-learning and explaining insights from multimodal
data sources. To address these limitations, we propose a novel and
generalizable framework, namely the Attention-Guided Concept Model (AGCM),
which provides learnable conceptual explanations by identifying what concepts
that lead to the predictions and where they are observed. AGCM is extendable to
any spatial and temporal signals through multimodal concept alignment and
co-learning, empowering stakeholders with deeper insights into the model's
decision-making process. We validate the efficiency of AGCM on well-established
Facial Expression Recognition benchmark datasets while also demonstrating its
generalizability on more complex real-world human behavior understanding
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuDoC: An Interactive <span class="highlight-title">Multimodal</span> Document-grounded Conversational AI
  System <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karan Taneja, Ashok K. Goel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal AI is an important step towards building effective tools to
leverage multiple modalities in human-AI communication. Building a multimodal
document-grounded AI system to interact with long documents remains a
challenge. Our work aims to fill the research gap of directly leveraging
grounded visuals from documents alongside textual content in documents for
response generation. We present an interactive conversational AI agent 'MuDoC'
based on GPT-4o to generate document-grounded responses with interleaved text
and figures. MuDoC's intelligent textbook interface promotes trustworthiness
and enables verification of system responses by allowing instant navigation to
source text and figures in the documents. We also discuss qualitative
observations based on MuDoC responses highlighting its strengths and
limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, AAAI-MAKE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Video Coding Meets <span class="highlight-title">Multimodal</span> Large Language Models: A Unified
  Paradigm for Video Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Zhang, Jinlong Li, Kecheng Chen, Meng Wang, Long Xu, Haoliang Li, Nicu Sebe, Sam Kwong, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing codecs are designed to eliminate intrinsic redundancies to create a
compact representation for compression. However, strong external priors from
Multimodal Large Language Models (MLLMs) have not been explicitly explored in
video compression. Herein, we introduce a unified paradigm for Cross-Modality
Video Coding (CMVC), which is a pioneering approach to explore multimodality
representation and video generative models in video coding. Specifically, on
the encoder side, we disentangle a video into spatial content and motion
components, which are subsequently transformed into distinct modalities to
achieve very compact representation by leveraging MLLMs. During decoding,
previously encoded components and video generation models are leveraged to
create multiple encoding-decoding modes that optimize video reconstruction
quality for specific decoding requirements, including Text-Text-to-Video (TT2V)
mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)
mode to achieve superb perceptual consistency. In addition, we propose an
efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)
tuning to guarantee perceptual quality, which allows the generated motion cues
to behave smoothly. Experiments on benchmarks indicate that TT2V achieves
effective semantic reconstruction, while IT2V exhibits competitive perceptual
consistency. These results highlight potential directions for future research
in video coding.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-13T00:00:00Z">2025-02-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">69</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Coherence Alignment for Large Language Model Representation
  Learning Through Tensor Field Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Gale, Godfrey Aldington, Harriet Thistlewood, Thomas Tattershall, Basil Wentworth, Vincent Enoasmo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning plays a central role in structuring internal
embeddings to capture the statistical properties of language, influencing the
coherence and contextual consistency of generated text. Statistical Coherence
Alignment is introduced as a method to enforce structured token representations
through tensor field convergence, guiding embeddings to reflect statistical
dependencies inherent in linguistic data. A mathematical framework is
established to quantify coherence alignment, integrating a loss function that
optimizes representational consistency across training iterations. Empirical
evaluations demonstrate that applying coherence constraints improves
perplexity, enhances classification accuracy, and refines rare word embeddings,
contributing to a more stable representation space. Comparative analyses with
baseline models reveal that the proposed method fosters a more interpretable
internal structure, ensuring that embeddings retain contextual dependencies
while mitigating representation collapse. The impact on coherence score
distributions suggests that the alignment mechanism strengthens semantic
integrity across diverse linguistic constructs, leading to a more balanced
organization of learned embeddings. Computational assessments indicate that
while the method introduces additional memory and training costs, the
structured optimization process justifies the trade-offs in applications
requiring heightened contextual fidelity. Experimental results validate the
effectiveness of coherence alignment in optimizing token representations,
providing insights into how statistical dependencies can be leveraged to
improve language model training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INJONGO: A Multicultural Intent Detection and Slot-filling <span class="highlight-title">Dataset</span> for
  16 African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Jesujoba O. Alabi, Andiswa Bukula, Jian Yun Zhuang, En-Shiun Annie Lee, Tadesse Kebede Guge, Israel Abebe Azime, Happy Buzaaba, Blessing Kudzaishe Sibanda, Godson K. Kalipe, Jonathan Mukiibi, Salomon Kabongo Kabenamualu, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Juliet W. Murage, Dietrich Klakow, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Slot-filling and intent detection are well-established tasks in
Conversational AI. However, current large-scale benchmarks for these tasks
often exclude evaluations of low-resource languages and rely on translations
from English benchmarks, thereby predominantly reflecting Western-centric
concepts. In this paper, we introduce Injongo -- a multicultural, open-source
benchmark dataset for 16 African languages with utterances generated by native
speakers across diverse domains, including banking, travel, home, and dining.
Through extensive experiments, we benchmark the fine-tuning multilingual
transformer models and the prompting large language models (LLMs), and show the
advantage of leveraging African-cultural utterances over Western-centric
utterances for improving cross-lingual transfer from the English language.
Experimental results reveal that current LLMs struggle with the slot-filling
task, with GPT-4o achieving an average performance of 26 F1-score. In contrast,
intent detection performance is notably better, with an average accuracy of
70.6%, though it still falls behind the fine-tuning baselines. Compared to the
English language, GPT-4o and fine-tuning baselines perform similarly on intent
detection, achieving an accuracy of approximately 81%. Our findings suggest
that the performance of LLMs is still behind for many low-resource African
languages, and more work is needed to further improve their downstream
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Acoustic Side-Channel Attacks on Keyboards Using <span class="highlight-title">Transformer</span>s
  and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of microphones in everyday devices and the growing
reliance on online services have amplified the risk of acoustic side-channel
attacks (ASCAs) targeting keyboards. This study explores deep learning
techniques, specifically vision transformers (VTs) and large language models
(LLMs), to enhance the effectiveness and applicability of such attacks. We
present substantial improvements over prior research, with the CoAtNet model
achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement
for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via
Zoom compared to previous benchmarks. We also evaluate transformer
architectures and language models, with the best VT model matching CoAtNet's
performance. A key advancement is the introduction of a noise mitigation method
for real-world scenarios. By using LLMs for contextual understanding, we detect
and correct erroneous keystrokes in noisy environments, enhancing ASCA
performance. Additionally, fine-tuned lightweight language models with Low-Rank
Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X
more parameters. This integration of VTs and LLMs improves the practical
applicability of ASCA mitigation, marking the first use of these technologies
to address ASCAs and error correction in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt and circumstance: A word-by-word LLM prompting approach to
  interlinear glossing for low-resource languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micha Elsner, David Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partly automated creation of interlinear glossed text (IGT) has the potential
to assist in linguistic documentation. We argue that LLMs can make this process
more accessible to linguists because of their capacity to follow
natural-language instructions. We investigate the effectiveness of a
retrieval-based LLM prompting approach to glossing, applied to the seven
languages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based
shared task baseline for every language in the morpheme-level score category,
and we show that a simple 3-best oracle has higher word-level scores than the
challenge winner (a tuned sequence model) in five languages. In a case study on
Tsez, we ask the LLM to automatically create and follow linguistic
instructions, reducing errors on a confusing grammatical feature. Our results
thus demonstrate the potential contributions which LLMs can make in interactive
systems for glossing, both in making suggestions to human annotators and
following directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Markovian Discrete Diffusion with Causal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete diffusion models have emerged as a flexible and controllable
paradigm for structured sequence modeling, yet they still lag behind causal
language models in expressiveness. To bridge the gap between two paradigms, we
introduce CaDDi, a causal discrete diffusion model that unifies sequential and
temporal modeling within a non-Markovian diffusion framework. Unlike
conventional diffusion models that operate step by step with no access to prior
states, CaDDi integrates the temporal trajectory, enabling more expressive and
controllable generation. Our approach also treats causal language models as a
special case, allowing seamless adoption of pretrained large language models
(LLMs) for discrete diffusion without the need for architectural modifications.
Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete
diffusion models on both natural language and biological sequence tasks,
narrowing the gap between diffusion-based methods and large-scale
autoregressive transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Widespread Adoption of Large Language Model-Assisted Writing Across
  Society 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Liang, Yaohui Zhang, Mihai Codreanu, Jiayu Wang, Hancheng Cao, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in large language models (LLMs) attracted significant
public and policymaker interest in its adoption patterns. In this paper, we
systematically analyze LLM-assisted writing across four domains-consumer
complaints, corporate communications, job postings, and international
organization press releases-from January 2022 to September 2024. Our dataset
includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3
million job postings, and 15,919 United Nations (UN) press releases. Using a
robust population-level statistical framework, we find that LLM usage surged
following the release of ChatGPT in November 2022. By late 2024, roughly 18% of
financial consumer complaint text appears to be LLM-assisted, with adoption
patterns spread broadly across regions and slightly higher in urban areas. For
corporate press releases, up to 24% of the text is attributable to LLMs. In job
postings, LLM-assisted writing accounts for just below 10% in small firms, and
is even more common among younger firms. UN press releases also reflect this
trend, with nearly 14% of content being generated or modified by LLMs. Although
adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by
2024, reflecting either saturation in LLM adoption or increasing subtlety of
more advanced models. Our study shows the emergence of a new reality in which
firms, consumers and even international organizations substantially rely on
generative AI for communications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partial Colexifications Improve Concept Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arne Rubehn, Johann-Mattis List
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the embedding of words has revolutionized the field of Natural Language
Processing, the embedding of concepts has received much less attention so far.
A dense and meaningful representation of concepts, however, could prove useful
for several tasks in computational linguistics, especially those involving
cross-linguistic data or sparse data from low resource languages. First methods
that have been proposed so far embed concepts from automatically constructed
colexification networks. While these approaches depart from automatically
inferred polysemies, attested across a larger number of languages, they are
restricted to the word level, ignoring lexical relations that would only hold
for parts of the words in a given language. Building on recently introduced
methods for the inference of partial colexifications, we show how they can be
used to improve concept embeddings in meaningful ways. The learned embeddings
are evaluated against lexical similarity ratings, recorded instances of
semantic shift, and word association data. We show that in all evaluation
tasks, the inclusion of partial colexifications lead to improved concept
representations and better results. Our results further show that the learned
embeddings are able to capture and represent different semantic relationships
between concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 63rd Annual Meeting of the Association for
  Computational Linguistics, Vienna, Austria</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FoNE: Precise Single-Token Number Embeddings via Fourier Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhou, Deqing Fu, Mahdi Soltanolkotabi, Robin Jia, Vatsal Sharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) typically represent numbers using multiple
tokens, which requires the model to aggregate these tokens to interpret
numerical values. This fragmentation makes both training and inference less
efficient and adversely affects the model's performance on number-related
tasks. Inspired by the observation that pre-trained LLMs internally learn
Fourier-like features for number tokens, we propose Fourier Number Embedding
(FoNE), a novel method that directly maps numbers into the embedding space with
their Fourier features. FoNE encodes each number as a single token with only
two embedding dimensions per digit, effectively capturing numerical values
without fragmentation. This compact representation accelerates both training
and inference. Compared to traditional subword and digit-wise embeddings, FoNE
not only reduces computational overhead but also achieves higher accuracy
across various numerical tasks including addition, subtraction and
multiplication. On 6-digit decimal addition, FoNE requires 64$\times$ less data
to achieve 99% accuracy than subword and digit-wise embeddings while using
3$\times$ and 6$\times$ fewer tokens per number, respectively. Furthermore,
FoNE is the only method that yields 100% accuracy on over 100,000 test examples
for addition, subtraction, and multiplication. The codes and visualization are
available at https://fouriernumber.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Them a Malicious Database: Exploiting Query Code to Jailbreak
  Aligned Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingsong Zou, Jingyu Xiao, Qing Li, Zhi Yan, Yuhang Wang, Li Xu, Wenxuan Wang, Kuofeng Gao, Ruoyu Li, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have demonstrated remarkable
potential in the field of natural language processing. Unfortunately, LLMs face
significant security and ethical risks. Although techniques such as safety
alignment are developed for defense, prior researches reveal the possibility of
bypassing such defenses through well-designed jailbreak attacks. In this paper,
we propose QueryAttack, a novel framework to systematically examine the
generalizability of safety alignment. By treating LLMs as knowledge databases,
we translate malicious queries in natural language into code-style structured
query to bypass the safety alignment mechanisms of LLMs. We conduct extensive
experiments on mainstream LLMs, ant the results show that QueryAttack achieves
high attack success rates (ASRs) across LLMs with different developers and
capabilities. We also evaluate QueryAttack's performance against common
defenses, confirming that it is difficult to mitigate with general defensive
techniques. To defend against QueryAttack, we tailor a defense method which can
reduce ASR by up to 64\% on GPT-4-1106. The code of QueryAttack can be found on
https://anonymous.4open.science/r/QueryAttack-334B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating GPT's Capability in Identifying Stages of Cognitive
  Impairment from Electronic Health Data <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Leng, Yingnan He, Colin Magdamo, Ana-Maria Vranceanu, Christine S. Ritchie, Shibani S. Mukerji, Lidia M. V. R. Moura, John R. Dickson, Deborah Blacker, Sudeshna Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying cognitive impairment within electronic health records (EHRs) is
crucial not only for timely diagnoses but also for facilitating research.
Information about cognitive impairment often exists within unstructured
clinician notes in EHRs, but manual chart reviews are both time-consuming and
error-prone. To address this issue, our study evaluates an automated approach
using zero-shot GPT-4o to determine stage of cognitive impairment in two
different tasks. First, we evaluated the ability of GPT-4o to determine the
global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who
visited the memory clinic at Massachusetts General Hospital (MGH), and achieved
a weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to
differentiate between normal cognition, mild cognitive impairment (MCI), and
dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o
attained a weighted kappa score of 0.91 in comparison to specialist chart
reviews and 0.96 on cases that the clinical adjudicators rated with high
confidence. Our findings demonstrate GPT-4o's potential as a scalable chart
review tool for creating research datasets and assisting diagnosis in clinical
settings in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Benefit and Limitation of Diffusion Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, Di He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion language models have emerged as a promising approach for text
generation. One would naturally expect this method to be an efficient
replacement for autoregressive models since multiple tokens can be sampled in
parallel during each diffusion step. However, its efficiency-accuracy trade-off
is not yet well understood. In this paper, we present a rigorous theoretical
analysis of a widely used type of diffusion language model, the Masked
Diffusion Model (MDM), and find that its effectiveness heavily depends on the
target evaluation metric. Under mild conditions, we prove that when using
perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling
steps regardless of sequence length, demonstrating that efficiency can be
achieved without sacrificing performance. However, when using the sequence
error rate--which is important for understanding the "correctness" of a
sequence, such as a reasoning chain--we show that the required sampling steps
must scale linearly with sequence length to obtain "correct" sequences, thereby
eliminating MDM's efficiency advantage over autoregressive models. Our analysis
establishes the first theoretical foundation for understanding the benefits and
limitations of MDMs. All theoretical findings are supported by empirical
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-CoT: Benchmarking Chain-of-Thought in Large <span class="highlight-title">Multimodal</span> Models for
  Reasoning Quality, Robustness, and Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions with Chain-of-Thought (CoT) has significantly enhanced
the reasoning capabilities of Large Language Models (LLMs), yet its impact on
Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth
investigation. In this paper, we introduce MME-CoT, a specialized benchmark
evaluating the CoT reasoning performance of LMMs, spanning six domains: math,
science, OCR, logic, space-time, and general scenes. As the first comprehensive
study in this area, we propose a thorough evaluation suite incorporating three
novel metrics that assess the reasoning quality, robustness, and efficiency at
a fine-grained level. Leveraging curated high-quality data and a unique
evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,
uncovering several key insights: 1) Models with reflection mechanism
demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and
demonstrating the highest quality results; 2) CoT prompting often degrades LMM
performance on perception-heavy tasks, suggesting a potentially harmful
overthinking behavior; and 3) Although the CoT quality is high, LMMs with
reflection exhibit significant inefficiency in both normal response and
self-correction phases. We hope MME-CoT serves as a foundation for advancing
multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mmecot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Encoder-free Architectures in 3D LMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encoder-free architectures have been preliminarily explored in the 2D visual
domain, yet it remains an open question whether they can be effectively applied
to 3D understanding scenarios. In this paper, we present the first
comprehensive investigation into the potential of encoder-free architectures to
overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs).
These challenges include the failure to adapt to varying point cloud
resolutions and the point features from the encoder not meeting the semantic
needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to
remove the encoder and enable the LLM to assume the role of the 3D encoder: 1)
We propose the LLM-embedded Semantic Encoding strategy in the pre-training
stage, exploring the effects of various point cloud self-supervised losses. And
we present the Hybrid Semantic Loss to extract high-level semantics. 2) We
introduce the Hierarchical Geometry Aggregation strategy in the instruction
tuning stage. This incorporates inductive bias into the LLM early layers to
focus on the local details of the point clouds. To the end, we present the
first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current
state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the
classification, captioning, and VQA tasks, respectively. Our results
demonstrate that the encoder-free architecture is highly promising for
replacing encoder-based architectures in the field of 3D understanding. The
code is released at https://github.com/Ivan-Tang-3D/ENEL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is released at https://github.com/Ivan-Tang-3D/ENEL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-LLM Coevolution: Evidence from Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a statistical analysis of arXiv paper abstracts, we report a marked drop
in the frequency of several words previously identified as overused by ChatGPT,
such as "delve", starting soon after they were pointed out in early 2024. The
frequency of certain other words favored by ChatGPT, such as "significant", has
instead kept increasing. These phenomena suggest that some authors of academic
papers have adapted their use of large language models (LLMs), for example, by
selecting outputs or applying modifications to the LLM-generated content. Such
coevolution and cooperation of humans and LLMs thus introduce additional
challenges to the detection of machine-generated text in real-world scenarios.
Estimating the impact of LLMs on academic writing by examining word frequency
remains feasible, and more attention should be paid to words that were already
frequently employed, including those that have decreased in frequency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfCite: Self-Supervised Alignment for Context Attribution in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to
generate high-quality, fine-grained, sentence-level citations for the
statements in their generated responses. Instead of only relying on costly and
labor-intensive annotations, SelfCite leverages a reward signal provided by the
LLM itself through context ablation: If a citation is necessary, removing the
cited text from the context should prevent the same response; if sufficient,
retaining the cited text alone should preserve the same response. This reward
can guide the inference-time best-of-N sampling strategy to improve citation
quality significantly, as well as be used in preference optimization to
directly fine-tune the models for generating better citations. The
effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3
points on the LongBench-Cite benchmark across five long-form question answering
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Implementation available at https://github.com/voidism/SelfCite</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-Valve: Length-Compressible Chain-of-Thought Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought significantly enhances a model's reasoning capability, but
it also comes with a considerable increase in inference costs due to long
chains. With the observation that the reasoning path can be easily compressed
under easy tasks but struggle on hard tasks, we explore the feasibility of
elastically controlling the length of reasoning paths with only one model,
thereby reducing the inference overhead of reasoning models dynamically based
on task difficulty. We introduce a new tuning and inference strategy named
CoT-Valve, designed to allow models to generate reasoning chains of varying
lengths. To achieve this, we propose to identify a direction in the parameter
space that, when manipulated, can effectively control the length of generated
CoT. Moreover, we show that this property is valuable for compressing the
reasoning chain. We construct datasets with chains from long to short for the
same questions and explore two enhanced strategies for CoT-Valve: (1) a precise
length-compressible CoT tuning method, and (2) a progressive chain length
compression approach. Our experiments show that CoT-Valve successfully enables
controllability and compressibility of the chain and shows better performance
than the prompt-based control. We applied this method to QwQ-32B-Preview,
reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor
performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with
only one additional incorrect answer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Code will be released at
  https://github.com/horseee/CoT-Valve</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Recognize Your Preferences? Evaluating Personalized Preference
  Following in LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used as chatbots, yet their
ability to personalize responses to user preferences remains limited. We
introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize
and adhere to user preferences in a long-context conversational setting.
PrefEval comprises 3,000 manually curated user preference and query pairs
spanning 20 topics. PrefEval contains user personalization or preference
information in both explicit and implicit forms, and evaluates LLM performance
using a generation and a classification task. With PrefEval, we evaluated the
aforementioned preference following capabilities of 10 open-source and
proprietary LLMs in multi-session conversations with varying context lengths up
to 100k tokens. We benchmark with various prompting, iterative feedback, and
retrieval-augmented generation methods. Our benchmarking effort reveals that
state-of-the-art LLMs face significant challenges in proactively following
users' preferences during conversations. In particular, in zero-shot settings,
preference following accuracy falls below 10% at merely 10 turns (~3k tokens)
across most evaluated models. Even with advanced prompting and retrieval
methods, preference following still deteriorates in long-context conversations.
Furthermore, we show that fine-tuning on PrefEval significantly improves
performance. We believe PrefEval serves as a valuable resource for measuring,
understanding, and enhancing LLMs' preference following abilities, paving the
way for personalized conversational agents. Our code and dataset are available
at https://prefeval.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 as oral presentation. Code and data at:
  https://prefeval.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logical forms complement probability in understanding language model
  (and human) performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Freda Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing interest in using large language models (LLMs) for
planning in natural language, understanding their behaviors becomes an
important research question. This work conducts a systematic investigation of
LLMs' ability to perform logical reasoning in natural language. We introduce a
controlled dataset of hypothetical and disjunctive syllogisms in propositional
and modal logic and use it as the testbed for understanding LLM performance.
Our results lead to novel insights in predicting LLM behaviors: in addition to
the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical
forms should be considered as orthogonal factors. In addition, we show
similarities and differences between the logical reasoning performances of
humans and LLMs by comparing LLM and human behavioral results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MorphNLI: A Stepwise Approach to Natural Language Inference Using Text
  Morphing <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Andrei Negru, Robert Vacareanu, Camelia Lemnaru, Mihai Surdeanu, Rodica Potolea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MorphNLI, a modular step-by-step approach to natural language
inference (NLI). When classifying the premise-hypothesis pairs into
{entailment, contradiction, neutral}, we use a language model to generate the
necessary edits to incrementally transform (i.e., morph) the premise into the
hypothesis. Then, using an off-the-shelf NLI model we track how the entailment
progresses with these atomic changes, aggregating these intermediate labels
into a final output. We demonstrate the advantages of our proposed method
particularly in realistic cross-domain settings, where our method always
outperforms strong baselines with improvements up to 12.6% (relative). Further,
our proposed approach is explainable as the atomic edits can be used to
understand the overall NLI label.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot generation of synthetic neurosurgical data with large language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin A. Barr, Eddie Guo, Emre Sezgin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical data is fundamental to advance neurosurgical research, but access is
often constrained by data availability, small sample sizes, privacy
regulations, and resource-intensive preprocessing and de-identification
procedures. Synthetic data offers a potential solution to challenges associated
with accessing and using real-world data (RWD). This study aims to evaluate the
capability of zero-shot generation of synthetic neurosurgical data with a large
language model (LLM), GPT-4o, by benchmarking with the conditional tabular
generative adversarial network (CTGAN). Synthetic datasets were compared to
real-world neurosurgical data to assess fidelity (means, proportions,
distributions, and bivariate correlations), utility (ML classifier performance
on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated
datasets matched or exceeded CTGAN performance, despite no fine-tuning or
access to RWD for pre-training. Datasets demonstrated high univariate and
bivariate fidelity to RWD without directly exposing any real patient records,
even at amplified sample size. Training an ML classifier on GPT-4o-generated
data and testing on RWD for a binary prediction task showed an F1 score (0.706)
with comparable performance to training on the CTGAN data (0.705) for
predicting postoperative functional status deterioration. GPT-4o demonstrated a
promising ability to generate high-fidelity synthetic neurosurgical data. These
findings also indicate that data synthesized with GPT-4o can effectively
augment clinical data with small sample sizes, and train ML models for
prediction of neurosurgical outcomes. Further investigation is necessary to
improve the preservation of distributional characteristics and boost classifier
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbodiedBench: Comprehensive Benchmarking <span class="highlight-title">Multi-modal</span> Large Language
  Models for Vision-Driven Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied
agents offers a promising avenue for tackling real-world tasks. While
language-centric embodied agents have garnered substantial attention,
MLLM-based embodied agents remain underexplored due to the lack of
comprehensive evaluation frameworks. To bridge this gap, we introduce
EmbodiedBench, an extensive benchmark designed to evaluate vision-driven
embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing
tasks across four environments, ranging from high-level semantic tasks (e.g.,
household) to low-level tasks involving atomic actions (e.g., navigation and
manipulation); and (2) six meticulously curated subsets evaluating essential
agent capabilities like commonsense reasoning, complex instruction
understanding, spatial awareness, visual perception, and long-term planning.
Through extensive experiments, we evaluated 13 leading proprietary and
open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel
at high-level tasks but struggle with low-level manipulation, with the best
model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a
multifaceted standardized evaluation platform that not only highlights existing
challenges but also offers valuable insights to advance MLLM-based embodied
agents. Our code is available at https://embodiedbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Gap! Choice Independence in Using Multilingual LLMs for
  Persuasive Co-Writing Tasks in Different Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyan Biswas, Alexander Erlei, Ujwal Gadiraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative AI have precipitated a proliferation of novel
writing assistants. These systems typically rely on multilingual large language
models (LLMs), providing globalized workers the ability to revise or create
diverse forms of content in different languages. However, there is substantial
evidence indicating that the performance of multilingual LLMs varies between
languages. Users who employ writing assistance for multiple languages are
therefore susceptible to disparate output quality. Importantly, recent research
has shown that people tend to generalize algorithmic errors across independent
tasks, violating the behavioral axiom of choice independence. In this paper, we
analyze whether user utilization of novel writing assistants in a charity
advertisement writing task is affected by the AI's performance in a second
language. Furthermore, we quantify the extent to which these patterns translate
into the persuasiveness of generated charity advertisements, as well as the
role of peoples' beliefs about LLM utilization in their donation choices. Our
results provide evidence that writers who engage with an LLM-based writing
assistant violate choice independence, as prior exposure to a Spanish LLM
reduces subsequent utilization of an English LLM. While these patterns do not
affect the aggregate persuasiveness of the generated advertisements, people's
beliefs about the source of an advertisement (human versus AI) do. In
particular, Spanish-speaking female participants who believed that they read an
AI-generated advertisement strongly adjusted their donation behavior downwards.
Furthermore, people are generally not able to adequately differentiate between
human-generated and LLM-generated ads. Our work has important implications for
the design, development, integration, and adoption of multilingual LLMs as
assistive agents -- particularly in writing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve LLM-based Automatic Essay Scoring with Linguistic Features <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Joey Hou, Alejandro Ciuba, Xiang Lorraine Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the
grading workload for instructors. Developing a scoring system capable of
handling essays across diverse prompts is challenging due to the flexibility
and diverse nature of the writing task. Existing methods typically fall into
two categories: supervised feature-based approaches and large language model
(LLM)-based methods. Supervised feature-based approaches often achieve higher
performance but require resource-intensive training. In contrast, LLM-based
methods are computationally efficient during inference but tend to suffer from
lower performance. This paper combines these approaches by incorporating
linguistic features into LLM-based scoring. Experimental results show that this
hybrid method outperforms baseline models for both in-domain and out-of-domain
writing prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the workshop Innovation and Responsibility in
  AI-Supported Education (iRaise) at the 2025 Conference on Artificial
  Intelligence (AAAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of
  Large Language Models to Generate Expert-Like Systems Engineering Artifacts
  and a Characterization of Failure Modes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylan G. Topcu, Mohammed Husain, Max Ofsa, Paul Wach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-purpose Large Language Models (LLMs), a subset of generative Artificial
Intelligence (AI), have recently made significant progress. While expectations
for LLMs to assist systems engineering (SE) tasks are paramount; the
interdisciplinary and complex nature of systems, along with the need to
synthesize deep-domain knowledge and operational context, raise questions
regarding the efficacy of LLMs to generate SE artifacts, particularly given
that they are trained using data that is broadly available on the internet. To
that end, we present results from an empirical exploration, where a human
expert-generated SE artifact was taken as a benchmark, parsed, and fed into
various LLMs through prompt engineering to generate segments of typical SE
artifacts. This procedure was applied without any fine-tuning or calibration to
document baseline LLM performance. We then adopted a two-fold mixed-methods
approach to compare AI generated artifacts against the benchmark. First, we
quantitatively compare the artifacts using natural language processing
algorithms and find that when prompted carefully, the state-of-the-art
algorithms cannot differentiate AI-generated artifacts from the human-expert
benchmark. Second, we conduct a qualitative deep dive to investigate how they
differ in terms of quality. We document that while the two-material appear very
similar, AI generated artifacts exhibit serious failure modes that could be
difficult to detect. We characterize these as: premature requirements
definition, unsubstantiated numerical estimates, and propensity to overspecify.
We contend that this study tells a cautionary tale about why the SE community
must be more cautious adopting AI suggested feedback, at least when generated
by multi-purpose LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objective quantification of mood states using large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Onysk, Quentin Huys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional states influence human behaviour and cognition, leading to diverse
thought trajectories. Similarly, Large Language Models (LLMs) showcase an
excellent level of response consistency across wide-ranging contexts (prompts).
We leverage these parallels to establish a framework for quantifying mental
states. Our approach utilises self-report questionnaires that reliably assess
these states due to their inherent sensitivity to patterns of co-occurring
responses. Specifically, we recruited a large sample of participants (N=422) to
investigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set
of depressive mood states measured with participants' open-ended responses to a
depression questionnaire. We show LLM responses to held-out multiple-choice
questions, given participants' open-ended answers, correlate strongly (r:
0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation
from mood representations. We explore a link between these representations and
factor analysis. Using ridge regression, we find depression-related subspaces
within LLM hidden states. We show these subspaces to be predictive of
participants' "Depression" and "Somatic & Emotional Distress" factor scores, as
well as suicidality severity. Overall, LLMs can provide quantitative measures
of mental states. The reliability of these hinges upon how informative the
questions we ask participants are. Used correctly, this approach could
supplement mental state assessment in a variety of settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main text - 9 pages, 5 figures;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Provenance Metadata for Determining the
  Relevance of Images and Videos in News Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Peterka, Matyas Bohacek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most effective misinformation campaigns are multimodal, often combining
text with images and videos taken out of context -- or fabricating them
entirely -- to support a given narrative. Contemporary methods for detecting
misinformation, whether in deepfakes or text articles, often miss the interplay
between multiple modalities. Built around a large language model, the system
proposed in this paper addresses these challenges. It analyzes both the
article's text and the provenance metadata of included images and videos to
determine whether they are relevant. We open-source the system prototype and
interactive web interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multilingual Mind : A <span class="highlight-title">Survey</span> of Multilingual Reasoning in Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Ghosh, Debayan Datta, Sriparna Saha, Chirag Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While reasoning and multilingual capabilities in Language Models (LMs) have
achieved remarkable progress in recent years, their integration into a unified
paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning
requires language models to handle logical reasoning across languages while
addressing misalignment, biases, and challenges in low-resource settings. This
survey provides the first in-depth review of multilingual reasoning in LMs. In
this survey, we provide a systematic overview of existing methods that leverage
LMs for multilingual reasoning, specifically outlining the challenges,
motivations, and foundational aspects of applying language models to reason
across diverse languages. We provide an overview of the standard data resources
used for training multilingual reasoning in LMs and the evaluation benchmarks
employed to assess their multilingual capabilities. Next, we analyze various
state-of-the-art methods and their performance on these benchmarks. Finally, we
explore future research opportunities to improve multilingual reasoning in LMs,
focusing on enhancing their ability to handle diverse languages and complex
reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-Level Reasoning Segmentation via Multi-turn Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexian Cai, Xiaocui Yang, Yongkang Liu, Daling Wang, Shi Feng, Yifei Zhang, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual perception systems focus on region-level segmentation in
single-turn dialogues, relying on complex and explicit query instructions. Such
systems cannot reason at the pixel level and comprehend dynamic user intent
that changes over interaction. Our work tackles this issue by introducing a
novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on
multi-turn conversations, tracking evolving user intent via multi-turn
interactions for fine-grained segmentation. To establish a benchmark for this
novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on
Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k
multi-turn conversational scenarios with segmentation targets. Building on
PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning
Segmentation framework, integrates pixel-level segmentation with robust
multi-turn conversation understanding, generating pixel-grounded explanations
aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in
pixel-level reasoning segmentation. Experimental results on the PRIST dataset
demonstrate that our method outperforms current segmentation-specific baselines
in terms of segmentation and LLM-based reasoning metrics. The code and data are
available at: https://github.com/ccccai239/PixelRIST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On multi-token prediction for efficient LLM inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somesh Mehra, Javier Alonso Garcia, Lukas Mauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We systematically investigate multi-token prediction (MTP) capabilities
within LLMs pre-trained for next-token prediction (NTP). We first show that
such models inherently possess MTP capabilities via numerical marginalization
over intermediate token probabilities, though performance is data-dependent and
improves with model scale. Furthermore, we explore the challenges of
integrating MTP heads into frozen LLMs and find that their hidden layers are
strongly specialized for NTP, making adaptation non-trivial. Finally, we show
that while joint training of MTP heads with the backbone improves performance,
it cannot fully overcome this barrier, prompting further research in this
direction. Our findings provide a deeper understanding of MTP applied to
pretrained LLMs, informing strategies for accelerating inference through
parallel token prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use
  a Different Evaluation Process than Human? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumi Goto, Yusuke Sakai, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the goals of automatic evaluation metrics in grammatical error
correction (GEC) is to rank GEC systems such that it matches human preferences.
However, current automatic evaluations are based on procedures that diverge
from human evaluation. Specifically, human evaluation derives rankings by
aggregating sentence-level relative evaluation results, e.g., pairwise
comparisons, using a rating algorithm, whereas automatic evaluation averages
sentence-level absolute scores to obtain corpus-level scores, which are then
sorted to determine rankings. In this study, we propose an aggregation method
for existing automatic evaluation metrics which aligns with human evaluation
methods to bridge this gap. We conducted experiments using various metrics,
including edit-based metrics, $n$-gram based metrics, and sentence-level
metrics, and show that resolving the gap improves results for the most of
metrics on the SEEDA benchmark. We also found that even BERT-based metrics
sometimes outperform the metrics of GPT-4. We publish our unified
implementation of the metrics and meta-evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind What You Ask For: Emotional and Rational Faces of Persuasion by
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiktoria Mieleszczenko-Kowszewicz, Beata Bajcar, Jolanta Babiak, Berenika Dyczek, Jakub Świstak, Przemysław Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Be careful what you ask for, you just might get it. This saying fits with the
way large language models (LLMs) are trained, which, instead of being rewarded
for correctness, are increasingly rewarded for pleasing the recipient. So, they
are increasingly effective at persuading us that their answers are valuable.
But what tricks do they use in this persuasion? In this study, we examine what
are the psycholinguistic features of the responses used by twelve different
language models. By grouping response content according to rational or
emotional prompts and exploring social influence principles employed by LLMs,
we ask whether and how we can mitigate the risks of LLM-driven mass
misinformation. We position this study within the broader discourse on
human-centred AI, emphasizing the need for interdisciplinary approaches to
mitigate cognitive and societal risks posed by persuasive AI responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQuARE: Sequential Question Answering Reasoning Engine for Enhanced
  Chain-of-Thought in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fleischer, Moshe Berchansky, Gad Markovits, Moshe Wasserblat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of Natural Language Processing, Large Language
Models (LLMs) are tasked with increasingly complex reasoning challenges.
Traditional methods like chain-of-thought prompting have shown promise but
often fall short in fully leveraging a model's reasoning capabilities. This
paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a
novel prompting technique designed to improve reasoning through a
self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts
models to generate and resolve multiple auxiliary questions before tackling the
main query, promoting a more thorough exploration of various aspects of a
topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models
across multiple question-answering datasets, demonstrate that SQuARE
significantly surpasses traditional CoT prompts and existing
rephrase-and-respond methods. By systematically decomposing queries, SQuARE
advances LLM capabilities in reasoning tasks. The code is publicly available at
https://github.com/IntelLabs/RAG-FiT/tree/square.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truth Knows No Language: Evaluating Truthfulness Beyond English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a professionally translated extension of the TruthfulQA
benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and
Spanish. Truthfulness evaluations of large language models (LLMs) have
primarily been conducted in English. However, the ability of LLMs to maintain
truthfulness across languages remains under-explored. Our study evaluates 12
state-of-the-art open LLMs, comparing base and instruction-tuned models using
human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our
findings reveal that, while LLMs perform best in English and worst in Basque
(the lowest-resourced language), overall truthfulness discrepancies across
languages are smaller than anticipated. Furthermore, we show that
LLM-as-a-Judge correlates more closely with human judgments than
multiple-choice metrics, and that informativeness plays a critical role in
truthfulness assessment. Our results also indicate that machine translation
provides a viable approach for extending truthfulness benchmarks to additional
languages, offering a scalable alternative to professional translation.
Finally, we observe that universal knowledge questions are better handled
across languages than context- and time-dependent ones, highlighting the need
for truthfulness evaluations that account for cultural and temporal
variability. Dataset and code are publicly available under open licenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Agents as Digital Representatives in Collective Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Jarrett, Miruna Pîslar, Michiel A. Bakker, Michael Henry Tessler, Raphael Köster, Jan Balaguer, Romuald Elie, Christopher Summerfield, Andrea Tacchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the process of collective decision-making, in which a group of
individuals interactively select a preferred outcome from among a universe of
alternatives. In this context, "representation" is the activity of making an
individual's preferences present in the process via participation by a proxy
agent -- i.e. their "representative". To this end, learned models of human
behavior have the potential to fill this role, with practical implications for
multi-agent scenario studies and mechanism design. In this work, we investigate
the possibility of training \textit{language agents} to behave in the capacity
of representatives of human agents, appropriately expressing the preferences of
those individuals whom they stand for. First, we formalize the setting of
\textit{collective decision-making} -- as the episodic process of interaction
between a group of agents and a decision mechanism. On this basis, we then
formalize the problem of \textit{digital representation} -- as the simulation
of an agent's behavior to yield equivalent outcomes from the mechanism.
Finally, we conduct an empirical case study in the setting of
\textit{consensus-finding} among diverse humans, and demonstrate the
feasibility of fine-tuning large language models to act as digital
representatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond English: The Impact of Prompt Translation Strategies across
  Languages and Tasks in Multilingual LLMs <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Mondshine, Tzuf Paz-Argaman, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in the multilingual capabilities of Large Language Models
(LLMs) across diverse tasks, English remains the dominant language for LLM
research and development. So, when working with a different language, this has
led to the widespread practice of pre-translation, i.e., translating the task
prompt into English before inference. Selective pre-translation, a more
surgical approach, focuses on translating specific prompt components. However,
its current use is sporagic and lacks a systematic research foundation.
Consequently, the optimal pre-translation strategy for various multilingual
settings and tasks remains unclear. In this work, we aim to uncover the optimal
setup for pre-translation by systematically assessing its use. Specifically, we
view the prompt as a modular entity, composed of four functional parts:
instruction, context, examples, and output, either of which could be translated
or not. We evaluate pre-translation strategies across 35 languages covering
both low and high-resource languages, on various tasks including Question
Answering (QA), Natural Language Inference (NLI), Named Entity Recognition
(NER), and Abstractive Summarization. Our experiments show the impact of
factors as similarity to English, translation quality and the size of
pre-trained data, on the model performance with pre-translation. We suggest
practical guidelines for choosing optimal strategies in various multilingual
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for NAACL findings 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Judge-free LLM Open-ended Generation Benchmark Based on the
  Distributional Hypothesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Imajo, Masanori Hirano, Shuji Suzuki, Hiroaki Mikami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the open-ended text generation of large language models (LLMs) is
challenging because of the lack of a clear ground truth and the high cost of
human or LLM-based assessments. We propose a novel benchmark that evaluates
LLMs using n-gram statistics and rules, without relying on human judgement or
LLM-as-a-judge approaches. Using 50 question and reference answer sets, we
introduce three new metrics based on n-grams and rules: Fluency, Truthfulness,
and Helpfulness. Our benchmark strongly correlates with GPT-4o-based
evaluations while requiring significantly fewer computational resources,
demonstrating its effectiveness as a scalable alternative for assessing LLMs'
open-ended generation capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When the LM misunderstood the human chuckled: Analyzing garden path
  effects in humans and language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models (LLMs) have shown human-like abilities in many
language tasks, sparking interest in comparing LLMs' and humans' language
processing. In this paper, we conduct a detailed comparison of the two on a
sentence comprehension task using garden-path constructions, which are
notoriously challenging for humans. Based on psycholinguistic research, we
formulate hypotheses on why garden-path sentences are hard, and test these
hypotheses on human participants and a large suite of LLMs using comprehension
questions. Our findings reveal that both LLMs and humans struggle with specific
syntactic complexities, with some models showing high correlation with human
comprehension. To complement our findings, we test LLM comprehension of
garden-path constructions with paraphrasing and text-to-image generation tasks,
and find that the results mirror the sentence comprehension question results,
further validating our findings on LLM understanding of these constructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparQLe: Speech Queries to Text Translation Through LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirbek Djanibekov, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing influence of Large Language Models (LLMs), there is
increasing interest in integrating speech representations with them to enable
more seamless multi-modal processing and speech understanding. This study
introduces a novel approach that leverages self-supervised speech
representations in combination with instruction-tuned LLMs for speech-to-text
translation. The proposed approach leverages a modality adapter to align
extracted speech features with instruction-tuned LLMs using English-language
data. Our experiments demonstrate that this method effectively preserves the
semantic content of the input speech and serves as an effective bridge between
self-supervised speech models and instruction-tuned LLMs, offering a promising
solution for various speech understanding applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of
  Techniques, Appeals, and Intent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiateng Liu, Lin Ai, Zizhou Liu, Payam Karisani, Zheng Hui, May Fung, Preslav Nakov, Julia Hirschberg, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Propaganda plays a critical role in shaping public opinion and fueling
disinformation. While existing research primarily focuses on identifying
propaganda techniques, it lacks the ability to capture the broader motives and
the impacts of such content. To address these challenges, we introduce
propainsight, a conceptual framework grounded in foundational social science
research, which systematically dissects propaganda into techniques, arousal
appeals, and underlying intent. propainsight offers a more granular
understanding of how propaganda operates across different contexts.
Additionally, we present propagaze, a novel dataset that combines
human-annotated data with high-quality synthetic data generated through a
meticulously designed pipeline. Our experiments show that off-the-shelf LLMs
struggle with propaganda analysis, but training with propagaze significantly
improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span
IoU in technique identification and 66.2% higher BertScore in appeal analysis
compared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited
human-annotated data in data-sparse and cross-domain scenarios, showing its
potential for comprehensive and generalizable propaganda analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeDex: Training LLMs to Better Self-Debug and Explain Code <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, Anoop Deoras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of code generation, self-debugging is crucial. It allows LLMs
to refine their generated code based on execution feedback. This is
particularly important because generating correct solutions in one attempt
proves challenging for complex tasks. Prior works on self-debugging mostly
focus on prompting methods by providing LLMs with few-shot examples, which work
poorly on small open-sourced LLMs. In this work, we propose LeDex, a training
framework that significantly improves the self-debugging capability of LLMs.
Intuitively, we observe that a chain of explanations on the wrong code followed
by code refinement helps LLMs better analyze the wrong code and do refinement.
We thus propose an automated pipeline to collect a high-quality dataset for
code explanation and refinement by generating a number of explanations and
refinement trajectories from the LLM itself or a larger teacher model and
filtering via execution verification. We perform supervised fine-tuning (SFT)
and further reinforcement learning (RL) on both success and failure
trajectories with a novel reward design considering code explanation and
refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by
9.30% over four benchmarks. RL training brings additional up to 3.54%
improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show
iterative refinement ability and can keep refining code continuously. Lastly,
our human evaluation shows that the LLMs trained with our framework generate
more useful code explanations and help developers better understand bugs in
source code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by The Thirty-eighth Annual Conference on
  Neural Information Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GroundCocoa: A Benchmark for Evaluating Compositional & Conditional
  Reasoning in Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsh Kohli, Sachin Kumar, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progress of large language models (LLMs) has seen them excel and
frequently surpass human performance on standard benchmarks. This has enabled
many downstream applications, such as LLM agents, to rely on their reasoning to
address complex task requirements. However, LLMs are known to unexpectedly
falter in simple tasks and under seemingly straightforward circumstances -
underscoring the need for better and more diverse evaluation setups to measure
their true capabilities. To this end, we choose to study compositional and
conditional reasoning, two aspects that are central to human cognition, and
introduce GroundCocoa - a lexically diverse benchmark connecting these
reasoning skills to the real-world problem of flight booking. Our task involves
aligning detailed user preferences with available flight options presented in a
multiple-choice format. Results indicate a significant disparity in performance
among current state-of-the-art LLMs with even the best performing model, GPT-4
Turbo, not exceeding 67% accuracy despite advanced prompting techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 17 figures, 3 tables. Accepted to NAACL 2025 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flaming-hot Initiation with Regular Execution Sampling for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhe Chen, Zhicheng Zhang, Guanlin Liu, Renjie Zheng, Wenlei Shi, Chen Dun, Zheng Wu, Xing Jin, Lin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the release of ChatGPT, large language models (LLMs) have demonstrated
remarkable capabilities across various domains. A key challenge in developing
these general capabilities is efficiently sourcing diverse, high-quality data.
This becomes especially critical in reasoning-related tasks with sandbox
checkers, such as math or code, where the goal is to generate correct solutions
to specific problems with higher probability. In this work, we introduce
Flaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet
highly effective method to efficiently find good responses. Our empirical
findings show that FIRE sampling enhances inference-time generation quality and
also benefits training in the alignment stage. Furthermore, we explore how FIRE
sampling improves performance by promoting diversity and analyze the impact of
employing FIRE at different positions within a response.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RePrompt: Planning by Automatic Prompt Engineering for Large Language
  Models Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhe Chen, Sven Koenig, Bistra Dilkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past year, large language models (LLMs) have had remarkable success in
domains outside the traditional natural language processing, and their capacity
is further expanded into the so-called LLM agents when connected with external
tools. In all domains, the prompt to the LLMs has been shown to make a big
difference in what the LLM would generate and thus affect the performance of
the LLM agents. Therefore, automatic prompt engineering (APE) has become an
important question for many researchers and users of LLMs. However, previous
works in APE rely on a final checker to evaluate the performance of the given
prompt -- a requirement that is hard to meet in the case of LLM agents, where
intermediate feedback is easier to obtain, and the final evaluation could be
expensive, inaccurate, or even missing. In this paper, we propose a novel
method, \textsc{RePrompt}, which does a ``gradient descent"-like approach to
optimize the step-by-step instructions in the prompts given to LLM agents,
based on the chat history obtained from interactions and reflections with LLM
agents. By leveraging intermediate feedback, \textsc{RePrompt} can optimize the
prompt without the need for a final solution checker. We evaluate our approach
on PDDL generation, TravelPlanner, and Meeting Planning to show that our method
could generally improve performance for different reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tulu 3: Pushing Frontiers in Open Language Model Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15124v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15124v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model post-training is applied to refine behaviors and unlock new
skills across a wide range of recent language models, but open recipes for
applying these techniques lag behind proprietary ones. The underlying training
data and recipes for post-training are simultaneously the most important pieces
of the puzzle and the portion with the least transparency. To bridge this gap,
we introduce Tulu 3, a family of fully-open state-of-the-art post-trained
models, alongside its data, code, and training recipes, serving as a
comprehensive guide for modern post-training techniques. Tulu 3, which builds
on Llama 3.1 base models, achieves results surpassing the instruct versions of
Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and
Claude 3.5-Haiku. The training algorithms for our models include supervised
finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we
call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we
introduce a multi-task evaluation scheme for post-training recipes with
development and unseen evaluations, standard benchmark implementations, and
substantial decontamination of existing open datasets on said benchmarks. We
conclude with analysis and discussion of training methods that did not reliably
improve performance.
  In addition to the Tulu 3 model weights and demo, we release the complete
recipe -- including datasets for diverse core skills, a robust toolkit for data
curation and evaluation, the training code and infrastructure, and, most
importantly, a detailed report for reproducing and further adapting the Tulu 3
approach to more domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added Tulu 3 405B results and additional analyses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abstraction Alignment: Comparing Model-Learned and Human-Encoded
  Conceptual Relationships 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While interpretability methods identify a model's learned concepts, they
overlook the relationships between concepts that make up its abstractions and
inform its ability to generalize to new data. To assess whether models' have
learned human-aligned abstractions, we introduce abstraction alignment, a
methodology to compare model behavior against formal human knowledge.
Abstraction alignment externalizes domain-specific human knowledge as an
abstraction graph, a set of pertinent concepts spanning levels of abstraction.
Using the abstraction graph as a ground truth, abstraction alignment measures
the alignment of a model's behavior by determining how much of its uncertainty
is accounted for by the human abstractions. By aggregating abstraction
alignment across entire datasets, users can test alignment hypotheses, such as
which human concepts the model has learned and where misalignments recur. In
evaluations with experts, abstraction alignment differentiates seemingly
similar errors, improves the verbosity of existing model-quality metrics, and
uncovers improvements to current human abstractions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, published in CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The 2021 Tokyo Olympics Multilingual News Article <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Novak, Erik Calcina, Dunja Mladenić, Marko Grobelnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a dataset of multilingual news articles covering
the 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from
1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and
published between July 1, 2021, and August 14, 2021. These articles are written
in nine languages from different language families and in different scripts. To
create the dataset, the raw news articles were first retrieved via a service
that collects and analyzes news articles. Then, the articles were grouped using
an online clustering algorithm, with each group containing articles reporting
on the same sub-event. Finally, the groups were manually annotated and
evaluated. The development of this dataset aims to provide a resource for
evaluating the performance of multilingual news clustering algorithms, for
which limited datasets are available. It can also be used to analyze the
dynamics and events of the 2021 Tokyo Olympics from different perspectives. The
dataset is available in CSV format and can be accessed from the CLARIN.SI
repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Echoes of Discord: Forecasting Hater Reactions to Counterspeech <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoying Song, Sharon Lisseth Perez, Xinchen Yu, Eduardo Blanco, Lingzi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech (HS) erodes the inclusiveness of online users and propagates
negativity and division. Counterspeech has been recognized as a way to mitigate
the harmful consequences. While some research has investigated the impact of
user-generated counterspeech on social media platforms, few have examined and
modeled haters' reactions toward counterspeech, despite the immediate
alteration of haters' attitudes being an important aspect of counterspeech.
This study fills the gap by analyzing the impact of counterspeech from the
hater's perspective, focusing on whether the counterspeech leads the hater to
reenter the conversation and if the reentry is hateful. We compile the Reddit
Echoes of Hate dataset (ReEco), which consists of triple-turn conversations
featuring haters' reactions, to assess the impact of counterspeech. To predict
haters' behaviors, we employ two strategies: a two-stage reaction predictor and
a three-way classifier. The linguistic analysis sheds insights on the language
of counterspeech to hate eliciting different haters' reactions. Experimental
results demonstrate that the 3-way classification model outperforms the
two-stage reaction predictor, which first predicts reentry and then determines
the reentry type. We conclude the study with an assessment showing the most
common errors identified by the best-performing model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Associative Recurrent Memory <span class="highlight-title">Transformer</span> <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, Mikhail Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of creating a neural architecture for very
long sequences that requires constant time for processing new information at
each time step. Our approach, Associative Recurrent Memory Transformer (ARMT),
is based on transformer self-attention for local context and segment-level
recurrence for storage of task specific information distributed over a long
context. We demonstrate that ARMT outperfors existing alternatives in
associative retrieval tasks and sets a new performance record in the recent
BABILong multi-task long-context benchmark by answering single-fact questions
over 50 million tokens with an accuracy of 79.9%. The source code for training
and evaluation is available on github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 Next Generation of Sequence Modeling Architectures Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ API Pack: A Massive Multi-Programming Language <span class="highlight-title">Dataset</span> for API Call
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09615v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09615v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce API Pack, a massive multi-programming language dataset
containing over one million instruction-API calls for improving the API call
generation capabilities of large language models. Our evaluation highlights
three key findings: First, fine-tuning on API Pack enables open-source models
to outperform GPT-3.5 and GPT-4 in generating code for entirely new API calls.
We show this by fine-tuning CodeLlama-13B on 20,000 Python instances from API
Pack. Second, fine-tuning on a large dataset in one language, combined with
smaller datasets from others, improves API generation accuracy across multiple
languages. Third, we confirm the benefits of larger datasets for API
generalization, as increasing fine-tuning data to one million instances
enhances generalization to new APIs. To support further research, we
open-source the API Pack dataset, trained model, and code at
https://github.com/zguo0525/API-Pack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Next Tokens via Second-to-Last Predictions with Generate and
  Refine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models like GPT aim to predict next tokens, while
autoencoding models such as BERT are trained on tasks such as predicting masked
tokens. We train a decoder-only architecture for predicting the second to last
token for a sequence of tokens. Our approach yields higher computational
training efficiency than BERT-style models by employing a structured
deterministic approach to masking tokens. We use our model to improve the next
token predictions of a standard GPT by combining both predictions in a
``generate-then-refine'' approach. We demonstrate on different variants of
GPT-2 and different datasets that (not unexpectedly) second to last token
predictions are much more accurate, i.e., more than 15\% higher accuracy than
standard next token predictions. The ``generate-then-refine'' approach also
demonstrates notable improvements in next-token predictions, yielding smaller
yet consistent and significant gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Intelligent Data Analysis (IDA), 2025, held in Konstanz,
  Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>s Learn Low Sensitivity Functions: Investigations and
  Implications <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers achieve state-of-the-art accuracy and robustness across many
tasks, but an understanding of their inductive biases and how those biases
differ from other neural network architectures remains elusive. In this work,
we identify the sensitivity of the model to token-wise random perturbations in
the input as a unified metric which explains the inductive bias of transformers
across different data modalities and distinguishes them from other
architectures. We show that transformers have lower sensitivity than MLPs,
CNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show
that this low-sensitivity bias has important implications: i) lower sensitivity
correlates with improved robustness; it can also be used as an efficient
intervention to further improve the robustness of transformers; ii) it
corresponds to flatter minima in the loss landscape; and iii) it can serve as a
progress measure for grokking. We support these findings with theoretical
results showing (weak) spectral bias of transformers in the NTK regime, and
improved robustness due to the lower sensitivity. The code is available at
https://github.com/estija/sensitivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. 24 pages, 19 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hello Again! LLM-powered Personalized Agent for Long-term Dialogue <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain dialogue systems have seen remarkable advancements with the
development of large language models (LLMs). Nonetheless, most existing
dialogue systems predominantly focus on brief single-session interactions,
neglecting the real-world demands for long-term companionship and personalized
interactions with chatbots. Crucial to addressing this real-world need are
event summary and persona management, which enable reasoning for appropriate
long-term dialogue responses. Recent progress in the human-like cognitive and
reasoning capabilities of LLMs suggests that LLM-based agents could
significantly enhance automated perception, decision-making, and
problem-solving. In response to this potential, we introduce a model-agnostic
framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three
independently tunable modules dedicated to event perception, persona
extraction, and response generation. For the event memory module, long and
short-term memory banks are employed to separately focus on historical and
ongoing sessions, while a topic-based retrieval mechanism is introduced to
enhance the accuracy of memory retrieval. Furthermore, the persona module
conducts dynamic persona modeling for both users and agents. The integration of
retrieved memories and extracted personas is subsequently fed into the
generator to induce appropriate responses. The effectiveness, generality, and
cross-domain capabilities of LD-Agent are empirically demonstrated across
various illustrative benchmarks, models, and tasks. The code is released at
https://github.com/leolee99/LD-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Zero-Shot Long-Context LLM Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Wang, Yihan Wang, Kai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the effectiveness of zero-shot compression techniques on
large language models (LLMs) under long-context. We identify the tendency for
computational errors to increase under long-context when employing certain
compression methods. We propose a hypothesis to explain the varied behavior of
different LLM compression techniques and explore remedies to mitigate the
performance decline observed in some techniques under long-context. This is a
course report for COS 598D Machine Learning and Systems by Prof. Kai Li at
Princeton University. Due to limited computational resources, our experiments
were conducted only on LLaMA-2-7B-32K.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Salamandra Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aitor Gonzalez-Agirre, Marc Pàmies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo, José Javier Saiz, Ferran Espuña, Jaume Prats, Javier Aula-Blasco, Mario Mina, Iñigo Pikabea, Adrián Rubio, Alexander Shvets, Anna Sallés, Iñaki Lacunza, Jorge Palomar, Júlia Falcão, Lucía Tormo, Luis Vasquez-Reina, Montserrat Marimon, Oriol Pareras, Valle Ruiz-Fernández, Marta Villegas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Salamandra, a suite of open-source decoder-only large
language models available in three different sizes: 2, 7, and 40 billion
parameters. The models were trained from scratch on highly multilingual data
that comprises text in 35 European languages and code. Our carefully curated
corpus is made exclusively from open-access data compiled from a wide variety
of sources. Along with the base models, supplementary checkpoints that were
fine-tuned on public-domain instruction data are also released for chat
applications. Additionally, we also share our preliminary experiments on
multimodality, which serve as proof-of-concept to showcase potential
applications for the Salamandra family. Our extensive evaluations on
multilingual benchmarks reveal that Salamandra has strong capabilities,
achieving competitive performance when compared to similarly sized open-source
models. We provide comprehensive evaluation results both on standard downstream
tasks as well as key aspects related to bias and safety.With this technical
report, we intend to promote open science by sharing all the details behind our
design choices, data curation strategy and evaluation methodology. In addition
to that, we deviate from the usual practice by making our training and
evaluation scripts publicly accessible. We release all models under a
permissive Apache 2.0 license in order to foster future research and facilitate
commercial use, thereby contributing to the open-source ecosystem of large
language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through
  Books <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangmitra Madhusudan, Robert Morabito, Skye Reid, Nikta Gohari Sadr, Ali Emami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Books, while often rich in cultural insights, can also mirror societal biases
of their eras - biases that Large Language Models (LLMs) may learn and
perpetuate during training. We introduce a novel method to trace and quantify
these biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising
593 fictional books across seven decades (1950-2019), to track bias evolution.
By fine-tuning LLMs on books from each decade and using targeted prompts, we
examine shifts in biases related to gender, sexual orientation, race, and
religion. Our findings indicate that LLMs trained on decade-specific books
manifest biases reflective of their times, with both gradual trends and notable
shifts. For example, model responses showed a progressive increase in the
portrayal of women in leadership roles (from 8% to 22%) from the 1950s to
2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly
aligning with third-wave feminism. Same-sex relationship references increased
markedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+
visibility. Concerningly, negative portrayals of Islam rose sharply in the
2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we
demonstrate that these biases stem mainly from the books' content and not the
models' architecture or initial training. Our study offers a new perspective on
societal bias trends by bridging AI, literary studies, and social science
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages (excluding references), accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Human Contribution in AI-Assisted Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Xie, Tao Qi, Jingwei Yi, Xiyuan Yang, Ryan Whalen, Junming Huang, Qian Ding, Yu Xie, Xing Xie, Fangzhao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing prevalence of generative artificial intelligence (AI), an
increasing amount of content is no longer exclusively generated by humans but
by generative AI models with human guidance. This shift presents notable
challenges for the delineation of originality due to the varying degrees of
human contribution in AI-assisted works. This study raises the research
question of measuring human contribution in AI-assisted content generation and
introduces a framework to address this question that is grounded in information
theory. By calculating mutual information between human input and AI-assisted
output relative to self-information of AI-assisted output, we quantify the
proportional information contribution of humans in content generation. Our
experimental results demonstrate that the proposed measure effectively
discriminates between varying degrees of human contribution across multiple
creative domains. We hope that this work lays a foundation for measuring human
contributions in AI-assisted content generation in the era of generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rationalization Models for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework for generating Chain-of-Thought (CoT) rationales to
enhance text-to-SQL model fine-tuning. These rationales consist of intermediate
SQL statements and explanations, serving as incremental steps toward
constructing the final SQL query. The process begins with manually annotating a
small set of examples, which are then used to prompt a large language model in
an iterative, dynamic few-shot knowledge distillation procedure from a teacher
model. A rationalization model is subsequently trained on the validated
decomposed queries, enabling extensive synthetic CoT annotations for
text-to-SQL datasets. To evaluate the approach, we fine-tune small language
models with and without these rationales on the BIRD dataset. Results indicate
that step-by-step query generation improves execution accuracy, especially for
moderately and highly complex queries, while also enhancing explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image
  Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Ma, Xiayang Xiao, Sihao Dong, Peidong Wang, HaiPeng Wang, Qingyun Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a powerful all-weather Earth observation tool, synthetic aperture radar
(SAR) remote sensing enables critical military reconnaissance, maritime
surveillance, and infrastructure monitoring. Although Vision language models
(VLMs) have made remarkable progress in natural language processing and image
understanding, their applications remain limited in professional domains due to
insufficient domain expertise. This paper innovatively proposes the first
large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which
contains approximately 2 million high-quality image-text pairs, encompasses
diverse scenarios with detailed target annotations. This dataset not only
supports several key tasks such as visual understanding and object detection
tasks, but also has unique innovative aspects: this study develop a
visual-language dataset and benchmark for the SAR domain, enabling and
evaluating VLMs' capabilities in SAR image interpretation, which provides a
paradigmatic framework for constructing multimodal datasets across various
remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the
effectiveness of the dataset has been fully verified. The project will be
released at https://github.com/JimmyMa99/SARChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of OM
tools. Our framework is implemented in a proof-of-concept system. Evaluations
of three Ontology Alignment Evaluation Initiative (OAEI) tracks over
state-of-the-art OM systems show that our system can achieve results very close
to the long-standing best performance on simple OM tasks and can significantly
improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Embeddings with Coupled Adam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Stollenwerk, Tobias Stollenwerk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their remarkable capabilities, LLMs learn word representations that
exhibit the undesirable yet poorly understood feature of anisotropy. In this
paper, we argue that the second moment in Adam is a cause of anisotropic
embeddings, and suggest a modified optimizer called Coupled Adam to mitigate
the problem. Our experiments demonstrate that Coupled Adam significantly
improves the quality of embeddings, while also leading to better upstream and
downstream performance on large enough datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures; figures corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Factual Consistency of News Summarization by Contrastive
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19347v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19347v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, Qianli Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent progress in news summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as "hallucinations" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, overgeneralizing, etc. These hallucinations are challenging to detect
through traditional methods, which poses great challenges for improving the
factual consistency of text summarization. In this paper, we propose
Contrastive Preference Optimization (CPO) to disentangle the LLMs' propensities
to generate faithful and fake content. Furthermore, we adopt a probing-based
specific training method to improve their capacity of distinguishing two types
of propensities. In this way, LLMs can execute the instructions more accurately
and have enhanced perception of hallucinations. Experimental results show that
CPO significantly improves the reliability of summarization based on LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The LLM Language Network: A Neuroscientific Approach for Identifying
  Causally Task-Relevant Units <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities on not just
language tasks, but also various tasks that are not linguistic in nature, such
as logical reasoning and social inference. In the human brain, neuroscience has
identified a core language system that selectively and causally supports
language processing. We here ask whether similar specialization for language
emerges in LLMs. We identify language-selective units within 18 popular LLMs,
using the same localization approach that is used in neuroscience. We then
establish the causal role of these units by demonstrating that ablating LLM
language-selective units -- but not random units -- leads to drastic deficits
in language tasks. Correspondingly, language-selective LLM units are more
aligned to brain recordings from the human language system than random units.
Finally, we investigate whether our localization method extends to other
cognitive domains: while we find specialized networks in some LLMs for
reasoning and social capabilities, there are substantial differences among
models. These findings provide functional and causal evidence for
specialization in large language models, and highlight parallels with the
functional organization in the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WarriorCoder: Learning from Expert Battles to Augment Code Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Feng, Pu Zhao, Qingfeng Sun, Can Xu, Fangkai Yang, Lu Wang, Qianli Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress achieved by code large language models (LLMs), their
remarkable abilities are largely dependent on fine-tuning on the high-quality
data, posing challenges for data collection and annotation. To address this,
current methods often design various data flywheels to collect complex code
instructions, enabling models to handle more intricate tasks. However, these
approaches typically rely on off-the-shelf datasets and data augmentation from
a limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which
restricts the diversity of the constructed data and makes it prone to systemic
biases. In this paper, we propose WarriorCoder, a novel paradigm learns from
expert battles to address these limitations. Specifically, we create an arena
where leading expert code LLMs challenge each other, with evaluations conducted
by impartial judges. This competitive framework generates novel training data
from scratch, leveraging the strengths of all participants. Experimental
results show that WarriorCoder achieves state-of-the-art performance compared
to previous models of the same size, even without relying on proprietary LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Prompt Internalization <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haebin Shin, Lei Ji, Yeyun Gong, Sungdong Kim, Eunbi Choi, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompts used in recent large language model based applications are often
fixed and lengthy, leading to significant computational overhead. To address
this challenge, we propose Generative Prompt Internalization (GenPI), a
lightweight method that employs a joint training approach. GenPI not only
replicates the behavior of models with prompt inputs but also generates the
content of the prompt along with reasons for why the model's behavior should
change accordingly. We demonstrate that our approach effectively internalizes
complex prompts across various agent-based application scenarios. For effective
training without interactions with the dedicated environments, we introduce a
data synthesis technique that autonomously collects conversational datasets by
swapping the roles of the agent and environment. This method is especially
useful in scenarios where only a predefined prompt is available without a
corresponding training dataset. By internalizing complex prompts, Generative
Prompt Internalization enables high performance and efficient inference without
the need for explicit prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial
  Stance for Summary Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J. Han, Hang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness evaluators based on large language models (LLMs) are often
fooled by the fluency of the text and struggle with identifying errors in the
summaries. We propose an approach to summary faithfulness evaluation in which
multiple LLM-based agents are assigned initial stances (regardless of what
their belief might be) and forced to come up with a reason to justify the
imposed belief, thus engaging in a multi-round debate to reach an agreement.
The uniformly distributed initial assignments result in a greater diversity of
stances leading to more meaningful debates and ultimately more errors
identified. Furthermore, by analyzing the recent faithfulness evaluation
datasets, we observe that naturally, it is not always the case for a summary to
be either faithful to the source document or not. We therefore introduce a new
dimension, ambiguity, and a detailed taxonomy to identify such special cases.
Experiments demonstrate our approach can help identify ambiguities, and have
even a stronger performance on non-ambiguous summaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Actionable Framework for Assessing Bias and Fairness in Large
  Language Model Use Cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10853v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10853v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Bouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can exhibit bias in a variety of ways. Such
biases can create or exacerbate unfair outcomes for certain groups within a
protected attribute, including, but not limited to sex, race, sexual
orientation, or age. In this paper, we propose a decision framework that allows
practitioners to determine which bias and fairness metrics to use for a
specific LLM use case. To establish the framework, we define bias and fairness
risks for LLMs, map those risks to a taxonomy of LLM use cases, and then define
various metrics to assess each type of risk. Instead of focusing solely on the
model itself, we account for both prompt-specific- and model-specific-risk by
defining evaluations at the level of an LLM use case, characterized by a model
and a population of prompts. Furthermore, because all of the evaluation metrics
are calculated solely using the LLM output, our proposed framework is highly
practical and easily actionable for practitioners. For streamlined
implementation, all evaluation metrics included in the framework are offered in
this paper's companion Python toolkit, LangFair. Finally, our experiments
demonstrate substantial variation in bias and fairness across use cases,
underscoring the importance of use-case-level assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LangFair repository: https://github.com/cvs-health/langfair</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Device Emoji Classifier Trained with GPT-based Data Augmentation for
  a Mobile Keyboard 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossam Amer, Joe Osborne, Michael Zaki, Mohamed Afify
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emojis improve communication quality among smart-phone users that use mobile
keyboards to exchange text. To predict emojis for users based on input text, we
should consider the on-device low memory and time constraints, ensure that the
on-device emoji classifier covers a wide range of emoji classes even though the
emoji dataset is typically imbalanced, and adapt the emoji classifier output to
user favorites. This paper proposes an on-device emoji classifier based on
MobileBert with reasonable memory and latency requirements for SwiftKey. To
account for the data imbalance, we utilize the widely used GPT to generate one
or more tags for each emoji class. For each emoji and corresponding tags, we
merge the original set with GPT-generated sentences and label them with this
emoji without human intervention to alleviate the data imbalance. At inference
time, we interpolate the emoji output with the user history for emojis for
better emoji classifications. Results show that the proposed on-device emoji
classifier deployed for SwiftKey increases the accuracy performance of emoji
prediction particularly on rare emojis and emoji engagement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepThink: Aligning Language Models with Domain-Specific User Intents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Mingxuan Luo, Yeyun Gong, Chen Lin, Jian Jiao, Yi Liu, Kaili Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning with synthesized instructions has been a common
practice for adapting LLMs to domain-specific QA tasks. However, the
synthesized instructions deviate from real user questions and expected answers.
This study proposes a novel framework called DeepThink to generate high-quality
instructions. DeepThink first generates a few seed questions to mimic actual
user questions, simulates conversations to uncover the hidden user needs, and
refines the answer by conversational contexts and the retrieved documents for
more comprehensive answers. Experiments demonstrate that DeepThink achieves an
average performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based
assistant on the real user test set in the advertising domain across dimensions
such as relevance, completeness, clarity, accuracy, and actionability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Large Language Model Performance with Gradient-Based Parameter
  Selection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoling Li, Xin Zhang, Xiao Liu, Yeyun Gong, Yifan Wang, Qi Chen, Peng Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized lots of fields of research.
Although it is well-known that fine-tuning is essential for enhancing the
capabilities of LLMs, existing research suggests that there is potential
redundancy in the fine-tuning process and therefore proposes to update only a
subset of parameters. However, these methods fail to leverage the task-specific
information to identify important parameters during training. Based on the
insight that gradients inherently contain information on task-specific data, we
propose Gradient-Mask Tuning (GMT), a method that selectively updates
parameters during training based on their gradient information. Specifically,
we compute the absolute values of the gradients and apply masking to those with
relatively smaller magnitudes. Our empirical results across various tasks
demonstrate that GMT not only outperforms traditional fine-tuning methods but
also elevates the upper limits of LLM performance. Further analysis indicates
that GMT exhibits insensitivity to mask ratio and possesses computational
efficiency comparable to vanilla SFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">63</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PUGS: Perceptual Uncertainty for Grasp Selection in Underwater
  Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onur Bagoren, Marc Micatka, Katherine A. Skinner, Aaron Marburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When navigating and interacting in challenging environments where sensory
information is imperfect and incomplete, robots must make decisions that
account for these shortcomings. We propose a novel method for quantifying and
representing such perceptual uncertainty in 3D reconstruction through occupancy
uncertainty estimation. We develop a framework to incorporate it into grasp
selection for autonomous manipulation in underwater environments. Instead of
treating each measurement equally when deciding which location to grasp from,
we present a framework that propagates uncertainty inherent in the multi-view
reconstruction process into the grasp selection. We evaluate our method with
both simulated and the real world data, showing that by accounting for
uncertainty, the grasp selection becomes robust against partial and noisy
measurements. Code will be made available at
https://onurbagoren.github.io/PUGS/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures Accepted to International Conference on Robotics
  and Automation (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Solver-Aided Hierarchical Language for LLM-Driven CAD Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin T. Jones, Felix Hähnlein, Zihan Zhang, Maaz Ahmad, Vladimir Kim, Adriana Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been enormously successful in solving a
wide variety of structured and unstructured generative tasks, but they struggle
to generate procedural geometry in Computer Aided Design (CAD). These
difficulties arise from an inability to do spatial reasoning and the necessity
to guide a model through complex, long range planning to generate complex
geometry. We enable generative CAD Design with LLMs through the introduction of
a solver-aided, hierarchical domain specific language (DSL) called AIDL, which
offloads the spatial reasoning requirements to a geometric constraint solver.
Additionally, we show that in the few-shot regime, AIDL outperforms even a
language with in-training data (OpenSCAD), both in terms of generating visual
results closer to the prompt and creating objects that are easier to
post-process and reason about.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the robustness of <span class="highlight-title">multimodal</span> language model towards distractions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Liu, Hao Chen, Jindong Wang, Wensheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although vision-language models (VLMs) have achieved significant success in
various applications such as visual question answering, their resilience to
prompt variations remains an under-explored area. Understanding how
distractions affect VLMs is crucial for improving their real-world
applicability, as inputs could have noisy and irrelevant information in many
practical scenarios. This paper aims to assess the robustness of VLMs against
both visual and textual distractions in the context of science question
answering. Built on the ScienceQA dataset, we developed a new benchmark that
introduces distractions in both the visual and textual contexts to evaluate the
reasoning capacity of VLMs amid these distractions. Our findings reveal that
most-of-the-art VLMs, including GPT-4, are vulnerable to various types of
distractions, experiencing noticeable degradation in reasoning capabilities
when confronted with distractions. Notably, models such as InternVL2
demonstrate a higher degree of robustness to these distractions. We also found
that models exhibit greater sensitivity to textual distractions than visual
ones. Additionally, we explored various mitigation strategies, such as prompt
engineering, to counteract the impact of distractions. While these strategies
improved solution accuracy, our analysis shows that there remain significant
opportunities for improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Deepfakes - A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tharindu Fernando, Darshana Priyasad, Sridha Sridharan, Arun Ross, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements in deep- fake generation technology
have led to unprecedented leaps in its realism and capabilities. Despite these
advances, we observe a notable lack of structured and deep analysis deepfake
technology. The principal aim of this survey is to contribute a thorough
theoretical analysis of state-of-the-art face deepfake generation and detection
methods. Furthermore, we provide a coherent and systematic evaluation of the
implications of deepfakes on face biometric recognition approaches. In
addition, we outline key applications of face deepfake technology, elucidating
both positive and negative applications of the technology, provide a detailed
discussion regarding the gaps in existing research, and propose key research
directions for further investigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Patient-Specific Surgical Planning for Bicuspid Aortic Valve
  Repair: Fully Automated Segmentation of the Aortic Valve in 4D CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaiyang Guo, Ningjun J Dong, Harold Litt, Natalie Yushkevich, Melanie Freas, Jessica Nunez, Victor Ferrari, Jilei Hao, Shir Goldfinger, Matthew A. Jolley, Joseph Bavaria, Nimesh Desai, Alison M. Pouch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The bicuspid aortic valve (BAV) is the most prevalent congenital heart defect
and may require surgery for complications such as stenosis, regurgitation, and
aortopathy. BAV repair surgery is effective but challenging due to the
heterogeneity of BAV morphology. Multiple imaging modalities can be employed to
assist the quantitative assessment of BAVs for surgical planning.
Contrast-enhanced 4D computed tomography (CT) produces volumetric temporal
sequences with excellent contrast and spatial resolution. Segmentation of the
aortic cusps and root in these images is an essential step in creating patient
specific models for visualization and quantification. While deep learning-based
methods are capable of fully automated segmentation, no BAV-specific model
exists. Among valve segmentation studies, there has been limited quantitative
assessment of the clinical usability of the segmentation results. In this work,
we developed a fully auto- mated multi-label BAV segmentation pipeline based on
nnU-Net. The predicted segmentations were used to carry out surgically relevant
morphological measurements including geometric cusp height, commissural angle
and annulus diameter, and the results were compared against manual
segmentation. Automated segmentation achieved average Dice scores of over 0.7
and symmetric mean distance below 0.7 mm for all three aortic cusps and the
root wall. Clinically relevant benchmarks showed good consistency between
manual and predicted segmentations. Overall, fully automated BAV segmentation
of 3D frames in 4D CT can produce clinically usable measurements for surgical
risk stratification, but the temporal consistency of segmentations needs to be
improved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8,
  ResNet50, and Inception-ResNet-v2 Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaa Awad, Salah A. Aly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thousands of individuals succumb annually to leukemia alone. As artificial
intelligence-driven technologies continue to evolve and advance, the question
of their applicability and reliability remains unresolved. This study aims to
utilize image processing and deep learning methodologies to achieve
state-of-the-art results for the detection of Acute Lymphoblastic Leukemia
(ALL) using data that best represents real-world scenarios. ALL is one of
several types of blood cancer, and it is an aggressive form of leukemia. In
this investigation, we examine the most recent advancements in ALL detection,
as well as the latest iteration of the YOLO series and its performance. We
address the question of whether white blood cells are malignant or benign.
Additionally, the proposed models can identify different ALL stages, including
early stages. Furthermore, these models can detect hematogones despite their
frequent misclassification as ALL. By utilizing advanced deep learning models,
namely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves
accuracy rates as high as 99.7%, demonstrating the effectiveness of these
algorithms across multiple datasets and various real-world situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 28 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging
  Illumination Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planetary exploration using aerial assets has the potential for unprecedented
scientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved
flight in Martian atmosphere is possible, future Mars rotocrafts will require
advanced navigation capabilities for long-range flights. One such critical
capability is Map-based Localization (MbL) which registers an onboard image to
a reference map during flight in order to mitigate cumulative drift from visual
odometry. However, significant illumination differences between rotocraft
observations and a reference map prove challenging for traditional MbL systems,
restricting the operational window of the vehicle. In this work, we investigate
a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model
for image registration that is more robust under large illumination differences
than prior models. The system is supported by a custom simulation framework
that uses real orbital maps to produce large amounts of realistic images of the
Martian terrain. Comprehensive evaluations show that our proposed system
outperforms prior MbL efforts in terms of localization accuracy under
significant lighting and scale variations. Furthermore, we demonstrate the
validity of our approach across a simulated Martian day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise Controlled CT Super-Resolution with Conditional Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Wang, Siyeop Yoon, Rui Hu, Baihui Yu, Duhgoon Lee, Rajiv Gupta, Li Zhang, Zhiqiang Chen, Dufan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the spatial resolution of CT images is a meaningful yet challenging
task, often accompanied by the issue of noise amplification. This article
introduces an innovative framework for noise-controlled CT super-resolution
utilizing the conditional diffusion model. The model is trained on hybrid
datasets, combining noise-matched simulation data with segmented details from
real data. Experimental results with real CT images validate the effectiveness
of our proposed framework, showing its potential for practical applications in
CT imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 8th International Conference on Image Formation in X-Ray Computed
  Tomography, Bamberg, Germany, August 5 - 9, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atom identification in bilayer moire materials with Gomb-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin C. Houston, Sumner B. Harris, Hao Wang, Yu-Chuan Lin, David B. Geohegan, Kai Xiao, Gerd Duscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moire patterns in van der Waals bilayer materials complicate the analysis of
atomic-resolution images, hindering the atomic-scale insight typically
attainable with scanning transmission electron microscopy. Here, we report a
method to detect the positions and identity of atoms in each of the individual
layers that compose bilayer heterostructures. We developed a deep learning
model, Gomb-Net, which can distinguish atomic species in each individual layer,
effectively deconvoluting the moire pattern to enable layer-specific mapping of
strain and dopant distributions, unlike other methods which struggle with
moire-induced complexity. Using this approach, we explored Se atom
substitutional sites in a twisted fractional Janus WS2-WS2(1-x)Se2x
heterostructure and found that layer specific implantation sites are unaffected
by the moire pattern's local energetic or electronic modulation. This
advancement enables atom-identification within material regimes where it was
not possible before, opening new insights into previously inaccessible material
physics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Muscle and Fat Segmentation in Computed Tomo<span class="highlight-title">graph</span>y for
  Comprehensive Body Composition Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqian Chen, Hanxue Gu, Yuwen Chen, Jicheng Yang, Haoyu Dong, Joseph Y. Cao, Adrian Camarena, Christopher Mantyh, Roy Colglazier, Maciej A. Mazurowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Body composition assessment using CT images can potentially be used for a
number of clinical applications, including the prognostication of
cardiovascular outcomes, evaluation of metabolic health, monitoring of disease
progression, assessment of nutritional status, prediction of treatment response
in oncology, and risk stratification for surgical and critical care outcomes.
While multiple groups have developed in-house segmentation tools for this
analysis, there are very limited publicly available tools that could be
consistently used across different applications. To mitigate this gap, we
present a publicly accessible, end-to-end segmentation and feature calculation
model specifically for CT body composition analysis. Our model performs
segmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and
visceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in
axial CT images. It also provides various body composition metrics, including
muscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle
area/volume, and skeletal muscle index (SMI), supporting both 2D and 3D
assessments. The model is shared for public use. To evaluate the model, the
segmentation was applied to both internal and external datasets, with body
composition metrics analyzed across different age, sex, and race groups. The
model achieved high dice coefficients on both internal and external datasets,
exceeding 89% for skeletal muscle, SAT, and VAT segmentation. The model
outperforms the benchmark by 2.40% on skeletal muscle and 10.26% on SAT
compared to the manual annotations given by the publicly available dataset.
Body composition metrics show mean relative absolute errors (MRAEs) under 10%
for all measures. Furthermore, the model provided muscular fat segmentation
with a Dice coefficient of 56.27%, which can be utilized for additional
analyses as needed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CellFlow: Simulating Cellular Morphology Changes via Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Yuchang Su, Chenyu Wang, Tianhong Li, Zoe Wefers, Jeffrey Nirschl, James Burgess, Daisy Ding, Alejandro Lozano, Emma Lundberg, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a virtual cell capable of accurately simulating cellular behaviors
in silico has long been a dream in computational biology. We introduce
CellFlow, an image-generative model that simulates cellular morphology changes
induced by chemical and genetic perturbations using flow matching. Unlike prior
methods, CellFlow models distribution-wise transformations from unperturbed to
perturbed cell states, effectively distinguishing actual perturbation effects
from experimental artifacts such as batch effects -- a major challenge in
biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined
perturbation (JUMP) datasets, CellFlow generates biologically meaningful cell
images that faithfully capture perturbation-specific morphological changes,
achieving a 35% improvement in FID scores and a 12% increase in mode-of-action
prediction accuracy over existing methods. Additionally, CellFlow enables
continuous interpolation between cellular states, providing a potential tool
for studying perturbation dynamics. These capabilities mark a significant step
toward realizing virtual cell modeling for biomedical research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A CNN Approach to Automated Detection and Classification of <span class="highlight-title">Brain</span> Tumors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Zahid Hasan, Abdullah Tamim, D. M. Asadujjaman, Md. Mahfujur Rahman, Md. Abu Ahnaf Mollick, Nosin Anjum Dristi,  Abdullah-Al-Noman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors require an assessment to ensure timely diagnosis and effective
patient treatment. Morphological factors such as size, location, texture, and
variable appearance com- plicate tumor inspection. Medical imaging presents
challenges, including noise and incomplete images. This research article
presents a methodology for processing Magnetic Resonance Imag- ing (MRI) data,
encompassing techniques for image classification and denoising. The effective
use of MRI images allows medical professionals to detect brain disorders,
including tumors. This research aims to categorize healthy brain tissue and
brain tumors by analyzing the provided MRI data. Unlike alternative methods
like Computed Tomography (CT), MRI technology offers a more detailed
representation of internal anatomical components, mak- ing it a suitable option
for studying data related to brain tumors. The MRI picture is first subjected
to a denoising technique utilizing an Anisotropic diffusion filter. The dataset
utilized for the models creation is a publicly accessible and validated Brain
Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE
was employed for data augmentation and dataset balancing. Convolutional Neural
Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for
the classification procedure. EfficientNet attained an accuracy of 98%, the
highest recorded.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embed Any NeRF: <span class="highlight-title">Graph</span> Meta-Networks for Neural Tasks on Arbitrary NeRF
  Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for
representing 3D objects and scenes by encoding shape and appearance information
into the weights of a neural network. Recent works have shown how such weights
can be used as input to frameworks processing them to solve deep learning
tasks. Yet, these frameworks can only process NeRFs with a specific, predefined
architecture. In this paper, we present the first framework that can ingest
NeRFs with multiple architectures and perform inference on architectures unseen
at training time. We achieve this goal by training a Graph Meta-Network in a
representation learning framework. Moreover, we show how a contrastive
objective is conducive to obtaining an architecture-agnostic latent space. In
experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates
robust performance in classification and retrieval tasks that either matches or
exceeds that of existing frameworks constrained to single architectures, thus
providing the first architecture-agnostic method to perform tasks on NeRFs by
processing their weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-CoT: Benchmarking Chain-of-Thought in Large <span class="highlight-title">Multimodal</span> Models for
  Reasoning Quality, Robustness, and Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions with Chain-of-Thought (CoT) has significantly enhanced
the reasoning capabilities of Large Language Models (LLMs), yet its impact on
Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth
investigation. In this paper, we introduce MME-CoT, a specialized benchmark
evaluating the CoT reasoning performance of LMMs, spanning six domains: math,
science, OCR, logic, space-time, and general scenes. As the first comprehensive
study in this area, we propose a thorough evaluation suite incorporating three
novel metrics that assess the reasoning quality, robustness, and efficiency at
a fine-grained level. Leveraging curated high-quality data and a unique
evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,
uncovering several key insights: 1) Models with reflection mechanism
demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and
demonstrating the highest quality results; 2) CoT prompting often degrades LMM
performance on perception-heavy tasks, suggesting a potentially harmful
overthinking behavior; and 3) Although the CoT quality is high, LMMs with
reflection exhibit significant inefficiency in both normal response and
self-correction phases. We hope MME-CoT serves as a foundation for advancing
multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mmecot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Encoder-free Architectures in 3D LMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encoder-free architectures have been preliminarily explored in the 2D visual
domain, yet it remains an open question whether they can be effectively applied
to 3D understanding scenarios. In this paper, we present the first
comprehensive investigation into the potential of encoder-free architectures to
overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs).
These challenges include the failure to adapt to varying point cloud
resolutions and the point features from the encoder not meeting the semantic
needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to
remove the encoder and enable the LLM to assume the role of the 3D encoder: 1)
We propose the LLM-embedded Semantic Encoding strategy in the pre-training
stage, exploring the effects of various point cloud self-supervised losses. And
we present the Hybrid Semantic Loss to extract high-level semantics. 2) We
introduce the Hierarchical Geometry Aggregation strategy in the instruction
tuning stage. This incorporates inductive bias into the LLM early layers to
focus on the local details of the point clouds. To the end, we present the
first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current
state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the
classification, captioning, and VQA tasks, respectively. Our results
demonstrate that the encoder-free architecture is highly promising for
replacing encoder-based architectures in the field of 3D understanding. The
code is released at https://github.com/Ivan-Tang-3D/ENEL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is released at https://github.com/Ivan-Tang-3D/ENEL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Kahana, Or Nathan, Eliahu Horwitz, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing numbers of publicly available models, there are probably
pretrained, online models for most tasks users require. However, current model
search methods are rudimentary, essentially a text-based search in the
documentation, thus users cannot find the relevant models. This paper presents
ProbeLog, a method for retrieving classification models that can recognize a
target concept, such as "Dog", without access to model metadata or training
data. Differently from previous probing methods, ProbeLog computes a descriptor
for each output dimension (logit) of each model, by observing its responses on
a fixed set of inputs (probes). Our method supports both logit-based retrieval
("find more logits like this") and zero-shot, text-based retrieval ("find all
logits corresponding to dogs"). As probing-based representations require
multiple costly feedforward passes through the model, we develop a method,
based on collaborative filtering, that reduces the cost of encoding
repositories by 3x. We demonstrate that ProbeLog achieves high retrieval
accuracy, both in real-world and fine-grained search tasks and is scalable to
full-size repositories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback
  Over Multi-Resolution Gaussians-on-Mesh <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wen, Alexander G. Schwing, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable rendering of an animatable human avatar from sparse inputs
relies on data priors and inductive biases extracted from training on large
data to avoid scene-specific optimization and to enable fast reconstruction.
This raises two main challenges: First, unlike iterative gradient-based
adjustment in scene-specific optimization, generalizable methods must
reconstruct the human shape representation in a single pass at inference time.
Second, rendering is preferably computationally efficient yet of high
resolution. To address both challenges we augment the recently proposed dual
shape representation, which combines the benefits of a mesh and Gaussian
points, in two ways. To improve reconstruction, we propose an iterative
feedback update framework, which successively improves the canonical human
shape representation during reconstruction. To achieve computationally
efficient yet high-resolution rendering, we study a coupled-multi-resolution
Gaussians-on-Mesh representation. We evaluate the proposed approach on the
challenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an
animatable representation from sparse inputs in less than 1s, renders views
with 95.1FPS at $1024 \times 1024$, and achieves PSNR/LPIPS*/FID of
24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in
rendering quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Project page: https://wenj.github.io/LIFe-GoM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Rectified Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengsheng Guo, Alexander G. Schwing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Variational Rectified Flow Matching, a framework that enhances
classic rectified flow matching by modeling multi-modal velocity vector-fields.
At inference time, classic rectified flow matching 'moves' samples from a
source distribution to the target distribution by solving an ordinary
differential equation via integration along a velocity vector-field. At
training time, the velocity vector-field is learnt by linearly interpolating
between coupled samples one drawn from the source and one drawn from the target
distribution randomly. This leads to ''ground-truth'' velocity vector-fields
that point in different directions at the same location, i.e., the velocity
vector-fields are multi-modal/ambiguous. However, since training uses a
standard mean-squared-error loss, the learnt velocity vector-field averages
''ground-truth'' directions and isn't multi-modal. In contrast, variational
rectified flow matching learns and samples from multi-modal flow directions. We
show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational
rectified flow matching leads to compelling results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexTrack: Towards Generalizable Neural Tracking Control for Dexterous
  Manipulation from Human References <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyi Liu, Jianibieke Adalibieke, Qianwei Han, Yuzhe Qin, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of developing a generalizable neural tracking
controller for dexterous manipulation from human references. This controller
aims to manage a dexterous robot hand to manipulate diverse objects for various
purposes defined by kinematic human-object interactions. Developing such a
controller is complicated by the intricate contact dynamics of dexterous
manipulation and the need for adaptivity, generalizability, and robustness.
Current reinforcement learning and trajectory optimization methods often fall
short due to their dependence on task-specific rewards or precise system
models. We introduce an approach that curates large-scale successful robot
tracking demonstrations, comprising pairs of human references and robot
actions, to train a neural controller. Utilizing a data flywheel, we
iteratively enhance the controller's performance, as well as the number and
quality of successful tracking demonstrations. We exploit available tracking
demonstrations and carefully integrate reinforcement learning and imitation
learning to boost the controller's performance in dynamic environments. At the
same time, to obtain high-quality tracking demonstrations, we individually
optimize per-trajectory tracking by leveraging the learned tracking controller
in a homotopy optimization method. The homotopy optimization, mimicking
chain-of-thought, aids in solving challenging trajectory tracking problems to
increase demonstration diversity. We showcase our success by training a
generalizable neural controller and evaluating it in both simulation and real
world. Our method achieves over a 10% improvement in success rates compared to
leading baselines. The project website with animated results is available at
https://meowuu7.github.io/DexTrack/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/
  Code: https://github.com/Meowuu7/DexTrack/ Video:
  https://youtu.be/zru1Z-DaiWE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, Zifan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RigAnything, a novel autoregressive transformer-based model, which
makes 3D assets rig-ready by probabilistically generating joints, skeleton
topologies, and assigning skinning weights in a template-free manner. Unlike
most existing auto-rigging methods, which rely on predefined skeleton template
and are limited to specific categories like humanoid, RigAnything approaches
the rigging problem in an autoregressive manner, iteratively predicting the
next joint based on the global input shape and the previous prediction. While
autoregressive models are typically used to generate sequential data,
RigAnything extends their application to effectively learn and represent
skeletons, which are inherently tree structures. To achieve this, we organize
the joints in a breadth-first search (BFS) order, enabling the skeleton to be
defined as a sequence of 3D locations and the parent index. Furthermore, our
model improves the accuracy of position prediction by leveraging diffusion
modeling, ensuring precise and consistent placement of joints within the
hierarchy. This formulation allows the autoregressive model to efficiently
capture both spatial and hierarchical relationships within the skeleton.
Trained end-to-end on both RigNet and Objaverse datasets, RigAnything
demonstrates state-of-the-art performance across diverse object types,
including humanoids, quadrupeds, marine creatures, insects, and many more,
surpassing prior methods in quality, robustness, generalizability, and
efficiency. Please check our website for more details:
https://www.liuisabella.com/RigAnything.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.liuisabella.com/RigAnything</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZeroBench: An Impossible Visual Benchmark for Contemporary Large
  <span class="highlight-title">Multimodal</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, Kai Han, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting
images and, by some measures, have poorer spatial cognition than small children
or animals. Despite this, they attain high scores on many popular visual
benchmarks, with headroom rapidly eroded by an ongoing surge of model progress.
To address this, there is a pressing need for difficult benchmarks that remain
relevant for longer. We take this idea to its limit by introducing ZeroBench-a
lightweight visual reasoning benchmark that is entirely impossible for
contemporary frontier LMMs. Our benchmark consists of 100 manually curated
questions and 334 less difficult subquestions. We evaluate 20 LMMs on
ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To
encourage progress in visual understanding, we publicly release ZeroBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Radiance Fields with 3D-aware 2D Representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyi Zhou, Xi Liu, Feng Luo, Siyu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent 3D reconstruction has shown great promise in empowering 3D semantic
understanding and 3D generation by distilling 2D features into the 3D space.
However, existing approaches struggle with the domain gap between 2D feature
space and 3D representations, resulting in degraded rendering performance. To
address this challenge, we propose a novel framework that integrates 3D
awareness into the 2D latent space. The framework consists of three stages: (1)
a correspondence-aware autoencoding method that enhances the 3D consistency of
2D latent representations, (2) a latent radiance field (LRF) that lifts these
3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field
(VAE-RF) alignment strategy that improves image decoding from the rendered 2D
representations. Extensive experiments demonstrate that our method outperforms
the state-of-the-art latent 3D reconstruction approaches in terms of synthesis
performance and cross-dataset generalizability across diverse indoor and
outdoor scenes. To our knowledge, this is the first work showing the radiance
field representations constructed from 2D latent representations can yield
photorealistic 3D reconstruction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025; Project page:
  https://latent-radiance-field.github.io/LRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Conditional Prior Distribution for Flow-Based Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Issachar, Mohammad Salama, Raanan Fattal, Sagie Benaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow-based generative models have recently shown impressive performance for
conditional generation tasks, such as text-to-image generation. However,
current methods transform a general unimodal noise distribution to a specific
mode of the target data distribution. As such, every point in the initial
source distribution can be mapped to every point in the target distribution,
resulting in long average paths. To this end, in this work, we tap into a
non-utilized property of conditional flow-based models: the ability to design a
non-trivial prior distribution. Given an input condition, such as a text
prompt, we first map it to a point lying in data space, representing an
``average" data point with the minimal average distance to all data points of
the same conditional mode (e.g., class). We then utilize the flow matching
formulation to map samples from a parametric distribution centered around this
point to the conditional target distribution. Experimentally, our method
significantly improves training times and generation efficiency (FID, KID and
CLIP alignment scores) compared to baselines, producing high quality samples
using fewer sampling steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance Segmentation of Scene Sketches Using Natural Image Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sketch segmentation involves grouping pixels within a sketch that belong to
the same object or instance. It serves as a valuable tool for sketch editing
tasks, such as moving, scaling, or removing specific components. While image
segmentation models have demonstrated remarkable capabilities in recent years,
sketches present unique challenges for these models due to their sparse nature
and wide variation in styles. We introduce SketchSeg, a method for instance
segmentation of raster scene sketches. Our approach adapts state-of-the-art
image segmentation and object detection models to the sketch domain by
employing class-agnostic fine-tuning and refining segmentation masks using
depth cues. Furthermore, our method organizes sketches into sorted layers,
where occluded instances are inpainted, enabling advanced sketch editing
applications. As existing datasets in this domain lack variation in sketch
styles, we construct a synthetic scene sketch segmentation dataset featuring
sketches with diverse brush strokes and varying levels of detail. We use this
dataset to demonstrate the robustness of our approach and will release it to
promote further research in the field.
  Project webpage: https://sketchseg.github.io/sketch-seg/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAIA: A Global, <span class="highlight-title">Multi-modal</span>, Multi-scale Vision-Language <span class="highlight-title">Dataset</span> for
  Remote Sensing Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous operation of Earth-orbiting satellites generates vast and
ever-growing archives of Remote Sensing (RS) images. Natural language presents
an intuitive interface for accessing, querying, and interpreting the data from
such archives. However, existing Vision-Language Models (VLMs) are
predominantly trained on web-scraped, noisy image-text data, exhibiting limited
exposure to the specialized domain of RS. This deficiency results in poor
performance on RS-specific tasks, as commonly used datasets often lack
detailed, scientifically accurate textual descriptions and instead emphasize
solely on attributes like date and location. To bridge this critical gap, we
introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and
multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated
RS image-text pairs, representing a diverse range of RS modalities associated
to different spatial resolutions. Unlike existing vision-language datasets in
RS, GAIA specifically focuses on capturing a diverse range of RS applications,
providing unique information about environmental changes, natural disasters,
and various other dynamic phenomena. The dataset provides a spatially and
temporally balanced distribution, spanning across the globe, covering the last
25 years with a balanced temporal distribution of observations. GAIA's
construction involved a two-stage process: (1) targeted web-scraping of images
and accompanying text from reputable RS-related sources, and (2) generation of
five high-quality, scientifically grounded synthetic captions for each image
using carefully crafted prompts that leverage the advanced vision-language
capabilities of GPT-4o. Our extensive experiments, including fine-tuning of
CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance
on RS image classification, cross-modal retrieval and image captioning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusing DeBias: a Recipe for Turning a Bug into a Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning model effectiveness in classification tasks is often challenged
by the quality and quantity of training data which, whenever containing strong
spurious correlations between specific attributes and target labels, can result
in unrecoverable biases in model predictions. Tackling these biases is crucial
in improving model generalization and trust, especially in real-world
scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting
as a plug-in for common methods in model debiasing while exploiting the
inherent bias-learning tendency of diffusion models. Our approach leverages
conditional diffusion models to generate synthetic bias-aligned images, used to
train a bias amplifier model, to be further employed as an auxiliary method in
different unsupervised debiasing approaches. Our proposed method, which also
tackles the common issue of training set memorization typical of this type of
tech- niques, beats current state-of-the-art in multiple benchmark datasets by
significant margins, demonstrating its potential as a versatile and effective
tool for tackling dataset bias in deep learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 Pages, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Calibrating Gaussian Splatting for Large Field of View
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a self-calibrating framework that jointly optimizes
camera parameters, lens distortion and 3D Gaussian representations, enabling
accurate and efficient scene reconstruction. In particular, our technique
enables high-quality scene reconstruction from Large field-of-view (FOV)
imagery taken with wide-angle lenses, allowing the scene to be modeled from a
smaller number of images. Our approach introduces a novel method for modeling
complex lens distortions using a hybrid network that combines invertible
residual networks with explicit grids. This design effectively regularizes the
optimization process, achieving greater accuracy than conventional camera
models. Additionally, we propose a cubemap-based resampling strategy to support
large FOV images without sacrificing resolution or introducing distortion
artifacts. Our method is compatible with the fast rasterization of Gaussian
Splatting, adaptable to a wide variety of camera lens distortion, and
demonstrates state-of-the-art performance on both synthetic and real-world
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://denghilbert.github.io/self-cali/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbodiedBench: Comprehensive Benchmarking <span class="highlight-title">Multi-modal</span> Large Language
  Models for Vision-Driven Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied
agents offers a promising avenue for tackling real-world tasks. While
language-centric embodied agents have garnered substantial attention,
MLLM-based embodied agents remain underexplored due to the lack of
comprehensive evaluation frameworks. To bridge this gap, we introduce
EmbodiedBench, an extensive benchmark designed to evaluate vision-driven
embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing
tasks across four environments, ranging from high-level semantic tasks (e.g.,
household) to low-level tasks involving atomic actions (e.g., navigation and
manipulation); and (2) six meticulously curated subsets evaluating essential
agent capabilities like commonsense reasoning, complex instruction
understanding, spatial awareness, visual perception, and long-term planning.
Through extensive experiments, we evaluated 13 leading proprietary and
open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel
at high-level tasks but struggle with low-level manipulation, with the best
model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a
multifaceted standardized evaluation platform that not only highlights existing
challenges but also offers valuable insights to advance MLLM-based embodied
agents. Our code is available at https://embodiedbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Shen, Cong Wang, Junyao Gao, Qin Guo, Jisheng Dang, Jinhui Tang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in conditional diffusion models have shown promise for
generating realistic TalkingFace videos, yet challenges persist in achieving
consistent head movement, synchronized facial expressions, and accurate lip
synchronization over extended generations. To address these, we introduce the
\textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel
(\textbf{MCDM}), which utilizes both archived and current clip motion priors to
enhance motion prediction and ensure temporal consistency. The model consists
of three key elements: (1) an archived-clip motion-prior that incorporates
historical frames and a reference frame to preserve identity and context; (2) a
present-clip motion-prior diffusion model that captures multimodal causality
for accurate predictions of head movements, lip sync, and expressions; and (3)
a memory-efficient temporal attention mechanism that mitigates error
accumulation by dynamically storing and updating motion features. We also
release the \textbf{TalkingFace-Wild} dataset, a multilingual collection of
over 200 hours of footage across 10 languages. Experimental results demonstrate
the effectiveness of MCDM in maintaining identity and motion continuity for
long-term TalkingFace generation. Code, models, and datasets will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteROI-D: System Design and Mapping for Stereo Depth Inference on
  Regions of Interest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Erhardt, Ziang Li, Reid Pinkham, Andrew Berkovich, Zhengya Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms have enabled high quality stereo depth estimation
to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy
consumption across the full image processing stack prevents stereo depth
algorithms from running effectively on battery-limited devices. This paper
introduces SteROI-D, a full stereo depth system paired with a mapping
methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity
at the system level to save energy. SteROI-D's flexible and heterogeneous
compute fabric supports diverse ROIs. Importantly, we introduce a systematic
mapping methodology to effectively handle dynamic ROIs, thereby maximizing
energy savings. Using these techniques, our 28nm prototype SteROI-D design
achieves up to 4.35x reduction in total system energy compared to a baseline
ASIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQ-GAN: Semantic Image Communications Using Masked Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Pezone, Sergio Barbarossa, Giuseppe Caire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach
integrating generative models to optimize image compression for
semantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic
semantic segmentation and a new specifically developed semantic-conditioned
adaptive mask module (SAMM) to selectively encode semantically significant
features of the images. SQ-GAN outperforms state-of-the-art image compression
schemes such as JPEG2000 and BPG across multiple metrics, including perceptual
quality and semantic segmentation accuracy on the post-decoding reconstructed
image, at extreme low compression rates expressed in bits per pixel.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and How Does CLIP Enable Domain and Compositional Generalization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable generalization performance of contrastive vision-language
models like CLIP is often attributed to the diversity of their training
distributions. However, key questions remain unanswered: Can CLIP generalize to
an entirely unseen domain when trained on a diverse mixture of domains (domain
generalization)? Can it generalize to unseen classes within partially seen
domains (compositional generalization)? What factors affect such
generalization? To answer these questions, we trained CLIP models on
systematically constructed training distributions with controlled domain
diversity and object class exposure. Our experiments show that domain diversity
is essential for both domain and compositional generalization, yet
compositional generalization can be surprisingly weaker than domain
generalization when the training distribution contains a suboptimal subset of
the test domain. Through data-centric and mechanistic analyses, we find that
successful generalization requires learning of shared representations already
in intermediate layers and shared circuitry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prior-Constrained Association Learning for Fine-Grained Generalized
  Category Discovery <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menglin Wang, Zhun Zhong, Xiaojin Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses generalized category discovery (GCD), the task of
clustering unlabeled data from potentially known or unknown categories with the
help of labeled instances from each known category. Compared to traditional
semi-supervised learning, GCD is more challenging because unlabeled data could
be from novel categories not appearing in labeled data. Current
state-of-the-art methods typically learn a parametric classifier assisted by
self-distillation. While being effective, these methods do not make use of
cross-instance similarity to discover class-specific semantics which are
essential for representation learning and category discovery. In this paper, we
revisit the association-based paradigm and propose a Prior-constrained
Association Learning method to capture and learn the semantic relations within
data. In particular, the labeled data from known categories provides a unique
prior for the association of unlabeled data. Unlike previous methods that only
adopts the prior as a pre or post-clustering refinement, we fully incorporate
the prior into the association process, and let it constrain the association
towards a reliable grouping outcome. The estimated semantic groups are utilized
through non-parametric prototypical contrast to enhance the representation
learning. A further combination of both parametric and non-parametric
classification complements each other and leads to a model that outperforms
existing methods by a significant margin. On multiple GCD benchmarks, we
perform extensive experiments and validate the effectiveness of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Provenance Metadata for Determining the
  Relevance of Images and Videos in News Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Peterka, Matyas Bohacek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most effective misinformation campaigns are multimodal, often combining
text with images and videos taken out of context -- or fabricating them
entirely -- to support a given narrative. Contemporary methods for detecting
misinformation, whether in deepfakes or text articles, often miss the interplay
between multiple modalities. Built around a large language model, the system
proposed in this paper addresses these challenges. It analyzes both the
article's text and the provenance metadata of included images and videos to
determine whether they are relevant. We open-source the system prototype and
interactive web interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Standardisation of Convex Ultrasound Data Through Geometric Analysis and
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Weld, Giovanni Faoro, Luke Dixon, Sophie Camp, Arianna Menciassi, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of ultrasound in healthcare has seen increased diversity and
importance. Unlike other medical imaging modalities, ultrasound research and
development has historically lagged, particularly in the case of applications
with data-driven algorithms. A significant issue with ultrasound is the extreme
variability of the images, due to the number of different machines available
and the possible combination of parameter settings. One outcome of this is the
lack of standardised and benchmarking ultrasound datasets. The method proposed
in this article is an approach to alleviating this issue of disorganisation.
For this purpose, the issue of ultrasound data sparsity is examined and a novel
perspective, approach, and solution is proposed; involving the extraction of
the underlying ultrasound plane within the image and representing it using
annulus sector geometry. An application of this methodology is proposed, which
is the extraction of scan lines and the linearisation of convex planes.
Validation of the robustness of the proposed method is performed on both
private and public data. The impact of deformation and the invertibility of
augmentation using the estimated annulus sector parameters is also studied.
Keywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation
  Networks for Quantitative Nanomaterial Analysis through Differentiable
  Rendering and Generative Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Possart, Leonid Mill, Florian Vollnhals, Tor Hildebrand, Peter Suter, Mathis Hoffmann, Jonas Utz, Daniel Augsburger, Mareike Thies, Mingxuan Wu, Fabian Wagner, George Sarau, Silke Christiansen, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanomaterials exhibit distinctive properties governed by parameters such as
size, shape, and surface characteristics, which critically influence their
applications and interactions across technological, biological, and
environmental contexts. Accurate quantification and understanding of these
materials are essential for advancing research and innovation. In this regard,
deep learning segmentation networks have emerged as powerful tools that enable
automated insights and replace subjective methods with precise quantitative
analysis. However, their efficacy depends on representative annotated datasets,
which are challenging to obtain due to the costly imaging of nanoparticles and
the labor-intensive nature of manual annotations. To overcome these
limitations, we introduce DiffRenderGAN, a novel generative model designed to
produce annotated synthetic data. By integrating a differentiable renderer into
a Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes
textural rendering parameters to generate realistic, annotated nanoparticle
images from non-annotated real microscopy images. This approach reduces the
need for manual intervention and enhances segmentation performance compared to
existing synthetic data methods by generating diverse and realistic data.
Tested on multiple ion and electron microscopy cases, including titanium
dioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),
DiffRenderGAN bridges the gap between synthetic and real data, advancing the
quantification and understanding of complex nanomaterial systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for
  Weakly-supervised Oriented Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yu, Xue Yang, Yansheng Li, Zhenjun Han, Feipeng Da, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the orientation of visual objects with compact rotated
bounding boxes (RBoxes) has become a prominent demand, which challenges
existing object detection paradigms that only use horizontal bounding boxes
(HBoxes). To equip the detectors with orientation awareness, supervised
regression/classification modules have been introduced at the high cost of
rotation annotation. Meanwhile, some existing datasets with oriented objects
are already annotated with horizontal boxes or even single points. It becomes
attractive yet remains open for effectively utilizing weaker single point and
horizontal annotations to train an oriented object detector (OOD). We develop
Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging
various labeling forms (Points, HBoxes, RBoxes, and their combination) in a
unified fashion. By only using HBox for training, our Wholly-WOOD achieves
performance very close to that of the RBox-trained counterpart on remote
sensing and other areas, significantly reducing the tedious efforts on
labor-intensive annotation for oriented objects. The source codes are available
at https://github.com/VisionXLab/whollywood (PyTorch-based) and
https://github.com/VisionXLab/whollywood-jittor (Jittor-based).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures, 9 tables, accepted by TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metamorphic Testing for Pose Estimation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Duran, Thomas Laurent, Ellen Rushe, Anthony Ventresque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose estimation systems are used in a variety of fields, from sports
analytics to livestock care. Given their potential impact, it is paramount to
systematically test their behaviour and potential for failure. This is a
complex task due to the oracle problem and the high cost of manual labelling
necessary to build ground truth keypoints. This problem is exacerbated by the
fact that different applications require systems to focus on different subjects
(e.g., human versus animal) or landmarks (e.g., only extremities versus whole
body and face), which makes labelled test data rarely reusable. To combat these
problems we propose MET-POSE, a metamorphic testing framework for pose
estimation systems that bypasses the need for manual annotation while assessing
the performance of these systems under different circumstances. MET-POSE thus
allows users of pose estimation systems to assess the systems in conditions
that more closely relate to their application without having to label an ad-hoc
test dataset or rely only on available datasets, which may not be adapted to
their application domain. While we define MET-POSE in general terms, we also
present a non-exhaustive list of metamorphic rules that represent common
challenges in computer vision applications, as well as a specific way to
evaluate these rules. We then experimentally show the effectiveness of MET-POSE
by applying it to Mediapipe Holistic, a state of the art human pose estimation
system, with the FLIC and PHOENIX datasets. With these experiments, we outline
numerous ways in which the outputs of MET-POSE can uncover faults in pose
estimation systems at a similar or higher rate than classic testing using hand
labelled data, and show that users can tailor the rule set they use to the
faults and level of accuracy relevant to their application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at 2025 IEEE Conference on Software Testing,
  Verification and Validation (ICST)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-Level Reasoning Segmentation via Multi-turn Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexian Cai, Xiaocui Yang, Yongkang Liu, Daling Wang, Shi Feng, Yifei Zhang, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual perception systems focus on region-level segmentation in
single-turn dialogues, relying on complex and explicit query instructions. Such
systems cannot reason at the pixel level and comprehend dynamic user intent
that changes over interaction. Our work tackles this issue by introducing a
novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on
multi-turn conversations, tracking evolving user intent via multi-turn
interactions for fine-grained segmentation. To establish a benchmark for this
novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on
Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k
multi-turn conversational scenarios with segmentation targets. Building on
PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning
Segmentation framework, integrates pixel-level segmentation with robust
multi-turn conversation understanding, generating pixel-grounded explanations
aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in
pixel-level reasoning segmentation. Experimental results on the PRIST dataset
demonstrate that our method outperforms current segmentation-specific baselines
in terms of segmentation and LLM-based reasoning metrics. The code and data are
available at: https://github.com/ccccai239/PixelRIST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redistribute Ensemble Training for Mitigating Memorization in Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoliu Guan, Yu Wu, Huayang Huang, Xiao Liu, Jiaxu Miao, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, known for their tremendous ability to generate high-quality
samples, have recently raised concerns due to their data memorization behavior,
which poses privacy risks. Recent methods for memory mitigation have primarily
addressed the issue within the context of the text modality in cross-modal
generation tasks, restricting their applicability to specific conditions. In
this paper, we propose a novel method for diffusion models from the perspective
of visual modality, which is more generic and fundamental for mitigating
memorization. Directly exposing visual data to the model increases memorization
risk, so we design a framework where models learn through proxy model
parameters instead. Specially, the training dataset is divided into multiple
shards, with each shard training a proxy model, then aggregated to form the
final model. Additionally, practical analysis of training losses illustrates
that the losses for easily memorable images tend to be obviously lower. Thus,
we skip the samples with abnormally low loss values from the current mini-batch
to avoid memorizing. However, balancing the need to skip memorization-prone
samples while maintaining sufficient training data for high-quality image
generation presents a key challenge. Thus, we propose IET-AGC+, which
redistributes highly memorizable samples between shards, to mitigate these
samples from over-skipping. Furthermore, we dynamically augment samples based
on their loss values to further reduce memorization. Extensive experiments and
analysis on four datasets show that our method successfully reduces memory
capacity while maintaining performance. Moreover, we fine-tune the pre-trained
diffusion models, e.g., Stable Diffusion, and decrease the memorization score
by 46.7\%, demonstrating the effectiveness of our method. Code is available in:
https://github.com/liuxiao-guan/IET_AGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures. arXiv admin note: substantial text overlap with
  arXiv:2407.15328</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Virtual Clinical Trials of Radiology AI with Conditional
  Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin D. Killeen, Bohua Wan, Aditya V. Kulkarni, Nathan Drenkow, Michael Oberst, Paul H. Yi, Mathias Unberath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) is poised to transform healthcare by enabling
personalized and efficient care through data-driven insights. Although
radiology is at the forefront of AI adoption, in practice, the potential of AI
models is often overshadowed by severe failures to generalize: AI models can
have performance degradation of up to 20% when transitioning from controlled
test environments to clinical use by radiologists. This mismatch raises
concerns that radiologists will be misled by incorrect AI predictions in
practice and/or grow to distrust AI, rendering these promising technologies
practically ineffectual. Exhaustive clinical trials of AI models on abundant
and diverse data is thus critical to anticipate AI model degradation when
encountering varied data samples. Achieving these goals, however, is
challenging due to the high costs of collecting diverse data samples and
corresponding annotations. To overcome these limitations, we introduce a novel
conditional generative AI model designed for virtual clinical trials (VCTs) of
radiology AI, capable of realistically synthesizing full-body CT images of
patients with specified attributes. By learning the joint distribution of
images and anatomical structures, our model enables precise replication of
real-world patient populations with unprecedented detail at this scale. We
demonstrate meaningful evaluation of radiology AI models through VCTs powered
by our synthetic CT study populations, revealing model degradation and
facilitating algorithmic auditing for bias-inducing data attributes. Our
generative AI approach to VCTs is a promising avenue towards a scalable
solution to assess model robustness, mitigate biases, and safeguard patient
care by enabling simpler testing and evaluation of AI models in any desired
range of diverse patient populations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone
  Scans with Deep Learning Based Methods Using Geometry and Morphometry
  Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Álvaro Heredia-Lidón, Alejandro Moñux-Bernal, Alejandro González, Luis M. Echeverry-Quiceno, Max Rubert, Aroa Casado, María Esther Esteban, Mireia Andreu-Montoriol, Susanna Gallardo, Cristina Ruffo, Neus Martínez-Abadías, Xavier Sevillano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional (3D) facial shape analysis has gained interest due to its
potential clinical applications. However, the high cost of advanced 3D facial
acquisition systems limits their widespread use, driving the development of
low-cost acquisition and reconstruction methods. This study introduces a novel
evaluation methodology that goes beyond traditional geometry-based benchmarks
by integrating morphometric shape analysis techniques, providing a statistical
framework for assessing facial morphology preservation. As a case study, we
compare smartphone-based 3D scans with state-of-the-art deep learning
reconstruction methods from 2D images, using high-end stereophotogrammetry
models as ground truth. This methodology enables a quantitative assessment of
global and local shape differences, offering a biologically meaningful
validation approach for low-cost 3D facial acquisition and reconstruction
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, Ohad Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models enable high-quality and diverse visual content synthesis.
However, they struggle to generate rare or unseen concepts. To address this
challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with
image generation models. We propose ImageRAG, a method that dynamically
retrieves relevant images based on a given text prompt, and uses them as
context to guide the generation process. Prior approaches that used retrieved
images to improve generation, trained models specifically for retrieval-based
generation. In contrast, ImageRAG leverages the capabilities of existing image
conditioning models, and does not require RAG-specific training. Our approach
is highly adaptable and can be applied across different model types, showing
significant improvement in generating rare and fine-grained concepts using
different base models.
  Our project page is available at: https://rotem-shalev.github.io/ImageRAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiao-Yuan Chin, I-Chao Shen, Yi-Ting Chiu, Bing-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to automatically complete a partial sketch that depicts a complex
scene, e.g., "a woman chatting with a man in the park", is very useful.
However, existing sketch generation methods create sketches from scratch; they
do not complete a partial sketch in the style of the original. To address this
challenge, we introduce AutoSketch, a styleaware vector sketch completion
method that accommodates diverse sketch styles. Our key observation is that the
style descriptions of a sketch in natural language preserve the style during
automatic sketch completion. Thus, we use a pretrained vision-language model
(VLM) to describe the styles of the partial sketches in natural language and
replicate these styles using newly generated strokes. We initially optimize the
strokes to match an input prompt augmented by style descriptions extracted from
the VLM. Such descriptions allow the method to establish a diffusion prior in
close alignment with that of the partial sketch. Next, we utilize the VLM to
generate an executable style adjustment code that adjusts the strokes to
conform to the desired style. We compare our method with existing methods
across various sketch styles and prompts, performed extensive ablation studies
and qualitative and quantitative evaluations, and demonstrate that AutoSketch
can support various sketch scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LATTE: Improving Latex Recognition for Tables and Formulae with
  Iterative Refinement <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Jiang, Shanchao Liang, Chengxiao Wang, Jiannan Wang, Lin Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portable Document Format (PDF) files are dominantly used for storing and
disseminating scientific research, legal documents, and tax information. LaTeX
is a popular application for creating PDF documents. Despite its advantages,
LaTeX is not WYSWYG -- what you see is what you get, i.e., the LaTeX source and
rendered PDF images look drastically different, especially for formulae and
tables. This gap makes it hard to modify or export LaTeX sources for formulae
and tables from PDF images, and existing work is still limited. First, prior
work generates LaTeX sources in a single iteration and struggles with complex
LaTeX formulae. Second, existing work mainly recognizes and extracts LaTeX
sources for formulae; and is incapable or ineffective for tables. This paper
proposes LATTE, the first iterative refinement framework for LaTeX recognition.
Specifically, we propose delta-view as feedback, which compares and pinpoints
the differences between a pair of rendered images of the extracted LaTeX source
and the expected correct image. Such delta-view feedback enables our fault
localization model to localize the faulty parts of the incorrect recognition
more accurately and enables our LaTeX refinement model to repair the incorrect
extraction more accurately. LATTE improves the LaTeX source extraction accuracy
of both LaTeX formulae and tables, outperforming existing techniques as well as
GPT-4V by at least 7.03% of exact match, with a success refinement rate of
46.08% (formula) and 25.51% (table).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by The 39th Annual AAAI Conference on
  Artificial Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning county from pixels: Corn yield prediction with
  attention-weighted multiple instance learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Wang, Yuchi Ma, Qunying Huang, Zhengwei Yang, Zhou Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing technology has become a promising tool in yield prediction.
Most prior work employs satellite imagery for county-level corn yield
prediction by spatially aggregating all pixels within a county into a single
value, potentially overlooking the detailed information and valuable insights
offered by more granular data. To this end, this research examines each county
at the pixel level and applies multiple instance learning to leverage detailed
information within a county. In addition, our method addresses the "mixed
pixel" issue caused by the inconsistent resolution between feature datasets and
crop mask, which may introduce noise into the model and therefore hinder
accurate yield prediction. Specifically, the attention mechanism is employed to
automatically assign weights to different pixels, which can mitigate the
influence of mixed pixels. The experimental results show that the developed
model outperforms four other machine learning models over the past five years
in the U.S. corn belt and demonstrates its best performance in 2022, achieving
a coefficient of determination (R2) value of 0.84 and a root mean square error
(RMSE) of 0.83. This paper demonstrates the advantages of our approach from
both spatial and temporal perspectives. Furthermore, through an in-depth study
of the relationship between mixed pixels and attention, it is verified that our
approach can capture critical feature information while filtering out noise
from mixed pixels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am writing to request the resubmission of my paper submitted to
  arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArchComplete: Autoregressive 3D Architectural Design Generation with
  Hierarchical Diffusion-Based Upsampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Rasoulzadeh, M. Bank, I. Kovacic, K. Schinegger, S. Rutzinger, M. Wimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D generative models have shown promising results but
often fall short in capturing the complexity of architectural geometries and
topologies and fine geometric details at high resolutions. To tackle this, we
present ArchComplete, a two-stage voxel-based 3D generative pipeline consisting
of a vector-quantised model, whose composition is modelled with an
autoregressive transformer for generating coarse shapes, followed by a
hierarchical upsampling strategy for further enrichment with fine structures
and details. Key to our pipeline is (i) learning a contextually rich codebook
of local patch embeddings, optimised alongside a 2.5D perceptual loss that
captures global spatial correspondence of projections onto three axis-aligned
orthogonal planes, and (ii) redefining upsampling as a set of conditional
diffusion models learning from a hierarchy of randomly cropped coarse-to-fine
local volumetric patches. Trained on our introduced dataset of 3D house models
with fully modelled exterior and interior, ArchComplete autoregressively
generates models at the resolution of $64^{3}$ and progressively refines them
up to $512^{3}$, with voxel sizes as small as $ \approx 9\text{cm}$.
ArchComplete solves a variety of tasks, including genetic interpolation and
variation, unconditional synthesis, shape and plan-drawing completion, as well
as geometric detailisation, while achieving state-of-the-art performance in
quality, diversity, and computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maritime Search and Rescue Missions with Aerial Images: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan P. Martinez-Esteso, Francisco J. Castellanos, Jorge Calvo-Zaragoza, Antonio Javier Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed of response by search and rescue teams at sea is of vital
importance, as survival may depend on it. Recent technological advancements
have led to the development of more efficient systems for locating individuals
involved in a maritime incident, such as the use of Unmanned Aerial Vehicles
(UAVs) equipped with cameras and other integrated sensors. Over the past
decade, several researchers have contributed to the development of automatic
systems capable of detecting people using aerial images, particularly by
leveraging the advantages of deep learning. In this article, we provide a
comprehensive review of the existing literature on this topic. We analyze the
methods proposed to date, including both traditional techniques and more
advanced approaches based on machine learning and neural networks.
Additionally, we take into account the use of synthetic data to cover a wider
range of scenarios without the need to deploy a team to collect data, which is
one of the major obstacles for these systems. Overall, this paper situates the
reader in the field of detecting people at sea using aerial images by quickly
identifying the most suitable methodology for each scenario, as well as
providing an in-depth discussion and direction for future trends.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint semi-supervised and contrastive learning enables domain
  generalization and multi-domain segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Gomariz, Yusuke Kikuchi, Yun Yvonna Li, Thomas Albrecht, Andreas Maunz, Daniela Ferrara, Huanxiang Lu, Orcun Goksel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their effectiveness, current deep learning models face challenges
with images coming from different domains with varying appearance and content.
We introduce SegCLR, a versatile framework designed to segment images across
different domains, employing supervised and contrastive learning simultaneously
to effectively learn from both labeled and unlabeled data. We demonstrate the
superior performance of SegCLR through a comprehensive evaluation involving
three diverse clinical datasets of 3D retinal Optical Coherence Tomography
(OCT) images, for the slice-wise segmentation of fluids with various network
configurations and verification across 10 different network initializations. In
an unsupervised domain adaptation context, SegCLR achieves results on par with
a supervised upper-bound model trained on the intended target domain. Notably,
we discover that the segmentation performance of SegCLR framework is marginally
impacted by the abundance of unlabeled data from the target domain, thereby we
also propose an effective domain generalization extension of SegCLR, known also
as zero-shot domain adaptation, which eliminates the need for any target domain
information. This shows that our proposed addition of contrastive loss in
standard supervised training for segmentation leads to superior models,
inherently more generalizable to both in- and out-of-domain test data. We
additionally propose a pragmatic solution for SegCLR deployment in realistic
scenarios with multiple domains containing labeled data. Accordingly, our
framework pushes the boundaries of deep-learning based segmentation in
multi-domain applications, regardless of data availability - labeled,
unlabeled, or nonexistent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Quality Control Of <span class="highlight-title">MRI</span> Images Using Synthetic Motion Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Bricout, Kang Ik K. Cho, Michael Harms, Ofer Pasternak, Carrie E. Bearden, Patrick D. McGorry, Rene S. Kahn, John Kane, Barnaby Nelson, Scott W. Woods, Martha E. Shenton, Sylvain Bouix, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MRI quality control (QC) is challenging due to unbalanced and limited
datasets, as well as subjective scoring, which hinder the development of
reliable automated QC systems. To address these issues, we introduce an
approach that pretrains a model on synthetically generated motion artifacts
before applying transfer learning for QC classification. This method not only
improves the accuracy in identifying poor-quality scans but also reduces
training time and resource requirements compared to training from scratch. By
leveraging synthetic data, we provide a more robust and resource-efficient
solution for QC automation in MRI, paving the way for broader adoption in
diverse research settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurgPLAN++: Universal Surgical Phase Localization Network for Online and
  Offline Inference <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Chen, Xingjian Luo, Jinlin Wu, Long Bai, Zhen Lei, Hongliang Ren, Sebastien Ourselin, Hongbin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical phase recognition is critical for assisting surgeons in
understanding surgical videos. Existing studies focused more on online surgical
phase recognition, by leveraging preceding frames to predict the current frame.
Despite great progress, they formulated the task as a series of frame-wise
classification, which resulted in a lack of global context of the entire
procedure and incoherent predictions. Moreover, besides online analysis,
accurate offline surgical phase recognition is also in significant clinical
need for retrospective analysis, and existing online algorithms do not fully
analyze the entire video, thereby limiting accuracy in offline analysis. To
overcome these challenges and enhance both online and offline inference
capabilities, we propose a universal Surgical Phase Localization Network, named
SurgPLAN++, with the principle of temporal detection. To ensure a global
understanding of the surgical procedure, we devise a phase localization
strategy for SurgPLAN++ to predict phase segments across the entire video
through phase proposals. For online analysis, to generate high-quality phase
proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the
streaming video into a pseudo-complete video through mirroring,
center-duplication, and down-sampling. For offline analysis, SurgPLAN++
capitalizes on its global phase prediction framework to continuously refine
preceding predictions during each online inference step, thereby significantly
improving the accuracy of phase recognition. We perform extensive experiments
to validate the effectiveness, and our SurgPLAN++ achieves remarkable
performance in both online and offline modes, which outperforms
state-of-the-art methods. The source code is available at
https://github.com/franciszchen/SurgPLAN-Plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted by IEEE ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyEmo: Scaling down Emotional Reasoning via Metric Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07062v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07062v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Gutierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TinyEmo, a family of small multi-modal language models
for emotional reasoning and classification. Our approach features: (1) a
synthetic emotional instruct dataset for both pre-training and fine-tuning
stages, (2) a Metric Projector that delegates classification from the language
model allowing for more efficient training and inference, (3) a multi-modal
large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated
framework for bias detection. TinyEmo is able to perform emotion classification
and emotional reasoning, all while using substantially fewer parameters than
comparable models. This efficiency allows us to freely incorporate more diverse
emotional datasets, enabling strong performance on classification tasks, with
our smallest model (700M parameters) outperforming larger state-of-the-art
models based on general-purpose MM-LLMs with over 7B parameters. Additionally,
the Metric Projector allows for interpretability and indirect bias detection in
large models without additional training, offering an approach to understand
and improve AI systems. We release code, models, and dataset at
https://github.com/ggcr/TinyEmo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am withdrawing this work in favour of the confidentiality of
  research ideas that are still under development.</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Opening Articulated Objects in the Real World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does it take to build mobile manipulation systems that can competently
operate on previously unseen objects in previously unseen environments? This
work answers this question using opening of articulated objects as a mobile
manipulation testbed. Specifically, our focus is on the end-to-end performance
on this task without any privileged information, i.e. the robot starts at a
location with the novel target articulated object in view, and has to approach
the object and successfully open it. We first develop a system for this task,
and then conduct 100+ end-to-end system tests across 13 real world test sites.
Our large-scale study reveals a number of surprising findings: a) modular
systems outperform end-to-end learned systems for this task, even when the
end-to-end learned systems are trained on 1000+ demonstrations, b) perception,
and not precise end-effector control, is the primary bottleneck to task
success, and c) state-of-the-art articulation parameter estimation models
developed in isolation struggle when faced with robot-centric viewpoints.
Overall, our findings highlight the limitations of developing components of the
pipeline in isolation and underscore the need for system-level research,
providing a pragmatic roadmap for building generalizable mobile manipulation
systems. Videos, code, and models are available on the project website:
https://arjung128.github.io/opening-articulated-objects/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage:
  https://arjung128.github.io/opening-articulated-objects/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heuristical Comparison of Vision <span class="highlight-title">Transformer</span>s Against Convolutional
  Neural Networks for Semantic Segmentation on Remote Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashim Dahal, Saydul Akbar Murad, Nick Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViT) have recently brought a new wave of research in the
field of computer vision. These models have performed particularly well in
image classification and segmentation. Research on semantic and instance
segmentation has accelerated with the introduction of the new architecture,
with over 80% of the top 20 benchmarks for the iSAID dataset based on either
the ViT architecture or the attention mechanism behind its success. This paper
focuses on the heuristic comparison of three key factors of using (or not
using) ViT for semantic segmentation of remote sensing aerial images on the
iSAID dataset. The experimental results observed during this research were
analyzed based on three objectives. First, we studied the use of a weighted
fused loss function to maximize the mean Intersection over Union (mIoU) score
and Dice score while minimizing entropy or class representation loss. Second,
we compared transfer learning on Meta's MaskFormer, a ViT-based semantic
segmentation model, against a generic UNet Convolutional Neural Network (CNN)
based on mIoU, Dice scores, training efficiency, and inference time. Third, we
examined the trade-offs between the two models in comparison to current
state-of-the-art segmentation models. We show that the novel combined weighted
loss function significantly boosts the CNN model's performance compared to
transfer learning with ViT. The code for this implementation can be found at:
https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of
  Images and Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents Sa2VA, the first unified model for dense grounded
understanding of both images and videos. Unlike existing multi-modal large
language models, which are often limited to specific modalities and tasks,
Sa2VA supports a wide range of image and video tasks, including referring
segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA
combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced
vision-language model, and unifies text, image, and video into a shared LLM
token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2
in producing precise masks, enabling a grounded, multi-modal understanding of
both static and dynamic visual content. Additionally, we introduce Ref-SAV, an
auto-labeled dataset containing over 72k object expressions in complex video
scenes, designed to boost model performance. We also manually validate 2k video
objects in the Ref-SAV datasets to benchmark referring video object
segmentation in complex environments. Experiments show that Sa2VA achieves
state-of-the-art across multiple tasks, particularly in referring video object
segmentation, highlighting its potential for complex real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lxtgh.github.io/project/sa2va</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for
  Remote Sensing Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiancheng Pan, Yanxing Liu, Yuqian Fu, Muyuan Ma, Jiahao Li, Danda Pani Paudel, Luc Van Gool, Xiaomeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, particularly open-vocabulary object detection, plays a
crucial role in Earth sciences, such as environmental monitoring, natural
disaster assessment, and land-use planning. However, existing open-vocabulary
detectors, primarily trained on natural-world images, struggle to generalize to
remote sensing images due to a significant data domain gap. Thus, this paper
aims to advance the development of open-vocabulary object detection in remote
sensing community. To achieve this, we first reformulate the task as Locate
Anything on Earth (LAE) with the goal of detecting any novel concepts on Earth.
We then developed the LAE-Label Engine which collects, auto-annotates, and
unifies up to 10 remote sensing datasets creating the LAE-1M - the first
large-scale remote sensing object detection dataset with broad category
coverage. Using the LAE-1M, we further propose and train the novel LAE-DINO
Model, the first open-vocabulary foundation object detector for the LAE task,
featuring Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt
Learning (VisGT) modules. DVC dynamically constructs vocabulary for each
training batch, while VisGT maps visual features to semantic space, enhancing
text features. We comprehensively conduct experiments on established remote
sensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class
LAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and
the effectiveness of the LAE-DINO method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArthroPhase: A Novel <span class="highlight-title">Dataset</span> and Method for Phase Recognition in
  Arthroscopic Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Bahari Malayeri, Matthias Seibold, Nicola Cavalcanti, Jonas Hein, Sascha Jecklin, Lazaros Vlachopoulos, Sandro Fucentese, Sandro Hodel, Philipp Furnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to advance surgical phase recognition in arthroscopic
procedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by
introducing the first arthroscopy dataset and developing a novel
transformer-based model. We aim to establish a benchmark for arthroscopic
surgical phase recognition by leveraging spatio-temporal features to address
the specific challenges of arthroscopic videos including limited field of view,
occlusions, and visual distortions. We developed the ACL27 dataset, comprising
27 videos of ACL surgeries, each labeled with surgical phases. Our model
employs a transformer-based architecture, utilizing temporal-aware frame-wise
feature extraction through a ResNet-50 and transformer layers. This approach
integrates spatio-temporal features and introduces a Surgical Progress Index
(SPI) to quantify surgery progression. The model's performance was evaluated
using accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80
datasets. The proposed model achieved an overall accuracy of 72.91% on the
ACL27 dataset. On the Cholec80 dataset, the model achieved a comparable
performance with the state-of-the-art methods with an accuracy of 92.4%. The
SPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80
datasets respectively, indicating reliable surgery progression estimation. This
study introduces a significant advancement in surgical phase recognition for
arthroscopy, providing a comprehensive dataset and a robust transformer-based
model. The results validate the model's effectiveness and generalizability,
highlighting its potential to improve surgical training, real-time assistance,
and operational efficiency in orthopedic surgery. The publicly available
dataset and code will facilitate future research and development in this
critical field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D
  Scenes <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Kampffmeyer, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in human motion synthesis have focused on specific types
of motions, such as human-scene interaction, locomotion or human-human
interaction, however, there is a lack of a unified system capable of generating
a diverse combination of motion types. In response, we introduce
Sitcom-Crafter, a comprehensive and extendable system for human motion
generation in 3D space, which can be guided by extensive plot contexts to
enhance workflow efficiency for anime and game designers. The system is
comprised of eight modules, three of which are dedicated to motion generation,
while the remaining five are augmentation modules that ensure consistent fusion
of motion sequences and system functionality. Central to the generation modules
is our novel 3D scene-aware human-human interaction module, which addresses
collision issues by synthesizing implicit 3D Signed Distance Function (SDF)
points around motion spaces, thereby minimizing human-scene collisions without
additional data collection costs. Complementing this, our locomotion and
human-scene interaction modules leverage existing methods to enrich the
system's motion generation capabilities. Augmentation modules encompass plot
comprehension for command generation, motion synchronization for seamless
integration of different motion types, hand pose retrieval to enhance motion
realism, motion collision revision to prevent human collisions, and 3D
retargeting to ensure visual fidelity. Experimental evaluations validate the
system's ability to generate high-quality, diverse, and physically realistic
motions, underscoring its potential for advancing creative workflows. Project
page: https://windvchen.github.io/Sitcom-Crafter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Project Page:
  https://windvchen.github.io/Sitcom-Crafter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 4-LEGS: 4D Language Embedded Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of neural representations has revolutionized our means for
digitally viewing a wide range of 3D scenes, enabling the synthesis of
photorealistic images rendered from novel views. Recently, several techniques
have been proposed for connecting these low-level representations with the
high-level semantics understanding embodied within the scene. These methods
elevate the rich semantic understanding from 2D imagery to 3D representations,
distilling high-dimensional spatial features onto 3D space. In our work, we are
interested in connecting language with a dynamic modeling of the world. We show
how to lift spatio-temporal features to a 4D representation based on 3D
Gaussian Splatting. This enables an interactive interface where the user can
spatiotemporally localize events in the video from text prompts. We demonstrate
our system on public 3D video datasets of people and animals performing various
actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Eurographics 2025. Project webpage:
  https://tau-vailab.github.io/4-LEGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Importance of Backbone to the Adversarial Robustness of Object
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Hang Chen, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is a critical component of various security-sensitive
applications, such as autonomous driving and video surveillance. However,
existing object detectors are vulnerable to adversarial attacks, which poses a
significant challenge to their reliability and security. Through experiments,
first, we found that existing works on improving the adversarial robustness of
object detectors give a false sense of security. Second, we found that
adversarially pre-trained backbone networks were essential for enhancing the
adversarial robustness of object detectors. We then proposed a simple yet
effective recipe for fast adversarial fine-tuning on object detectors with
adversarially pre-trained backbones. Without any modifications to the structure
of object detectors, our recipe achieved significantly better adversarial
robustness than previous works. Finally, we explored the potential of different
modern object detector designs for improving adversarial robustness with our
recipe and demonstrated interesting findings, which inspired us to design
state-of-the-art (SOTA) robust detectors. Our empirical results set a new
milestone for adversarially robust object detection. Code and trained
checkpoints are available at https://github.com/thu-ml/oddefense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TIFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Yan, Yu Zheng, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skins wrapping around our bodies, leathers covering over the sofa, sheet
metal coating the car - it suggests that objects are enclosed by a series of
continuous surfaces, which provides us with informative geometry prior for
objectness deduction. In this paper, we propose Gaussian-Det which leverages
Gaussian Splatting as surface representation for multi-view based 3D object
detection. Unlike existing monocular or NeRF-based methods which depict the
objects via discrete positional data, Gaussian-Det models the objects in a
continuous manner by formulating the input Gaussians as feature descriptors on
a mass of partial surfaces. Furthermore, to address the numerous outliers
inherently introduced by Gaussian splatting, we accordingly devise a Closure
Inferring Module (CIM) for the comprehensive surface-based objectness
deduction. CIM firstly estimates the probabilistic feature residuals for
partial surfaces given the underdetermined nature of Gaussian Splatting, which
are then coalesced into a holistic representation on the overall surface
closure of the object proposal. In this way, the surface information
Gaussian-Det exploits serves as the prior on the quality and reliability of
objectness and the information basis of proposal refinement. Experiments on
both synthetic and real-world datasets demonstrate that Gaussian-Det
outperforms various existing approaches, in terms of both average precision and
recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADBM: Adversarial diffusion bridge model for reliable adversarial
  purification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yining Liu, Yingzhe He, Jie Shi, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently Diffusion-based Purification (DiffPure) has been recognized as an
effective defense method against adversarial examples. However, we find
DiffPure which directly employs the original pre-trained diffusion models for
adversarial purification, to be suboptimal. This is due to an inherent
trade-off between noise purification performance and data recovery quality.
Additionally, the reliability of existing evaluations for DiffPure is
questionable, as they rely on weak adaptive attacks. In this work, we propose a
novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs
a reverse bridge from the diffused adversarial data back to its original clean
examples, enhancing the purification capabilities of the original diffusion
models. Through theoretical analysis and experimental validation across various
scenarios, ADBM has proven to be a superior and robust defense mechanism,
offering significant promise for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion <span class="highlight-title">Transformer</span> Policy: Scaling Diffusion <span class="highlight-title">Transformer</span> for
  Generalist Vision-Language-Action Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large vision-language action models pretrained on diverse robot
datasets have demonstrated the potential for generalizing to new environments
with a few in-domain data. However, those approaches usually predict individual
discretized or continuous action by a small action head, which limits the
ability in handling diverse action spaces. In contrast, we model the continuous
action sequence with a large multi-modal diffusion transformer, dubbed as
Diffusion Transformer Policy, in which we directly denoise action chunks by a
large transformer model rather than a small action head for action embedding.
By leveraging the scaling capability of transformers, the proposed approach can
effectively model continuous end-effector actions across large diverse robot
datasets, and achieve better generalization performance. Extensive experiments
demonstrate the effectiveness and generalization of Diffusion Transformer
Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world
Franka arm, achieving consistent better performance on Real-to-Sim benchmark
SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.
Specifically, without bells and whistles, the proposed approach achieves
state-of-the-art performance with only a single third-view camera stream in the
Calvin task ABC->D, improving the average number of tasks completed in a row of
5 to 3.6, and the pretraining stage significantly facilitates the success
sequence length on the Calvin by over 1.2. Project Page:
https://zhihou7.github.io/dit_policy_vla/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data and Decision Traceability for the Welder's Arc 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasir Latif, Latha Pratti, Samya Bagchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space Protocol is applying the principles derived from MITRE and NIST's
Supply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a
complex multi party system to achieve introspection, auditing, and replay of
data and decisions that ultimately lead to a end decision. The core goal of
decision traceability is to ensure transparency, accountability, and integrity
within the WA system. This is accomplished by providing a clear, auditable path
from the system's inputs all the way to the final decision. This traceability
enables the system to track the various algorithms and data flows that have
influenced a particular outcome.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prioritized Ranking Experimental Design Using Recommender Systems in
  Two-Sided Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahyar Habibi, Zahra Khanalizadeh, Negar Ziaeian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interdependencies between units in online two-sided marketplaces complicate
estimating causal effects in experimental settings. We propose a novel
experimental design to mitigate the interference bias in estimating the total
average treatment effect (TATE) of item-side interventions in online two-sided
marketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the
recommender system as an instrument for experimentation. TSPR strategically
prioritizes items based on their treatment status in the listings displayed to
users. We designed TSPR to provide users with a coherent platform experience by
ensuring access to all items and a consistent realization of their treatment by
all users. We evaluate our experimental design through simulations using a
search impression dataset from an online travel agency. Our methodology closely
estimates the true simulated TATE, while a baseline item-side estimator
significantly overestimates TATE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on LLM-based News Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongyao Wang, Veronica Liesaputra, Zhiyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News recommender systems play a critical role in mitigating the information
overload problem. In recent years, due to the successful applications of large
language model technologies, researchers have utilized Discriminative Large
Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve
the performance of news recommender systems. Although several recent surveys
review significant challenges for deep learning-based news recommender systems,
such as fairness, privacy-preserving, and responsibility, there is a lack of a
systematic survey on Large Language Model (LLM)-based news recommender systems.
In order to review different core methodologies and explore potential issues
systematically, we categorize DLLM-based and GLLM-based news recommender
systems under the umbrella of LLM-based news recommender systems. In this
survey, we first overview the development of deep learning-based news
recommender systems. Then, we review LLM-based news recommender systems based
on three aspects: news-oriented modeling, user-oriented modeling, and
prediction-oriented modeling. Next, we examine the challenges from various
perspectives, including datasets, benchmarking tools, and methodologies.
Furthermore, we conduct extensive experiments to analyze how large language
model technologies affect the performance of different news recommender
systems. Finally, we comprehensively explore the future directions for
LLM-based news recommendations in the era of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FARM: Frequency-Aware Model for Cross-Domain Live-Streaming
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Li, Ruochen Yang, Shuang Wen, Shen Wang, Yueyang Liu, Guoquan Wang, Weisong Hu, Qiang Luo, Jiawei Sheng, Tingwen Liu, Jiangxia Cao, Shuang Yang, Zhaojie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live-streaming services have attracted widespread popularity due to their
real-time interactivity and entertainment value. Users can engage with
live-streaming authors by participating in live chats, posting likes, or
sending virtual gifts to convey their preferences and support. However, the
live-streaming services faces serious data-sparsity problem, which can be
attributed to the following two points: (1) User's valuable behaviors are
usually sparse, e.g., like, comment and gift, which are easily overlooked by
the model, making it difficult to describe user's personalized preference. (2)
The main exposure content on our platform is short-video, which is 9 times
higher than the exposed live-streaming, leading to the inability of
live-streaming content to fully model user preference. To this end, we propose
a Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation, termed
as FARM. Specifically, we first present the intra-domain frequency aware module
to enable our model to perceive user's sparse yet valuable behaviors, i.e.,
high-frequency information, supported by the Discrete Fourier Transform (DFT).
To transfer user preference across the short-video and live-streaming domains,
we propose a novel preference align before fuse strategy, which consists of two
parts: the cross-domain preference align module to align user preference in
both domains with contrastive learning, and the cross-domain preference fuse
module to further fuse user preference in both domains using a serious of
tailor-designed attention mechanisms. Extensive offline experiments and online
A/B testing on Kuaishou live-streaming services demonstrate the effectiveness
and superiority of FARM. Our FARM has been deployed in online live-streaming
services and currently serves hundreds of millions of users on Kuaishou.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Jensen Gap for Max-Min Group Fairness Optimization in
  Recommendation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Yuxin Li, Wenjie Wang, Liang Pang, Jun Xu, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group max-min fairness (MMF) is commonly used in fairness-aware recommender
systems (RS) as an optimization objective, as it aims to protect marginalized
item groups and ensures a fair competition platform. However, our theoretical
analysis indicates that integrating MMF constraint violates the assumption of
sample independence during optimization, causing the loss function to deviate
from linear additivity. Such nonlinearity property introduces the Jensen gap
between the model's convergence point and the optimal point if mini-batch
sampling is applied. Both theoretical and empirical studies show that as the
mini-batch size decreases and the group size increases, the Jensen gap will
widen accordingly. Some methods using heuristic re-weighting or debiasing
strategies have the potential to bridge the Jensen gap. However, they either
lack theoretical guarantees or suffer from heavy computational costs. To
overcome these limitations, we first theoretically demonstrate that the
MMF-constrained objective can be essentially reformulated as a group-weighted
optimization objective. Then we present an efficient and effective algorithm
named FairDual, which utilizes a dual optimization technique to minimize the
Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a
sub-linear convergence rate to the globally optimal solution and the Jensen gap
can be well bounded under a mini-batch sampling strategy with random shuffle.
Extensive experiments conducted using six large-scale RS backbone models on
three publicly available datasets demonstrate that FairDual outperforms all
baselines in terms of both accuracy and fairness. Our data and codes are shared
at https://github.com/XuChen0427/FairDual.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for
  <span class="highlight-title">Graph</span>-RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Huang, Shiqi Zhang, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval
in Large Language Model (LLM)-based question answering. It is particularly
useful in domains such as biomedicine, law, and political science, where
retrieval often requires multi-hop reasoning over proprietary documents. Some
existing Graph-RAG systems construct KNN graphs based on text chunk relevance,
but this coarse-grained approach fails to capture entity relationships within
texts, leading to sub-par retrieval and generation quality. To address this,
recent solutions leverage LLMs to extract entities and relationships from text
chunks, constructing triplet-based knowledge graphs. However, this approach
incurs significant indexing costs, especially for large document collections.
  To ensure a good result accuracy while reducing the indexing cost, we propose
KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small
set of key text chunks and leverages an LLM to construct a knowledge graph
skeleton. It then builds a text-keyword bipartite graph from all text chunks,
serving as a lightweight alternative to a full knowledge graph. During
retrieval, KET-RAG searches both structures: it follows the local search
strategy of existing Graph-RAG systems on the skeleton while mimicking this
search on the bipartite graph to improve retrieval quality. We evaluate eight
solutions on two real-world datasets, demonstrating that KET-RAG outperforms
all competitors in indexing cost, retrieval effectiveness, and generation
quality. Notably, it achieves comparable or superior retrieval quality to
Microsoft's Graph-RAG while reducing indexing costs by over an order of
magnitude. Additionally, it improves the generation quality by up to 32.4%
while lowering indexing costs by around 20%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI
  Suggestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Fasano, Yashar Deldjoo, Tommaso di Noia, Bianca Lau, Sina Adham-Khiabani, Eric Morris, Xia Liu, Ganga Chinna Rao Devarapu, Liam O'Faolain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This demo paper presents AirSense-R, a privacy-preserving mobile application
that provides real-time, pollution-aware recommendations for points of interest
(POIs) in urban environments. By combining real-time air quality monitoring
data with user preferences, the proposed system aims to help users make
health-conscious decisions about the locations they visit. The application
utilizes collaborative filtering for personalized suggestions, and federated
learning for privacy protection, and integrates air pollutant readings from
AirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland.
Additionally, the AirSENCE prediction engine can be employed to detect anomaly
readings and interpolate for air quality readings in areas with sparse sensor
coverage. This system offers a promising, health-oriented POI recommendation
solution that adapts dynamically to current urban air quality conditions while
safeguarding user privacy. The code of AirTOWN and a demonstration video is
made available at the following repo:
https://github.com/AirtownApp/Airtown-Application.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Ads Retrieval at Walmart eCommerce with Language Models
  Progressively Trained on Multiple Knowledge Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaodong Wang, Weizhi Du, Md Omar Faruk Rokon, Pooshpendu Adhikary, Yanbing Xue, Jiaxuan Xu, Jianghong Zhou, Kuang-chih Lee, Musen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sponsored search in e-commerce poses several unique and complex challenges.
These challenges stem from factors such as the asymmetric language structure
between search queries and product names, the inherent ambiguity in user search
intent, and the vast volume of sparse and imbalanced search corpus data. The
role of the retrieval component within a sponsored search system is pivotal,
serving as the initial step that directly affects the subsequent ranking and
bidding systems. In this paper, we present an end-to-end solution tailored to
optimize the ads retrieval system on Walmart.com. Our approach is to pretrain
the BERT-like classification model with product category information, enhancing
the model's understanding of Walmart product semantics. Second, we design a
two-tower Siamese Network structure for embedding structures to augment
training efficiency. Third, we introduce a Human-in-the-loop Progressive Fusion
Training method to ensure robust model performance. Our results demonstrate the
effectiveness of this pipeline. It enhances the search relevance metric by up
to 16% compared to a baseline DSSM-based model. Moreover, our large-scale
online A/B testing demonstrates that our approach surpasses the ad revenue of
the existing production model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Power of Large Language Model for Denoising
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyao Wang, Zhi Zheng, Yongduo Sui, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are crucial for personalizing user experiences but often
depend on implicit feedback data, which can be noisy and misleading. Existing
denoising studies involve incorporating auxiliary information or learning
strategies from interaction data. However, they struggle with the inherent
limitations of external knowledge and interaction data, as well as the
non-universality of certain predefined assumptions, hindering accurate noise
identification. Recently, large language models (LLMs) have gained attention
for their extensive world knowledge and reasoning abilities, yet their
potential in enhancing denoising in recommendations remains underexplored. In
this paper, we introduce LLaRD, a framework leveraging LLMs to improve
denoising in recommender systems, thereby boosting overall recommendation
performance. Specifically, LLaRD generates denoising-related knowledge by first
enriching semantic insights from observational data via LLMs and inferring
user-item preference knowledge. It then employs a novel Chain-of-Thought (CoT)
technique over user-item interaction graphs to reveal relation knowledge for
denoising. Finally, it applies the Information Bottleneck (IB) principle to
align LLM-generated denoising knowledge with recommendation targets, filtering
out noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's
effectiveness in enhancing denoising and recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 4 tables. Accecpted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Member-Group Relations via Multi-View <span class="highlight-title">Graph</span> Filtering for
  Effective Group Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chae-Hyun Kim, Yoon-Ryung Choi, Jin-Duk Park, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group recommendation aims at providing optimized recommendations tailored to
diverse groups, enabling groups to enjoy appropriate items. On the other hand,
most existing group recommendation methods are built upon deep neural network
(DNN) architectures designed to capture the intricate relationships between
member-level and group-level interactions. While these DNN-based approaches
have proven their effectiveness, they require complex and expensive training
procedures to incorporate group-level interactions in addition to member-level
interactions. To overcome such limitations, we introduce Group-GF, a new
approach for extremely fast recommendations of items to each group via
multi-view graph filtering (GF) that offers a holistic view of complex
member-group dynamics, without the need for costly model training.
Specifically, in Group-GF, we first construct three item similarity graphs
manifesting different viewpoints for GF. Then, we discover a distinct
polynomial graph filter for each similarity graph and judiciously aggregate the
three graph filters. Extensive experiments demonstrate the effectiveness of
Group-GF in terms of significantly reducing runtime and achieving
state-of-the-art recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to
  appear) (Please cite our conference version.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Criteria-Aware <span class="highlight-title">Graph</span> Filtering: Extremely Fast Yet Accurate
  Multi-Criteria Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Duk Park, Jaemin Yoo, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-criteria (MC) recommender systems, which utilize MC rating information
for recommendation, are increasingly widespread in various e-commerce domains.
However, the MC recommendation using training-based collaborative filtering,
requiring consideration of multiple ratings compared to single-criterion
counterparts, often poses practical challenges in achieving state-of-the-art
performance along with scalable model training. To solve this problem, we
propose CA-GF, a training-free MC recommendation method, which is built upon
criteria-aware graph filtering for efficient yet accurate MC recommendations.
Specifically, first, we construct an item-item similarity graph using an MC
user-expansion graph. Next, we design CA-GF composed of the following key
components, including 1) criterion-specific graph filtering where the optimal
filter for each criterion is found using various types of polynomial low-pass
filters and 2) criteria preference-infused aggregation where the smoothed
signals from each criterion are aggregated. We demonstrate that CA-GF is (a)
efficient: providing the computational efficiency, offering the extremely fast
runtime of less than 0.2 seconds even on the largest benchmark dataset, (b)
accurate: outperforming benchmark MC recommendation methods, achieving
substantial accuracy gains up to 24% compared to the best competitor, and (c)
interpretable: providing interpretations for the contribution of each criterion
to the model prediction based on visualizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to
  appear) (Please cite our conference version.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Contextual-Aware Position Encoding for Sequential Recommendation <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yuan, Guohao Cai, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR), which encodes user activity to predict the
next action, has emerged as a widely adopted strategy in developing commercial
personalized recommendation systems. A critical component of modern SR models
is the attention mechanism, which synthesizes users' historical activities.
This mechanism is typically order-invariant and generally relies on position
encoding (PE). Conventional SR models simply assign a learnable vector to each
position, resulting in only modest gains compared to traditional recommendation
models. Moreover, limited research has been conducted on position encoding
tailored for sequential recommendation, leaving a significant gap in addressing
its unique requirements. To bridge this gap, we propose a novel
Contextual-Aware Position Encoding method for sequential recommendation,
abbreviated as CAPE. To the best of our knowledge, CAPE is the first PE method
specifically designed for sequential recommendation. Comprehensive experiments
conducted on benchmark SR datasets demonstrate that CAPE consistently enhances
multiple mainstream backbone models and achieves state-of-the-art performance,
across small and large scale model size. Furthermore, we deployed CAPE in an
industrial setting on a real-world commercial platform, clearly showcasing the
effectiveness of our approach. Our source code is available at
https://github.com/yjdy/CAPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'25 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The 2021 Tokyo Olympics Multilingual News Article <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Novak, Erik Calcina, Dunja Mladenić, Marko Grobelnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a dataset of multilingual news articles covering
the 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from
1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and
published between July 1, 2021, and August 14, 2021. These articles are written
in nine languages from different language families and in different scripts. To
create the dataset, the raw news articles were first retrieved via a service
that collects and analyzes news articles. Then, the articles were grouped using
an online clustering algorithm, with each group containing articles reporting
on the same sub-event. Finally, the groups were manually annotated and
evaluated. The development of this dataset aims to provide a resource for
evaluating the performance of multilingual news clustering algorithms, for
which limited datasets are available. It can also be used to analyze the
dynamics and events of the 2021 Tokyo Olympics from different perspectives. The
dataset is available in CSV format and can be accessed from the CLARIN.SI
repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABXI: Invariant Interest Adaptation for Task-Guided Cross-Domain
  Sequential Recommendation <span class="chip">WWW '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingtian Bian, Marcus Vinícius de Carvalho, Tieying Li, Jiaxing Xu, Hui Fang, Yiping Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Sequential Recommendation (CDSR) has recently gained attention
for countering data sparsity by transferring knowledge across domains. A common
approach merges domain-specific sequences into cross-domain sequences, serving
as bridges to connect domains. One key challenge is to correctly extract the
shared knowledge among these sequences and appropriately transfer it. Most
existing works directly transfer unfiltered cross-domain knowledge rather than
extracting domain-invariant components and adaptively integrating them into
domain-specific modelings. Another challenge lies in aligning the
domain-specific and cross-domain sequences. Existing methods align these
sequences based on timestamps, but this approach can cause prediction
mismatches when the current tokens and their targets belong to different
domains. In such cases, the domain-specific knowledge carried by the current
tokens may degrade performance. To address these challenges, we propose the
A-B-Cross-to-Invariant Learning Recommender (ABXI). Specifically, leveraging
LoRA's effectiveness for efficient adaptation, ABXI incorporates two types of
LoRAs to facilitate knowledge adaptation. First, all sequences are processed
through a shared encoder that employs a domain LoRA for each sequence, thereby
preserving unique domain characteristics. Next, we introduce an invariant
projector that extracts domain-invariant interests from cross-domain
representations, utilizing an invariant LoRA to adapt these interests into
modeling each specific domain. Besides, to avoid prediction mismatches, all
domain-specific sequences are aligned to match the domains of the cross-domain
ground truths. Experimental results on three datasets demonstrate that our
approach outperforms other CDSR counterparts by a large margin. The codes are
available in https://github.com/DiMarzioBian/ABXI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WebConf '25 (WWW '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of OM
tools. Our framework is implemented in a proof-of-concept system. Evaluations
of three Ontology Alignment Evaluation Initiative (OAEI) tracks over
state-of-the-art OM systems show that our system can achieve results very close
to the long-standing best performance on simple OM tasks and can significantly
improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaSeRec: Retrieval-Augmented Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinping Zhao, Baotian Hu, Yan Zhong, Shouzheng Huang, Zihao Zheng, Meng Wang, Haofen Wang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although prevailing supervised and self-supervised learning augmented
sequential recommendation (SeRec) models have achieved improved performance
with powerful neural network architectures, we argue that they still suffer
from two limitations: (1) Preference Drift, where models trained on past data
can hardly accommodate evolving user preference; and (2) Implicit Memory, where
head patterns dominate parametric learning, making it harder to recall long
tails. In this work, we explore retrieval augmentation in SeRec, to address
these limitations. Specifically, we propose a Retrieval-Augmented Sequential
Recommendation framework, named RaSeRec, the main idea of which is to maintain
a dynamic memory bank to accommodate preference drifts and retrieve relevant
memories to augment user modeling explicitly. It consists of two stages: (i)
collaborative-based pre-training, which learns to recommend and retrieve; (ii)
retrieval-augmented fine-tuning, which learns to leverage retrieved memories.
Extensive experiments on three datasets fully demonstrate the superiority and
effectiveness of RaSeRec. The implementation code is available at
https://github.com/HITsz-TMG/RaSeRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PeaPOD: Personalized Prompt Distillation for Generative Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Ramos, Bin Wu, Aldo Lipani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, researchers have investigated the capabilities of Large Language
Models (LLMs) for generative recommender systems. Existing LLM-based
recommender models are trained by adding user and item IDs to a discrete prompt
template. However, the disconnect between IDs and natural language makes it
difficult for the LLM to learn the relationship between users. To address this
issue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to
distill user preferences as personalized soft prompts. Considering the
complexities of user preferences in the real world, we maintain a shared set of
learnable prompts that are dynamically weighted based on the user's interests
to construct the user-personalized prompt in a compositional manner.
Experimental results on three real-world datasets demonstrate the effectiveness
of our PeaPOD model on sequential recommendation, top-n recommendation, and
explanation generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Model for Interest Refinement in Multi-Interest Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yankun Le, Haoran Li, Baoyuan Ou, Yingjie Qin, Zhixuan Yang, Ruilong Su, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-interest candidate matching plays a pivotal role in personalized
recommender systems, as it captures diverse user interests from their
historical behaviors. Most existing methods utilize attention mechanisms to
generate interest representations by aggregating historical item embeddings.
However, these methods only capture overall item-level relevance, leading to
coarse-grained interest representations that include irrelevant information. To
address this issue, we propose the Diffusion Multi-Interest model (DMI), a
novel framework for refining user interest representations at the dimension
level. Specifically, DMI first introduces controllable noise into
coarse-grained interest representations at the dimensional level. Then, in the
iterative reconstruction process, DMI combines a cross-attention mechanism and
an item pruning strategy to reconstruct the personalized interest vectors with
the guidance of tailored collaborative information. Extensive experiments
demonstrate the effectiveness of DMI, surpassing state-of-the-art methods on
offline evaluations and an online A/B test. Successfully deployed in the
real-world recommender system, DMI effectively enhances user satisfaction and
system performance at scale, serving the major traffic of hundreds of millions
of daily active users. \footnote{The code will be released for reproducibility
once the paper is accepted.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Visualization Recommendation with Hier-SUCB 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03375v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03375v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songwen Hu, Ryan A. Rossi, Tong Yu, Junda Wu, Handong Zhao, Sungchul Kim, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualization recommendation aims to enable rapid visual analysis of massive
datasets. In real-world scenarios, it is essential to quickly gather and
comprehend user preferences to cover users from diverse backgrounds, including
varying skill levels and analytical tasks. Previous approaches to personalized
visualization recommendations are non-interactive and rely on initial user data
for new users. As a result, these models cannot effectively explore options or
adapt to real-time feedback. To address this limitation, we propose an
interactive personalized visualization recommendation (PVisRec) system that
learns on user feedback from previous interactions. For more interactive and
accurate recommendations, we propose Hier-SUCB, a contextual combinatorial
semi-bandit in the PVisRec setting. Theoretically, we show an improved overall
regret bound with the same rank of time but an improved rank of action space.
We further demonstrate the effectiveness of Hier-SUCB through extensive
experiments where it is comparable to offline methods and outperforms other
bandit algorithms in the setting of visualization recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Sparse Mixture Of Experts Text Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zach Nussbaum, Brandon Duderstadt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based text embedding models have improved their performance on
benchmarks like MIRACL and BEIR by increasing their parameter counts. However,
this scaling approach introduces significant deployment challenges, including
increased inference latency and memory usage. These challenges are particularly
severe in retrieval-augmented generation (RAG) applications, where large
models' increased memory requirements constrain dataset ingestion capacity, and
their higher latency directly impacts query-time performance. While causal
language models have addressed similar efficiency challenges using Mixture of
Experts (MoE) architectures, this approach hasn't been successfully adapted to
the general text embedding setting. In this paper, we introduce Nomic Embed v2,
the first general purpose MoE text embedding model. Our model outperforms
models in the same parameter class on both monolingual and multilingual
benchmarks while also maintaining competitive performance with models twice its
size. We open-source all code, models, and evaluation data to ensure full
reproducibility of our training pipeline at
\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual-based spatial audio generation system for multi-speaker
  environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojing Liu, Ogulcan Gurelli, Yan Wang, Joshua Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multimedia applications such as films and video games, spatial audio
techniques are widely employed to enhance user experiences by simulating 3D
sound: transforming mono audio into binaural formats. However, this process is
often complex and labor-intensive for sound designers, requiring precise
synchronization of audio with the spatial positions of visual components. To
address these challenges, we propose a visual-based spatial audio generation
system - an automated system that integrates face detection YOLOv8 for object
detection, monocular depth estimation, and spatial audio techniques. Notably,
the system operates without requiring additional binaural dataset training. The
proposed system is evaluated against existing Spatial Audio generation system
using objective metrics. Experimental results demonstrate that our method
significantly improves spatial consistency between audio and video, enhances
speech quality, and performs robustly in multi-speaker scenarios. By
streamlining the audio-visual alignment process, the proposed system enables
sound engineers to achieve high-quality results efficiently, making it a
valuable tool for professionals in multimedia production.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework
  with Interactive Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zhouyang Wang, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous advancement of vision language models (VLMs) technology,
remarkable research achievements have emerged in the dermatology field, the
fourth most prevalent human disease category. However, despite these
advancements, VLM still faces explainable problems to user in diagnosis due to
the inherent complexity of dermatological conditions, existing tools offer
relatively limited support for user comprehension. We propose SkinGEN, a
diagnosis-to-generation framework that leverages the stable diffusion(SD) model
to generate reference demonstrations from diagnosis results provided by VLM,
thereby enhancing the visual explainability for users. Through extensive
experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for
skin condition image generation. We conduct a user study with 32 participants
evaluating both the system performance and explainability. Results demonstrate
that SkinGEN significantly improves users' comprehension of VLM predictions and
fosters increased trust in the diagnostic process. This work paves the way for
more transparent and user-centric VLM applications in dermatology and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-12T00:00:00Z">2025-02-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ask in Any Modality: A Comprehensive <span class="highlight-title">Survey</span> on <span class="highlight-title">Multimodal</span>
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) struggle with hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information enhancing factual and updated grounding. Recent advances in
multimodal learning have led to the development of Multimodal RAG,
incorporating multiple modalities such as text, images, audio, and video to
enhance the generated outputs. However, cross-modal alignment and reasoning
introduce unique challenges to Multimodal RAG, distinguishing it from
traditional unimodal RAG. This survey offers a structured and comprehensive
analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,
evaluation, methodologies, and innovations in retrieval, fusion, augmentation,
and generation. We precisely review training strategies, robustness
enhancements, and loss functions, while also exploring the diverse Multimodal
RAG scenarios. Furthermore, we discuss open challenges and future research
directions to support advancements in this evolving field. This survey lays the
foundation for developing more capable and reliable AI systems that effectively
leverage multimodal dynamic external knowledge bases. Resources are available
at https://github.com/llm-lab-org/Multimodal-RAG-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion
  in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonduk Seo, Seunghyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion is widely used in Information Retrieval (IR) to improve
search outcomes by enriching queries with additional contextual information.
Although recent Large Language Model (LLM) based methods generate
pseudo-relevant content and expanded terms via multiple prompts, they often
yield repetitive, narrow expansions that lack the diverse context needed to
retrieve all relevant information. In this paper, we introduce QA-Expand, a
novel and effective framework for query expansion. It first generates multiple
relevant questions from the initial query and subsequently produces
corresponding pseudo-answers as surrogate documents. A feedback model further
rewrites and filters these answers to ensure only the most informative
augmentations are incorporated. Extensive experiments on benchmarks such as
BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up
to 13% over state-of-the-art methods, offering a robust solution for modern
retrieval challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Topics through Weighting Aspect Keywords 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Nazari, Michael Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling often requires examining topics from multiple perspectives to
uncover hidden patterns, especially in less explored areas. This paper presents
an approach to address this need, utilizing weighted keywords from various
aspects derived from a domain knowledge. The research method starts with
standard topic modeling. Then, it adds a process consisting of four key steps.
First, it defines keywords for each aspect. Second, it gives weights to these
keywords based on their relevance. Third, it calculates relevance scores for
aspect-weighted keywords and topic keywords to create aspect-topic models.
Fourth, it uses these scores to tune relevant new documents. Finally, the
generated topic models are interpreted and validated. The findings show that
top-scoring documents are more likely to be about the same aspect of a topic.
This highlights the model's effectiveness in finding the related documents to
the aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composite Sketch+Text Queries for Retrieving Objects with Elusive Names
  and Complex Interactions <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-native speakers with limited vocabulary often struggle to name specific
objects despite being able to visualize them, e.g., people outside Australia
searching for numbats. Further, users may want to search for such elusive
objects with difficult-to-sketch interactions, e.g., numbat digging in the
ground. In such common but complex situations, users desire a search interface
that accepts composite multimodal queries comprising hand-drawn sketches of
difficult-to-name but easy-to-draw objects and text describing
difficult-to-sketch but easy-to-verbalize object attributes or interaction with
the scene. This novel problem statement distinctly differs from the previously
well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image
retrieval) problems. To study this under-explored task, we curate a dataset,
CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M
queries and 108K natural scene images. Further, as a solution to this problem,
we propose a pretrained multimodal transformer-based baseline, STNET
(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant
objects in the natural scene image, and encodes the text and image to perform
image retrieval. In addition to contrastive learning, we propose multiple
training objectives that improve the performance of our model. Extensive
experiments show that our proposed method outperforms several state-of-the-art
retrieval methods for text-only, sketch-only, and composite query modalities.
We make the dataset and code available at our project website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2024, 9 pages. Project Website:
  https://vl2g.github.io/projects/cstbir</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Free Counterfactual Subset Selection at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Hieu Nguyen, Viet Hung Doan, Anh Tuan Nguyen, Jun Jo, Quoc Viet Hung Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring transparency in AI decision-making requires interpretable
explanations, particularly at the instance level. Counterfactual explanations
are a powerful tool for this purpose, but existing techniques frequently depend
on synthetic examples, introducing biases from unrealistic assumptions, flawed
models, or skewed data. Many methods also assume full dataset availability, an
impractical constraint in real-time environments where data flows continuously.
In contrast, streaming explanations offer adaptive, real-time insights without
requiring persistent storage of the entire dataset. This work introduces a
scalable, model-free approach to selecting diverse and relevant counterfactual
examples directly from observed data. Our algorithm operates efficiently in
streaming settings, maintaining $O(\log k)$ update complexity per item while
ensuring high-quality counterfactual selection. Empirical evaluations on both
real-world and synthetic datasets demonstrate superior performance over
baseline methods, with robust behavior even under adversarial conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Scaling Law in Industrial Recommendation Systems with a
  Three-step Paradigm based Large User Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bencheng Yan, Shilei Liu, Zhiyuan Zeng, Zihao Wang, Yizhen Zhang, Yujin Yuan, Langming Liu, Jiaqi Liu, Di Wang, Wenbo Su, Wang Pengjie, Jian Xu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in autoregressive Large Language Models (LLMs) have
achieved significant milestones, largely attributed to their scalability, often
referred to as the "scaling law". Inspired by these achievements, there has
been a growing interest in adapting LLMs for Recommendation Systems (RecSys) by
reformulating RecSys tasks into generative problems. However, these End-to-End
Generative Recommendation (E2E-GR) methods tend to prioritize idealized goals,
often at the expense of the practical advantages offered by traditional Deep
Learning based Recommendation Models (DLRMs) in terms of in features,
architecture, and practices. This disparity between idealized goals and
practical needs introduces several challenges and limitations, locking the
scaling law in industrial RecSys. In this paper, we introduce a large user
model (LUM) that addresses these limitations through a three-step paradigm,
designed to meet the stringent requirements of industrial settings while
unlocking the potential for scalable recommendations. Our extensive
experimental evaluations demonstrate that LUM outperforms both state-of-the-art
DLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with
performance improvements observed as the model scales up to 7 billion
parameters. Additionally, we have successfully deployed LUM in an industrial
application, where it achieved significant gains in an A/B test, further
validating its effectiveness and practicality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoLoRec: A Generalizable and Efficient Framework for LLM-Based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Hou, Chenxi Bai, Le Wu, Hao Liu, Kun Zhang, Kai Zhang, Richang Hong, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success in recent
years, owing to their impressive generalization capabilities and rich world
knowledge. To capitalize on the potential of using LLMs as recommender systems,
mainstream approaches typically focus on two paradigms. The first paradigm
designs multi-domain or multi-task instruction data for generalizable
recommendation, so as to align LLMs with general recommendation areas and deal
with cold-start recommendation. The second paradigm enhances domain-specific
recommendation tasks with parameter-efficient fine-tuning techniques, in order
to improve models under the warm recommendation scenarios. While most previous
works treat these two paradigms separately, we argue that they have
complementary advantages, and combining them together would be helpful.
  To that end, in this paper, we propose a generalizable and efficient
LLM-based recommendation framework MoLoRec. Our approach starts by
parameter-efficient fine-tuning a domain-general module with general
recommendation instruction data, to align LLM with recommendation knowledge.
Then, given users' behavior of a specific domain, we construct a
domain-specific instruction dataset and apply efficient fine-tuning to the
pre-trained LLM. After that, we provide approaches to integrate the above
domain-general part and domain-specific part with parameters mixture. Please
note that, MoLoRec is efficient with plug and play, as the domain-general
module is trained only once, and any domain-specific plug-in can be efficiently
merged with only domain-specific fine-tuning. Extensive experiments on multiple
datasets under both warm and cold-start recommendation scenarios validate the
effectiveness and generality of the proposed MoLoRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wisdom of the Crowds in Forecasting: Forecast Summarization for
  Supporting Future Event Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Saha, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future Event Prediction (FEP) is an essential activity whose demand and
application range across multiple domains. While traditional methods like
simulations, predictive and time-series forecasting have demonstrated promising
outcomes, their application in forecasting complex events is not entirely
reliable due to the inability of numerical data to accurately capture the
semantic information related to events. One forecasting way is to gather and
aggregate collective opinions on the future to make predictions as cumulative
perspectives carry the potential to help estimating the likelihood of upcoming
events. In this work, we organize the existing research and frameworks that aim
to support future event prediction based on crowd wisdom through aggregating
individual forecasts. We discuss the challenges involved, available datasets,
as well as the scope of improvement and future research directions for this
task. We also introduce a novel data model to represent individual forecast
statements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixDec Sampling: A Soft Link-based Sampling Method of <span class="highlight-title">Graph</span> Neural
  Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangjin Xie, Yuxin Chen, Ruipeng Wang, Kai Ouyang, Zihan Zhang, Hai-Tao Zheng, Buyue Qian, Hansen Zheng, Bo Hu, Chengxiang Zhuo, Zang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks have been widely used in recent recommender systems,
where negative sampling plays an important role. Existing negative sampling
methods restrict the relationship between nodes as either hard positive pairs
or hard negative pairs. This leads to the loss of structural information, and
lacks the mechanism to generate positive pairs for nodes with few neighbors. To
overcome limitations, we propose a novel soft link-based sampling method,
namely MixDec Sampling, which consists of Mixup Sampling module and Decay
Sampling module. The Mixup Sampling augments node features by synthesizing new
nodes and soft links, which provides sufficient number of samples for nodes
with few neighbors. The Decay Sampling strengthens the digestion of graph
structure information by generating soft links for node embedding learning. To
the best of our knowledge, we are the first to model sampling relationships
between nodes by soft links in GNN-based recommender systems. Extensive
experiments demonstrate that the proposed MixDec Sampling can significantly and
consistently improve the recommendation performance of several representative
GNN-based models on various recommendation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SS4Rec: Continuous-Time Sequential Recommendation with State Space
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xiao, Huiying Wang, Qifeng Zhou, Qing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation is a key area in the field of recommendation
systems aiming to model user interest based on historical interaction sequences
with irregular intervals. While previous recurrent neural network-based and
attention-based approaches have achieved significant results, they have
limitations in capturing system continuity due to the discrete characteristics.
In the context of continuous-time modeling, state space model (SSM) offers a
potential solution, as it can effectively capture the dynamic evolution of user
interest over time. However, existing SSM-based approaches ignore the impact of
irregular time intervals within historical user interactions, making it
difficult to model complexed user-item transitions in sequences. To address
this issue, we propose a hybrid SSM-based model called SS4Rec for
continuous-time sequential recommendation. SS4Rec integrates a time-aware SSM
to handle irregular time intervals and a relation-aware SSM to model contextual
dependencies, enabling it to infer user interest from both temporal and
sequential perspectives. In the training process, the time-aware SSM and the
relation-aware SSM are discretized by variable stepsizes according to user
interaction time intervals and input data, respectively. This helps capture the
continuous dependency from irregular time intervals and provides time-specific
personalized recommendations. Experimental studies on five benchmark datasets
demonstrate the superiority and effectiveness of SS4Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Filtering Meets Spectrum Shift: Connecting User-Item
  Interaction with <span class="highlight-title">Graph</span>-Structured Side Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhang He, Cong Xu, Jun Wang, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Network (GNN) has demonstrated their superiority in
collaborative filtering, where the user-item (U-I) interaction bipartite graph
serves as the fundamental data format. However, when graph-structured side
information (e.g., multimodal similarity graphs or social networks) is
integrated into the U-I bipartite graph, existing graph collaborative filtering
methods fall short of achieving satisfactory performance. We quantitatively
analyze this problem from a spectral perspective. Recall that a bipartite graph
possesses a full spectrum within the range of [-1, 1], with the highest
frequency exactly achievable at -1 and the lowest frequency at 1; however, we
observe as more side information is incorporated, the highest frequency of the
augmented adjacency matrix progressively shifts rightward. This spectrum shift
phenomenon has caused previous approaches built for the full spectrum [-1, 1]
to assign mismatched importance to different frequencies. To this end, we
propose Spectrum Shift Correction (dubbed SSC), incorporating shifting and
scaling factors to enable spectral GNNs to adapt to the shifted spectrum.
Unlike previous paradigms of leveraging side information, which necessitate
tailored designs for diverse data types, SSC directly connects traditional
graph collaborative filtering with any graph-structured side information.
Experiments on social and multimodal recommendation demonstrate the
effectiveness of SSC, achieving relative improvements of up to 23% without
incurring any additional computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Training for Recommendation with Language-based User Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaolin Gao, Joyce Zhou, Yijia Dai, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in natural language-based user profiles for
recommender systems, which aims to enhance transparency and scrutability
compared with embedding-based methods. Existing studies primarily generate
these profiles using zero-shot inference from large language models (LLMs), but
their quality remains insufficient, leading to suboptimal recommendation
performance. In this paper, we introduce LangPTune, the first end-to-end
training framework to optimize LLM-generated user profiles. Our method
significantly outperforms zero-shot approaches by explicitly training the LLM
for the recommendation objective. Through extensive evaluations across diverse
training configurations and benchmarks, we demonstrate that LangPTune not only
surpasses zero-shot baselines but can also matches the performance of
state-of-the-art embedding-based methods. Finally, we investigate whether the
training procedure preserves the interpretability of these profiles compared to
zero-shot inference through both GPT-4 simulations and crowdworker user
studies. Implementation of LangPTune can be found at
https://github.com/ZhaolinGao/LangPTune.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic-Aware Knowledge <span class="highlight-title">Graph</span> with Large Language Models for
  Interoperability in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20163v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20163v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhye Jeon, Seokho Ahn, Young-Duk Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of knowledge graphs in recommender systems has become one of the
common approaches to addressing data sparsity and cold start problems. Recent
advances in large language models (LLMs) offer new possibilities for processing
side and context information within knowledge graphs. However, consistent
integration across various systems remains challenging due to the need for
domain expert intervention and differences in system characteristics. To
address these issues, we propose a consistent approach that extracts both
general and specific topics from both side and context information using LLMs.
First, general topics are iteratively extracted and updated from side
information. Then, specific topics are extracted using context information.
Finally, to address synonymous topics generated during the specific topic
extraction process, a refining algorithm processes and resolves these issues
effectively. This approach allows general topics to capture broad knowledge
across diverse item characteristics, while specific topics emphasize detailed
attributes, providing a more comprehensive understanding of the semantic
features of items and the preferences of users. Experimental results
demonstrate significant improvements in recommendation performance across
diverse knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODE-ACCORD: A Corpus of building regulatory data for rule generation
  towards automatic compliance checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02231v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02231v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Amna Dridi, Mohamed Medhat Gaber, Pouyan Parsafard, Nicoleta Bocaneala, Katja Breitenfelder, Gonçal Costa, Maria Hedblom, Mihaela Juganaru-Mathieu, Thamer Mecharnia, Sumee Park, He Tan, Abdel-Rahman H. Tawil, Edlira Vakaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and
Construction (AEC) sector necessitates automating the interpretation of
building regulations to achieve its full potential. Converting textual rules
into machine-readable formats is challenging due to the complexities of natural
language and the scarcity of resources for advanced Machine Learning (ML).
Addressing these challenges, we introduce CODE-ACCORD, a dataset of 862
sentences from the building regulations of England and Finland. Only the
self-contained sentences, which express complete rules without needing
additional context, were considered as they are essential for ACC. Each
sentence was manually annotated with entities and relations by a team of 12
annotators to facilitate machine-readable rule generation, followed by careful
curation to ensure accuracy. The final dataset comprises 4,297 entities and
4,329 relations across various categories, serving as a robust ground truth.
CODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,
including text classification, entity recognition, and relation extraction. It
enables applying recent trends, such as deep neural networks and large language
models, to ACC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of an article submitted to the Scientific Data
  Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intent Alignment between Interaction and Language Spaces for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent-based recommender systems have garnered significant attention for
uncovering latent fine-grained preferences. Intents, as underlying factors of
interactions, are crucial for improving recommendation interpretability. Most
methods define intents as learnable parameters updated alongside interactions.
However, existing frameworks often overlook textual information (e.g., user
reviews, item descriptions), which is crucial for alleviating the sparsity of
interaction intents. Exploring these multimodal intents, especially the
inherent differences in representation spaces, poses two key challenges: i) How
to align multimodal intents and effectively mitigate noise issues; ii) How to
extract and match latent key intents across modalities. To tackle these
challenges, we propose a model-agnostic framework, Intent Representation
Learning with Large Language Model (IRLLRec), which leverages large language
models (LLMs) to construct multimodal intents and enhance recommendations.
Specifically, IRLLRec employs a dual-tower architecture to learn multimodal
intent representations. Next, we propose pairwise and translation alignment to
eliminate inter-modal differences and enhance robustness against noisy input
features. Finally, to better match textual and interaction-based intents, we
employ momentum distillation to perform teacher-student learning on fused
intent representations. Empirical evaluations on three datasets show that our
IRLLRec framework outperforms baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Parameter Update Balancing Algorithm for Multi-task Ranking Models in
  Recommendation Systems <span class="chip">ICDM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yuan, Guohao Cai, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task ranking models have become essential for modern real-world
recommendation systems. While most recommendation researches focus on designing
sophisticated models for specific scenarios, achieving performance improvement
for multi-task ranking models across various scenarios still remains a
significant challenge. Training all tasks naively can result in inconsistent
learning, highlighting the need for the development of multi-task optimization
(MTO) methods to tackle this challenge. Conventional methods assume that the
optimal joint gradient on shared parameters leads to optimal parameter updates.
However, the actual update on model parameters may deviates significantly from
gradients when using momentum based optimizers such as Adam, and we design and
execute statistical experiments to support the observation. In this paper, we
propose a novel Parameter Update Balancing algorithm for multi-task
optimization, denoted as PUB. In contrast to traditional MTO method which are
based on gradient level tasks fusion or loss level tasks fusion, PUB is the
first work to optimize multiple tasks through parameter update balancing.
Comprehensive experiments on benchmark multi-task ranking datasets demonstrate
that PUB consistently improves several multi-task backbones and achieves
state-of-the-art performance. Additionally, experiments on benchmark computer
vision datasets show the great potential of PUB in various multi-task learning
scenarios. Furthermore, we deployed our method for an industrial evaluation on
the real-world commercial platform, HUAWEI AppGallery, where PUB significantly
enhances the online multi-task ranking model, efficiently managing the primary
traffic of a crucial channel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FIRE: Fact-checking with Iterative Retrieval and Verification <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohan Xie, Rui Xing, Yuxia Wang, Jiahui Geng, Hasan Iqbal, Dhruv Sahnan, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking long-form text is challenging, and it is therefore common
practice to break it down into multiple atomic claims. The typical approach to
fact-checking these atomic claims involves retrieving a fixed number of pieces
of evidence, followed by a verification step. However, this method is usually
not cost-effective, as it underutilizes the verification model's internal
knowledge of the claim and fails to replicate the iterative reasoning process
in human search strategies. To address these limitations, we propose FIRE, a
novel agent-based framework that integrates evidence retrieval and claim
verification in an iterative manner. Specifically, FIRE employs a unified
mechanism to decide whether to provide a final answer or generate a subsequent
search query, based on its confidence in the current judgment. We compare FIRE
with other strong fact-checking frameworks and find that it achieves slightly
better performance while reducing large language model (LLM) costs by an
average of 7.6 times and search costs by 16.5 times. These results indicate
that FIRE holds promise for application in large-scale fact-checking
operations. Our code is available at https://github.com/mbzuai-nlp/fire.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 8 tables, accepted to Findings of NAACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisCo: <span class="highlight-title">Graph</span>-Based Disentangled Contrastive Learning for Cold-Start
  Cross-Domain Recommendation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15005v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15005v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used in various real-world applications, but
they often encounter the persistent challenge of the user cold-start problem.
Cross-domain recommendation (CDR), which leverages user interactions from one
domain to improve prediction performance in another, has emerged as a promising
solution. However, users with similar preferences in the source domain may
exhibit different interests in the target domain. Therefore, directly
transferring embeddings may introduce irrelevant source-domain collaborative
information. In this paper, we propose a novel graph-based disentangled
contrastive learning framework to capture fine-grained user intent and filter
out irrelevant collaborative information, thereby avoiding negative transfer.
Specifically, for each domain, we use a multi-channel graph encoder to capture
diverse user intents. We then construct the affinity graph in the embedding
space and perform multi-step random walks to capture high-order user similarity
relationships. Treating one domain as the target, we propose a disentangled
intent-wise contrastive learning approach, guided by user similarity, to refine
the bridging of user intents across domains. Extensive experiments on four
benchmark CDR datasets demonstrate that DisCo consistently outperforms existing
state-of-the-art baselines, thereby validating the effectiveness of both DisCo
and its components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Quantification and Decomposition for LLM-based
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of large language models (LLMs) for
recommendation, we demonstrate that LLMs often exhibit uncertainty in their
recommendations. To ensure the trustworthy use of LLMs in generating
recommendations, we emphasize the importance of assessing the reliability of
recommendations generated by LLMs. We start by introducing a novel framework
for estimating the predictive uncertainty to quantitatively measure the
reliability of LLM-based recommendations. We further propose to decompose the
predictive uncertainty into recommendation uncertainty and prompt uncertainty,
enabling in-depth analyses of the primary source of uncertainty. Through
extensive experiments, we (1) demonstrate predictive uncertainty effectively
indicates the reliability of LLM-based recommendations, (2) investigate the
origins of uncertainty with decomposed uncertainty measures, and (3) propose
uncertainty-aware prompting for a lower predictive uncertainty and enhanced
recommendation. Our source code and model weights are available at
https://github.com/WonbinKweon/UNC_LLM_REC_WWW2025
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELASTIC: Efficient Linear Attention for Sequential Interest Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09380v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09380v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Deng, Shiyao Wang, Song Lu, Yinfeng Li, Xinchen Luo, Yuanjun Liu, Peixing Xu, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art sequential recommendation models heavily rely on
transformer's attention mechanism. However, the quadratic computational and
memory complexities of self attention have limited its scalability for modeling
users' long range behaviour sequences. To address this problem, we propose
ELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,
requiring only linear time complexity and decoupling model capacity from
computational cost. Specifically, ELASTIC introduces a fixed length interest
experts with linear dispatcher attention mechanism which compresses the
long-term behaviour sequences to a significantly more compact representation
which reduces up to 90% GPU memory usage with x2.7 inference speed up. The
proposed linear dispatcher attention mechanism significantly reduces the
quadratic complexity and makes the model feasible for adequately modeling
extremely long sequences. Moreover, in order to retain the capacity for
modeling various user interests, ELASTIC initializes a vast learnable interest
memory bank and sparsely retrieves compressed user's interests from the memory
with a negligible computational overhead. The proposed interest memory
retrieval technique significantly expands the cardinality of available interest
space while keeping the same computational cost, thereby striking a trade-off
between recommendation accuracy and efficiency. To validate the effectiveness
of our proposed ELASTIC, we conduct extensive experiments on various public
datasets and compare it with several strong sequential recommenders.
Experimental results demonstrate that ELASTIC consistently outperforms
baselines by a significant margin and also highlight the computational
efficiency of ELASTIC when modeling long sequences. We will make our
implementation code publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We hereby withdraw this paper from arXiv due to incomplete
  experiments. Upon further review, we have determined that additional
  experimental work is necessary to fully validate our findings and conclusions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DOGR: Leveraging Document-Oriented Contrastive Learning in Generative
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghao Lu, Xin Dong, Yuansheng Zhou, Lei Cheng, Chuan Yuan, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval constitutes an innovative approach in information
retrieval, leveraging generative language models (LM) to generate a ranked list
of document identifiers (docid) for a given query. It simplifies the retrieval
pipeline by replacing the large external index with model parameters. However,
existing works merely learned the relationship between queries and document
identifiers, which is unable to directly represent the relevance between
queries and documents. To address the above problem, we propose a novel and
general generative retrieval framework, namely Leveraging Document-Oriented
Contrastive Learning in Generative Retrieval (DOGR), which leverages
contrastive learning to improve generative retrieval tasks. It adopts a
two-stage learning strategy that captures the relationship between queries and
documents comprehensively through direct interactions. Furthermore, negative
sampling methods and corresponding contrastive learning objectives are
implemented to enhance the learning of semantic representations, thereby
promoting a thorough comprehension of the relationship between queries and
documents. Experimental results demonstrate that DOGR achieves state-of-the-art
performance compared to existing generative retrieval methods on two public
benchmark datasets. Further experiments have shown that our framework is
generally effective for common identifier construction techniques.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Centric Foundation Models: Perception, Generation and Agentic
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human understanding and generation are critical for modeling digital humans
and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)
inspired by the success of generalist models, such as large language and vision
models, have emerged to unify diverse human-centric tasks into a single
framework, surpassing traditional task-specific approaches. In this survey, we
present a comprehensive overview of HcFMs by proposing a taxonomy that
categorizes current approaches into four groups: (1) Human-centric Perception
Foundation Models that capture fine-grained features for multi-modal 2D and 3D
understanding. (2) Human-centric AIGC Foundation Models that generate
high-fidelity, diverse human-related content. (3) Unified Perception and
Generation Models that integrate these capabilities to enhance both human
understanding and synthesis. (4) Human-centric Agentic Foundation Models that
extend beyond perception and generation to learn human-like intelligence and
interactive behaviors for humanoid embodied tasks. We review state-of-the-art
techniques, discuss emerging challenges and future research directions. This
survey aims to serve as a roadmap for researchers and practitioners working
towards more robust, versatile, and intelligent digital human and embodiments
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "You'll Be Alice Adventuring in Wonderland!" Processes, Challenges, and
  Opportunities of Creating Animated Virtual Reality Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin-Ping Yuan, Feilin Han, Liwenhan Xie, Junjie Zhang, Jian Zhao, Huamin Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animated virtual reality (VR) stories, combining the presence of VR and the
artistry of computer animation, offer a compelling way to deliver messages and
evoke emotions. Motivated by the growing demand for immersive narrative
experiences, more creators are creating animated VR stories. However, a
holistic understanding of their creation processes and challenges involved in
crafting these stories is still limited. Based on semi-structured interviews
with 21 animated VR story creators, we identify ten common stages in their
end-to-end creation processes, ranging from idea generation to evaluation,
which form diverse workflows that are story-driven or visual-driven.
Additionally, we highlight nine unique issues that arise during the creation
process, such as a lack of reference material for multi-element plots, the
absence of specific functionalities for story integration, and inadequate
support for audience evaluation. We compare the creation of animated VR stories
to general XR applications and distill several future research opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conditionally accepted to the ACM Conference on Human Factors in
  Computing Systems (CHI'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composite Sketch+Text Queries for Retrieving Objects with Elusive Names
  and Complex Interactions <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-native speakers with limited vocabulary often struggle to name specific
objects despite being able to visualize them, e.g., people outside Australia
searching for numbats. Further, users may want to search for such elusive
objects with difficult-to-sketch interactions, e.g., numbat digging in the
ground. In such common but complex situations, users desire a search interface
that accepts composite multimodal queries comprising hand-drawn sketches of
difficult-to-name but easy-to-draw objects and text describing
difficult-to-sketch but easy-to-verbalize object attributes or interaction with
the scene. This novel problem statement distinctly differs from the previously
well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image
retrieval) problems. To study this under-explored task, we curate a dataset,
CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M
queries and 108K natural scene images. Further, as a solution to this problem,
we propose a pretrained multimodal transformer-based baseline, STNET
(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant
objects in the natural scene image, and encodes the text and image to perform
image retrieval. In addition to contrastive learning, we propose multiple
training objectives that improve the performance of our model. Extensive
experiments show that our proposed method outperforms several state-of-the-art
retrieval methods for text-only, sketch-only, and composite query modalities.
We make the dataset and code available at our project website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2024, 9 pages. Project Website:
  https://vl2g.github.io/projects/cstbir</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by
  Silhouette Masks and Fashion Styles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Zhou, Haijun Zhang, Qun Li, Jianghong Ma, Xiaofei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to recommend outfits has gained considerable attention in both academia
and industry in recent years. Many studies have been carried out regarding
fashion compatibility learning, to determine whether the fashion items in an
outfit are compatible or not. These methods mainly focus on evaluating the
compatibility of existing outfits and rarely consider applying such knowledge
to 'design' new fashion items. We propose the new task of generating
complementary and compatible fashion items based on an arbitrary number of
given fashion items. In particular, given some fashion items that can make up
an outfit, the aim of this paper is to synthesize photo-realistic images of
other, complementary, fashion items that are compatible with the given ones. To
achieve this, we propose an outfit generation framework, referred to as
COutfitGAN, which includes a pyramid style extractor, an outfit generator, a
UNet-based real/fake discriminator, and a collocation discriminator. To train
and evaluate this framework, we collected a large-scale fashion outfit dataset
with over 200K outfits and 800K fashion items from the Internet. Extensive
experiments show that COutfitGAN outperforms other baselines in terms of
similarity, authenticity, and compatibility measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E2LVLM:Evidence-Enhanced Large Vision-Language Model for <span class="highlight-title">Multimodal</span>
  Out-of-Context Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wu, Yumeng Fu, Nan Yu, Guohong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies in Large Vision-Language Models (LVLMs) have demonstrated
impressive advancements in multimodal Out-of-Context (OOC) misinformation
detection, discerning whether an authentic image is wrongly used in a claim.
Despite their success, the textual evidence of authentic images retrieved from
the inverse search is directly transmitted to LVLMs, leading to inaccurate or
false information in the decision-making phase. To this end, we present E2LVLM,
a novel evidence-enhanced large vision-language model by adapting textual
evidence in two levels. First, motivated by the fact that textual evidence
provided by external tools struggles to align with LVLMs inputs, we devise a
reranking and rewriting strategy for generating coherent and contextually
attuned content, thereby driving the aligned and effective behavior of LVLMs
pertinent to authentic images. Second, to address the scarcity of news domain
datasets with both judgment and explanation, we generate a novel OOC multimodal
instruction-following dataset by prompting LVLMs with informative content to
acquire plausible explanations. Further, we develop a multimodal
instruction-tuning strategy with convincing explanations for beyond detection.
This scheme contributes to E2LVLM for multimodal OOC misinformation detection
and explanation. A multitude of experiments demonstrate that E2LVLM achieves
superior performance than state-of-the-art methods, and also provides
compelling rationales for judgments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Learned Image Compression via Cross Window-based Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21144v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21144v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Mudgal, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression methods have demonstrated superior
rate-distortion performance compared to traditional image compression methods.
Recent methods utilize convolutional neural networks (CNN), variational
autoencoders (VAE), invertible neural networks (INN), and transformers. Despite
their significant contributions, a main drawback of these models is their poor
performance in capturing local redundancy. Therefore, to leverage global
features along with local redundancy, we propose a CNN-based solution
integrated with a feature encoding module. The feature encoding module encodes
important features before feeding them to the CNN and then utilizes cross-scale
window-based attention, which further captures local redundancy. Cross-scale
window-based attention is inspired by the attention mechanism in transformers
and effectively enlarges the receptive field. Both the feature encoding module
and the cross-scale window-based attention module in our architecture are
flexible and can be incorporated into any other network architecture. We
evaluate our method on the Kodak and CLIC datasets and demonstrate that our
approach is effective and on par with state-of-the-art methods. Our code is
publicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted and presented in ISVC'24. Copyrights stay with ISVC
  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive
  Modality Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models, particularly following GPT-4o, have
sparked increasing interest in developing omni-modal models capable of
understanding more modalities. While some open-source alternatives have
emerged, there is still a notable lag behind specialized single-modality models
in performance. In this paper, we present Ola, an Omni-modal language model
that achieves competitive performance across image, video, and audio
understanding compared to specialized counterparts. The core design of Ola lies
in its progressive modality alignment strategy that extends the supporting
modality of the language model progressively. Our training pipeline begins with
the most distinct modalities: image and text, then gradually expands the skill
sets of the model using speech data that connects language and audio knowledge,
and video data that connects all modalities. The progressive learning pipeline
also enables us to maintain a relatively small size of the cross-modal
alignment data, making developing omni-modal from existing vision-language
models easy and less costly. Moreover, to unlock an advanced interactive
experience like GPT-4o, we further design a sentence-wise decoding solution for
streaming speech generation. Extensive experiments demonstrate that Ola
surpasses existing open omni-modal LLMs across all modalities while achieving
highly competitive performance compared to state-of-the-art specialized models
of similar sizes. We aim to make Ola a fully open omni-modal understanding
solution to advance future research in this emerging field. Model weights,
code, and data are open-sourced at https://github.com/Ola-Omni/Ola.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeSuite: Improving MLLMs for Long Video Understanding via Grounded
  Tuning <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in short video understanding. However, understanding long-form
videos still remains challenging for MLLMs. This paper proposes TimeSuite, a
collection of new designs to adapt the existing short-form video MLLMs for long
video understanding, including a simple yet efficient framework to process long
video sequence, a high-quality video dataset for grounded tuning of MLLMs, and
a carefully-designed instruction tuning task to explicitly incorporate the
grounding supervision in the traditional QA format. Specifically, based on
VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by
implementing a token shuffling to compress long video tokens and introducing
Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of
visual representation. Meanwhile, we introduce the TimePro, a comprehensive
grounding-centric instruction tuning dataset composed of 9 tasks and 349k
high-quality grounded annotations. Notably, we design a new instruction tuning
task type, called Temporal Grounded Caption, to peform detailed video
descriptions with the corresponding time stamps prediction. This explicit
temporal location prediction will guide MLLM to correctly attend on the visual
content when generating description, and thus reduce the hallucination risk
caused by the LLMs. Experimental results demonstrate that our TimeSuite
provides a successful solution to enhance the long video understanding
capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the
benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T
exhibits robust zero-shot temporal grounding capabilities, significantly
outperforming the existing state-of-the-art MLLMs. After fine-tuning, it
performs on par with the traditional supervised expert models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routing Experts: Learning to Route Dynamic Experts in <span class="highlight-title">Multi-modal</span> Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Zhaoxi Ke, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, mixture of experts (MoE) has become a popular paradigm for
achieving the trade-off between modal capacity and efficiency of multi-modal
large language models (MLLMs). Different from previous efforts, we are
dedicated to exploring the dynamic expert path in an already exist MLLM and
show that a standard MLLM can be also a mixture of experts. To approach this
target, we propose a novel dynamic expert scheme for MLLMs, termed Routing
Experts (RoE), which can achieve example-dependent optimal path routing without
obvious structure tweaks. Meanwhile, a new regularization of structure sparsity
is also introduced to enforce MLLMs to learn more short-cut inference, ensuring
the efficiency. In addition, we also realize the first attempt of aligning the
training and inference schemes of MLLMs in terms of network routing. To
validate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,
LLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL
benchmarks. The experiment results not only show the great advantages of our
RoE in improving MLLMs' efficiency, but also yield obvious advantages than
MoE-LLaVA in both performance and speed, e.g., an average performance gain of
3.3% on 5 benchmarks while being faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image-to-video generation methods have demonstrated success in
enabling control over one or two visual elements, such as camera trajectory or
object motion. However, these methods are unable to offer control over multiple
visual elements due to limitations in data and network efficacy. In this paper,
we introduce VidCRAFT3, a novel framework for precise image-to-video generation
that enables control over camera motion, object motion, and lighting direction
simultaneously. To better decouple control over each visual element, we propose
the Spatial Triple-Attention Transformer, which integrates lighting direction,
text, and image in a symmetric way. Since most real-world video datasets lack
lighting annotations, we construct a high-quality synthetic video dataset, the
VideoLightingDirection (VLD) dataset. This dataset includes lighting direction
annotations and objects of diverse appearance, enabling VidCRAFT3 to
effectively handle strong light transmission and reflection effects.
Additionally, we propose a three-stage training strategy that eliminates the
need for training data annotated with multiple visual elements (camera motion,
object motion, and lighting direction) simultaneously. Extensive experiments on
benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing
high-quality video content, surpassing existing state-of-the-art methods in
terms of control granularity and visual coherence. All code and data will be
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music for All: Exploring Multicultural Representations in Music
  Generation Models <span class="chip">NAACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Music-Language Models has greatly enhanced the automatic music
generation capability of AI systems, but they are also limited in their
coverage of the musical genres and cultures of the world. We present a study of
the datasets and research papers for music generation and quantify the bias and
under-representation of genres. We find that only 5.7% of the total hours of
existing music datasets come from non-Western genres, which naturally leads to
disparate performance of the models across genres. We then investigate the
efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating
this bias. Our experiments with two popular models -- MusicGen and Mustango,
for two underrepresented non-Western music traditions -- Hindustani Classical
and Turkish Makam music, highlight the promises as well as the non-triviality
of cross-genre adaptation of music through small datasets, implying the need
for more equitable baseline music-language models that are designed for
cross-cultural transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, accepted to NAACL'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-11T00:00:00Z">2025-02-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document retrieval is a core component of question-answering systems, as it
enables conditioning answer generation on new and large-scale corpora. While
effective, the standard practice of encoding documents into high-dimensional
embeddings for similarity search entails large memory and compute footprints,
and also makes it hard to inspect the inner workings of the system. In this
paper, we propose a tree-based method for organizing and representing reference
documents at various granular levels, which offers the flexibility to balance
cost and utility, and eases the inspection of the corpus content and retrieval
operations. Our method, called ReTreever, jointly learns a routing function per
internal node of a binary tree such that query and reference documents are
assigned to similar tree branches, hence directly optimizing for retrieval
performance. Our evaluations show that ReTreever generally preserves full
representation accuracy. Its hierarchical structure further provides strong
coarse representations and enhances transparency by indirectly learning
meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever
achieves the best retrieval accuracy at the lowest latency, proving that this
family of techniques can be viable in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ exHarmony: Authorship and Citations for Benchmarking the <span class="highlight-title">Review</span>er
  Assignment Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Ebrahimi, Sara Salamat, Negar Arabzadeh, Mahdi Bashari, Ebrahim Bagheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The peer review process is crucial for ensuring the quality and reliability
of scholarly work, yet assigning suitable reviewers remains a significant
challenge. Traditional manual methods are labor-intensive and often
ineffective, leading to nonconstructive or biased reviews. This paper
introduces the exHarmony (eHarmony but for connecting experts to manuscripts)
benchmark, designed to address these challenges by re-imagining the Reviewer
Assignment Problem (RAP) as a retrieval task. Utilizing the extensive data from
OpenAlex, we propose a novel approach that considers a host of signals from the
authors, most similar experts, and the citation relations as potential
indicators for a suitable reviewer for a manuscript. This approach allows us to
develop a standard benchmark dataset for evaluating the reviewer assignment
problem without needing explicit labels. We benchmark various methods,
including traditional lexical matching, static neural embeddings, and
contextualized neural embeddings, and introduce evaluation metrics that assess
both relevance and diversity in the context of RAP. Our results indicate that
while traditional methods perform reasonably well, contextualized embeddings
trained on scholarly literature show the best performance. The findings
underscore the importance of further research to enhance the diversity and
effectiveness of reviewer assignments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IU4Rec: Interest Unit-Based Product Organization and Recommendation for
  E-Commerce Platform <span class="chip">KDD25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wu, Xiaojie Li, Lin Wang, Jialiang Zhou, Di Wu, Qinye Xie, Qingheng Zhang, Yin Zhang, Shuguang Han, Fei Huang, Junfeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recommendation systems typically follow a product-based paradigm
utilizing user-product interactions to identify the most engaging items for
users. However, this product-based paradigm has notable drawbacks for
Xianyu~\footnote{Xianyu is China's largest online C2C e-commerce platform where
a large portion of the product are post by individual sellers}. Most of the
product on Xianyu posted from individual sellers often have limited stock
available for distribution, and once the product is sold, it's no longer
available for distribution. This result in most items distributed product on
Xianyu having relatively few interactions, affecting the effectiveness of
traditional recommendation depending on accumulating user-item interactions. To
address these issues, we introduce \textbf{IU4Rec}, an \textbf{I}nterest
\textbf{U}nit-based two-stage \textbf{Rec}ommendation system framework. We
first group products into clusters based on attributes such as category, image,
and semantics. These IUs are then integrated into the Recommendation system,
delivering both product and technological innovations. IU4Rec begins by
grouping products into clusters based on attributes such as category, image,
and semantics, forming Interest Units (IUs). Then we redesign the
recommendation process into two stages. In the first stage, the focus is on
recommend these Interest Units, capturing broad-level interests. In the second
stage, it guides users to find the best option among similar products within
the selected Interest Unit. User-IU interactions are incorporated into our
ranking models, offering the advantage of more persistent IU behaviors compared
to item-specific interactions. Experimental results on the production dataset
and online A/B testing demonstrate the effectiveness and superiority of our
proposed IU-centric recommendation approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at KDD25 ADS. This work has already been deployed on the
  Xianyu platform in Alibaba. arXiv admin note: substantial text overlap with
  arXiv:2403.06747</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Patterns Behind Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Chengcheng Ma, XuanQi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive framework for time series prediction
using a hybrid model that combines ARIMA and LSTM. The model incorporates
feature engineering techniques, including embedding and PCA, to transform raw
data into a lower-dimensional representation while retaining key information.
The embedding technique is used to convert categorical data into continuous
vectors, facilitating the capture of complex relationships. PCA is applied to
reduce dimensionality and extract principal components, enhancing model
performance and computational efficiency. To handle both linear and nonlinear
patterns in the data, the ARIMA model captures linear trends, while the LSTM
model models complex nonlinear dependencies. The hybrid model is trained on
historical data and achieves high accuracy, as demonstrated by low RMSE and MAE
scores. Additionally, the paper employs the run test to assess the randomness
of sequences, providing insights into the underlying patterns. Ablation studies
are conducted to validate the roles of different components in the model,
demonstrating the significance of each module. The paper also utilizes the SHAP
method to quantify the impact of traditional advantages on the predicted
results, offering a detailed understanding of feature importance. The KNN
method is used to determine the optimal prediction interval, further enhancing
the model's accuracy. The results highlight the effectiveness of combining
traditional statistical methods with modern deep learning techniques for robust
time series forecasting in Sports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ETimeline: An Extensive Timeline Generation <span class="highlight-title">Dataset</span> based on Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Liu, Yanan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timeline generation is of great significance for a comprehensive
understanding of the development of events over time. Its goal is to organize
news chronologically, which helps to identify patterns and trends that may be
obscured when viewing news in isolation, making it easier to track the
development of stories and understand the interrelationships between key
events. Timelines are now common in various commercial products, but academic
research in this area is notably scarce. Additionally, the current datasets are
in need of refinement for enhanced utility and expanded coverage. In this
paper, we propose ETimeline, which encompasses over $13,000$ news articles,
spanning $600$ bilingual timelines across $28$ news domains. Specifically, we
gather a candidate pool of more than $120,000$ news articles and employ the
large language model (LLM) Pipeline to improve performance, ultimately yielding
the ETimeline. The data analysis underscores the appeal of ETimeline.
Additionally, we also provide the news pool data for further research and
analysis. This work contributes to the advancement of timeline generation
research and supports a wide range of tasks, including topic generation and
event relationships. We believe that this dataset will serve as a catalyst for
innovative research and bridge the gap between academia and industry in
understanding the practical application of technology services. The dataset is
available at https://zenodo.org/records/11392212
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Gao, Liang Pang, Shicheng Xu, Leigang Qu, Tat-Seng Chua, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of AI-generated content (AIGC), the creation of
high-quality AI-generated videos has become faster and easier, resulting in the
Internet being flooded with all kinds of video content. However, the impact of
these videos on the content ecosystem remains largely unexplored. Video
information retrieval remains a fundamental approach for accessing video
content. Building on the observation that retrieval models often favor
AI-generated content in ad-hoc and image retrieval tasks, we investigate
whether similar biases emerge in the context of challenging video retrieval,
where temporal and visual factors may further influence model behavior. To
explore this, we first construct a comprehensive benchmark dataset containing
both real and AI-generated videos, along with a set of fair and rigorous
metrics to assess bias. This benchmark consists of 13,000 videos generated by
two state-of-the-art open-source video generation models. We meticulously
design a suite of rigorous metrics to accurately measure this preference,
accounting for potential biases arising from the limited frame rate and
suboptimal quality of AIGC videos. We then applied three off-the-shelf video
retrieval models to perform retrieval tasks on this hybrid dataset. Our
findings reveal a clear preference for AI-generated videos in retrieval.
Further investigation shows that incorporating AI-generated videos into the
training set of retrieval models exacerbates this bias. Unlike the preference
observed in image modalities, we find that video retrieval bias arises from
both unseen visual and temporal information, making the root causes of video
bias a complex interplay of these two factors. To mitigate this bias, we
fine-tune the retrieval models using a contrastive learning approach. The
results of this study highlight the potential implications of AI-generated
videos on retrieval systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt-Based Document Modifications In Ranking Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niv Bardas, Tommy Mordo, Oren Kurland, Moshe Tennenholtz, Gal Zur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study prompting-based approaches with Large Language Models (LLMs) for
modifying documents so as to promote their ranking in a competitive search
setting. Our methods are inspired by prior work on leveraging LLMs as rankers.
We evaluate our approach by deploying it as a bot in previous ranking
competitions and in competitions we organized. Our findings demonstrate that
our approach effectively improves document ranking while preserving high levels
of faithfulness to the original content and maintaining overall document
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CreAgent: Towards Long-Term Evaluation of Recommender System under
  Platform-Creator Information Asymmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopeng Ye, Chen Xu, Zhongxiang Sun, Jun Xu, Gang Wang, Zhenhua Dong, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the long-term sustainability of recommender systems (RS) emerges as
a crucial issue. Traditional offline evaluation methods for RS typically focus
on immediate user feedback, such as clicks, but they often neglect the
long-term impact of content creators. On real-world content platforms, creators
can strategically produce and upload new items based on user feedback and
preference trends. While previous studies have attempted to model creator
behavior, they often overlook the role of information asymmetry. This asymmetry
arises because creators primarily have access to feedback on the items they
produce, while platforms possess data on the entire spectrum of user feedback.
Current RS simulators, however, fail to account for this asymmetry, leading to
inaccurate long-term evaluations. To address this gap, we propose CreAgent, a
Large Language Model (LLM)-empowered creator simulation agent. By incorporating
game theory's belief mechanism and the fast-and-slow thinking framework,
CreAgent effectively simulates creator behavior under conditions of information
asymmetry. Additionally, we enhance CreAgent's simulation ability by
fine-tuning it using Proximal Policy Optimization (PPO). Our credibility
validation experiments show that CreAgent aligns well with the behaviors
between real-world platform and creator, thus improving the reliability of
long-term RS evaluations. Moreover, through the simulation of RS involving
CreAgents, we can explore how fairness- and diversity-aware RS algorithms
contribute to better long-term performance for various stakeholders. CreAgent
and the simulation platform are publicly available at
https://github.com/shawnye2000/CreAgent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow Matching for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkai Liu, Yangtian Zhang, Jianling Wang, Rex Ying, James Caverlee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have shown great promise in collaborative filtering by
capturing the underlying distribution of user interests and preferences.
However, existing approaches struggle with inaccurate posterior approximations
and misalignment with the discrete nature of recommendation data, limiting
their expressiveness and real-world performance. To address these limitations,
we propose FlowCF, a novel flow-based recommendation system leveraging flow
matching for collaborative filtering. We tailor flow matching to the unique
challenges in recommendation through two key innovations: (1) a behavior-guided
prior that aligns with user behavior patterns to handle the sparse and
heterogeneous user-item interactions, and (2) a discrete flow framework to
preserve the binary nature of implicit feedback while maintaining the benefits
of flow matching, such as stable training and efficient inference. Extensive
experiments demonstrate that FlowCF achieves state-of-the-art recommendation
accuracy across various datasets with the fastest inference speed, making it a
compelling approach for real-world recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Negative Reservoir for Incremental Learning in Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonios Valkanas, Yuening Wang, Yingxue Zhang, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become an integral part of online platforms. Every
day the volume of training data is expanding and the number of user
interactions is constantly increasing. The exploration of larger and more
expressive models has become a necessary pursuit to improve user experience.
However, this progression carries with it an increased computational burden. In
commercial settings, once a recommendation system model has been trained and
deployed it typically needs to be updated frequently as new client data arrive.
Cumulatively, the mounting volume of data is guaranteed to eventually make full
batch retraining of the model from scratch computationally infeasible. Naively
fine-tuning solely on the new data runs into the well-documented problem of
catastrophic forgetting. Despite the fact that negative sampling is a crucial
part of training with implicit feedback, no specialized technique exists that
is tailored to the incremental learning framework. In this work, we propose a
personalized negative reservoir strategy, which is used to obtain negative
samples for the standard triplet loss of graph-based recommendation systems.
Our technique balances alleviation of forgetting with plasticity by encouraging
the model to remember stable user preferences and selectively forget when user
interests change. We derive the mathematical formulation of a negative sampler
to populate and update the reservoir. We integrate our design in three SOTA and
commonly used incremental recommendation models. We show that these concrete
realizations of our negative reservoir framework achieve state-of-the-art
results for standard benchmarks using multiple top-k evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faux Polyglot: A Study on Information Disparity in Multilingual Large
  Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Sharma, Kenton Murray, Ziang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the multilingual capability of LLMs offers new opportunities to
overcome the language barrier, do these capabilities translate into real-life
scenarios where linguistic divide and knowledge conflicts between multilingual
sources are known occurrences? In this paper, we studied LLM's linguistic
preference in a cross-language RAG-based information search setting. We found
that LLMs displayed systemic bias towards information in the same language as
the query language in both document retrieval and answer generation.
Furthermore, in scenarios where no information is in the language of the query,
LLMs prefer documents in high-resource languages during generation, potentially
reinforcing the dominant views. Such bias exists for both factual and
opinion-based queries. Our results highlight the linguistic divide within
multilingual LLMs in information search systems. The seemingly beneficial
multilingual capability of LLMs may backfire on information parity by
reinforcing language-specific information cocoons or filter bubbles further
marginalizing low-resource views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized
  Recommendation Systems <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuli Wang, Xue Wei, Senjie Kou, Chi Wang, Wenshuai Chen, Qi Tang, Yinhua Zhu, Xiong Xiao, Xingxing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking plays a crucial role in modern multi-stage recommender systems by
rearranging the initial ranking list. Due to the inherent challenges of
combinatorial search spaces, some current research adopts an
evaluator-generator paradigm, with a generator generating feasible sequences
and an evaluator selecting the best sequence based on the estimated list
utility. However, these methods still face two issues. Firstly, due to the goal
inconsistency problem between the evaluator and generator, the generator tends
to fit the local optimal solution of exposure distribution rather than
combinatorial space optimization. Secondly, the strategy of generating target
items one by one is difficult to achieve optimality because it ignores the
information of subsequent items.
  To address these issues, we propose a utilizing Neighbor Lists model for
Generative Reranking (NLGR), which aims to improve the performance of the
generator in the combinatorial space. NLGR follows the evaluator-generator
paradigm and improves the generator's training and generating methods.
Specifically, we use neighbor lists in combination space to enhance the
training process, making the generator perceive the relative scores and find
the optimization direction. Furthermore, we propose a novel sampling-based
non-autoregressive generation method, which allows the generator to jump
flexibly from the current list to any neighbor list. Extensive experiments on
public and industrial datasets validate NLGR's effectiveness and we have
successfully deployed NLGR on the Meituan food delivery platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SampleLLM: Optimizing Tabular Data Synthesis in Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Gao, Zhaocheng Du, Xiaopeng Li, Yichao Wang, Xiangyang Li, Huifeng Guo, Ruiming Tang, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data synthesis is crucial in machine learning, yet existing general
methods-primarily based on statistical or deep learning models-are highly
data-dependent and often fall short in recommender systems. This limitation
arises from their difficulty in capturing complex distributions and
understanding feature relationships from sparse and limited data, along with
their inability to grasp semantic feature relations. Recently, Large Language
Models (LLMs) have shown potential in generating synthetic data samples through
few-shot learning and semantic understanding. However, they often suffer from
inconsistent distribution and lack of diversity due to their inherent
distribution disparity with the target dataset. To address these challenges and
enhance tabular data synthesis for recommendation tasks, we propose a novel
two-stage framework named SampleLLM to improve the quality of LLM-based tabular
data synthesis for recommendations by ensuring better distribution alignment.
In the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and
diverse exemplars to generate data that closely aligns with the target dataset
distribution, even when input samples are limited. The second stage uses an
advanced feature attribution-based importance sampling method to refine feature
relationships within the synthesized data, reducing any distribution biases
introduced by the LLM. Experimental results on three recommendation datasets,
two general datasets, and online deployment illustrate that SampleLLM
significantly surpasses existing methods for recommendation tasks and holds
promise for a broader range of tabular data scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIM: <span class="highlight-title">Multi-modal</span> Content Interest Modeling Paradigm for User Behavior
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00321v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00321v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bencheng Yan, Si Chen, Shichang Jia, Jianyu Liu, Yueran Liu, Chenghan Fu, Wanxian Guan, Hui Zhao, Xiang Zhang, Kai Zhang, Wenbo Su, Pengjie Wang, Jian Xu, Bo Zheng, Baolin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction is a crucial task in recommendation
systems, online searches, and advertising platforms, where accurately capturing
users' real interests in content is essential for performance. However,
existing methods heavily rely on ID embeddings, which fail to reflect users'
true preferences for content such as images and titles. This limitation becomes
particularly evident in cold-start and long-tail scenarios, where traditional
approaches struggle to deliver effective results. To address these challenges,
we propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which
consists of three key stages: Pre-training, Content-Interest-Aware Supervised
Fine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training
stage adapts foundational models to domain-specific data, enabling the
extraction of high-quality multi-modal embeddings. The C-SFT stage bridges the
semantic gap between content and user interests by leveraging user behavior
signals to guide the alignment of embeddings with user preferences. Finally,
the CiUBM stage integrates multi-modal embeddings and ID-based collaborative
filtering signals into a unified framework. Comprehensive offline experiments
and online A/B tests conducted on the Taobao, one of the world's largest
e-commerce platforms, demonstrated the effectiveness and efficiency of MIM
method. The method has been successfully deployed online, achieving a
significant increase of +14.14% in CTR and +4.12% in RPM, showcasing its
industrial applicability and substantial impact on platform performance. To
promote further research, we have publicly released the code and dataset at
https://pan.quark.cn/s/8fc8ec3e74f3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distinguished Quantized Guidance for Diffusion-based Sequence
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Mao, Shuchang Liu, Haoyang Liu, Haozhe Liu, Xiang Li, Lantao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have emerged as promising approaches for sequential
recommendation due to their strong ability to model data distributions and
generate high-quality items. Existing work typically adds noise to the next
item and progressively denoises it guided by the user's interaction sequence,
generating items that closely align with user interests. However, we identify
two key issues in this paradigm. First, the sequences are often heterogeneous
in length and content, exhibiting noise due to stochastic user behaviors. Using
such sequences as guidance may hinder DMs from accurately understanding user
interests. Second, DMs are prone to data bias and tend to generate only the
popular items that dominate the training dataset, thus failing to meet the
personalized needs of different users. To address these issues, we propose
Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation
(DiQDiff), which aims to extract robust guidance to understand user interests
and generate distinguished items for personalized user interests within DMs. To
extract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)
to quantize sequences into semantic vectors (e.g., collaborative signals and
category interests) using a codebook, which can enrich the guidance to better
understand user interests. To generate distinguished items, DiQDiff
personalizes the generation through Contrastive Discrepancy Maximization (CDM),
which maximizes the distance between denoising trajectories using contrastive
loss to prevent biased generation for different users. Extensive experiments
are conducted to compare DiQDiff with multiple baseline models across four
widely-used datasets. The superior recommendation performance of DiQDiff
against leading approaches demonstrates its effectiveness in sequential
recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AURO: Reinforcement Learning for Adaptive User Retention Optimization in
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghai Xue, Qingpeng Cai, Bin Yang, Lantao Hu, Peng Jiang, Kun Gai, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Reinforcement Learning (RL) has garnered increasing attention
for its ability of optimizing user retention in recommender systems. A primary
obstacle in this optimization process is the environment non-stationarity
stemming from the continual and complex evolution of user behavior patterns
over time, such as variations in interaction rates and retention propensities.
These changes pose significant challenges to existing RL algorithms for
recommendations, leading to issues with dynamics and reward distribution
shifts. This paper introduces a novel approach called \textbf{A}daptive
\textbf{U}ser \textbf{R}etention \textbf{O}ptimization (AURO) to address this
challenge. To navigate the recommendation policy in non-stationary
environments, AURO introduces an state abstraction module in the policy
network. The module is trained with a new value-based loss function, aligning
its output with the estimated performance of the current policy. As the policy
performance of RL is sensitive to environment drifts, the loss function enables
the state abstraction to be reflective of environment changes and notify the
recommendation policy to adapt accordingly. Additionally, the non-stationarity
of the environment introduces the problem of implicit cold start, where the
recommendation policy continuously interacts with users displaying novel
behavior patterns. AURO encourages exploration guarded by performance-based
rejection sampling to maintain a stable recommendation quality in the
cost-sensitive online environment. Extensive empirical analysis are conducted
in a user retention simulator, the MovieLens dataset, and a live short-video
recommendation platform, demonstrating AURO's superior performance against all
evaluated baseline algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Web Conference 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Memory Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lu, Zheng Chai, Yuchao Zheng, Zhe Chen, Deping Xie, Peng Xu, Xun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehai Min, Zhiyang Xu, Guilin Qi, Lifu Huang, Chenyu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing information retrieval (IR) models often assume a homogeneous
structure for knowledge sources and user queries, limiting their applicability
in real-world settings where retrieval is inherently heterogeneous and diverse.
In this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous
knowledge retriever that (1) builds a unified retrieval space for heterogeneous
knowledge and (2) follows diverse user instructions to retrieve knowledge of
specified types. UniHGKR consists of three principal stages: heterogeneous
self-supervised pretraining, text-anchored embedding alignment, and
instruction-aware retriever fine-tuning, enabling it to generalize across
varied retrieval contexts. This framework is highly scalable, with a BERT-based
version and a UniHGKR-7B version trained on large language models. Also, we
introduce CompMix-IR, the first native heterogeneous knowledge retrieval
benchmark. It includes two retrieval scenarios with various instructions, over
9,400 question-answer (QA) pairs, and a corpus of 10 million entries, covering
four different types of data. Extensive experiments show that UniHGKR
consistently outperforms state-of-the-art methods on CompMix-IR, achieving up
to 6.36% and 54.23% relative improvements in two scenarios, respectively.
Finally, by equipping our retriever for open-domain heterogeneous QA systems,
we achieve a new state-of-the-art result on the popular ConvMix task, with an
absolute improvement of up to 5.90 points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025, Main, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Domain Scaling for Personalized Sequential Modeling in
  Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chai, Hui Lu, Di Chen, Qin Ren, Yuchao Zheng, Xun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users generally exhibit complex behavioral patterns and diverse intentions in
multiple business scenarios of super applications like Douyin, presenting great
challenges to current industrial multi-domain recommenders. To mitigate the
discrepancies across diverse domains, researches and industrial practices
generally emphasize sophisticated network structures to accomodate diverse data
distributions, while neglecting the inherent understanding of user behavioral
sequence from the multi-domain perspective. In this paper, we present Adaptive
Domain Scaling (ADS) model, which comprehensively enhances the personalization
capability in target-aware sequence modeling across multiple domains.
Specifically, ADS comprises of two major modules, including personalized
sequence representation generation (PSRG) and personalized candidate
representation generation (PCRG). The modules contribute to the tailored
multi-domain learning by dynamically learning both the user behavioral sequence
item representation and the candidate target item representation under
different domains, facilitating adaptive user intention understanding.
Experiments are performed on both a public dataset and two billion-scaled
industrial datasets, and the extensive results verify the high effectiveness
and compatibility of ADS. Besides, we conduct online experiments on two
influential business scenarios including Douyin Advertisement Platform and
Douyin E-commerce Service Platform, both of which show substantial business
improvements. Currently, ADS has been fully deployed in many recommendation
services at ByteDance, serving billions of users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate
  Personalized News Recommendation <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09401v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09401v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunyong Ko, Seongeun Ryu, Sang-Wook Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized news recommendation aims to assist users in finding news
articles that align with their interests, which plays a pivotal role in
mitigating users' information overload problem. Although many recent works have
been studied for better personalized news recommendation, the following
challenges should be explored more: (C1) Comprehending manifold intents coupled
within a news article, (C2) Differentiating varying post-read preferences of
news articles, and (C3) Addressing the cold-start user problem. To tackle the
aforementioned challenges together, in this paper, we propose a novel
personalized news recommendation framework (CROWN) that employs (1)
category-guided intent disentanglement for (C1), (2) consistency-based news
representation for (C2), and (3) GNN-enhanced hybrid user representation for
(C3). Furthermore, we incorporate a category prediction into the training
process of CROWN as an auxiliary task, which provides supplementary supervisory
signals to enhance intent disentanglement. Extensive experiments on two
real-world datasets reveal that (1) CROWN provides consistent performance
improvements over ten state-of-the-art news recommendation methods and (2) the
proposed strategies significantly improve the accuracy of CROWN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 9 tables, the ACM Web Conference (WWW) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RALLRec: Improving Retrieval Augmented Large Language Model
  Recommendation with Representation Learning <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xu, Sichun Luo, Xiangyu Chen, Haoming Huang, Hanxu Hou, Linqi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been integrated into recommendation systems
to enhance user behavior comprehension. The Retrieval Augmented Generation
(RAG) technique is further incorporated into these systems to retrieve more
relevant items and improve system performance. However, existing RAG methods
rely primarily on textual semantics and often fail to incorporate the most
relevant items, limiting the effectiveness of the systems.
  In this paper, we propose Representation learning for retrieval-Augmented
Large Language model Recommendation (RALLRec). Specifically, we enhance textual
semantics by prompting LLMs to generate more detailed item descriptions,
followed by joint representation learning of textual and collaborative
semantics, which are extracted by the LLM and recommendation models,
respectively. Considering the potential time-varying characteristics of user
interest, a simple yet effective reranking method is further introduced to
capture the dynamics of user preference. We conducted extensive experiments on
three real-world datasets, and the evaluation results validated the
effectiveness of our method. Code is made public at
https://github.com/JianXu95/RALLRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TheWebConf'25 (WWW'25) as a Short Paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RenderBox: Expressive Performance Rendering with Text Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zhang, Akira Maezawa, Simon Dixon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive music performance rendering involves interpreting symbolic scores
with variations in timing, dynamics, articulation, and instrument-specific
techniques, resulting in performances that capture musical can emotional
intent. We introduce RenderBox, a unified framework for text-and-score
controlled audio performance generation across multiple instruments, applying
coarse-level controls through natural language descriptions and granular-level
controls using music scores. Based on a diffusion transformer architecture and
cross-attention joint conditioning, we propose a curriculum-based paradigm that
trains from plain synthesis to expressive performance, gradually incorporating
controllable factors such as speed, mistakes, and style diversity.
  RenderBox achieves high performance compared to baseline models across key
metrics such as FAD and CLAP, and also tempo and pitch accuracy under different
prompting tasks. Subjective evaluation further demonstrates that RenderBox is
able to generate controllable expressive performances that sound natural and
musically engaging, aligning well with prompts and intent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for
egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K
ego-view videos and 7K scene-text aware questions that reflect real-user needs
in outdoor driving and indoor house-keeping activities. The questions are
designed to elicit identification and reasoning on scene text in an egocentric
and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10
prominent multimodal large language models. Currently, all models struggle, and
the best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the
severe deficiency of these techniques in egocentric QA assistance. Our further
investigations suggest that precise temporal grounding and multi-frame
reasoning, along with high resolution and auxiliary scene-text inputs, are key
for better performance. With thorough analyses and heuristic suggestions, we
hope EgoTextVQA can serve as a solid testbed for research in egocentric
scene-text QA assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression under ultra-low bitrates remains challenging for both
conventional learned image compression (LIC) and generative vector-quantized
(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy
quantization, while generative VQ modeling gives poor fidelity due to the
mismatch between learned generative priors and specific inputs. In this work,
we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream
framework that utilizes both generative VQ-modeling and diffusion models, as
well as conventional LIC, to achieve both high fidelity and high perceptual
quality. Different from previous hybrid methods that directly use pre-trained
LIC models to generate low-quality fidelity-preserving information from heavily
quantized latent, we use diffusion models to extract high-quality complimentary
fidelity information from the ground-truth input, which can enhance the system
performance in several aspects: improving indices map prediction, enhancing the
fidelity-preserving output of the LIC stream, and refining conditioned image
reconstruction with VQ-latent correction. In addition, our diffusion model is
based on a dense representative vector (DRV), which is lightweight with very
simple sampling schedulers. Extensive experiments demonstrate that our
HDCompression outperforms the previous conventional LIC, generative
VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative
visualization, providing balanced robust compression performance at ultra-low
bitrates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Audio Codec Identification Using Overlapping LCS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farzane Jafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio data are widely exchanged over telecommunications networks. Due to the
limitations of network resources, these data are typically compressed before
transmission. Various methods are available for compressing audio data. To
access such audio information, it is first necessary to identify the codec used
for compression. One of the most effective approaches for audio codec
identification involves analyzing the content of received packets. In these
methods, statistical features extracted from the packets are utilized to
determine the codec employed. This paper proposes a novel method for audio
codec classification based on features derived from the overlapped longest
common sub-string and sub-sequence (LCS). The simulation results, which
achieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of
the proposed method over conventional approaches. This method divides each 8 KB
packet into fifteen 1 KB packets with a 50% overlap. The results indicate that
this division has no significant impact on the simulation outcomes, while
significantly speeding up the feature extraction, being eight times faster than
the traditional method for extracting LCS features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioVL-QR: Egocentric Biochemical Vision-and-Language <span class="highlight-title">Dataset</span> Using Micro
  QR Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomohiro Nishimoto, Taichi Nishimura, Koki Yamamoto, Keisuke Shirai, Hirotaka Kameko, Yuto Haneji, Tomoya Yoshida, Keiya Kajimura, Taiyu Cui, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Shinsuke Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces BioVL-QR, a biochemical vision-and-language dataset
comprising 23 egocentric experiment videos, corresponding protocols, and
vision-and-language alignments. A major challenge in understanding biochemical
videos is detecting equipment, reagents, and containers because of the
cluttered environment and indistinguishable objects. Previous studies assumed
manual object annotation, which is costly and time-consuming. To address the
issue, we focus on Micro QR Codes. However, detecting objects using only Micro
QR Codes is still difficult due to blur and occlusion caused by object
manipulation. To overcome this, we propose an object labeling method combining
a Micro QR Code detector with an off-the-shelf hand object detector. As an
application of the method and BioVL-QR, we tackled the task of localizing the
procedural steps in an instructional video. The experimental results show that
using Micro QR Codes and our method improves biochemical video understanding.
Data and code are available through https://nishi10mo.github.io/BioVL-QR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-10T00:00:00Z">2025-02-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repository-level Code Search with Neural Retrieval Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Gandhi, Luyu Gao, Jamie Callan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a multi-stage reranking system for repository-level code
search, which leverages the vastly available commit histories of large
open-source repositories to aid in bug fixing. We define the task of
repository-level code search as retrieving the set of files from the current
state of a code repository that are most relevant to addressing a user's
question or bug. The proposed approach combines BM25-based retrieval over
commit messages with neural reranking using CodeBERT to identify the most
pertinent files. By learning patterns from diverse repositories and their
commit histories, the system can surface relevant files for the task at hand.
The system leverages both commit messages and source code for relevance
matching, and is evaluated in both normal and oracle settings. Experiments on a
new dataset created from 7 popular open-source repositories demonstrate
substantial improvements of up to 80% in MAP, MRR and P@1 over the BM25
baseline, across a diverse set of queries, demonstrating the effectiveness this
approach. We hope this work aids LLM agents as a tool for better code search
and understanding. Our code and results obtained are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RSAttAE: An Information-Aware Attention-based Autoencoder Recommender
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Dadashzadeh Taromi, Sina Heydari, Mohsen Hooshmand, Majid Ramezani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a crucial role in modern life, including information
retrieval, the pharmaceutical industry, retail, and entertainment. The
entertainment sector, in particular, attracts significant attention and
generates substantial profits. This work proposes a new method for predicting
unknown user-movie ratings to enhance customer satisfaction. To achieve this,
we utilize the MovieLens 100K dataset. Our approach introduces an
attention-based autoencoder to create meaningful representations and the
XGBoost method for rating predictions. The results demonstrate that our
proposal outperforms most of the existing state-of-the-art methods.
Availability: github.com/ComputationIASBS/RecommSys
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiveForesighter: Generating Future Information for Live-Streaming
  Recommendations at Kuaishou 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Lu, Jiangxia Cao, Xu Kuan, Wei Cheng, Wei Jiang, Jiaming Zhang, Yang Shuang, Liu Zhaojie, Liyin Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live-streaming, as a new-generation media to connect users and authors, has
attracted a lot of attention and experienced rapid growth in recent years.
Compared with the content-static short-video recommendation, the live-streaming
recommendation faces more challenges in giving our users a satisfactory
experience: (1) Live-streaming content is dynamically ever-changing along time.
(2) valuable behaviors (e.g., send digital-gift, buy products) always require
users to watch for a long-time (>10 min). Combining the two attributes, here
raising a challenging question for live-streaming recommendation: How to
discover the live-streamings that the content user is interested in at the
current moment, and further a period in the future?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Collaborative and Semantic Knowledge Fusion for Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longtao Xiao, Haozhao Wang, Cheng Wang, Linfei Ji, Yifan Wang, Jieming Zhu, Zhenhua Dong, Rui Zhang, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent surge in interest surrounding generative paradigms,
generative recommendation has increasingly attracted the attention of
researchers in the recommendation community. This paradigm generally consists
of two stages. In the first stage, pretrained semantic embeddings or
collaborative ID embeddings are quantized to create item codes, aiming to
capture and preserve rich semantic or collaborative knowledge within these
codes. The second stage involves utilizing these discrete codes to perform an
autoregressive sequence generation task. Existing methods often either overlook
collaborative or semantic knowledge, or combine the two roughly. In this paper,
we observe that naively concatenating representations from semantic and
collaborative modality leads to a semantic domination issue, where the
resulting representation is overly influenced by semantic information,
effectively overshadowing the collaborative representation. Consequently,
downstream recommendation tasks fail to fully exploit the knowledge from both
modalities, resulting in suboptimal performance. To address this, we propose a
progressive collaborative and semantic knowledge fusion model for generative
recommendation, named PRORec, which integrates semantic and collaborative
knowledge with a unified code through a two-stage framework. Specifically, in
the first stage, we propose a cross-modality knowledge alignment task, which
integrates semantic knowledge into collaborative embeddings, enhancing their
representational capability. In the second stage, we propose an in-modality
knowledge distillation task, designed to effectively capture and integrate
knowledge from both semantic and collaborative modalities. Extensive
experiments on three widely used benchmarks validate the effectiveness of our
approach, demonstrating its superiority compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyun Zhao, Hongyi Yuan, Jingjing Liu, Haichao Chen, Huaiyuan Ying, Songchi Zhou, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity retrieval plays a crucial role in the utilization of Electronic Health
Records (EHRs) and is applied across a wide range of clinical practices.
However, a comprehensive evaluation of this task is lacking due to the absence
of a public benchmark. In this paper, we propose the development and release of
a novel benchmark for evaluating entity retrieval in EHRs, with a particular
focus on the semantic gap issue. Using discharge summaries from the MIMIC-III
dataset, we incorporate ICD codes and prescription labels associated with the
notes as queries, and annotate relevance judgments using GPT-4. In total, we
use 1,000 patient notes, generate 1,246 queries, and provide over 77,000
relevance annotations. To offer the first assessment of the semantic gap, we
introduce a novel classification system for relevance matches. Leveraging
GPT-4, we categorize each relevant pair into one of five categories: string,
synonym, abbreviation, hyponym, and implication. Using the proposed benchmark,
we evaluate several retrieval methods, including BM25, query expansion, and
state-of-the-art dense retrievers. Our findings show that BM25 provides a
strong baseline but struggles with semantic matches. Query expansion
significantly improves performance, though it slightly reduces string match
capabilities. Dense retrievers outperform traditional methods, particularly for
semantic matches, and general-domain dense retrievers often surpass those
trained specifically in the biomedical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, and the dataset will be made public upon reception of
  our paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FunduSAM: A Specialized Deep Learning Model for Enhanced Optic Disc and
  Cup Segmentation in Fundus Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchen Yu, Yongwei Nie, Fei Qi, Wenxiong Liao, Hongmin Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) has gained popularity as a versatile image
segmentation method, thanks to its strong generalization capabilities across
various domains. However, when applied to optic disc (OD) and optic cup (OC)
segmentation tasks, SAM encounters challenges due to the complex structures,
low contrast, and blurred boundaries typical of fundus images, leading to
suboptimal performance. To overcome these challenges, we introduce a novel
model, FunduSAM, which incorporates several Adapters into SAM to create a deep
network specifically designed for OD and OC segmentation. The FunduSAM utilizes
Adapter into each transformer block after encoder for parameter fine-tuning
(PEFT). It enhances SAM's feature extraction capabilities by designing a
Convolutional Block Attention Module (CBAM), addressing issues related to
blurred boundaries and low contrast. Given the unique requirements of OD and OC
segmentation, polar transformation is used to convert the original fundus OD
images into a format better suited for training and evaluating FunduSAM. A
joint loss is used to achieve structure preservation between the OD and OC,
while accurate segmentation. Extensive experiments on the REFUGE dataset,
comprising 1,200 fundus images, demonstrate the superior performance of
FunduSAM compared to five mainstream approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Knowledge Integration in Retrieval-Augmented Generation with
  Self-Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Weng, Fengbin Zhu, Tong Ye, Haoyan Liu, Fuli Feng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), which integrates external knowledge
into Large Language Models (LLMs), has proven effective in enabling LLMs to
produce more accurate and reliable responses. However, it remains a significant
challenge how to effectively integrate external retrieved knowledge with
internal parametric knowledge in LLMs. In this work, we propose a novel
Self-Selection RAG framework, where the LLM is made to select from pairwise
responses generated with internal parametric knowledge solely and with external
retrieved knowledge together to achieve enhanced accuracy. To this end, we
devise a Self-Selection-RGP method to enhance the capabilities of the LLM in
both generating and selecting the correct answer, by training the LLM with
Direct Preference Optimization (DPO) over a curated Retrieval Generation
Preference (RGP) dataset. Experimental results with two open-source LLMs (i.e.,
Llama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our
approach over other baseline methods on Natural Questions (NQ) and TrivialQA
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Copyright Protection for Knowledge Bases of Retrieval-augmented
  Language Models via Ownership Verification with Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly integrated into real-world
applications through retrieval-augmented generation (RAG) mechanisms to
supplement their responses with up-to-date and domain-specific knowledge.
However, the valuable and often proprietary nature of the knowledge bases used
in RAG introduces the risk of unauthorized usage by adversaries. Existing
methods that can be generalized as watermarking techniques to protect these
knowledge bases typically involve poisoning attacks. However, these methods
require to alter the results of verification samples (\eg, generating incorrect
outputs), inevitably making them susceptible to anomaly detection and even
introduce new security risks. To address these challenges, we propose \name{}
for `harmless' copyright protection of knowledge bases. Instead of manipulating
LLM's final output, \name{} implants distinct verification behaviors in the
space of chain-of-thought (CoT) reasoning, maintaining the correctness of the
final answer. Our method has three main stages: (1) \textbf{Generating CoTs}:
For each verification question, we generate two CoTs, including a target CoT
for building watermark behaviors; (2) \textbf{Optimizing Watermark Phrases and
Target CoTs}: We optimize them to minimize retrieval errors under the black-box
setting of suspicious LLM, ensuring that the watermarked verification queries
activate the target CoTs without being activated in non-watermarked ones; (3)
\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to
statistically verify whether a suspicious LLM is augmented with the protected
knowledge base by comparing its responses to watermarked and benign
verification queries. Our experiments on diverse benchmarks demonstrate that
\name{} effectively protects knowledge bases against unauthorized usage while
preserving the integrity and performance of the RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Conversational and Collaborative Signals for Conversational
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Bin Rabiah, Nafis Sadeq, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommendation systems (CRS) leverage contextual information
from conversations to generate recommendations but often struggle due to a lack
of collaborative filtering (CF) signals, which capture user-item interaction
patterns essential for accurate recommendations. We introduce Reddit-ML32M, a
dataset that links Reddit conversations with interactions on MovieLens 32M, to
enrich item representations by leveraging collaborative knowledge and
addressing interaction sparsity in conversational datasets. We propose an
LLM-based framework that uses Reddit-ML32M to align LLM-generated
recommendations with CF embeddings, refining rankings for better performance.
We evaluate our framework against three sets of baselines: CF-based
recommenders using only interactions from CRS tasks, traditional CRS models,
and LLM-based methods relying on conversational context without item
representations. Our approach achieves consistent improvements, including a
12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the
best-performing baseline that relies on conversational context but lacks
collaborative item representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARM: Unlocking the Future of Recommendation Systems through Memory
  Augmentation and Scalable Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling-law has guided the language model designing for past years, however,
it is worth noting that the scaling laws of NLP cannot be directly applied to
RecSys due to the following reasons: (1) The amount of training samples and
model parameters is typically not the bottleneck for the model. Our
recommendation system can generate over 50 billion user samples daily, and such
a massive amount of training data can easily allow our model parameters to
exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the
stability and robustness of the recommendation system, it is essential to
control computational complexity FLOPs carefully. Considering the above
differences with LLM, we can draw a conclusion that: for a RecSys model,
compared to model parameters, the computational complexity FLOPs is a more
expensive factor that requires careful control. In this paper, we propose our
milestone work, MARM (Memory Augmented Recommendation Model), which explores a
new cache scaling-laws successfully.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-based SPARQL Query Generation from Natural Language over Federated
  Knowledge <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06062v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06062v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Emonet, Jerven Bolleman, Severine Duvaud, Tarcisio Mendes de Farias, Ana Claudia Sima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential
  Recommendation <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Liu, Sixiao Zhang, Cheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) systems excel at capturing users' dynamic
preferences by leveraging their interaction histories. Most existing SR systems
assign a single embedding vector to each item to represent its features, and
various types of models are adopted to combine these item embeddings into a
sequence representation vector to capture the user intent. However, we argue
that this representation alone is insufficient to capture an item's
multi-faceted nature (e.g., movie genres, starring actors). Besides, users
often exhibit complex and varied preferences within these facets (e.g., liking
both action and musical films in the facet of genre), which are challenging to
fully represent. To address the issues above, we propose a novel structure
called Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential
Recommendation (FAME). We leverage sub-embeddings from each head in the last
multi-head attention layer to predict the next item separately. This approach
captures the potential multi-faceted nature of items without increasing model
complexity. A gating mechanism integrates recommendations from each head and
dynamically determines their importance. Furthermore, we introduce a
Mixture-of-Experts (MoE) network in each attention head to disentangle various
user preferences within each facet. Each expert within the MoE focuses on a
specific preference. A learnable router network is adopted to compute the
importance weight for each expert and aggregate them. We conduct extensive
experiments on four public sequential recommendation datasets and the results
demonstrate the effectiveness of our method over existing baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by WSDM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Identity-Aware Cross-Modal Retrieval: a <span class="highlight-title">Dataset</span> and a Baseline <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Messina, Lucia Vadicamo, Leo Maltese, Claudio Gennaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep learning have significantly enhanced
content-based retrieval methods, notably through models like CLIP that map
images and texts into a shared embedding space. However, these methods often
struggle with domain-specific entities and long-tail concepts absent from their
training data, particularly in identifying specific individuals. In this paper,
we explore the task of identity-aware cross-modal retrieval, which aims to
retrieve images of persons in specific contexts based on natural language
queries. This task is critical in various scenarios, such as for searching and
browsing personalized video collections or large audio-visual archives
maintained by national broadcasters. We introduce a novel dataset, COCO Person
FaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched
with deepfake-generated faces from VGGFace2. This dataset addresses the lack of
large-scale datasets needed for training and evaluating models for this task.
Our experiments assess the performance of different CLIP variations repurposed
for this task, including our architecture, Identity-aware CLIP (Id-CLIP), which
achieves competitive retrieval performance through targeted fine-tuning. Our
contributions lay the groundwork for more robust cross-modal retrieval systems
capable of recognizing long-tail identities and contextual nuances. Data and
code are available at https://github.com/mesnico/IdCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as full paper at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LemmaHead: RAG Assisted Proof Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15797v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15797v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbo Yang, Mingqi Yan, Hongyi Zhao, Tianshuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing the logic necessary to solve mathematical problems or write
mathematical proofs is one of the more difficult objectives for large language
models (LLMS). Currently, the most popular methods in literature consists of
fine-tuning the model on written mathematical content such as academic
publications and textbooks, so that the model can learn to emulate the style of
mathematical writing. In this project, we explore the effectiveness of using
retrieval augmented generation (RAG) to address gaps in the mathematical
reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements
queries to the model with relevant mathematical context, with particular focus
on context from published textbooks. To measure our model's performance in
mathematical reasoning, our testing paradigm focuses on the task of automated
theorem proving via generating proofs to a given mathematical claim in the Lean
formal language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epidemiology-informed Network for Robust Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Tong Chen, Xinyi Gao, Wentao Zhang, Lizhen Cui, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid spread of rumors on social media has posed significant challenges
to maintaining public trust and information integrity. Since an information
cascade process is essentially a propagation tree, recent rumor detection
models leverage graph neural networks to additionally capture information
propagation patterns, thus outperforming text-only solutions. Given the
variations in topics and social impact of the root node, different source
information naturally has distinct outreach capabilities, resulting in
different heights of propagation trees. This variation, however, impedes the
data-driven design of existing graph-based rumor detectors. Given a shallow
propagation tree with limited interactions, it is unlikely for graph-based
approaches to capture sufficient cascading patterns, questioning their ability
to handle less popular news or early detection needs. In contrast, a deep
propagation tree is prone to noisy user responses, and this can in turn
obfuscate the predictions. In this paper, we propose a novel
Epidemiology-informed Network (EIN) that integrates epidemiological knowledge
to enhance performance by overcoming data-driven methods sensitivity to data
quality. Meanwhile, to adapt epidemiology theory to rumor detection, it is
expected that each users stance toward the source information will be
annotated. To bypass the costly and time-consuming human labeling process, we
take advantage of large language models to generate stance labels, facilitating
optimization objectives for learning epidemiology-informed representations. Our
experimental results demonstrate that the proposed EIN not only outperforms
state-of-the-art methods on real-world datasets but also exhibits enhanced
robustness across varying tree depths.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cardiverse: Harnessing LLMs for Novel Card Game Prototyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prototyping of computer games, particularly card games, requires
extensive human effort in creative ideation and gameplay evaluation. Recent
advances in Large Language Models (LLMs) offer opportunities to automate and
streamline these processes. However, it remains challenging for LLMs to design
novel game mechanics beyond existing databases, generate consistent gameplay
environments, and develop scalable gameplay AI for large-scale evaluations.
This paper addresses these challenges by introducing a comprehensive automated
card game prototyping framework. The approach highlights a graph-based indexing
method for generating novel game designs, an LLM-driven system for consistent
game code generation validated by gameplay records, and a gameplay AI
constructing method that uses an ensemble of LLM-generated action-value
functions optimized through self-play. These contributions aim to accelerate
card game prototyping, reduce human labor, and lower barriers to entry for game
developers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Musical Representations for Music Performance Question
  Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Diao, Chunhui Zhang, Tingxuan Wu, Ming Cheng, Zhongyu Ouyang, Weiyi Wu, Jiang Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music performances are representative scenarios for audio-visual modeling.
Unlike common scenarios with sparse audio, music performances continuously
involve dense audio signals throughout. While existing multimodal learning
methods on the audio-video QA demonstrate impressive capabilities in general
scenarios, they are incapable of dealing with fundamental problems within the
music performances: they underexplore the interaction between the multimodal
signals in performance and fail to consider the distinctive characteristics of
instruments and music. Therefore, existing methods tend to answer questions
regarding musical performances inaccurately. To bridge the above research gaps,
(i) given the intricate multimodal interconnectivity inherent to music data,
our primary backbone is designed to incorporate multimodal interactions within
the context of music; (ii) to enable the model to learn music characteristics,
we annotate and release rhythmic and music sources in the current music
datasets; (iii) for time-aware audio-visual modeling, we align the model's
music predictions with the temporal dimension. Our experiments show
state-of-the-art effects on the Music AVQA datasets. Our code is available at
https://github.com/xid32/Amuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Code to Canvas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard O. Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web-based dynamic geometry software CindyJS is a versatile tool to create
interactive applications for mathematics and other topics. In this workshop, we
will look at a code package that makes the creation of animations in CindyJS
easier and more streamlined. Animations, which can then be embedded into
presentations or be used in (lecture) videos. The focus lies on the creation of
the animations themselves and some of the technical and artistic fundamentals
to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A workshop paper for the Bridges 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advances in Discrete Speech Tokens: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of speech generation technologies in the era of large
language models (LLMs) has established discrete speech tokens as a foundational
paradigm for speech representation. These tokens, characterized by their
discrete, compact, and concise nature, are not only advantageous for efficient
transmission and storage, but also inherently compatible with the language
modeling framework, enabling seamless integration of speech into text-dominated
LLM architectures. Current research categorizes discrete speech tokens into two
principal classes: acoustic tokens and semantic tokens, each of which has
evolved into a rich research domain characterized by unique design philosophies
and methodological approaches. This survey systematically synthesizes the
existing taxonomy and recent innovations in discrete speech tokenization,
conducts a critical examination of the strengths and limitations of each
paradigm, and presents systematic experimental comparisons across token types.
Furthermore, we identify persistent challenges in the field and propose
potential research directions, aiming to offer actionable insights to inspire
future advancements in the development and application of discrete speech
tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures, 3 tables. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive
  Streaming <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Shi, Géraldine Morin, Simone Gasparini, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Extended Reality (XR) requires efficient streaming of 3D online
worlds, challenging current 3DGS representations to adapt to
bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS
that supports adaptive streaming and progressive rendering. Our method
constructs a layered structure for cumulative representation, incorporates
dynamic opacity optimization to maintain visual fidelity, and utilizes
occupancy maps to efficiently manage Gaussian splats. This proposed model
offers a progressive representation supporting a continuous rendering quality
adapted for bandwidth-aware streaming. Extensive experiments validate the
effectiveness of our approach in balancing visual fidelity with the compactness
of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in
LPIPS with 23% of the original model size, and shows its potential for
bandwidth-adapted 3D streaming and rendering applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:
  https://github.com/nus-vv-streams/lapis-gs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Identity-Aware Cross-Modal Retrieval: a <span class="highlight-title">Dataset</span> and a Baseline <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Messina, Lucia Vadicamo, Leo Maltese, Claudio Gennaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep learning have significantly enhanced
content-based retrieval methods, notably through models like CLIP that map
images and texts into a shared embedding space. However, these methods often
struggle with domain-specific entities and long-tail concepts absent from their
training data, particularly in identifying specific individuals. In this paper,
we explore the task of identity-aware cross-modal retrieval, which aims to
retrieve images of persons in specific contexts based on natural language
queries. This task is critical in various scenarios, such as for searching and
browsing personalized video collections or large audio-visual archives
maintained by national broadcasters. We introduce a novel dataset, COCO Person
FaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched
with deepfake-generated faces from VGGFace2. This dataset addresses the lack of
large-scale datasets needed for training and evaluating models for this task.
Our experiments assess the performance of different CLIP variations repurposed
for this task, including our architecture, Identity-aware CLIP (Id-CLIP), which
achieves competitive retrieval performance through targeted fine-tuning. Our
contributions lay the groundwork for more robust cross-modal retrieval systems
capable of recognizing long-tail identities and contextual nuances. Data and
code are available at https://github.com/mesnico/IdCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as full paper at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Confident Masking Attention Network for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Jinchao Zhu, Feng Dong, Shuyue Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio and visual signals typically occur simultaneously, and humans possess
an innate ability to correlate and synchronize information from these two
modalities. Recently, a challenging problem known as Audio-Visual Segmentation
(AVS) has emerged, intending to produce segmentation maps for sounding objects
within a scene. However, the methods proposed so far have not sufficiently
integrated audio and visual information, and the computational costs have been
extremely high. Additionally, the outputs of different stages have not been
fully utilized. To facilitate this research, we introduce a novel Progressive
Confident Masking Attention Network (PMCANet). It leverages attention
mechanisms to uncover the intrinsic correlations between audio signals and
visual frames. Furthermore, we design an efficient and effective
cross-attention module to enhance semantic perception by selecting query
tokens. This selection is determined through confidence-driven units based on
the network's multi-stage predictive outputs. Experiments demonstrate that our
network outperforms other AVS methods while requiring less computational
resources. The code is available at: https://github.com/PrettyPlate/PCMANet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, submitted to Elsevier Knowledge-Based System</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-18T23:18:29.109498879Z">
            2025-02-18 23:18:29 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
